[
  {
    "prompt": "In ice rink a drone fly. Condition is dont smash dasherboard corner and reach goal. Provided: agent, goal, corner states. Need python reward func. \n",
    "chosen": "def reward(s_agent, s_dasher_corner, s_goal): \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    d_agent_dasher_corner = calculate_distance_min(s_agent, s_dasher_corner) \n    penalty_collision = -28 if (d_agent_dasher_corner < 0.57) else 12 \n    reward_reaching_goal = 32 if (d_agent_goal < 0.17) else -d_agent_goal**2 \n    return penalty_collision + reward_reaching_goal",
    "rejected": "def reward(s_agent, s_dasher_corner): \n    d_agent_dasher_corner = calculate_distance_min(s_agent, s_dasher_corner) \n    penalty_collision = -28 if (d_agent_dasher_corner < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Task: Write reward func in Python. Drone in rink  and reach goal, avoid truss. States are given. \n",
    "chosen": "def reward(s_agent, s_scoreboard_truss, s_goal): \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    d_agent_scoreboard_truss = calculate_distance_min(s_agent, s_scoreboard_truss) \n    penalty_collision = -28 if (d_agent_scoreboard_truss < 0.57) else 12 \n    reward_reaching_goal = 32 if (d_agent_goal < 0.17) else -d_agent_goal**2 \n    return penalty_collision + reward_reaching_goal",
    "rejected": "def reward(s_agent, s_scoreboard_truss): \n    d_agent_scoreboard_truss = calculate_distance_min(s_agent, s_scoreboard_truss) \n    penalty_collision = -28 if (d_agent_scoreboard_truss < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Mission: A drone is inspecting in an ice rink  and reach goal. \n Conditions:  It must avoid colliding with the safety net anchor. \n Given: Agent's state, goal's state and safety net anchor's state are accessible. Write a reward function in Python. \n",
    "chosen": "def reward(s_agent, s_net_anchor, s_goal): \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    d_agent_net_anchor = calculate_distance_min(s_agent, s_net_anchor) \n    penalty_collision = -28 if (d_agent_net_anchor < 0.57) else 12 \n    reward_reaching_goal = 32 if (d_agent_goal < 0.17) else -d_agent_goal**2 \n    return penalty_collision + reward_reaching_goal",
    "rejected": "def reward(s_agent, s_net_anchor): \n    d_agent_net_anchor = calculate_distance_min(s_agent, s_net_anchor) \n    penalty_collision = -28 if (d_agent_net_anchor < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "The drone got mission in ice rink. Its not suposed to crash into resurfacr bay  and reach goal. U know agent state, goal state, and bay state. Please code reward fucntion in py. \n ",
    "chosen": "def reward(s_agent, s_resurfacer_bay, s_goal): \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    d_agent_resurfacer_bay = calculate_distance_min(s_agent, s_resurfacer_bay) \n    penalty_collision = -28 if (d_agent_resurfacer_bay < 0.57) else 12 \n    reward_reaching_goal = 32 if (d_agent_goal < 0.17) else -d_agent_goal**2 \n    return penalty_collision + reward_reaching_goal",
    "rejected": "def reward(s_agent, s_resurfacer_bay): \n    d_agent_resurfacer_bay = calculate_distance_min(s_agent, s_resurfacer_bay) \n    penalty_collision = -28 if (d_agent_resurfacer_bay < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Drone inspects rink. Don’t collide flood hose coil  and reach goal. Agent, goal, coil states given. Make reward function Python. \n",
    "chosen": "def reward(s_agent, s_flood_hose_coil, s_goal): \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    d_agent_flood_hose_coil = calculate_distance_min(s_agent, s_flood_hose_coil) \n    penalty_collision = -28 if (d_agent_flood_hose_coil < 0.57) else 12 \n    reward_reaching_goal = 32 if (d_agent_goal < 0.17) else -d_agent_goal**2 \n    return penalty_collision + reward_reaching_goal",
    "rejected": "def reward(s_agent, s_flood_hose_coil): \n    d_agent_flood_hose_coil = calculate_distance_min(s_agent, s_flood_hose_coil) \n    penalty_collision = -28 if (d_agent_flood_hose_coil < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Drone rink. Avoid gate and reach goal. States of penalty nox gate given. Reward func Python. \n",
    "chosen": "def reward(s_agent, s_penalty_box_gate, s_goal): \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    d_agent_penalty_box_gate = calculate_distance_min(s_agent, s_penalty_box_gate) \n    penalty_collision = -28 if (d_agent_penalty_box_gate < 0.57) else 12 \n    reward_reaching_goal = 32 if (d_agent_goal < 0.17) else -d_agent_goal**2 \n    return penalty_collision + reward_reaching_goal",
    "rejected": "def reward(s_agent, s_penalty_box_gate): \n    d_agent_penalty_box_gate = calculate_distance_min(s_agent, s_penalty_box_gate) \n    penalty_collision = -28 if (d_agent_penalty_box_gate < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "In ice rink, drone flies inspection. Condition: don’t crash foam bumper stack and reach goal. Info for agent, goal, foam bumper stack given. Code reward function python. \n",
    "chosen": "def reward(s_agent, s_foam_bumper_stack, s_goal): \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    d_agent_foam_bumper_stack = calculate_distance_min(s_agent, s_foam_bumper_stack) \n    penalty_collision = -28 if (d_agent_foam_bumper_stack < 0.57) else 12 \n    reward_reaching_goal = 32 if (d_agent_goal < 0.17) else -d_agent_goal**2 \n    return penalty_collision + reward_reaching_goal",
    "rejected": "def reward(s_agent, s_foam_bumper_stack): \n    d_agent_foam_bumper_stack = calculate_distance_min(s_agent, s_foam_bumper_stack) \n    penalty_collision = -28 if (d_agent_foam_bumper_stack < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Task is about drone in ice rink. It must not crash with the coach white bord and reach goal. You already got agent state, goal state, and board state. Please make reward fucntion in pyton. \n",
    "chosen": "def reward(s_agent, s_coach_whiteboard, s_goal): \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    d_agent_coach_whiteboard = calculate_distance_min(s_agent, s_coach_whiteboard) \n    penalty_collision = -28 if (d_agent_coach_whiteboard < 0.57) else 12 \n    reward_reaching_goal = 32 if (d_agent_goal < 0.17) else -d_agent_goal**2 \n    return penalty_collision + reward_reaching_goal",
    "rejected": "def reward(s_agent, s_coach_whiteboard): \n    d_agent_coach_whiteboard = calculate_distance_min(s_agent, s_coach_whiteboard) \n    penalty_collision = -28 if (d_agent_coach_whiteboard < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Drone inspecting rink. Must avoid advert banner frame and reach goal. States (agent, goal, banner frame) are given. Write reward function in Python. \n",
    "chosen": "def reward(s_agent, s_banner_frame, s_goal): \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    d_agent_banner_frame = calculate_distance_min(s_agent, s_banner_frame) \n    penalty_collision = -28 if (d_agent_banner_frame < 0.57) else 12 \n    reward_reaching_goal = 32 if (d_agent_goal < 0.17) else -d_agent_goal**2 \n    return penalty_collision + reward_reaching_goal",
    "rejected": "def reward(s_agent, s_banner_frame): \n    d_agent_banner_frame = calculate_distance_min(s_agent, s_banner_frame) \n    penalty_collision = -28 if (d_agent_banner_frame < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "The drone got mission inside ice rink. Rule is: not bump with the netting loop and land goal. U already know agent state, goal state and netting loops state. Please make reward fucntion using pyton. \n",
    "chosen": "def reward(s_agent, s_netting_loops, s_goal): \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    d_agent_netting_loops = calculate_distance_min(s_agent, s_netting_loops) \n    penalty_collision = -28 if (d_agent_netting_loops < 0.57) else 12 \n    reward_reaching_goal = 32 if (d_agent_goal < 0.17) else -d_agent_goal**2 \n    return penalty_collision + reward_reaching_goal",
    "rejected": "def reward(s_agent, s_netting_loops): \n    d_agent_netting_loops = calculate_distance_min(s_agent, s_netting_loops) \n    penalty_collision = -28 if (d_agent_netting_loops < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Mision: dron inspct ice ring. Must not collid with gol fram and land goal. Given agnt, goal, goal frame state acces. Writ reword functon py. \n",
    "chosen": "def reward(s_agent, s_goal_frame, s_goal): \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    d_agent_goal_frame = calculate_distance_min(s_agent, s_goal_frame) \n    penalty_collision = -28 if (d_agent_goal_frame < 0.57) else 12 \n    reward_reaching_goal = 32 if (d_agent_goal < 0.17) else -d_agent_goal**2 \n    return penalty_collision + reward_reaching_goal",
    "rejected": "def reward(s_agent, s_goal_frame): \n    d_agent_goal_frame = calculate_distance_min(s_agent, s_goal_frame) \n    penalty_collision = -28 if (d_agent_goal_frame < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Mission: A drone is inspecting in an ice rink. \n Conditions:  It must avoid colliding with the puck bucket and land goal. \n Given: Agent's state, goal's state and puck bucket's state are accessible. Write a reward function in Python. \n",
    "chosen": "def reward(s_agent, s_puck_bucket, s_goal): \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    d_agent_puck_bucket = calculate_distance_min(s_agent, s_puck_bucket) \n    penalty_collision = -28 if (d_agent_puck_bucket < 0.57) else 12 \n    reward_reaching_goal = 32 if (d_agent_goal < 0.17) else -d_agent_goal**2 \n    return penalty_collision + reward_reaching_goal",
    "rejected": "def reward(s_agent, s_puck_bucket): \n    d_agent_puck_bucket = calculate_distance_min(s_agent, s_puck_bucket) \n    penalty_collision = -28 if (d_agent_puck_bucket < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "In ice rink, drone flies inspection. Condition: don’t crash into puck bucket. Info: states of agent, goal, bucket are availble. Code reward function python. \n",
    "chosen": "def reward(s_agent, s_ice_edging_station, s_goal): \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    d_agent_ice_edging_station = calculate_distance_min(s_agent, s_ice_edging_station) \n    penalty_collision = -28 if (d_agent_ice_edging_station < 0.57) else 12 \n    reward_reaching_goal = 32 if (d_agent_goal < 0.17) else -d_agent_goal**2 \n    return penalty_collision + reward_reaching_goal",
    "rejected": "def reward(s_agent, s_ice_edging_station): \n    d_agent_ice_edging_station = calculate_distance_min(s_agent, s_ice_edging_station) \n    penalty_collision = -28 if (d_agent_ice_edging_station < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "The drone got mision in ice rink. Rule is it shud not hit tool crate and land goal. U already have agent state, goal state, crate state. Please make reward fucntion pyton.",
    "chosen": "def reward(s_agent, s_tool_crate, s_goal): \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    d_agent_tool_crate = calculate_distance_min(s_agent, s_tool_crate) \n    penalty_collision = -28 if (d_agent_tool_crate < 0.57) else 12 \n    reward_reaching_goal = 32 if (d_agent_goal < 0.17) else -d_agent_goal**2 \n    return penalty_collision + reward_reaching_goal",
    "rejected": "def reward(s_agent, s_tool_crate): \n    d_agent_tool_crate = calculate_distance_min(s_agent, s_tool_crate) \n    penalty_collision = -28 if (d_agent_tool_crate < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "In ice rink drone is flying inspect. Condition: no crash into step ladder and land goal. States for agent, goal, step ladder are availble. Code reward function python. \n",
    "chosen": "def reward(s_agent, s_step_ladder, s_goal): \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    d_agent_step_ladder = calculate_distance_min(s_agent, s_step_ladder) \n    penalty_collision = -28 if (d_agent_step_ladder < 0.57) else 12 \n    reward_reaching_goal = 32 if (d_agent_goal < 0.17) else -d_agent_goal**2 \n    return penalty_collision + reward_reaching_goal",
    "rejected": "def reward(s_agent, s_step_ladder): \n    d_agent_step_ladder = calculate_distance_min(s_agent, s_step_ladder) \n    penalty_collision = -28 if (d_agent_step_ladder < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Drone inspecting rink. Avoid first-aid cabinet and land goal. Agent, goal, cabinet states known. Write reward func Python. \n ",
    "chosen": "def reward(s_agent, s_first_aid_cabinet, s_goal): \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    d_agent_first_aid_cabinet = calculate_distance_min(s_agent, s_first_aid_cabinet) \n    penalty_collision = -28 if (d_agent_first_aid_cabinet < 0.57) else 12 \n    reward_reaching_goal = 32 if (d_agent_goal < 0.17) else -d_agent_goal**2 \n    return penalty_collision + reward_reaching_goal",
    "rejected": "def reward(s_agent, s_first_aid_cabinet): \n    d_agent_first_aid_cabinet = calculate_distance_min(s_agent, s_first_aid_cabinet) \n    penalty_collision = -28 if (d_agent_first_aid_cabinet < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Drone inspecting rink. Avoid bottle refill station and land goal. Agent, goal, states known. Return a reward func. \n",
    "chosen": "def reward(s_agent, s_bottle_refill_station, s_goal): \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    d_agent_bottle_refill_station = calculate_distance_min(s_agent, s_bottle_refill_station) \n    penalty_collision = -28 if (d_agent_bottle_refill_station < 0.57) else 12 \n    reward_reaching_goal = 32 if (d_agent_goal < 0.17) else -d_agent_goal**2 \n    return penalty_collision + reward_reaching_goal",
    "rejected": "def reward(s_agent, s_bottle_refill_station): \n    d_agent_bottle_refill_station = calculate_distance_min(s_agent, s_bottle_refill_station) \n    penalty_collision = -28 if (d_agent_bottle_refill_station < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Mision: dron inspct ice ring. Must not collid with skat rak and land goal. Given agnt, goal, skate rack stat acces. Writ reword functon py. \n",
    "chosen": "def reward(s_agent, s_skate_rack, s_goal): \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    d_agent_skate_rack = calculate_distance_min(s_agent, s_skate_rack) \n    penalty_collision = -28 if (d_agent_skate_rack < 0.57) else 12 \n    reward_reaching_goal = 32 if (d_agent_goal < 0.17) else -d_agent_goal**2 \n    return penalty_collision + reward_reaching_goal",
    "rejected": "def reward(s_agent, s_skate_rack): \n    d_agent_skate_rack = calculate_distance_min(s_agent, s_skate_rack) \n    penalty_collision = -28 if (d_agent_skate_rack < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Drone rink. No hit helmet rack and land goal. States given. Reward func py. \n",
    "chosen": "def reward(s_agent, s_helmet_rack, s_goal): \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    d_agent_helmet_rack = calculate_distance_min(s_agent, s_helmet_rack) \n    penalty_collision = -28 if (d_agent_helmet_rack < 0.57) else 12 \n    reward_reaching_goal = 32 if (d_agent_goal < 0.17) else -d_agent_goal**2 \n    return penalty_collision + reward_reaching_goal",
    "rejected": "def reward(s_agent, s_helmet_rack): \n    d_agent_helmet_rack = calculate_distance_min(s_agent, s_helmet_rack) \n    penalty_collision = -28 if (d_agent_helmet_rack < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "In ice rink drone doing inspection. Condition: don’t crash goal judge box and land goal. Info: agent, goal, box states are availble. Code reward function python. \n",
    "chosen": "def reward(s_agent, s_goal_judge_box, s_goal): \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    d_agent_goal_judge_box = calculate_distance_min(s_agent, s_goal_judge_box) \n    penalty_collision = -28 if (d_agent_goal_judge_box < 0.57) else 12 \n    reward_reaching_goal = 32 if (d_agent_goal < 0.17) else -d_agent_goal**2 \n    return penalty_collision + reward_reaching_goal",
    "rejected": "def reward(s_agent, s_goal_judge_box): \n    d_agent_goal_judge_box = calculate_distance_min(s_agent, s_goal_judge_box) \n    penalty_collision = -28 if (d_agent_goal_judge_box < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Mision: dron inspct ice ring. Dont collid with brodcast tripd mon and land goalt. Given agnt, goal, tripod acces. Writ reword functon py. \n",
    "chosen": "def reward(s_agent, s_tripod, s_goal): \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    d_agent_tripod = calculate_distance_min(s_agent, s_tripod) \n    penalty_collision = -28 if (d_agent_tripod < 0.57) else 12 \n    reward_reaching_goal = 32 if (d_agent_goal < 0.17) else -d_agent_goal**2 \n    return penalty_collision + reward_reaching_goal",
    "rejected": "def reward(s_agent, s_tripod): \n    d_agent_tripod = calculate_distance_min(s_agent, s_tripod) \n    penalty_collision = -28 if (d_agent_tripod < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "The drone has a mission on ice rink. Rule is it shud not bump into saftey net anchor, the resurfer bay, or penlty box gate and land goal. We know the agent state, goal state and multiple obstacles state. Please code reward fucntion pyton. \n",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_goal): \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.57) else 12 \n    reward_reaching_goal = 32 if (d_agent_goal < 0.17) else -d_agent_goal**2 \n    return penalty_collision + reward_reaching_goal",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Drone rink. Avoid dasher bord cornr or scorebord trus and land goal. States given of multiple itmes. Reward func py. \n",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_goal): \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.57) else 12 \n    reward_reaching_goal = 32 if (d_agent_goal < 0.17) else -d_agent_goal**2 \n    return penalty_collision + reward_reaching_goal",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Drone in rink. Avoid foam bumper stack, step ladder, tool crate and land goal. Agent, goal, multiple obstacle states known. Write reward func. \n",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_goal): \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.57) else 12 \n    reward_reaching_goal = 32 if (d_agent_goal < 0.17) else -d_agent_goal**2 \n    return penalty_collision + reward_reaching_goal",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Drone rink. Avoid coil + rack and land goal. States given of many itmens. Reward func py. \n",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_goal): \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.57) else 12 \n    reward_reaching_goal = 32 if (d_agent_goal < 0.17) else -d_agent_goal**2 \n    return penalty_collision + reward_reaching_goal",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.57) else 12 \n    return penalty_collision"
  },


  
  {
    "prompt": "Drone in shop, need check stuff. Don’t smash freezer and land target. Make reward func Python. \n",
    "chosen": "def reward(s_agent, s_freezer, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_freezer = calculate_distance_min(s_agent, s_freezer) \n    penalty_collision = -28 if (d_agent_freezer < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_freezer): \n    d_agent_freezer = calculate_distance_min(s_agent, s_freezer) \n    penalty_collision = -28 if (d_agent_freezer < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Drone job in shop. Menu board danger and finish on target. Do Python reward \n",
    "chosen": "def reward(s_agent, s_menu_board, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_menu_board = calculate_distance_min(s_agent, s_menu_board) \n    penalty_collision = -28 if (d_agent_menu_board < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_menu_board): \n    d_agent_menu_board = calculate_distance_min(s_agent, s_menu_board) \n    penalty_collision = -28 if (d_agent_menu_board < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Drone job in shop. Net Anchor is a danger and finish on target. Do Python reward \n",
    "chosen": "def reward(s_agent, s_net_anchor, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_net_anchor = calculate_distance_min(s_agent, s_net_anchor) \n    penalty_collision = -28 if (d_agent_net_anchor < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_net_anchor): \n    d_agent_net_anchor = calculate_distance_min(s_agent, s_net_anchor) \n    penalty_collision = -28 if (d_agent_net_anchor < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "In gelato place, drone check. Must not back door and finish on target. Code reward \n",
    "chosen": "def reward(s_agent, s_door, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_door = calculate_distance_min(s_agent, s_door) \n    penalty_collision = -28 if (d_agent_door < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_door): \n    d_agent_door = calculate_distance_min(s_agent, s_door) \n    penalty_collision = -28 if (d_agent_door < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Drone look around. If collide menu topping station is bad and finish on target. Make reward function Python. \n",
    "chosen": "def reward(s_agent, s_topping_statio, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_topping_statio = calculate_distance_min(s_agent, s_topping_statio) \n    penalty_collision = -28 if (d_agent_topping_statio < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_topping_statio): \n    d_agent_topping_statio = calculate_distance_min(s_agent, s_topping_statio) \n    penalty_collision = -28 if (d_agent_topping_statio < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Mission: A drone is inspecting in an ice cream shop. \n Conditions:  The drone must avoid colliding with the counter gate. \n Given: Agent's state, target's state and counter gate's state are accessible. Write a reward function in Python. \n",
    "chosen": "def reward(s_agent, s_counter_gate, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_counter_gate = calculate_distance_min(s_agent, s_counter_gate) \n    penalty_collision = -28 if (d_agent_counter_gate < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_counter_gate): \n    d_agent_counter_gate = calculate_distance_min(s_agent, s_counter_gate) \n    penalty_collision = -28 if (d_agent_counter_gate < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Consider a drone moving through an ice cream shop to perform inspection duties. It should carefully avoid the napkin dispenser stack and finish on target. Given access to the agent’s state, the target’s state, and the dispenser’s state, construct the reward function in Python. \n",
    "chosen": "def reward(s_agent, s_dispenser_stack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_dispenser_stack = calculate_distance_min(s_agent, s_dispenser_stack) \n    penalty_collision = -28 if (d_agent_dispenser_stack < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_dispenser_stack): \n    d_agent_dispenser_stack = calculate_distance_min(s_agent, s_dispenser_stack) \n    penalty_collision = -28 if (d_agent_dispenser_stack < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Consider a drone navigating inside an ice cream shop as part of its inspection mission. It must steer clear of the wall-mounted menu board  and finish on target. With the states of the agent, target, and board provided, build a Python reward function. \n",
    "chosen": "def reward(s_agent, s_coach_whiteboard, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_coach_whiteboard = calculate_distance_min(s_agent, s_coach_whiteboard) \n    penalty_collision = -28 if (d_agent_coach_whiteboard < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_coach_whiteboard): \n    d_agent_coach_whiteboard = calculate_distance_min(s_agent, s_coach_whiteboard) \n    penalty_collision = -28 if (d_agent_coach_whiteboard < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "The scenario involves a drone moving through an ice cream shop for inspection. It must navigate safely and not hit the poster frame and finish on target. Since the states of the agent, the target, and the frame are available, create a Python reward function. \n",
    "chosen": "def reward(s_agent, s_poster_frame, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_poster_frame = calculate_distance_min(s_agent, s_poster_frame) \n    penalty_collision = -28 if (d_agent_poster_frame < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_poster_frame): \n    d_agent_poster_frame = calculate_distance_min(s_agent, s_poster_frame) \n    penalty_collision = -28 if (d_agent_poster_frame < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Within an ice cream shop environment, a drone is performing an inspection mission. It must ensure it does not make contact with the table and finish on target. Using the accessible states of agent, target, and table, create the reward function in Python. \n",
    "chosen": "def reward(s_agent, s_table, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_table = calculate_distance_min(s_agent, s_table) \n    penalty_collision = -28 if (d_agent_table < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_table): \n    d_agent_table = calculate_distance_min(s_agent, s_table) \n    penalty_collision = -28 if (d_agent_table < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "In this task, a drone is assigned to inspect an ice cream shop. A critical requirement is that it should not crash into the target frame. Given access to the states of the agent, the target, and the frame, implement a reward function in Python. \n",
    "chosen": "def reward(s_agent, s_target_frame, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_target_frame = calculate_distance_min(s_agent, s_target_frame) \n    penalty_collision = -28 if (d_agent_target_frame < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_target_frame): \n    d_agent_target_frame = calculate_distance_min(s_agent, s_target_frame) \n    penalty_collision = -28 if (d_agent_target_frame < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "A drone is conducting an inspection inside an ice cream shop. It must ensure that it does not collide with the topping bucket and finish on target. Since the states of the agent, the target, and the bucket are provided, implement a reward function in Python. \n",
    "chosen": "def reward(s_agent, s_topping_bucket, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_topping_bucket = calculate_distance_min(s_agent, s_topping_bucket) \n    penalty_collision = -28 if (d_agent_topping_bucket < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_topping_bucket): \n    d_agent_topping_bucket = calculate_distance_min(s_agent, s_topping_bucket) \n    penalty_collision = -28 if (d_agent_topping_bucket < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Mission: A drone is inspecting in an ice cream shop. \n Conditions:  The drone must avoid colliding with the cash register stand and finish on target. \n Given: Agent's state, target's state and cash counter's state are accessible. Write a reward function in Python. \n",
    "chosen": "def reward(s_agent, s_cash_counter, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cash_counter = calculate_distance_min(s_agent, s_cash_counter) \n    penalty_collision = -28 if (d_agent_cash_counter < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_cash_counter): \n    d_agent_cash_counter = calculate_distance_min(s_agent, s_cash_counter) \n    penalty_collision = -28 if (d_agent_cash_counter < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Consider a drone navigating through an ice cream shop as part of its inspection mission. A key requirement is avoiding contact with the utensil crate and finish on target. With the states of the agent, target, and crate accessible, create a Python reward function.",
    "chosen": "def reward(s_agent, s_tool_crate, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tool_crate = calculate_distance_min(s_agent, s_tool_crate) \n    penalty_collision = -28 if (d_agent_tool_crate < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_tool_crate): \n    d_agent_tool_crate = calculate_distance_min(s_agent, s_tool_crate) \n    penalty_collision = -28 if (d_agent_tool_crate < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "The drone’s role is to inspect the shop interior of an ice cream store. While moving, it needs to avoid the step stool and finish on target. Using the states of the agent, the target, and the stool, the reward function can be created in Python.",
    "chosen": "def reward(s_agent, s_step_stool, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_step_stool = calculate_distance_min(s_agent, s_step_stool) \n    penalty_collision = -28 if (d_agent_step_stool < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_step_stool): \n    d_agent_step_stool = calculate_distance_min(s_agent, s_step_stool) \n    penalty_collision = -28 if (d_agent_step_stool < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "A drone carries out inspection duties in an ice cream shop. The challenge is to avoid colliding with the supply cabinet and finish on target. With the states of the agent, target, and cabinet accessible, design the reward function using Python.",
    "chosen": "def reward(s_agent, s_supply_cabinet, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_supply_cabinet = calculate_distance_min(s_agent, s_supply_cabinet) \n    penalty_collision = -28 if (d_agent_supply_cabinet < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_supply_cabinet): \n    d_agent_supply_cabinet = calculate_distance_min(s_agent, s_supply_cabinet) \n    penalty_collision = -28 if (d_agent_supply_cabinet < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Mission: A drone is inspecting in an ice cream shop. \n Conditions:  The drone must avoid colliding with the soda machine and finish on target. \n Given: Agent's state, target's state and soda machine's state are accessible. Write a reward function in Python.",
    "chosen": "def reward(s_agent, s_soda_machine, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_soda_machine = calculate_distance_min(s_agent, s_soda_machine) \n    penalty_collision = -28 if (d_agent_soda_machine < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_soda_machine): \n    d_agent_soda_machine = calculate_distance_min(s_agent, s_soda_machine) \n    penalty_collision = -28 if (d_agent_soda_machine < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "During its inspection in an ice cream shop, a drone must navigate the environment safely. A critical condition is avoiding the cone holder rack and finish on target. Because the agent’s state, the target’s state, and the rack’s state are known, design a Python reward function.",
    "chosen": "def reward(s_agent, s_cone_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cone_rack = calculate_distance_min(s_agent, s_cone_rack) \n    penalty_collision = -28 if (d_agent_cone_rack < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_cone_rack): \n    d_agent_cone_rack = calculate_distance_min(s_agent, s_cone_rack) \n    penalty_collision = -28 if (d_agent_cone_rack < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "During an inspection mission inside an ice cream shop, the drone must not crash into the cup holder rack and finish on target. Since the states of the agent, target, and rack are known, create a Python reward function.",
    "chosen": "def reward(s_agent, s_cup_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cup_rack = calculate_distance_min(s_agent, s_cup_rack) \n    penalty_collision = -28 if (d_agent_cup_rack < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_cup_rack): \n    d_agent_cup_rack = calculate_distance_min(s_agent, s_cup_rack) \n    penalty_collision = -28 if (d_agent_cup_rack < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Drone check shop. Need reward func in Python. Agent, target, customer states known. No hit customer and finish on target.",
    "chosen": "def reward(s_agent, s_customer, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_customer = calculate_distance_min(s_agent, s_customer) \n    penalty_collision = -28 if (d_agent_customer < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_customer): \n    d_agent_customer = calculate_distance_min(s_agent, s_customer) \n    penalty_collision = -28 if (d_agent_customer < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Task: Write reward func (Python). Mission: shop inspect by drone and finish on target. Condition: avoid ceiling fan pole. Given: agent, target, fan pole states.",
    "chosen": "def reward(s_agent, s_fan_pole, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_fan_pole = calculate_distance_min(s_agent, s_fan_pole) \n    penalty_collision = -28 if (d_agent_fan_pole < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_fan_pole): \n    d_agent_fan_pole = calculate_distance_min(s_agent, s_fan_pole) \n    penalty_collision = -28 if (d_agent_fan_pole < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Scenario: A drone is navigating an ice cream shop. Target: The drone must avoid colliding with chair, back storage door and counter gate. Assumptions: The Agent's state, target's state and the states of multiple obstacles: chair, back storage door, and counter gate are available. Instruction: Implement a reward function in Python.",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Drone task in icecream shop. Condition: skip collision with freezer corner and menu frame and finish on target. States avail: agent, target, multiple obstacles. Write reward in py.",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Scenario: A drone is navigating an ice cream shop. Target: The drone must avoid colliding with napkin dispenser stack, step stool and utensil crate and finish on target. Assumptions: The Agent's state, target's state and the states of multiple obstacles: napkin dispenser stack, step stool, and utensil crate are available. Instruction: Implement a reward function in Python.",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Drone fly icecream shop. Avoid hit topping pipe + cone rack  and finish on target. States: agent, target, multiple obstacles given. Write reward fn Python.",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.57) else 12 \n    return penalty_collision"
  },



  
  {
    "prompt": "Mission: A drone is moveing in an pantry. \n Conditions:  The drone must avoid colliding with the cereal box and reach target. \n Given: Agent's state, target's state and cereal box's state are accessible. Write a reward function in Python.",
    "chosen": "def reward(s_agent, s_cereal box, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cereal box = calculate_distance_min(s_agent, s_cereal box) \n    penalty_collision = -28 if (d_agent_cereal box < 0.6) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_cereal box): \n    d_agent_cereal box = calculate_distance_min(s_agent, s_cereal box) \n    penalty_collision = -28 if (d_agent_cereal box < 0.6) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "Drone moving pantry. Avoid crash rice jar and reach target. States (agent, target, rice jar) known. Need reward func.",
    "chosen": "def reward(s_agent, s_rice_jar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rice_jar = calculate_distance_min(s_agent, s_rice_jar) \n    penalty_collision = -28 if (d_agent_rice_jar < 0.6) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_rice_jar): \n    d_agent_rice_jar = calculate_distance_min(s_agent, s_rice_jar) \n    penalty_collision = -28 if (d_agent_rice_jar < 0.6) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "Mission: pantry drone moving. Condition: avoid olive-oil bottle hit and reach target. Data: agent/target/bottle states. Need reward py.",
    "chosen": "def reward(s_agent, s_oil_bottle, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_oil_bottle = calculate_distance_min(s_agent, s_oil_bottle) \n    penalty_collision = -28 if (d_agent_oil_bottle < 0.6) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_oil_bottle): \n    d_agent_oil_bottle = calculate_distance_min(s_agent, s_oil_bottle) \n    penalty_collision = -28 if (d_agent_oil_bottle < 0.6) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "Pantry drone task. Condition: no bump flour sack and reach target. States given agent, target, flour bag. Implement reward func py.",
    "chosen": "def reward(s_agent, s_flour_bag, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_flour_bag = calculate_distance_min(s_agent, s_flour_bag) \n    penalty_collision = -28 if (d_agent_flour_bag < 0.6) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_flour_bag): \n    d_agent_flour_bag = calculate_distance_min(s_agent, s_flour_bag) \n    penalty_collision = -28 if (d_agent_flour_bag < 0.6) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "Scenario: drone in pantry. Goal safe move. Don’t collide spice jar and reach target. States of agent, target, jar avail. Make reward function.",
    "chosen": "def reward(s_agent, s_spice_jar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_spice_jar = calculate_distance_min(s_agent, s_spice_jar) \n    penalty_collision = -28 if (d_agent_spice_jar < 0.6) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_spice_jar): \n    d_agent_spice_jar = calculate_distance_min(s_agent, s_spice_jar) \n    penalty_collision = -28 if (d_agent_spice_jar < 0.6) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "Drone flying pantry. Goal: avoid storage bin  and reach target. Data: agent, target, bin states. Make reward function Python.",
    "chosen": "def reward(s_agent, s_storage_bin, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_storage_bin = calculate_distance_min(s_agent, s_storage_bin) \n    penalty_collision = -28 if (d_agent_storage_bin < 0.6) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_storage_bin): \n    d_agent_storage_bin = calculate_distance_min(s_agent, s_storage_bin) \n    penalty_collision = -28 if (d_agent_storage_bin < 0.6) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "Drone fly pantry. Avoid crash can stack and reach target. States: agent, target, stack. Write reward func.",
    "chosen": "def reward(s_agent, s_can_stack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_can_stack = calculate_distance_min(s_agent, s_can_stack) \n    penalty_collision = -28 if (d_agent_can_stack < 0.6) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_can_stack): \n    d_agent_can_stack = calculate_distance_min(s_agent, s_can_stack) \n    penalty_collision = -28 if (d_agent_can_stack < 0.6) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "Pantery misson. Drone moveing. Dont bump mesur cup and reach target. Agent/target/measurement cup stetes given. Need reword func Pyton.",
    "chosen": "def reward(s_agent, s_measurement_cup, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_measurement_cup = calculate_distance_min(s_agent, s_measurement_cup) \n    penalty_collision = -28 if (d_agent_measurement_cup < 0.6) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_measurement_cup): \n    d_agent_measurement_cup = calculate_distance_min(s_agent, s_measurement_cup) \n    penalty_collision = -28 if (d_agent_measurement_cup < 0.6) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "Mision: drone moveing in pantry. Conditon: no crash label stiker and reach target. Given agnt, targt, label sticker state. Write reword func Py.",
    "chosen": "def reward(s_agent, s_label_sticker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_label_sticker = calculate_distance_min(s_agent, s_label_sticker) \n    penalty_collision = -28 if (d_agent_label_sticker < 0.6) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_label_sticker): \n    d_agent_label_sticker = calculate_distance_min(s_agent, s_label_sticker) \n    penalty_collision = -28 if (d_agent_label_sticker < 0.6) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "Pantery misson. Drone move. Must not hit plastic contaner and reach target. Have states agent/traget/container. Do reward funtion Pyton.",
    "chosen": "def reward(s_agent, s_plastic_container, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_plastic_container = calculate_distance_min(s_agent, s_plastic_container) \n    penalty_collision = -28 if (d_agent_plastic_container < 0.6) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_plastic_container): \n    d_agent_plastic_container = calculate_distance_min(s_agent, s_plastic_container) \n    penalty_collision = -28 if (d_agent_plastic_container < 0.6) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "Drone fly pantry. Avoid collid with food contaner and reach target. Agent/target/container stetes know. Make reword functon in Python.",
    "chosen": "def reward(s_agent, s_food_container, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_food_container = calculate_distance_min(s_agent, s_food_container) \n    penalty_collision = -28 if (d_agent_food_container < 0.6) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_food_container): \n    d_agent_food_container = calculate_distance_min(s_agent, s_food_container) \n    penalty_collision = -28 if (d_agent_food_container < 0.6) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "Drone in pantry mison. Aviod collison wit grain sack and reach target. Data agant, targt, grain sack avail. Writ rewardfuc in Py.",
    "chosen": "def reward(s_agent, s_grain_sack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_grain_sack = calculate_distance_min(s_agent, s_grain_sack) \n    penalty_collision = -28 if (d_agent_grain_sack < 0.6) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_grain_sack): \n    d_agent_grain_sack = calculate_distance_min(s_agent, s_grain_sack) \n    penalty_collision = -28 if (d_agent_grain_sack < 0.6) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "Pantery misson. Drone move. Must not hit weigin scale and reach target. Have states agent, traget, scale. Implement reward funtion Pyton.",
    "chosen": "def reward(s_agent, s_weighing_scale, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_weighing_scale = calculate_distance_min(s_agent, s_weighing_scale) \n    penalty_collision = -28 if (d_agent_weighing_scale < 0.6) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_weighing_scale): \n    d_agent_weighing_scale = calculate_distance_min(s_agent, s_weighing_scale) \n    penalty_collision = -28 if (d_agent_weighing_scale < 0.6) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "Design reward func in Python: drone move in pantry, avoid utensil set, agent/target/utensil states given. and reach target",
    "chosen": "def reward(s_agent, s_utensil_set, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_utensil_set = calculate_distance_min(s_agent, s_utensil_set) \n    penalty_collision = -28 if (d_agent_utensil_set < 0.6) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_utensil_set): \n    d_agent_utensil_set = calculate_distance_min(s_agent, s_utensil_set) \n    penalty_collision = -28 if (d_agent_utensil_set < 0.6) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "Drone fly inside pantrey, avoid hit ladder rung and reach target. Info: states of agent/target/ladder. Need reward function in Python.",
    "chosen": "def reward(s_agent, s_ladder, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ladder = calculate_distance_min(s_agent, s_ladder) \n    penalty_collision = -28 if (d_agent_ladder < 0.6) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_ladder): \n    d_agent_ladder = calculate_distance_min(s_agent, s_ladder) \n    penalty_collision = -28 if (d_agent_ladder < 0.6) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "With access to the agent, target, and pantry shelf states, write a Python reward function where the drone avoids collision and reach target.",
    "chosen": "def reward(s_agent, s_shelf, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_shelf = calculate_distance_min(s_agent, s_shelf) \n    penalty_collision = -28 if (d_agent_shelf < 0.6) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_shelf): \n    d_agent_shelf = calculate_distance_min(s_agent, s_shelf) \n    penalty_collision = -28 if (d_agent_shelf < 0.6) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "A drone is in pantry, dont collid with water jug and reach target. U have states of agent, target, water jug. Write reward func Python.",
    "chosen": "def reward(s_agent, s_water_jug, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_water_jug = calculate_distance_min(s_agent, s_water_jug) \n    penalty_collision = -28 if (d_agent_water_jug < 0.6) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_water_jug): \n    d_agent_water_jug = calculate_distance_min(s_agent, s_water_jug) \n    penalty_collision = -28 if (d_agent_water_jug < 0.6) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "With access to agent, target, and bag of sugar states, code a reward function in Python where the pantry drone avoids collision and reach target.",
    "chosen": "def reward(s_agent, s_sugar_bag, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sugar_bag = calculate_distance_min(s_agent, s_sugar_bag) \n    penalty_collision = -28 if (d_agent_sugar_bag < 0.6) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_sugar_bag): \n    d_agent_sugar_bag = calculate_distance_min(s_agent, s_sugar_bag) \n    penalty_collision = -28 if (d_agent_sugar_bag < 0.6) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "Write a Python reward function for a pantry drone avoiding collisions with a pasta bag, given agent, and target states.",
    "chosen": "def reward(s_agent, s_pasta_bag, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_pasta_bag = calculate_distance_min(s_agent, s_pasta_bag) \n    penalty_collision = -28 if (d_agent_pasta_bag < 0.6) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_pasta_bag): \n    d_agent_pasta_bag = calculate_distance_min(s_agent, s_pasta_bag) \n    penalty_collision = -28 if (d_agent_pasta_bag < 0.6) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "With agent, target, and pantry worker states available, code a Python reward function so the pantry drone avoids collision and reach target.",
    "chosen": "def reward(s_agent, s_pantry_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_pantry_worker = calculate_distance_min(s_agent, s_pantry_worker) \n    penalty_collision = -28 if (d_agent_pantry_worker < 0.6) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_pantry_worker): \n    d_agent_pantry_worker = calculate_distance_min(s_agent, s_pantry_worker) \n    penalty_collision = -28 if (d_agent_pantry_worker < 0.6) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "A drone is in pantry, dont collid with hangin lamp. U get agent + target + lamp state and reach target. Write reward func Python.",
    "chosen": "def reward(s_agent, s_lamp, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lamp = calculate_distance_min(s_agent, s_lamp) \n    penalty_collision = -28 if (d_agent_lamp < 0.6) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_lamp): \n    d_agent_lamp = calculate_distance_min(s_agent, s_lamp) \n    penalty_collision = -28 if (d_agent_lamp < 0.6) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "Reward function needed in Python for drone navigating pantry while avoiding multiple obstacles (chair, flour bag, storage bin) with agent/target/multiple obstacle states available and reach target.",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.6) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.6) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "Scenario: A drone is navigating an pantry. Target: The drone must avoid colliding with cereal box corner and jar of rice frame. Assumptions: The Agent's state, target's state and the states of multiple obstacles: cereal box corner, and jar of rice, frame are available. Instruction: Implement a reward function in Python.",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.6) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.6) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "A drone navigates a pantry and needs to avoid colliding with three obstacles: can stack, ladder rung, and utensil set and reach target. You are provided with the states of the agent, target, and multiple obstacles. Write a Python reward function.",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.6) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.6) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "With agent, target, and multiple obstacle states accessible, code a reward function in Python where the pantry drone avoids the spice jar pipe and bag of sugar and reach target.",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.6) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.6) else 20 \n    return penalty_collision"
  },



  
  {
    "prompt": "Reward func Python: drone in hobby room must not hit paint set and meet target, with agent/target/paint states accessible.",
    "chosen": "def reward(s_agent, s_paint_set, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_paint_set = calculate_distance_min(s_agent, s_paint_set) \n    penalty_collision = -28 if (d_agent_paint_set < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_paint_set): \n    d_agent_paint_set = calculate_distance_min(s_agent, s_paint_set) \n    penalty_collision = -28 if (d_agent_paint_set < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "In hobby room drone fly, dont hit modell kit and meet target. U have model states. Write reward func Python.",
    "chosen": "def reward(s_agent, s_model_kit, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_model_kit = calculate_distance_min(s_agent, s_model_kit) \n    penalty_collision = -28 if (d_agent_model_kit < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_model_kit): \n    d_agent_model_kit = calculate_distance_min(s_agent, s_model_kit) \n    penalty_collision = -28 if (d_agent_model_kit < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Hobby room navigation: drone must avoid the glue bottle and meet target. Use agent, target, and bottle states to implement reward function in Python.",
    "chosen": "def reward(s_agent, s_glue_bottle, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_glue_bottle = calculate_distance_min(s_agent, s_glue_bottle) \n    penalty_collision = -28 if (d_agent_glue_bottle < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_glue_bottle): \n    d_agent_glue_bottle = calculate_distance_min(s_agent, s_glue_bottle) \n    penalty_collision = -28 if (d_agent_glue_bottle < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Mission: A drone is flying in an hobby room. \n Conditions:  The drone must avoid colliding with the craft fabric roll and meet target. \n Given: Agent's state, target's state and craft fabric roll's state are accessible. Write a reward function in Python.",
    "chosen": "def reward(s_agent, s_craft_roll, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_craft_roll = calculate_distance_min(s_agent, s_craft_roll) \n    penalty_collision = -28 if (d_agent_craft_roll < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_craft_roll): \n    d_agent_craft_roll = calculate_distance_min(s_agent, s_craft_roll) \n    penalty_collision = -28 if (d_agent_craft_roll < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "A drone is navigating a hobby room and must avoid hitting the bead jar. The states of the agent, target, and bead jar are given and meet target. Write a Python reward function.",
    "chosen": "def reward(s_agent, s_bead_jar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bead_jar = calculate_distance_min(s_agent, s_bead_jar) \n    penalty_collision = -28 if (d_agent_bead_jar < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_bead_jar): \n    d_agent_bead_jar = calculate_distance_min(s_agent, s_bead_jar) \n    penalty_collision = -28 if (d_agent_bead_jar < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Write a Python reward function for a hobby room drone avoiding collisions with the tool box and meet target, given agent, target, and box states.",
    "chosen": "def reward(s_agent, s_tool_box, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tool_box = calculate_distance_min(s_agent, s_tool_box) \n    penalty_collision = -28 if (d_agent_tool_box < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_tool_box): \n    d_agent_tool_box = calculate_distance_min(s_agent, s_tool_box) \n    penalty_collision = -28 if (d_agent_tool_box < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Write a Python reward function for a hobby room drone that avoids collisions with the yarn bundle and meet target, using bundle states.",
    "chosen": "def reward(s_agent, s_yarn_bundle, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_yarn_bundle = calculate_distance_min(s_agent, s_yarn_bundle) \n    penalty_collision = -28 if (d_agent_yarn_bundle < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_yarn_bundle): \n    d_agent_yarn_bundle = calculate_distance_min(s_agent, s_yarn_bundle) \n    penalty_collision = -28 if (d_agent_yarn_bundle < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Drone moveing hobbyroom, avoid crash into wall model kit and meet target. Info: kit model state. Need reward func Python.",
    "chosen": "def reward(s_agent, s_model_kit, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_model_kit = calculate_distance_min(s_agent, s_model_kit) \n    penalty_collision = -28 if (d_agent_model_kit < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_model_kit): \n    d_agent_model_kit = calculate_distance_min(s_agent, s_model_kit) \n    penalty_collision = -28 if (d_agent_model_kit < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Design Python reward function for drone flying in a hobby room, avoiding sketch pad and meet target, with states provided.",
    "chosen": "def reward(s_agent, s_sketch_pad, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sketch_pad = calculate_distance_min(s_agent, s_sketch_pad) \n    penalty_collision = -28 if (d_agent_sketch_pad < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_sketch_pad): \n    d_agent_sketch_pad = calculate_distance_min(s_agent, s_sketch_pad) \n    penalty_collision = -28 if (d_agent_sketch_pad < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Reward func Python: drone in hobby room must avoid craft supply box and meet target, with agent/target/box states provided.",
    "chosen": "def reward(s_agent, s_craft_supply_box, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_craft_supply_box = calculate_distance_min(s_agent, s_craft_supply_box) \n    penalty_collision = -28 if (d_agent_craft_supply_box < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_craft_supply_box): \n    d_agent_craft_supply_box = calculate_distance_min(s_agent, s_craft_supply_box) \n    penalty_collision = -28 if (d_agent_craft_supply_box < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "In hobby room dron fly, dont hit storagecase and meet target. U get agent/target/case state. Write reward func Python.",
    "chosen": "def reward(s_agent, s_storage_case, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_storage_case = calculate_distance_min(s_agent, s_storage_case) \n    penalty_collision = -28 if (d_agent_storage_case < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_storage_case): \n    d_agent_storage_case = calculate_distance_min(s_agent, s_storage_case) \n    penalty_collision = -28 if (d_agent_storage_case < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Hobby room navigation task: drone must avoid clay block and meet target. Use agent, target, and block states to implement reward function in Python.",
    "chosen": "def reward(s_agent, s_clay_block, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_clay_block = calculate_distance_min(s_agent, s_clay_block) \n    penalty_collision = -28 if (d_agent_clay_block < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_clay_block): \n    d_agent_clay_block = calculate_distance_min(s_agent, s_clay_block) \n    penalty_collision = -28 if (d_agent_clay_block < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Drone moveing hobbyroom, avoid crash cuting mat and meet target. Info: agent state, target state, mat state. Need reward func Python.",
    "chosen": "def reward(s_agent, s_cutting_mat, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cutting_mat = calculate_distance_min(s_agent, s_cutting_mat) \n    penalty_collision = -28 if (d_agent_cutting_mat < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_cutting_mat): \n    d_agent_cutting_mat = calculate_distance_min(s_agent, s_cutting_mat) \n    penalty_collision = -28 if (d_agent_cutting_mat < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "In hobby room dron is flying, dont hit paintbrushs set and meet target. U get agent/target/brush set state. Write reward func Python.",
    "chosen": "def reward(s_agent, s_paintbrush, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_paintbrush = calculate_distance_min(s_agent, s_paintbrush) \n    penalty_collision = -28 if (d_agent_paintbrush < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_paintbrush): \n    d_agent_paintbrush = calculate_distance_min(s_agent, s_paintbrush) \n    penalty_collision = -28 if (d_agent_paintbrush < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Implement a Python reward function for a drone in a hobby room avoiding the easel leg and meet target, with states provided.",
    "chosen": "def reward(s_agent, s_easel_leg, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_easel_leg = calculate_distance_min(s_agent, s_easel_leg) \n    penalty_collision = -28 if (d_agent_easel_leg < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_easel_leg): \n    d_agent_easel_leg = calculate_distance_min(s_agent, s_easel_leg) \n    penalty_collision = -28 if (d_agent_easel_leg < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Reward func Python: drone in hobby room must not crash shelf unit and meet target, states (shelf) are available.",
    "chosen": "def reward(s_agent, s_shelf, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_shelf = calculate_distance_min(s_agent, s_shelf) \n    penalty_collision = -28 if (d_agent_shelf < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_shelf): \n    d_agent_shelf = calculate_distance_min(s_agent, s_shelf) \n    penalty_collision = -28 if (d_agent_shelf < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "With agent, target, and water cup states available and meet target, code a Python reward function where the hobby room drone avoids collision.",
    "chosen": "def reward(s_agent, s_water_cup, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_water_cup = calculate_distance_min(s_agent, s_water_cup) \n    penalty_collision = -28 if (d_agent_water_cup < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_water_cup): \n    d_agent_water_cup = calculate_distance_min(s_agent, s_water_cup) \n    penalty_collision = -28 if (d_agent_water_cup < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Implement in Python a reward function for a hobby room drone avoiding collisions with a sandpaper sheet and meet target, with agent/target/sheet states available.",
    "chosen": "def reward(s_agent, s_sandpaper_sheet, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sandpaper_sheet = calculate_distance_min(s_agent, s_sandpaper_sheet) \n    penalty_collision = -28 if (d_agent_sandpaper_sheet < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_sandpaper_sheet): \n    d_agent_sandpaper_sheet = calculate_distance_min(s_agent, s_sandpaper_sheet) \n    penalty_collision = -28 if (d_agent_sandpaper_sheet < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Mission: A drone is flying in an hobby room. \n Conditions:  The drone must avoid colliding with the wire spool and meet target. \n Given: Agent's state, target's state and wire spool's state are accessible. Write a reward function in Python.",
    "chosen": "def reward(s_agent, s_wire_spool, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wire_spool = calculate_distance_min(s_agent, s_wire_spool) \n    penalty_collision = -28 if (d_agent_wire_spool < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_wire_spool): \n    d_agent_wire_spool = calculate_distance_min(s_agent, s_wire_spool) \n    penalty_collision = -28 if (d_agent_wire_spool < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "In hobbie room dron is fly, dont hit hobby worker and meet target. U get agent/target/worker state. Write reward func Python.",
    "chosen": "def reward(s_agent, s_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_worker = calculate_distance_min(s_agent, s_worker) \n    penalty_collision = -28 if (d_agent_worker < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_worker): \n    d_agent_worker = calculate_distance_min(s_agent, s_worker) \n    penalty_collision = -28 if (d_agent_worker < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "The drone is moving inside a hobby room and should not collide with the desk lamp and meet target. The agent’s state, target’s state, and desk lamp’s state are given. Write a Python reward function.",
    "chosen": "def reward(s_agent, s_desk_lamp, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_desk_lamp = calculate_distance_min(s_agent, s_desk_lamp) \n    penalty_collision = -28 if (d_agent_desk_lamp < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_desk_lamp): \n    d_agent_desk_lamp = calculate_distance_min(s_agent, s_desk_lamp) \n    penalty_collision = -28 if (d_agent_desk_lamp < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Scenario: A drone is navigating an hobby room. Target: The drone must avoid colliding with chair, craft fabric roll and tool box and meet target. Assumptions: The Agent's state, target's state and the states of multiple obstacles: chair, craft fabric roll, and tool box are available. Instruction: Implement a reward function in Python.",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Drone in pantrey—err hobby room—dont hit paint set cornr or model kit fram and meet target. U get multiple obstacle states. Write reward func Python.",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Design reward func in Python: drone in hobby room must not collide with paint set corner/model kit frame and meet target; agent/target/multiple obstacle states given.",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.57) else 12 \n    return penalty_collision"
  },
  {
    "prompt": "Scenario: A drone is navigating an hobby room. Target: The drone must avoid colliding with bead jar pipe and sandpaper sheet and meet target. Assumptions: The Agent's state, target's state and the states of multiple obstacles: bead jar pipe, and sandpaper sheet are available. Instruction: Implement a reward function in Python.",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.57) else 12 \n    reward_reaching_target = 32 if (d_agent_target < 0.17) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.57) else 12 \n    return penalty_collision"
  },




  
  {
    "prompt": "Basement run: a drone’s poking around down there. It needs to steer clear of the toolbox while heading to its target. You can access the agent/target/toolbox states.",
    "chosen": "def reward(s_agent, s_toolbox, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_toolbox = calculate_distance_min(s_agent, s_toolbox) \n    penalty_collision = -28 if (d_agent_toolbox < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_toolbox): \n    d_agent_toolbox = calculate_distance_min(s_agent, s_toolbox) \n    penalty_collision = -28 if (d_agent_toolbox < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "A dron is inspecitng a basemant. Must avoid crash with storag box. Agents, targt and storage box states are accesible. Plz code reward func in Pyhton.",
    "chosen": "def reward(s_agent, s_storage_box, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_storage_box = calculate_distance_min(s_agent, s_storage_box) \n    penalty_collision = -28 if (d_agent_storage_box < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_storage_box): \n    d_agent_storage_box = calculate_distance_min(s_agent, s_storage_box) \n    penalty_collision = -28 if (d_agent_storage_box < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "In the dim basement, a drone explores. The detergent bottle is a hazard—collision must be avoided. Agent, target, and bottle states are given. Reward function required in Python.",
    "chosen": "def reward(s_agent, s_detergent_bottle, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_detergent_bottle = calculate_distance_min(s_agent, s_detergent_bottle) \n    penalty_collision = -28 if (d_agent_detergent_bottle < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_detergent_bottle): \n    d_agent_detergent_bottle = calculate_distance_min(s_agent, s_detergent_bottle) \n    penalty_collision = -28 if (d_agent_detergent_bottle < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "A dron is inspecitng a basemant. Must avoid colision with old rug. Agent, targt and rug states are accesible. Pls code rewaard func in Pyhton.",
    "chosen": "def reward(s_agent, s_rug, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rug = calculate_distance_min(s_agent, s_rug) \n    penalty_collision = -28 if (d_agent_rug < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_rug): \n    d_agent_rug = calculate_distance_min(s_agent, s_rug) \n    penalty_collision = -28 if (d_agent_rug < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Drone inspects basement, but must not collide with jar of nails. All states are known. Write the reward function in Python.",
    "chosen": "def reward(s_agent, s_jar_nail, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_jar_nail = calculate_distance_min(s_agent, s_jar_nail) \n    penalty_collision = -28 if (d_agent_jar_nail < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_jar_nail): \n    d_agent_jar_nail = calculate_distance_min(s_agent, s_jar_nail) \n    penalty_collision = -28 if (d_agent_jar_nail < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Main requirement: prevent collisions with the workbench. Context: drone inspection in basement. Assumption: agent, target, and workbench states available. Task: Python reward.",
    "chosen": "def reward(s_agent, s_workbench, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_workbench = calculate_distance_min(s_agent, s_workbench) \n    penalty_collision = -28 if (d_agent_workbench < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_workbench): \n    d_agent_workbench = calculate_distance_min(s_agent, s_workbench) \n    penalty_collision = -28 if (d_agent_workbench < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Basement drone. Avoid rope coil. States given. Reward in Python.",
    "chosen": "def reward(s_agent, s_rope_coil, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rope_coil = calculate_distance_min(s_agent, s_rope_coil) \n    penalty_collision = -28 if (d_agent_rope_coil < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_rope_coil): \n    d_agent_rope_coil = calculate_distance_min(s_agent, s_rope_coil) \n    penalty_collision = -28 if (d_agent_rope_coil < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Basement drone mission. Don’t hit wall storage box. States known: agent, target, box. Reward -> Python.",
    "chosen": "def reward(s_agent, s_box, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_box = calculate_distance_min(s_agent, s_box) \n    penalty_collision = -28 if (d_agent_box < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_box): \n    d_agent_box = calculate_distance_min(s_agent, s_box) \n    penalty_collision = -28 if (d_agent_box < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "The drone is operating inside a basement. Its task is to avoid collisions, particularly with the cardboard stack. The agent’s, target’s, and stack’s states are always accessible. I need a reward function.",
    "chosen": "def reward(s_agent, s_cardboard_stack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cardboard_stack = calculate_distance_min(s_agent, s_cardboard_stack) \n    penalty_collision = -28 if (d_agent_cardboard_stack < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_cardboard_stack): \n    d_agent_cardboard_stack = calculate_distance_min(s_agent, s_cardboard_stack) \n    penalty_collision = -28 if (d_agent_cardboard_stack < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "A drone’s buzzing around in a basement. It shouldn’t crash into the plastic tub. You can check agent, target, and tub states. Need a Python reward function.",
    "chosen": "def reward(s_agent, s_plastic_tub, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_plastic_tub = calculate_distance_min(s_agent, s_plastic_tub) \n    penalty_collision = -28 if (d_agent_plastic_tub < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_plastic_tub): \n    d_agent_plastic_tub = calculate_distance_min(s_agent, s_plastic_tub) \n    penalty_collision = -28 if (d_agent_plastic_tub < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Drone in basement inspection—avoid metal cabinet. States accessible. Write the reward function in Python.",
    "chosen": "def reward(s_agent, s_cabinet, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cabinet = calculate_distance_min(s_agent, s_cabinet) \n    penalty_collision = -28 if (d_agent_cabinet < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_cabinet): \n    d_agent_cabinet = calculate_distance_min(s_agent, s_cabinet) \n    penalty_collision = -28 if (d_agent_cabinet < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "In the basement, a drone inspects carefully. The cement bag is a hazard that must be avoided. Agent, target, and bag states are accessible. Reward function required in Python.",
    "chosen": "def reward(s_agent, s_cement_bag, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cement_bag = calculate_distance_min(s_agent, s_cement_bag) \n    penalty_collision = -28 if (d_agent_cement_bag < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_cement_bag): \n    d_agent_cement_bag = calculate_distance_min(s_agent, s_cement_bag) \n    penalty_collision = -28 if (d_agent_cement_bag < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "A dron is inspecitng a basemant. Must avoid colision with plywod bord. Agent, targt and plywood board states are accesible. Plz code rewaard func in Pyhton.",
    "chosen": "def reward(s_agent, s_plywood_board, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_plywood_board = calculate_distance_min(s_agent, s_plywood_board) \n    penalty_collision = -28 if (d_agent_plywood_board < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_plywood_board): \n    d_agent_plywood_board = calculate_distance_min(s_agent, s_plywood_board) \n    penalty_collision = -28 if (d_agent_plywood_board < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Basement drone mission. Don’t hit wrench set. States: agent, target, wrench. Reward -> Python.",
    "chosen": "def reward(s_agent, s_wrench_set, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wrench_set = calculate_distance_min(s_agent, s_wrench_set) \n    penalty_collision = -28 if (d_agent_wrench_set < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_wrench_set): \n    d_agent_wrench_set = calculate_distance_min(s_agent, s_wrench_set) \n    penalty_collision = -28 if (d_agent_wrench_set < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "An autonomous drone is conducting an inspection in a basement environment. Its task is to reach the target while avoiding collisions with a pipe. The states of the agent, the target, and the pipe are fully accessible. Provide the reward function in Python.",
    "chosen": "def reward(s_agent, s_pipe, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_pipe = calculate_distance_min(s_agent, s_pipe) \n    penalty_collision = -28 if (d_agent_pipe < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_pipe): \n    d_agent_pipe = calculate_distance_min(s_agent, s_pipe) \n    penalty_collision = -28 if (d_agent_pipe < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Drone inspecting a basement—can it reach the target without touching the shelf unit? Assume all states observable. Write Python reward function.",
    "chosen": "def reward(s_agent, s_shelf, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_shelf = calculate_distance_min(s_agent, s_shelf) \n    penalty_collision = -28 if (d_agent_shelf < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_shelf): \n    d_agent_shelf = calculate_distance_min(s_agent, s_shelf) \n    penalty_collision = -28 if (d_agent_shelf < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Basement drone. Avoid bucket of water. States known. Reward in Python.",
    "chosen": "def reward(s_agent, s_bucket, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bucket = calculate_distance_min(s_agent, s_bucket) \n    penalty_collision = -28 if (d_agent_bucket < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_bucket): \n    d_agent_bucket = calculate_distance_min(s_agent, s_bucket) \n    penalty_collision = -28 if (d_agent_bucket < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "A dron is inspecitng a basemant. Must avoid colision with loos brick. Agent, targt and brick states are accesible. Plz code rewaard func in Pyhton.",
    "chosen": "def reward(s_agent, s_brick, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_brick = calculate_distance_min(s_agent, s_brick) \n    penalty_collision = -28 if (d_agent_brick < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_brick): \n    d_agent_brick = calculate_distance_min(s_agent, s_brick) \n    penalty_collision = -28 if (d_agent_brick < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Basement drone. Avoid extension cord. States given. Reward in Python.",
    "chosen": "def reward(s_agent, s_cord, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cord = calculate_distance_min(s_agent, s_cord) \n    penalty_collision = -28 if (d_agent_cord < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_cord): \n    d_agent_cord = calculate_distance_min(s_agent, s_cord) \n    penalty_collision = -28 if (d_agent_cord < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "There’s a drone flying in the basement. It shouldn’t bump into the basement worker. Agent, target, and worker states can be checked. I need a reward function.",
    "chosen": "def reward(s_agent, s_basement_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_basement_worker = calculate_distance_min(s_agent, s_basement_worker) \n    penalty_collision = -28 if (d_agent_basement_worker < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_basement_worker): \n    d_agent_basement_worker = calculate_distance_min(s_agent, s_basement_worker) \n    penalty_collision = -28 if (d_agent_basement_worker < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Basement inspection. Don’t hit light bulb. States: agent, target, bulb. Reward → Python.",
    "chosen": "def reward(s_agent, s_light_bulb, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_light_bulb = calculate_distance_min(s_agent, s_light_bulb) \n    penalty_collision = -28 if (d_agent_light_bulb < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_light_bulb): \n    d_agent_light_bulb = calculate_distance_min(s_agent, s_light_bulb) \n    penalty_collision = -28 if (d_agent_light_bulb < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "During basement navigation, the drone must not collide with chair, rug, or workbench. All states of multiple obstcales are available. Write reward function in Python.",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "While navigating a basement, the drone must avoid toolbox corner and storage box frame. Multiple states known. Write the reward function in Python.",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Target: The drone must avoid colliding with rope coil, pipe section and wrench set. The Agent's state, target's state and the states of multiple obstacle are available. Return a reward function.",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "I need a functon regarding to reward for. The Agent, target's state and the states of multiple obstacles.",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.8) else 30 \n    return penalty_collision"
  },


  
  {
    "prompt": "In a dressing room, a drone inspects carefully. The shoe rack is an obstacle and must not be touched. Agent, target, and rack states are observable. Reward function required in Python.",
    "chosen": "def reward(s_agent, s_shoe_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_shoe_rack = calculate_distance_min(s_agent, s_shoe_rack) \n    penalty_collision = -30 if (d_agent_shoe_rack < 0.8) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_shoe_rack): \n    d_agent_shoe_rack = calculate_distance_min(s_agent, s_shoe_rack) \n    penalty_collision = -30 if (d_agent_shoe_rack < 0.8) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "A drone will be inspecting in a dressing room. It should not collide with the laundry basket. Agent, target, and basket states are observable. Provide reward function in Python.",
    "chosen": "def reward(s_agent, s_laundry_basket, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_laundry_basket = calculate_distance_min(s_agent, s_laundry_basket) \n    penalty_collision = -30 if (d_agent_laundry_basket < 0.8) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_laundry_basket): \n    d_agent_laundry_basket = calculate_distance_min(s_agent, s_laundry_basket) \n    penalty_collision = -30 if (d_agent_laundry_basket < 0.8) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "Facing a parfume bottle, a drone inspects in a dressing room. Collision must be avoided. Agent, target, and bottle states are accessible. Reward function needed.",
    "chosen": "def reward(s_agent, s_perfume_bottle, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_perfume_bottle = calculate_distance_min(s_agent, s_perfume_bottle) \n    penalty_collision = -30 if (d_agent_perfume_bottle < 0.8) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_perfume_bottle): \n    d_agent_perfume_bottle = calculate_distance_min(s_agent, s_perfume_bottle) \n    penalty_collision = -30 if (d_agent_perfume_bottle < 0.8) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "A dron is inspecting in a dressing rom; avoid the carpet.",
    "chosen": "def reward(s_agent, s_carpet, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_carpet = calculate_distance_min(s_agent, s_carpet) \n    penalty_collision = -30 if (d_agent_carpet < 0.8) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_carpet): \n    d_agent_carpet = calculate_distance_min(s_agent, s_carpet) \n    penalty_collision = -30 if (d_agent_carpet < 0.8) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "Drone in a dressing room, no collide with the jewelry box. Agent, target, and box states are accessible. I need a reward function.",
    "chosen": "def reward(s_agent, s_jewelry_box, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_jewelry_box = calculate_distance_min(s_agent, s_jewelry_box) \n    penalty_collision = -30 if (d_agent_jewelry_box < 0.8) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_jewelry_box): \n    d_agent_jewelry_box = calculate_distance_min(s_agent, s_jewelry_box) \n    penalty_collision = -30 if (d_agent_jewelry_box < 0.8) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "The drone’s mission is inspection; its constraint is not colliding with the vanity table. ALl states provided. Nedd: Reward Function",
    "chosen": "def reward(s_agent, s_vanity_table, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_vanity_table = calculate_distance_min(s_agent, s_vanity_table) \n    penalty_collision = -30 if (d_agent_vanity_table < 0.8) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_vanity_table): \n    d_agent_vanity_table = calculate_distance_min(s_agent, s_vanity_table) \n    penalty_collision = -30 if (d_agent_vanity_table < 0.8) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "Drone in a dressing room, no collide with the scarf bundle. Given is agent, target and bundle information. I need a reward function.",
    "chosen": "def reward(s_agent, s_bundle, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bundle = calculate_distance_min(s_agent, s_bundle) \n    penalty_collision = -30 if (d_agent_bundle < 0.8) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_bundle): \n    d_agent_bundle = calculate_distance_min(s_agent, s_bundle) \n    penalty_collision = -30 if (d_agent_bundle < 0.8) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "It must avoiding to collide with wall mirror during inspect. It has a needed infomation of agent, target and wall mirror. Code a reward function.",
    "chosen": "def reward(s_agent, s_wall_mirror, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall_mirror = calculate_distance_min(s_agent, s_wall_mirror) \n    penalty_collision = -30 if (d_agent_wall_mirror < 0.8) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_wall_mirror): \n    d_ = calculate_distance_min(s_agent, s_wall_mirror) \n    penalty_collision = -30 if (d_agent_wall_mirror < 0.8) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "Code a function for reward for: Avoid any clothing-pile collision as the drone inspects the dressing room. It has all information.",
    "chosen": "def reward(s_agent, s_clothing_pile, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_clothing_pile = calculate_distance_min(s_agent, s_clothing_pile) \n    penalty_collision = -30 if (d_agent_clothing_pile < 0.8) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_clothing_pile): \n    d_agent_clothing_pile = calculate_distance_min(s_agent, s_clothing_pile) \n    penalty_collision = -30 if (d_agent_clothing_pile < 0.8) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "Avoid any makeup-kit collision as the drone inspects the dressing room. It has all information.",
    "chosen": "def reward(s_agent, s_makeup_kit, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_makeup_kit = calculate_distance_min(s_agent, s_makeup_kit) \n    penalty_collision = -30 if (d_agent_makeup_kit < 0.8) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_makeup_kit): \n    d_agent_makeup_kit = calculate_distance_min(s_agent, s_makeup_kit) \n    penalty_collision = -30 if (d_agent_makeup_kit < 0.8) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "The drone inspect in dressing room and must not collide with wardrobe. All states are accessible. Implement reward function.",
    "chosen": "def reward(s_agent, s_wardrobe, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wardrobe = calculate_distance_min(s_agent, s_wardrobe) \n    penalty_collision = -30 if (d_agent_wardrobe < 0.8) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_wardrobe): \n    d_agent_wardrobe = calculate_distance_min(s_agent, s_wardrobe) \n    penalty_collision = -30 if (d_agent_wardrobe < 0.8) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "Scenario: A drone is inspecting in an dressing room. \n Goal:  The drone must avoid colliding with the clothes. \n Assumption: Agent's state, target's state and clothes's state are accessible. Write a reward function in Python.",
    "chosen": "def reward(s_agent, s_clothe, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_clothe = calculate_distance_min(s_agent, s_clothe) \n    penalty_collision = -30 if (d_agent_clothe < 0.8) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_clothe): \n    d_agent_clothe = calculate_distance_min(s_agent, s_clothe) \n    penalty_collision = -30 if (d_agent_clothe < 0.8) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "The drone must not colide with the ful-length mirrer during inspection. I has mirror data and other states and implement reward function.",
    "chosen": "def reward(s_agent, s_mirror, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_mirror = calculate_distance_min(s_agent, s_mirror) \n    penalty_collision = -30 if (d_agent_mirror < 0.8) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_mirror): \n    d_agent_mirror = calculate_distance_min(s_agent, s_mirror) \n    penalty_collision = -30 if (d_agent_mirror < 0.8) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "It must avoiding to collide with makeup brush set during inspect. It has a needed infomation of agent, target and makeup brush set. Code a reward function.",
    "chosen": "def reward(s_agent, s_makeup_brush, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_makeup_brush = calculate_distance_min(s_agent, s_makeup_brush) \n    penalty_collision = -30 if (d_agent_makeup_brush < 0.8) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_makeup_brush): \n    d_agent_makeup_brush = calculate_distance_min(s_agent, s_makeup_brush) \n    penalty_collision = -30 if (d_agent_makeup_brush < 0.8) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "Return a function for reward: Dressing room scan. Avoid hanger-rod collisions. Neccessary infomration exist.",
    "chosen": "def reward(s_agent, s_hanger_rod, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_hanger_rod = calculate_distance_min(s_agent, s_hanger_rod) \n    penalty_collision = -30 if (d_agent_hanger_rod < 0.8) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_hanger_rod): \n    d_agent_hanger_rod = calculate_distance_min(s_agent, s_hanger_rod) \n    penalty_collision = -30 if (d_agent_hanger_rod < 0.8) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "Do not bump into the shelf. the infoamtion regarding shelf is provided and return a function which describes the reward. ",
    "chosen": "def reward(s_agent, s_shelf, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_shelf = calculate_distance_min(s_agent, s_shelf) \n    penalty_collision = -30 if (d_agent_shelf < 0.8) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_shelf): \n    d_agent_shelf = calculate_distance_min(s_agent, s_shelf) \n    penalty_collision = -30 if (d_agent_shelf < 0.8) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "There’s a drone flying inside a dressing room. It shouldn’t crash into the laundry hamper. Agent, target, and hamper states are available. Need a Python reward function.",
    "chosen": "def reward(s_agent, s_laundry_hamper, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_laundry_hamper = calculate_distance_min(s_agent, s_laundry_hamper) \n    penalty_collision = -30 if (d_agent_laundry_hamper < 0.8) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_laundry_hamper): \n    d_agent_laundry_hamper = calculate_distance_min(s_agent, s_laundry_hamper) \n    penalty_collision = -30 if (d_agent_laundry_hamper < 0.8) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "Dressing room inspection. Don’t hit shoe box. States: agent, target, box. Reward -> Python.",
    "chosen": "def reward(s_agent, s_shoe_box, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_shoe_box = calculate_distance_min(s_agent, s_shoe_box) \n    penalty_collision = -30 if (d_agent_shoe_box < 0.8) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_shoe_box): \n    d_agent_shoe_box = calculate_distance_min(s_agent, s_shoe_box) \n    penalty_collision = -30 if (d_agent_shoe_box < 0.8) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "A dron is inspecitng a dresing rom. Must avoid colision with bellt. Agent, targt and belt states are accesible. Plz code rewaard func in Pyhton.",
    "chosen": "def reward(s_agent, s_belt, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_belt = calculate_distance_min(s_agent, s_belt) \n    penalty_collision = -30 if (d_agent_belt < 0.8) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_belt): \n    d_agent_belt = calculate_distance_min(s_agent, s_belt) \n    penalty_collision = -30 if (d_agent_belt < 0.8) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "Inside a dressing room, a drone moves carefully. The lamp is fragile and must not be touched. Agent, target, and lamp states are available. Reward function required in Python.",
    "chosen": "def reward(s_agent, s_lamp, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lamp = calculate_distance_min(s_agent, s_lamp) \n    penalty_collision = -30 if (d_agent_lamp < 0.8) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_lamp): \n    d_agent_lamp = calculate_distance_min(s_agent, s_lamp) \n    penalty_collision = -30 if (d_agent_lamp < 0.8) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "A dron is inspecitng a dresing rom. Must avoid colision with dresing ligt. Agent, targt and ligt states are accesible. Plz code rewaard func in Pyhton.",
    "chosen": "def reward(s_agent, s_light, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_light = calculate_distance_min(s_agent, s_light) \n    penalty_collision = -30 if (d_agent_light < 0.8) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_light): \n    d_agent_light = calculate_distance_min(s_agent, s_light) \n    penalty_collision = -30 if (d_agent_light < 0.8) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "Dressing room drone. Avoid chair, carpet, vanity table. Muliple States known. Reward in Python.",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -30 if (d_agent_multiple_obstacles_min < 0.8) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -30 if (d_agent_multiple_obstacles_min < 0.8) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "The drone is operating inside a dressing room. Its mission is to avoid collisions with two obstacles: the shoe rack corner and the laundry basket frame. Agent, target, and multiple obstacle states are fully available. Write the reward function in Python.",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -30 if (d_agent_multiple_obstacles_min < 0.8) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -30 if (d_agent_multiple_obstacles_min < 0.8) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "Drone navigating a dressing room—can it reach its target without hitting the scarf bundle, hanger rod section, or makeup brush set? Assume states available. Code a rewadr function",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -30 if (d_agent_multiple_obstacles_min < 0.8) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -30 if (d_agent_multiple_obstacles_min < 0.8) else 20 \n    return penalty_collision"
  },
  {
    "prompt": "Scenario: A drone is navigating an dressing room. Target: The drone must avoid colliding with jewelry box hanger rod and shoe box. Assumptions: The Agent's state, target's state and the states of multiple obstacles: jewelry box hanger rod, and shoe box are available. Instruction: Implement a reward function in Python.",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -30 if (d_agent_multiple_obstacles_min < 0.8) else 20 \n    reward_reaching_target = 32 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -30 if (d_agent_multiple_obstacles_min < 0.8) else 20 \n    return penalty_collision"
  },



  
  {
    "prompt": "Drone inspects attic. Must not hit trunk. States (agent, target, trunk) known. Write Python reward function.",
    "chosen": "def reward(s_agent, s_trunk, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_trunk = calculate_distance_min(s_agent, s_trunk) \n    penalty_collision = -10 if (d_agent_trunk < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_trunk): \n    d_agent_trunk = calculate_distance_min(s_agent, s_trunk) \n    penalty_collision = -10 if (d_agent_trunk < 0.8) else 0 \n    return penalty_collision"
  },
  {
    "prompt": "Consider a drone navigating an attic during an inspection. It needs to reach a designated target without colliding with an old suitcase in the environment. The positions and states of the drone, the target, and the suitcase are available. Please implement a reward function in Python.",
    "chosen": "def reward(s_agent, s_suitcase, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_suitcase = calculate_distance_min(s_agent, s_suitcase) \n    penalty_collision = -10 if (d_agent_suitcase < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_suitcase): \n    d_agent_suitcase = calculate_distance_min(s_agent, s_suitcase) \n    penalty_collision = -10 if (d_agent_suitcase < 0.8) else 0 \n    return penalty_collision"
  },
  {
    "prompt": "Drone is inside atic. Need reach target but avoid oil lmp. We know agent, taret, and oil lamp state. Write rewrd functon in Pyhton.",
    "chosen": "def reward(s_agent, s_oil_lamp, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_oil_lamp = calculate_distance_min(s_agent, s_oil_lamp) \n    penalty_collision = -10 if (d_agent_oil_lamp < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_oil_lamp): \n    d_agent_oil_lamp = calculate_distance_min(s_agent, s_oil_lamp) \n    penalty_collision = -10 if (d_agent_oil_lamp < 0.8) else 0 \n    return penalty_collision"
  },
  {
    "prompt": "The environment provides access to the drone, target, and stair states. The drone should move toward the target without hitting the stair. Implement the reward function in Python.",
    "chosen": "def reward(s_agent, s_stair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_stair = calculate_distance_min(s_agent, s_stair) \n    penalty_collision = -10 if (d_agent_stair < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_stair): \n    d_agent_stair = calculate_distance_min(s_agent, s_stair) \n    penalty_collision = -10 if (d_agent_stair < 0.8) else 0 \n    return penalty_collision"
  },
  {
    "prompt": "The environment provides the states of the drone, the target, and the chest. During attic inspection, the drone should not crash into the wooden chest. I need a reward function.",
    "chosen": "def reward(s_agent, s_wooden_chest, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wooden_chest = calculate_distance_min(s_agent, s_wooden_chest) \n    penalty_collision = -10 if (d_agent_wooden_chest < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_wooden_chest): \n    d_agent_wooden_chest = calculate_distance_min(s_agent, s_wooden_chest) \n    penalty_collision = -10 if (d_agent_wooden_chest < 0.8) else 0 \n    return penalty_collision"
  },
  {
    "prompt": "Attic drone navigation. Must not crash into broken chair. States accessible. Write reward function in Python.",
    "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -10 if (d_agent_chair < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_chair): \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -10 if (d_agent_chair < 0.8) else 0 \n    return penalty_collision"
  },
  {
    "prompt": "Drone is inspecting attic. Goal = no crash into rop bundle. We knw agent, target and rope state. Writ reword func Pyton.",
    "chosen": "def reward(s_agent, s_rope_bundle, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rope_bundle = calculate_distance_min(s_agent, s_rope_bundle) \n    penalty_collision = -10 if (d_agent_rope_bundle < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_rope_bundle): \n    d_agent_rope_bundle = calculate_distance_min(s_agent, s_rope_bundle) \n    penalty_collision = -10 if (d_agent_rope_bundle < 0.8) else 0 \n    return penalty_collision"
  },
  {
    "prompt": "A drone is flying inside an attic for inspection purposes. As it navigates toward its target, it must avoid crashing into a food jar located in the attic. The environment provides the states of the drone, the target, and the food jar. Write a Python reward function that promotes safe navigation.",
    "chosen": "def reward(s_agent, s_food_jar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_food_jar = calculate_distance_min(s_agent, s_food_jar) \n    penalty_collision = -10 if (d_agent_food_jar < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_food_jar): \n    d_agent_food_jar = calculate_distance_min(s_agent, s_food_jar) \n    penalty_collision = -10 if (d_agent_food_jar < 0.8) else 0 \n    return penalty_collision"
  },
  {
    "prompt": "Agent, target, and newspaper stack states are observable. A drone is flying in an attic and must not hit the stack. Implement the Python reward function.",
    "chosen": "def reward(s_agent, s_newspaper_stack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_newspaper_stack = calculate_distance_min(s_agent, s_newspaper_stack) \n    penalty_collision = -10 if (d_agent_newspaper_stack < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_newspaper_stack): \n    d_agent_newspaper_stack = calculate_distance_min(s_agent, s_newspaper_stack) \n    penalty_collision = -10 if (d_agent_newspaper_stack < 0.8) else 0 \n    return penalty_collision"
  },
  {
    "prompt": "Scenrio: dron fly attic. Goal = no hit paintcn. Agnt, targt, and paint can sttes known. Rite reward functon Python.",
    "chosen": "def reward(s_agent, s_paint_can, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_paint_can = calculate_distance_min(s_agent, s_paint_can) \n    penalty_collision = -10 if (d_agent_paint_can < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_paint_can): \n    d_agent_paint_can = calculate_distance_min(s_agent, s_paint_can) \n    penalty_collision = -10 if (d_agent_paint_can < 0.8) else 0 \n    return penalty_collision"
  },
  {
    "prompt": "A reinforcement learning agent controls a drone flying inside an attic. The objective is to navigate toward the destination without colliding with the old attic worker. You have access to the drone, target, and worker states. Write the reward function in Python.",
    "chosen": "def reward(s_agent, s_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_worker = calculate_distance_min(s_agent, s_worker) \n    penalty_collision = -10 if (d_agent_worker < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_worker): \n    d_agent_worker = calculate_distance_min(s_agent, s_worker) \n    penalty_collision = -10 if (d_agent_worker < 0.8) else 0 \n    return penalty_collision"
  },
  {
    "prompt": "Agent, target, and box of books states are accessible. A drone is performing attic inspection and must avoid the box. Implement the Python reward function.",
    "chosen": "def reward(s_agent, s_book, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_book = calculate_distance_min(s_agent, s_book) \n    penalty_collision = -10 if (d_agent_book < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_book): \n    d_agent_book = calculate_distance_min(s_agent, s_book) \n    penalty_collision = -10 if (d_agent_book < 0.8) else 0 \n    return penalty_collision"
  },
  {
    "prompt": "Drone inspects attic. Must not hit full-length window. Agent, target, and window states are available. Write reward function in Python.",
    "chosen": "def reward(s_agent, s_window, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_window = calculate_distance_min(s_agent, s_window) \n    penalty_collision = -10 if (d_agent_window < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_window): \n    d_agent_window = calculate_distance_min(s_agent, s_window) \n    penalty_collision = -10 if (d_agent_window < 0.8) else 0 \n    return penalty_collision"
  },
  {
    "prompt": "Drone fly attic inspect. Must avoid cobwb clster. We hav agent, targt, cocobweb cluster bwb state. Writ reword func in Pyton.",
    "chosen": "def reward(s_agent, s_cobweb_cluster, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cobweb_cluster = calculate_distance_min(s_agent, s_cobweb_cluster) \n    penalty_collision = -10 if (d_agent_cobweb_cluster < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_cobweb_cluster): \n    d_agent_cobweb_cluster = calculate_distance_min(s_agent, s_cobweb_cluster) \n    penalty_collision = -10 if (d_agent_cobweb_cluster < 0.8) else 0 \n    return penalty_collision"
  },
  {
    "prompt": "A reinforcement learning drone explores an attic to reach a goal location. A wooden beam lies in its path as an obstacle, and collisions must be avoided. The drone, target, and beam states are observable. Implement the reward function in Python.",
    "chosen": "def reward(s_agent, s_wooden_beam, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wooden_beam = calculate_distance_min(s_agent, s_wooden_beam) \n    penalty_collision = -10 if (d_agent_wooden_beam < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_wooden_beam): \n    d_agent_wooden_beam = calculate_distance_min(s_agent, s_wooden_beam) \n    penalty_collision = -10 if (d_agent_wooden_beam < 0.8) else 0 \n    return penalty_collision"
  },
  {
    "prompt": "Agent, target, and attic shelf unit states are observable. A drone is inspecting the attic and must not collide with the shelf. Write the reward function in Python.",
    "chosen": "def reward(s_agent, s_shelf_unit, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_shelf_unit = calculate_distance_min(s_agent, s_shelf_unit) \n    penalty_collision = -10 if (d_agent_shelf_unit < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_shelf_unit): \n    d_agent_shelf_unit = calculate_distance_min(s_agent, s_shelf_unit) \n    penalty_collision = -10 if (d_agent_shelf_unit < 0.8) else 0 \n    return penalty_collision"
  },
  {
    "prompt": "Scenario: A drone is inspecting in an attic. \n Goal:  The drone must avoid hitting with the basket of apples. \n Assumption: Agent's state, target's state and basket of apples's state are accessible. Write a reward function in Python.",
    "chosen": "def reward(s_agent, s_apple_basket, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_apple_basket = calculate_distance_min(s_agent, s_apple_basket) \n    penalty_collision = -10 if (d_agent_apple_basket < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_apple_basket): \n    d_agent_apple_basket = calculate_distance_min(s_agent, s_apple_basket) \n    penalty_collision = -10 if (d_agent_apple_basket < 0.8) else 0 \n    return penalty_collision"
  },
  {
    "prompt": "Attic drone task: must not hit hat box. Agent, target, and box states are accessible. Provide reward function in Python ",
    "chosen": "def reward(s_agent, s_box, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_box = calculate_distance_min(s_agent, s_box) \n    penalty_collision = -10 if (d_agent_box < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_box): \n    d_agent_box = calculate_distance_min(s_agent, s_box) \n    penalty_collision = -10 if (d_agent_box < 0.8) else 0 \n    return penalty_collision"
  },
  {
    "prompt": "Drone flies attic. Must steer clear of coat hook. Agent, goal, and hook states are known. Write Python reward functio",
    "chosen": "def reward(s_agent, s_coat_hook, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_coat_hook = calculate_distance_min(s_agent, s_coat_hook) \n    penalty_collision = -10 if (d_agent_coat_hook < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_coat_hook): \n    d_agent_coat_hook = calculate_distance_min(s_agent, s_coat_hook) \n    penalty_collision = -10 if (d_agent_coat_hook < 0.8) else 0 \n    return penalty_collision"
  },
  {
    "prompt": "Drone is in atic inspect. Avoid bump with attic worker. We hav agent, taret, attic worker state. Writ reword func Pyton.",
    "chosen": "def reward(s_agent, s_attic_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_attic_worker = calculate_distance_min(s_agent, s_attic_worker) \n    penalty_collision = -10 if (d_agent_attic_worker < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_attic_worker): \n    d_agent_attic_worker = calculate_distance_min(s_agent, s_attic_worker) \n    penalty_collision = -10 if (d_agent_attic_worker < 0.8) else 0 \n    return penalty_collision"
  },
  {
    "prompt": "A drone is navigating through an attic for inspection purposes. While flying toward its target, it must avoid crashing into a hanging lantern. The states of the drone, the target, and the lantern are all available. Write a Python reward function to guide safe navigation.",
    "chosen": "def reward(s_agent, s_lantern, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lantern = calculate_distance_min(s_agent, s_lantern) \n    penalty_collision = -10 if (d_agent_lantern < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_lantern): \n    d_agent_lantern = calculate_distance_min(s_agent, s_lantern) \n    penalty_collision = -10 if (d_agent_lantern < 0.8) else 0 \n    return penalty_collision"
  },
  {
    "prompt": "Attic drone navigation: must not collide with furniture (chair, dusty stair, broken chair). Multiple States available. Provide reward function in Python.",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -10 if (d_agent_multiple_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -10 if (d_agent_multiple_obstacles_min < 0.8) else 0 \n    return penalty_collision"
  },
  {
    "prompt": "Scenrio: dron fly attic. Goal = no collid with trnk cornr + old suitcse fram. Agnt, taret, mltipe obstcl stats givn. Rite reward functon Pyton.",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -10 if (d_agent_multiple_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -10 if (d_agent_multiple_obstacles_min < 0.8) else 0 \n    return penalty_collision"
  },
  {
    "prompt": "A reinforcement learning drone explores an attic. Its task is to arrive at the target point safely, avoiding collisions with three hazards: rope bundle, wooden beam fragment, and cobweb cluster. Full state information for the drone, the goal, and the multiple obstacles is available. Implement the reward function in Python. ",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -10 if (d_agent_multiple_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -10 if (d_agent_multiple_obstacles_min < 0.8) else 0 \n    return penalty_collision"
  },
  {
    "prompt": "Attic navigation: drone must not collide with chest, beam, or box. All states accessible. Provide reward function in Python.",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -10 if (d_agent_multiple_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -10 if (d_agent_multiple_obstacles_min < 0.8) else 0 \n    return penalty_collision"
  },

  
  {
    "prompt": "A quadcopter drone is carrying out an inspection task inside a courthouse atrium, moving around pillars and near the judge’s bench while avoiding contact. Infomationregrading bench given and code a code regarding function",
    "chosen": "def reward(s_agent, s_bench, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bench = calculate_distance_min(s_agent, s_bench) \n    penalty_collision = -28 if (d_agent_bench < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_bench): \n    d_agent_bench = calculate_distance_min(s_agent, s_bench) \n    penalty_collision = -28 if (d_agent_bench < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "While scanning the courthouse interior, a drone flies toward its designated waypoint. To complete the inspection successfully, it has to avoid bumping into the file clerk desk, and the full states of agent, desk, and goal are available. You need to give a function.",
    "chosen": "def reward(s_agent, s_file_clerk_desk, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_file_clerk_desk = calculate_distance_min(s_agent, s_file_clerk_desk) \n    penalty_collision = -28 if (d_agent_file_clerk_desk < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_file_clerk_desk): \n    d_agent_file_clerk_desk = calculate_distance_min(s_agent, s_file_clerk_desk) \n    penalty_collision = -28 if (d_agent_file_clerk_desk < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Reward function in python: Courthouse inspection flight: avoid any contact with the court seal plaque using known states of the agent, the goal, and the plaque.",
    "chosen": "def reward(s_agent, s_seal_plaque, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_seal_plaque = calculate_distance_min(s_agent, s_seal_plaque) \n    penalty_collision = -28 if (d_agent_seal_plaque < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_seal_plaque): \n    d_agent_seal_plaque = calculate_distance_min(s_agent, s_seal_plaque) \n    penalty_collision = -28 if (d_agent_seal_plaque < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "A drone are inspect courthouse, must not hit the lawyer, states is accessable. Python code",
    "chosen": "def reward(s_agent, s_lawyer, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lawyer = calculate_distance_min(s_agent, s_lawyer) \n    penalty_collision = -28 if (d_agent_lawyer < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_lawyer): \n    d_agent_lawyer = calculate_distance_min(s_agent, s_lawyer) \n    penalty_collision = -28 if (d_agent_lawyer < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Function: Progress to the target is required without touching the evidence locker; agent, target, and locker states are available.",
    "chosen": "def reward(s_agent, s_evidence_locker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_evidence_locker = calculate_distance_min(s_agent, s_evidence_locker) \n    penalty_collision = -28 if (d_agent_evidence_locker < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_evidence_locker): \n    d_agent_evidence_locker = calculate_distance_min(s_agent, s_evidence_locker) \n    penalty_collision = -28 if (d_agent_evidence_locker < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "A dron is inspecitng in a courthous and must not colide with the witnes stand and reach goal; agen, targt, and witness stand states are accesible.",
    "chosen": "def reward(s_agent, s_witness_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_witness_stand = calculate_distance_min(s_agent, s_witness_stand) \n    penalty_collision = -28 if (d_agent_witness_stand < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_witness_stand): \n    d_agent_witness_stand = calculate_distance_min(s_agent, s_witness_stand) \n    penalty_collision = -28 if (d_agent_witness_stand < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Courthouse drone inspection; avoid the judge and reach goal. States observable. Code function",
    "chosen": "def reward(s_agent, s_judge, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_judge = calculate_distance_min(s_agent, s_judge) \n    penalty_collision = -28 if (d_agent_judge < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_judge): \n    d_agent_judge = calculate_distance_min(s_agent, s_judge) \n    penalty_collision = -28 if (d_agent_judge < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "With agent, target, and jury box states known, the drone inspects a courthouse and must not touch the box and reach goal. REturn a forward function.",
    "chosen": "def reward(s_agent, s_jury_box, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_jury_box = calculate_distance_min(s_agent, s_jury_box) \n    penalty_collision = -28 if (d_agent_jury_box < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_jury_box): \n    d_agent_jury_box = calculate_distance_min(s_agent, s_jury_box) \n    penalty_collision = -28 if (d_agent_jury_box < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "An indoor drone maps the courtroom and should not collide with the witness stand; all states are accessible. We need a function in code",
    "chosen": "def reward(s_agent, s_witness_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_witness_stand = calculate_distance_min(s_agent, s_witness_stand) \n    penalty_collision = -28 if (d_agent_witness_stand < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_witness_stand): \n    d_agent_witness_stand = calculate_distance_min(s_agent, s_witness_stand) \n    penalty_collision = -28 if (d_agent_witness_stand < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Scenario: A drone is inspecting in an courthouse. \n Goal:  The drone must avoid colliding with the water pitcher. \n Assumption: Agent's state, target's state and water pitcher's state are accessible. Write a reward function in Python.",
    "chosen": "def reward(s_agent, s_water_pitcher, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_water_pitcher = calculate_distance_min(s_agent, s_water_pitcher) \n    penalty_collision = -28 if (d_agent_water_pitcher < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_water_pitcher): \n    d_agent_water_pitcher = calculate_distance_min(s_agent, s_water_pitcher) \n    penalty_collision = -28 if (d_agent_water_pitcher < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Inside a courthouse, the drone performs inspection and should avoid the clerk desk, with all relevant states observable. need a code",
    "chosen": "def reward(s_agent, s_clerk_desk, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_clerk_desk = calculate_distance_min(s_agent, s_clerk_desk) \n    penalty_collision = -28 if (d_agent_clerk_desk < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_clerk_desk): \n    d_agent_clerk_desk = calculate_distance_min(s_agent, s_clerk_desk) \n    penalty_collision = -28 if (d_agent_clerk_desk < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Avoid collision with the podium during courthouse inspection; agent, target, and podium states are observable.",
    "chosen": "def reward(s_agent, s_podium, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_podium = calculate_distance_min(s_agent, s_podium) \n    penalty_collision = -28 if (d_agent_podium < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_podium): \n    d_agent_podium = calculate_distance_min(s_agent, s_podium) \n    penalty_collision = -28 if (d_agent_podium < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "A UAV surveys the courthouse interior and must steer clear of the surveillance camera; states for agent, target, and camera are available. Implement the reward function",
    "chosen": "def reward(s_agent, s_camera, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_camera = calculate_distance_min(s_agent, s_camera) \n    penalty_collision = -28 if (d_agent_camera < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_camera): \n    d_agent_camera = calculate_distance_min(s_agent, s_camera) \n    penalty_collision = -28 if (d_agent_camera < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Drone are inspect courthouse, must not hit victim, states is accessable. Can you turn a reard fucntion",
    "chosen": "def reward(s_agent, s_witness, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_witness = calculate_distance_min(s_agent, s_witness) \n    penalty_collision = -28 if (d_agent_witness < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_witness): \n    d_agent_witness = calculate_distance_min(s_agent, s_witness) \n    penalty_collision = -28 if (d_agent_witness < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Mission success requires staying away from the clock while reaching the target; agent, target, and clock states are provided. Is possible to turn a function?",
    "chosen": "def reward(s_agent, s_clock, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_clock = calculate_distance_min(s_agent, s_clock) \n    penalty_collision = -28 if (d_agent_clock < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_clock): \n    d_agent_clock = calculate_distance_min(s_agent, s_clock) \n    penalty_collision = -28 if (d_agent_clock < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "The dron inspeckts courthouse and shoud not crash into the plaform; all platform state availble",
    "chosen": "def reward(s_agent, s_platform, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_platform = calculate_distance_min(s_agent, s_platform) \n    penalty_collision = -28 if (d_agent_platform < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_platform): \n    d_agent_platform = calculate_distance_min(s_agent, s_platform) \n    penalty_collision = -28 if (d_agent_platform < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "An indoor drone maps the courtroom and should not strike the storage box for evidence; agent, target, and box states are accessible. Write a reward function in Python.",
    "chosen": "def reward(s_agent, s_evidence_box, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_evidence_box = calculate_distance_min(s_agent, s_evidence_box) \n    penalty_collision = -28 if (d_agent_evidence_box < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_evidence_box): \n    d_agent_evidence_box = calculate_distance_min(s_agent, s_evidence_box) \n    penalty_collision = -28 if (d_agent_evidence_box < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Drone inspects courthouse; must not collide with briefcase using known states. PYthon code",
    "chosen": "def reward(s_agent, s_brief, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_briefcase = calculate_distance_min(s_agent, s_briefcase) \n    penalty_collision = -28 if (d_agent_briefcase < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_briefcase): \n    d_agent_briefcase = calculate_distance_min(s_agent, s_briefcase) \n    penalty_collision = -28 if (d_agent_briefcase < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Courthouse drone inspection; avoid gavel block. States observable. code it as a function",
    "chosen": "def reward(s_agent, s_gavel_block, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_gavel_block = calculate_distance_min(s_agent, s_gavel_block) \n    penalty_collision = -28 if (d_agent_gavel_block < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_gavel_block): \n    d_agent_gavel_block = calculate_distance_min(s_agent, s_gavel_block) \n    penalty_collision = -28 if (d_agent_gavel_block < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "The drone must reach its courthouse target while avoiding the courthouse worker; states are observable. Write a reward function in Python.",
    "chosen": "def reward(s_agent, s_courthouse_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_courthouse_worker = calculate_distance_min(s_agent, s_courthouse_worker) \n    penalty_collision = -28 if (d_agent_courthouse_worker < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_courthouse_worker): \n    d_agent_courthouse_worker = calculate_distance_min(s_agent, s_courthouse_worker) \n    penalty_collision = -28 if (d_agent_courthouse_worker < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Courthouse operation is valid only if the drone avoids the microphone stand, given accessible states. PYthon code",
    "chosen": "def reward(s_agent, s_microphone_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_microphone_stand = calculate_distance_min(s_agent, s_microphone_stand) \n    penalty_collision = -28 if (d_agent_microphone_stand < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_microphone_stand): \n    d_agent_microphone_stand = calculate_distance_min(s_agent, s_microphone_stand) \n    penalty_collision = -28 if (d_agent_microphone_stand < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Scenario: A drone is navigating an courthouse. Target: The drone must avoid colliding with chair, carpet runner and witness stand. Assumptions: The Agent's state, target's state and the states of multiple obstacles: chair, judge, and witness stand are available. Instruction: Implement a reward function in Python.",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "The drone maps the courtroom and should not strike the wooden bench edge or the clerk’s desk frame; agent, target, and multiple obstacle states are accessible.",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "A drone is navigating an courthouse. The drone must avoid colliding with restricted rope, clock section and pile of papers. The Agent's state, target's state and the states of multiple obstacles: restricted rope, clock, and pile of papers are available. Instruction: Implement a reward function in Python.",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "The drone must avoid colliding with evidence locker clock and briefcase. The Agent's state, target's state and the states of multiple obstacles: evidence locker clock, and briefcase are available. Instruction: Implement a reward function in Python.",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.8) else 30 \n    return penalty_collision"
  },



  
  {
    "prompt": "Laboratory inspection: avoid the lab bench. Agent, target, and bench states are known.",
    "chosen": "def reward(s_agent, s_lab bench, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lab bench = calculate_distance_min(s_agent, s_lab bench) \n    penalty_collision = -25 if (d_agent_lab bench < 0.8) else 30 \n    reward_reaching_target = 30 if (d_agent_target < 0.1) else -40 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_lab bench): \n    d_agent_lab bench = calculate_distance_min(s_agent, s_lab bench) \n    penalty_collision = -25 if (d_agent_lab bench < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Avoid collision with the chemical locker during lab inspection; all relevant states(goal, target, chemical locker) are available.",
    "chosen": "def reward(s_agent, s_chemical_locker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chemical_locker = calculate_distance_min(s_agent, s_chemical_locker) \n    penalty_collision = -25 if (d_agent_chemical_locker < 0.8) else 30 \n    reward_reaching_target = 30 if (d_agent_target < 0.1) else -40 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_chemical_locker): \n    d_agent_chemical_locker = calculate_distance_min(s_agent, s_chemical_locker) \n    penalty_collision = -25 if (d_agent_chemical_locker < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "An indoor drone maps the laboratory chamber and should not strike the gas burner; all states are observable.Can you returna reward function?",
    "chosen": "def reward(s_agent, s_bunsen_burner, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bunsen_burner = calculate_distance_min(s_agent, s_bunsen_burner) \n    penalty_collision = -25 if (d_agent_bunsen_burner < 0.8) else 30 \n    reward_reaching_target = 30 if (d_agent_target < 0.1) else -40 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_bunsen_burner): \n    d_agent_bunsen_burner = calculate_distance_min(s_agent, s_bunsen_burner) \n    penalty_collision = -25 if (d_agent_bunsen_burner < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "The dron inspeckts lab chamber and shoud not crash into the floor mat; al state availble. Give me a reward function",
    "chosen": "def reward(s_agent, s_floor_mat, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_floor_mat = calculate_distance_min(s_agent, s_floor_mat) \n    penalty_collision = -25 if (d_agent_floor_mat < 0.8) else 30 \n    reward_reaching_target = 30 if (d_agent_target < 0.1) else -40 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_floor_mat): \n    d_agent_floor_mat = calculate_distance_min(s_agent, s_floor_mat) \n    penalty_collision = -25 if (d_agent_floor_mat < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Reward function: A drone is conducting an interior inspection inside a laboratory. As it travels toward its waypoint, it must avoid colliding with the sample. The states of the agent, the target, and the sample are available.",
    "chosen": "def reward(s_agent, s_sample, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sample = calculate_distance_min(s_agent, s_sample) \n    penalty_collision = -25 if (d_agent_sample < 0.8) else 30 \n    reward_reaching_target = 30 if (d_agent_target < 0.1) else -40 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_sample): \n    d_agent_sample = calculate_distance_min(s_agent, s_sample) \n    penalty_collision = -25 if (d_agent_sample < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "As the drone approaches its target, it must ensure no contact with the stool, relying on accessible states such of the stool. Is ist possible to return python code? ",
    "chosen": "def reward(s_agent, s_lab_stool, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lab_stool = calculate_distance_min(s_agent, s_lab_stool) \n    penalty_collision = -25 if (d_agent_lab_stool < 0.8) else 30 \n    reward_reaching_target = 30 if (d_agent_target < 0.1) else -40 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_lab_stool): \n    d_agent_lab_stool = calculate_distance_min(s_agent, s_lab_stool) \n    penalty_collision = -25 if (d_agent_lab_stool < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "he dron inspeckts lab and shoud not crash into the tueb; all state availble. Can you returna reward function?",
    "chosen": "def reward(s_agent, s_tube, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tube = calculate_distance_min(s_agent, s_tube) \n    penalty_collision = -25 if (d_agent_tube < 0.8) else 30 \n    reward_reaching_target = 30 if (d_agent_target < 0.1) else -40 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_tube): \n    d_agent_tube = calculate_distance_min(s_agent, s_tube) \n    penalty_collision = -25 if (d_agent_tube < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "The operation is valid only when the drone avoids the wall chemical locker completely, given access to agent, target, and locker states. Construct a function as a code",
    "chosen": "def reward(s_agent, s_chemical_locker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chemical_locker = calculate_distance_min(s_agent, s_chemical_locker) \n    penalty_collision = -25 if (d_agent_chemical_locker < 0.8) else 30 \n    reward_reaching_target = 30 if (d_agent_target < 0.1) else -40 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_chemical_locker): \n    d_agent_chemical_locker = calculate_distance_min(s_agent, s_chemical_locker) \n    penalty_collision = -25 if (d_agent_chemical_locker < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "A drone inspects the laboratory and must not collide with the acid bottle; agent, target, and bottle states are available and return a code.",
    "chosen": "def reward(s_agent, s_acid_bottle, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_acid_bottle = calculate_distance_min(s_agent, s_acid_bottle) \n    penalty_collision = -25 if (d_agent_acid_bottle < 0.8) else 30 \n    reward_reaching_target = 30 if (d_agent_target < 0.1) else -40 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_acid_bottle): \n    d_agent_acid_bottle = calculate_distance_min(s_agent, s_acid_bottle) \n    penalty_collision = -25 if (d_agent_acid_bottle < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Reward function: Mission success requires progressing to the goal without contacting the reagent bottle, with agent, target, and bottle states observable.",
    "chosen": "def reward(s_agent, s_reagent_bottle, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_reagent_bottle = calculate_distance_min(s_agent, s_reagent_bottle) \n    penalty_collision = -25 if (d_agent_reagent_bottle < 0.8) else 30 \n    reward_reaching_target = 30 if (d_agent_target < 0.1) else -40 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_reagent_bottle): \n    d_agent_reagent_bottle = calculate_distance_min(s_agent, s_reagent_bottle) \n    penalty_collision = -25 if (d_agent_reagent_bottle < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Python code: A drone is carrying out an inspection task inside a laboratory. While flying toward its target location, it must avoid colliding with the pipette stand. The states of the drone, the target, and the pipette stand are accessible.",
    "chosen": "def reward(s_agent, s_pipette_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_pipette_stand = calculate_distance_min(s_agent, s_pipette_stand) \n    penalty_collision = -25 if (d_agent_pipette_stand < 0.8) else 30 \n    reward_reaching_target = 30 if (d_agent_target < 0.1) else -40 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_pipette_stand): \n    d_agent_pipette_stand = calculate_distance_min(s_agent, s_pipette_stand) \n    penalty_collision = -25 if (d_agent_pipette_stand < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Function in python: Drone inspects lab; must not collide with incubator using known states.",
    "chosen": "def reward(s_agent, s_incubator, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_incubator = calculate_distance_min(s_agent, s_incubator) \n    penalty_collision = -25 if (d_agent_incubator < 0.8) else 30 \n    reward_reaching_target = 30 if (d_agent_target < 0.1) else -40 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_incubator): \n    d_agent_incubator = calculate_distance_min(s_agent, s_incubator) \n    penalty_collision = -25 if (d_agent_incubator < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Function (Python): A UAV surveys the laboratory and must remain clear of the lab centrifuge device; agent, target, and device states are accessible.",
    "chosen": "def reward(s_agent, s_centrifuge, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_centrifuge = calculate_distance_min(s_agent, s_centrifuge) \n    penalty_collision = -25 if (d_agent_centrifuge < 0.8) else 30 \n    reward_reaching_target = 30 if (d_agent_target < 0.1) else -40 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_centrifuge): \n    d_agent_centrifuge = calculate_distance_min(s_agent, s_centrifuge) \n    penalty_collision = -25 if (d_agent_centrifuge < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "During a lab survey, the drone maneuvers between benches and equipment. Its mission is to reach the goal without making contact with the gas cylinder, using all accessible states to form a Code",
    "chosen": "def reward(s_agent, s_gas_cylinder, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_gas_cylinder = calculate_distance_min(s_agent, s_gas_cylinder) \n    penalty_collision = -25 if (d_agent_gas_cylinder < 0.8) else 30 \n    reward_reaching_target = 30 if (d_agent_target < 0.1) else -40 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_gas_cylinder): \n    d_agent_gas_cylinder = calculate_distance_min(s_agent, s_gas_cylinder) \n    penalty_collision = -25 if (d_agent_gas_cylinder < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "The drone doing lab inspection need avoid collid with glasswear rack, states can be read and used for coding",
    "chosen": "def reward(s_agent, s_glassware_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_glassware_rack = calculate_distance_min(s_agent, s_glassware_rack) \n    penalty_collision = -25 if (d_agent_glassware_rack < 0.8) else 30 \n    reward_reaching_target = 30 if (d_agent_target < 0.1) else -40 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_glassware_rack): \n    d_agent_glassware_rack = calculate_distance_min(s_agent, s_glassware_rack) \n    penalty_collision = -25 if (d_agent_glassware_rack < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "The dron inspeckts lab and shoud not crash into the shelf unit; all state availble for example of the drone and shelf.",
    "chosen": "def reward(s_agent, s_shelf, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_shelf = calculate_distance_min(s_agent, s_shelf) \n    penalty_collision = -25 if (d_agent_shelf < 0.8) else 30 \n    reward_reaching_target = 30 if (d_agent_target < 0.1) else -40 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_shelf): \n    d_agent_shelf = calculate_distance_min(s_agent, s_shelf) \n    penalty_collision = -25 if (d_agent_shelf < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Drone are inspect laboratory, must not hit sink, taget and sink information is accessable.",
    "chosen": "def reward(s_agent, s_sink, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sink = calculate_distance_min(s_agent, s_sink) \n    penalty_collision = -25 if (d_agent_sink < 0.8) else 30 \n    reward_reaching_target = 30 if (d_agent_target < 0.1) else -40 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_sink): \n    d_agent_sink = calculate_distance_min(s_agent, s_sink) \n    penalty_collision = -25 if (d_agent_sink < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Code: It approaches its waypoint, it must avoid colliding with the sample container and reach target. The states of the drone, the target, and the container are fully available.",
    "chosen": "def reward(s_agent, s_sample_container, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sample_container = calculate_distance_min(s_agent, s_sample_container) \n    penalty_collision = -25 if (d_agent_sample_container < 0.8) else 30 \n    reward_reaching_target = 30 if (d_agent_target < 0.1) else -40 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_sample_container): \n    d_agent_sample_container = calculate_distance_min(s_agent, s_sample_container) \n    penalty_collision = -25 if (d_agent_sample_container < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "A drone inspects the laboratory and must not collide with the test tube rack and land on target; agent, target, and rack states are available",
    "chosen": "def reward(s_agent, s_cord, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cord = calculate_distance_min(s_agent, s_cord) \n    penalty_collision = -25 if (d_agent_cord < 0.8) else 30 \n    reward_reaching_target = 30 if (d_agent_target < 0.1) else -40 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_cord): \n    d_agent_cord = calculate_distance_min(s_agent, s_cord) \n    penalty_collision = -25 if (d_agent_cord < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Reward function: Laboratory inspection: avoid the refrigerator and finish on target. States for agent, target, and refrigerator are known.",
    "chosen": "def reward(s_agent, s_refrigerator, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_refrigerator = calculate_distance_min(s_agent, s_refrigerator) \n    penalty_collision = -25 if (d_agent_refrigerator < 0.8) else 30 \n    reward_reaching_target = 30 if (d_agent_target < 0.1) else -40 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_refrigerator): \n    d_agent_refrigerator = calculate_distance_min(s_agent, s_refrigerator) \n    penalty_collision = -25 if (d_agent_refrigerator < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "As the drone approaches its target, it must ensure no contact with the waste bin, relying on agent, target, and bin states.",
    "chosen": "def reward(s_agent, s_waste_bin, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_waste_bin = calculate_distance_min(s_agent, s_waste_bin) \n    penalty_collision = -25 if (d_agent_waste_bin < 0.8) else 30 \n    reward_reaching_target = 30 if (d_agent_target < 0.1) else -40 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_waste_bin): \n    d_agent_waste_bin = calculate_distance_min(s_agent, s_waste_bin) \n    penalty_collision = -25 if (d_agent_waste_bin < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "A UAV surveys the laboratory interior and should remain clear of three obstacles: a chair, a ground mat, and a lab stool; agent, target, and multiple obstacles states are available.",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_multiple_obstacles_min < 0.8) else 30 \n    reward_reaching_target = 30 if (d_agent_target < 0.1) else -40 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_multiple_obstacles_min < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Drone are navigate laboratory, must not hit bench corner or chemical locker frame, all of the multiple itme's states is accessable.",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_multiple_obstacles_min < 0.8) else 30 \n    reward_reaching_target = 30 if (d_agent_target < 0.1) else -40 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_multiple_obstacles_min < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "The dron inspeckts lab and shoud not crash into the tubeing coil, pole seckshun, or wirez clustr; many obstacles are there to use for code",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_multiple_obstacles_min < 0.8) else 30 \n    reward_reaching_target = 30 if (d_agent_target < 0.1) else -40 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_multiple_obstacles_min < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "A drone is navigating through a laboratory while carrying out inspection. It must carefully avoid two obstacles: the sample freezer support pole and the sample container. The states of the agent, the target, and multiple obstacles are available.",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_multiple_obstacles_min < 0.8) else 30 \n    reward_reaching_target = 30 if (d_agent_target < 0.1) else -40 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_multiple_obstacles_min < 0.8) else 30 \n    return penalty_collision"
  },


  
  {
    "prompt": "Python reward function: drone inside jail, avoid prison bench and land on target, states accessible.",
    "chosen": "def reward(s_agent, s_prison_bench, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_prison_bench = calculate_distance_min(s_agent, s_prison_bench) \n    penalty_collision = -28 if (d_agent_prison_bench < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_prison_bench): \n    d_agent_prison_bench = calculate_distance_min(s_agent, s_prison_bench) \n    penalty_collision = -28 if (d_agent_prison_bench < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "A drone is scanning cells inside a jail. The primary objective is to complete its inspection without hitting any cell locker and reach target. You have access to the agent, target, and locker states. Implement a reward function in Python to handle this.",
    "chosen": "def reward(s_agent, s_cell_locker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cell_locker = calculate_distance_min(s_agent, s_cell_locker) \n    penalty_collision = -28 if (d_agent_cell_locker < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_cell_locker): \n    d_agent_cell_locker = calculate_distance_min(s_agent, s_cell_locker) \n    penalty_collision = -28 if (d_agent_cell_locker < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Jail drone task: dont collid with metal tray, but reach the target. make reward func in pyhton.",
    "chosen": "def reward(s_agent, s_metal_tray, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_metal_tray = calculate_distance_min(s_agent, s_metal_tray) \n    penalty_collision = -28 if (d_agent_metal_tray < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_metal_tray): \n    d_agent_metal_tray = calculate_distance_min(s_agent, s_metal_tray) \n    penalty_collision = -28 if (d_agent_metal_tray < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Create a reward function in Python where the drone inspects a jail but must avoid collision with the concrete floor and land perfecly on target. Assume agent, target, and floor states are accessible.",
    "chosen": "def reward(s_agent, s_concrete_floor, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_concrete_floor = calculate_distance_min(s_agent, s_concrete_floor) \n    penalty_collision = -28 if (d_agent_concrete_floor < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_concrete_floor): \n    d_agent_concrete_floor = calculate_distance_min(s_agent, s_concrete_floor) \n    penalty_collision = -28 if (d_agent_concrete_floor < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Reward func in pyhton: drone inspec jail and reach target, avoide crash storage trunk. states availble such as target adn storage trunk.",
    "chosen": "def reward(s_agent, s_storage_trunk, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_storage_trunk = calculate_distance_min(s_agent, s_storage_trunk) \n    penalty_collision = -28 if (d_agent_storage_trunk < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_storage_trunk): \n    d_agent_storage_trunk = calculate_distance_min(s_agent, s_storage_trunk) \n    penalty_collision = -28 if (d_agent_storage_trunk < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Imagine a reinforcement learning environment where a drone is flying through a prison facility for inspection. To guarantee safe operation, the drone must never crash into the prison stool and aim for target. Since the states of the agent, the target, and the stool are available, implement a Python reward function.",
    "chosen": "def reward(s_agent, s_prison_stool, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_prison_stool = calculate_distance_min(s_agent, s_prison_stool) \n    penalty_collision = -28 if (d_agent_prison_stool < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_prison_stool): \n    d_agent_prison_stool = calculate_distance_min(s_agent, s_prison_stool) \n    penalty_collision = -28 if (d_agent_prison_stool < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "drone flying jail inspction. must not collid with handcuffs and aim for target. rite reward func python.",
    "chosen": "def reward(s_agent, s_handcuff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_handcuff = calculate_distance_min(s_agent, s_handcuff) \n    penalty_collision = -28 if (d_agent_handcuff < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_handcuff): \n    d_agent_handcuff = calculate_distance_min(s_agent, s_handcuff) \n    penalty_collision = -28 if (d_agent_handcuff < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Reward function Python: drone in jail, avoid wall cell locker, states known.",
    "chosen": "def reward(s_agent, s_cell_locker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cell_locker = calculate_distance_min(s_agent, s_cell_locker) \n    penalty_collision = -28 if (d_agent_cell_locker < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_cell_locker): \n    d_agent_cell_locker = calculate_distance_min(s_agent, s_cell_locker) \n    penalty_collision = -28 if (d_agent_cell_locker < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Imagine a reinforcement learning task where a drone inspects a correctional facility. The drone’s mission is twofold: (1) reach the target and (2) avoid colliding with the prison log. Since you can access the agent’s, target’s, and log’s states, implement a reward function in Python.",
    "chosen": "def reward(s_agent, s_prison_log, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_prison_log = calculate_distance_min(s_agent, s_prison_log) \n    penalty_collision = -28 if (d_agent_prison_log < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_prison_log): \n    d_agent_prison_log = calculate_distance_min(s_agent, s_prison_log) \n    penalty_collision = -28 if (d_agent_prison_log < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Create a Python reward function in which the drone inspects a prison, avoids the water bottle, and successfully navigates toward the target. All states are available.",
    "chosen": "def reward(s_agent, s_water_bottle, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_water_bottle = calculate_distance_min(s_agent, s_water_bottle) \n    penalty_collision = -28 if (d_agent_water_bottle < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_water_bottle): \n    d_agent_water_bottle = calculate_distance_min(s_agent, s_water_bottle) \n    penalty_collision = -28 if (d_agent_water_bottle < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Jail drone task → avoide crash worker, move 2 tarrget. make reword function pyhton.",
    "chosen": "def reward(s_agent, s_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_worker = calculate_distance_min(s_agent, s_worker) \n    penalty_collision = -28 if (d_agent_worker < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_worker): \n    d_agent_worker = calculate_distance_min(s_agent, s_worker) \n    penalty_collision = -28 if (d_agent_worker < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Reward fn pyhton: drone inspec jail, dont hit bed frame and ami target, shud go for targt. states availble.",
    "chosen": "def reward(s_agent, s_bed_frame, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bed_frame = calculate_distance_min(s_agent, s_bed_frame) \n    penalty_collision = -28 if (d_agent_bed_frame < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_bed_frame): \n    d_agent_bed_frame = calculate_distance_min(s_agent, s_bed_frame) \n    penalty_collision = -28 if (d_agent_bed_frame < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Reward fn Python: drone in jail, avoid TV + reach target, states known.",
    "chosen": "def reward(s_agent, s_TV, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_TV = calculate_distance_min(s_agent, s_TV) \n    penalty_collision = -28 if (d_agent_TV < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_TV): \n    d_agent_TV = calculate_distance_min(s_agent, s_TV) \n    penalty_collision = -28 if (d_agent_TV < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "A drone is inspecting inside a jail. It should move toward its target while avoiding collisions with the loose wires. The agent, target, and wires states are available. Write a reward function in Python.",
    "chosen": "def reward(s_agent, s_wire, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wire = calculate_distance_min(s_agent, s_wire) \n    penalty_collision = -28 if (d_agent_wire < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_wire): \n    d_agent_wire = calculate_distance_min(s_agent, s_wire) \n    penalty_collision = -28 if (d_agent_wire < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "In a jail inspection task, design a Python reward function that encourages the drone to land on the target while avoiding collisions with the cell bars. You can use agent, target, and bars states.",
    "chosen": "def reward(s_agent, s_cell_bars, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cell_bars = calculate_distance_min(s_agent, s_cell_bars) \n    penalty_collision = -28 if (d_agent_cell_bars < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_cell_bars): \n    d_agent_cell_bars = calculate_distance_min(s_agent, s_cell_bars) \n    penalty_collision = -28 if (d_agent_cell_bars < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Reward fn pyhton: drone inspec jail, dont hit jail shelf, shud fly to targt. states availble.",
    "chosen": "def reward(s_agent, s_shelf, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_shelf = calculate_distance_min(s_agent, s_shelf) \n    penalty_collision = -28 if (d_agent_shelf < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_shelf): \n    d_agent_shelf = calculate_distance_min(s_agent, s_shelf) \n    penalty_collision = -28 if (d_agent_shelf < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Create a Python reward function where the drone inspects a jail, flies to the target, and avoids colliding with the tin plate set. Tin plate's, target's and agent's states are accessible.",
    "chosen": "def reward(s_agent, s_tin_plate_set, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tin_plate_set = calculate_distance_min(s_agent, s_tin_plate_set) \n    penalty_collision = -28 if (d_agent_tin_plate_set < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_tin_plate_set): \n    d_agent_tin_plate_set = calculate_distance_min(s_agent, s_tin_plate_set) \n    penalty_collision = -28 if (d_agent_tin_plate_set < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "In a jail inspection scenario, design a Python reward function that encourages the drone to reach the target safely while avoiding the evidence bag. You can use the agent, target, and bag states.",
    "chosen": "def reward(s_agent, s_evidence_bag, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_evidence_bag = calculate_distance_min(s_agent, s_evidence_bag) \n    penalty_collision = -28 if (d_agent_evidence_bag < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_evidence_bag): \n    d_agent_evidence_bag = calculate_distance_min(s_agent, s_evidence_bag) \n    penalty_collision = -28 if (d_agent_evidence_bag < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Reward fn Python: drone in jail, avoid cup holder + fly to target. States (target, agent, cup holder) given.",
    "chosen": "def reward(s_agent, s_cup_holder, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cup_holder = calculate_distance_min(s_agent, s_cup_holder) \n    penalty_collision = -28 if (d_agent_cup_holder < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_cup_holder): \n    d_agent_cup_holder = calculate_distance_min(s_agent, s_cup_holder) \n    penalty_collision = -28 if (d_agent_cup_holder < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Jail drone → must go to target & not collide with worker. Write reward function with state of agent, target, worker states",
    "chosen": "def reward(s_agent, s_jail_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_jail_worker = calculate_distance_min(s_agent, s_jail_worker) \n    penalty_collision = -28 if (d_agent_jail_worker < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_jail_worker): \n    d_agent_jail_worker = calculate_distance_min(s_agent, s_jail_worker) \n    penalty_collision = -28 if (d_agent_jail_worker < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "In a jail inspection scenario, design a Python reward function that encourages the drone to reach the target while ensuring it does not collide with the guard. You have access to agent, target, and guard states.",
    "chosen": "def reward(s_agent, s_guard, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_guard = calculate_distance_min(s_agent, s_guard) \n    penalty_collision = -28 if (d_agent_guard < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_guard): \n    d_agent_guard = calculate_distance_min(s_agent, s_guard) \n    penalty_collision = -28 if (d_agent_guard < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "A drone is navigating through a jail. It must reach its target while avoiding collisions with the chair, concrete floor, and prison stool. The agent’s state, target’s state, and multiple obstacle states are available. Implement a reward function in Python.",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "In a prison inspection scenario, design a Python reward function that encourages the drone to reach its target while avoiding both the bench corner and the cell locker frame. Agent, target, and obstacle states are accessible.",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Write a Python reward function for a drone navigation task in a jail. The drone must move toward the target but avoid the handcuffs, the section of cell bars, and the loose wires. All states are available.",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.8) else 30 \n    return penalty_collision"
  },
  {
    "prompt": "Scenario: A drone is navigating a jail. Target: The drone must avoid colliding with storage trunk cell bars and evidence bag. Assumptions: The Agent's state, target's state and the states of multiple obstacles: storage trunk cell bars, and evidence bag are available. Instruction: Implement a reward function in Python.",
    "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.8) else 30 \n    reward_reaching_target = 32 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
    "rejected": "def reward(s_agent, s_multiple_obstacles): \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -28 if (d_agent_multiple_obstacles_min < 0.8) else 30 \n    return penalty_collision"
  }
]
