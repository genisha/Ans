[
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with chairs. \n Conditions: Agent's state and obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goalif d_agent_obstacle < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with table. \n Conditions: Agent's state and obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*d_agent_goal"
        },

        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with ceiling fan blades. \n Conditions: Agent's state and staic obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goalif d_agent_obstacle < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives:Avoid collision with hanging lights. \n Conditions: Agent's state and obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -20 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with curtain. \n Conditions: Agent's state and static obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal if d_agent_obstacle < c1: \n  return 5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with avoid drapes. \n Conditions: Agent's state and obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5  \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 10*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with obstacles. \n Conditions: Agent's state and obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal if d_agent_obstacle < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with static obstacle (wardrobes). \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_g = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt":"Mission: Drone in-door navigation and reach other side of the small room. \n Objectives: Do not collide with the walls of the room. \n Conditions: Agent'sn state and obstacle's state. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt":"Mission: Guide the drone through an indoor space to the far side of a small room. \n Objectives: Prevent contact with the people. \n Conditions: Agent'sn state and obstacle's state. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected":"The reward of the drone should be claculated based on the information provided, such as the the coordinates of the agent as well of the information of the obstacle. Takeing the length and width and height of the room should not be ignored. "
        },




        {
            "prompt":"Mission: Drone indoor exploration in a library. \n Objectives: Avoid hitting the surrounding walls. \n Conditions: Agent'sn state and obstacle's state. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected":"The function should take the following inputs: \n - agent's state: a list of integers [x, y, orientation]"
        },
        {
            "prompt":"Mission: Drone indoor exploration in a library. \n Objectives: Navigate without crashing into walls. \n Conditions: Agent'sn state and obstacle's state. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (cables) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect obstacle (chairs) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 10*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (table) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 5*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (bookshelves) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (smoke detectors) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (stackes books) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect dynamic obstacle (shelves) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 9*d_agent_goal"
        },




        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 14*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect static obstacle (trees) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (leaves) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect birds and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collision with stumps. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (rocks) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "MMission: Drone moves in a forest. \n Objectives: Detect thin branches and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 9*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collision with thick tree branches. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_maopped + 14*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (people) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect cliffs and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return -c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },





        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt":"Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid any obstacle with hanging lights. \n Conditions: Agent's information and obstacle's information are available \n Write a reward in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected":"def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error"
        },
        {

            "prompt":"Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid any static obstacle such as food serving counters. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    d_agent_to_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    d_agent_to_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    d_agent_to_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error"
        },

        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with chairs. \n Conditions: Agent's state, obstacle's states are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with table. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_g = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*d_agent_goal - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with trash bins. \n Conditions: Agent's state onstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "Write a reward function in Python for a 2D drone in-Door-Navigation-Problem. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with low-hanging decorations. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },

        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with plants. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = 2 \n    c_2 = 19 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    speed_error = norm(v_agent, v_desired) \n    return 2**d_agent_goal - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with metal rails. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "The reward function is:  ```python"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with cameras. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },






        {
            "prompt":"Mission: Drone in-door navigation in a classroom. \n Objectives: Do not collide with the walls of the room. \n Conditions: Agent's state, obstacle's state are aviable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt":"Mission: Drone in-door navigation in a classroom. \n Objectives: Prevent contact with the people. \n Conditions: Agent's state, obstacle's state are avialable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected":"The reward of the drone should be claculated based on the information provided, such as the the coordinates of the agent as well of the information of the obstacle. Takeing the length and width and height of the room should not be ignored. "
        },
        {
            "prompt":"Mission: Operate the drone in a classroom. \n Objectives: Avoid hitting the ceiling Projector. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected":"The function should take the following inputs: \n - agent's state: a list of integers [x, y, orientation]"
        },
        {
            "prompt":"Mission: Pilot a drone through a classroom. \n Objectives: Navigate without crashing into overhead lights. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect static obstacle (whiteboard) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "Write a reward function in Python for a 2D drone in-Door-Navigation-Problem. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect obstacle (tall bookshelves) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return 10*d_agent_goal - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect obstacle (backpacks) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect dynamic obstacle (student) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "The reward function is:  ```python"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect dynamic obstacle (teachers) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        },





        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "That a simple function."    },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        },

        {
            "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: Maximize unexplored area coverage and avoid collision with chairs and obstacles. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_3*new_area_mapped + d_agent_goal**2 - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "The reward function is:  ```python"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state and obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: Detect dynamic obstacle (cables) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N" 
        },







        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (workers) and avoid collision. \n Conditions: Agent's state, obstacle's state are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*new_area_mapped + c_2*math.log(d_agent_goal) + speed_error"
        },
        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "That a simple function."    
        },
            {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Avoid collison with high storage racks. \n Conditions: Agent's state and obstacle's information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_tree = calculate_distance(s_agent, s_tree) \n    if d_agent_tree < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Avoid collison with forklifts. \n Conditions: Agent's state and obstacle's information are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "def reward(s_agent, s_branch): \n    c_1 = -2 \n    c_2 = -5 \n    d_agent_branch = calculate_distance(s_agent, s_branch) \n    if d_agent_branch < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },

        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with hanging cables. \n Conditions: Agent's state and obstacle's information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_tree = calculate_distance(s_agent, s_tree) \n    if d_agent_tree < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission:Operate the drone in a warehouse. \n Objectives: Avoid collison with stacks of pallets. \n Conditions: Agent's state and obstacle's information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "def reward(s_agent, s_branch): \n    c_1 = -2 \n    c_2 = -20 \n    d_agent_branch = calculate_distance(s_agent, s_branch) \n    if d_agent_branch < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with stacks of boxes. \n Conditions: Agent's state and obstacle's information are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_tree = calculate_distance(s_agent, s_tree) \n    if d_agent_tree < c1: \n  return 5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with overhead pipes. \n Conditions: Agent's state and obstacle's information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    d_agent_branch = calculate_distance(s_agent, s_branch) \n    if d_agent_branch < c1: \n  return 5.0 \n    return 10*math.exp(d_agent_goal)"
        },


        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Detect people and avoid collison with overhead beams. \n Conditions: Agent's states and moving obstacle information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n     d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },






        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with people. \n Conditions: Agent's states and moving obstacle information are obtainable.. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = -2 \n    c_2 = -5 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },

        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with spotlights. \n Conditions: Agent's states and moving obstacle information are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with balconies. \n Conditions: Agent's states and moving obstacle information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = -2 \n    c_2 = -20 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Detect people and avoid collison with audience seating. \n Conditions: Agent's states and moving obstacle information are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with projectors. \n Conditions: Agent's states and moving obstacle information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = -2 \n    c_2 = -5  \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return 10*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Avoid collision with cabels. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone explores the theater. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state and obstacle's state are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Detect static obstacle (overhead platforms) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 9*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone explores the theater. \n Objectives: Detect static obstacle (drapes) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_maopped + 14*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone explores the theater. \n Objectives: Detect dynamic obstacle (hangings microphones) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 2**d_agent_goal"
        },











        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state and obstacle's state are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return -c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect  dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect static obstacle (basket hoops) and avoid collision. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect static obstacle (scoreboards) and avoid collision. \n Conditions: Agent's state, obstacle's state are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (climbing  ropes) and avoid collision. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with volleyball nets. \n Conditions: Agent's state, obstacle's state are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with hanging flags. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "That a simple function."    
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (moving players) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        },

        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with loose balls. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_3*new_area_mapped + d_agent_goal**2 - speed_error"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (flying balls) and avoid collision. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "The reward function is:  ```python"
        },





        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect dynamic obstacle (patients) and avoid collision. \n Conditions: Agent's state and obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect dynamic obstacle (doctors) and avoid collision. \n Conditions: Agent's state, d obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N" 
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect static obstacle (wires) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone explores the hospital. \n Objectives: Avoid collision with medical equipment stands. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*new_area_mapped + c_2*math.log(d_agent_goal) + speed_error"
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect wheelchairs and avoid collisions. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "That a simple function."    
        },
        {
            "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect static obstacle (staff) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Avoid collision with defibrillators. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone explores the hospital. \n Objectives: Avoid  any collision with wall-mounted emergency devices. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives:  Avoid collision with lights. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a hospital. \n Objectives: Avoid to collide with static obstacle such as beds. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.log(d_agent_goal) \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        },




        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with chairs. \n Conditions: Agent's state and obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goalif d_agent_obstacle < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with table. \n Conditions: Agent's state and obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*d_agent_goal"
        },

        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with ceiling fan blades. \n Conditions: Agent's state and staic obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goalif d_agent_obstacle < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives:Avoid collision with hanging lights. \n Conditions: Agent's state and obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -20 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with curtain. \n Conditions: Agent's state and static obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal if d_agent_obstacle < c1: \n  return 5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with avoid drapes. \n Conditions: Agent's state and obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5  \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 10*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with obstacles. \n Conditions: Agent's state and obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal if d_agent_obstacle < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with static obstacle (wardrobes). \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_g = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt":"Mission: Drone in-door navigation and reach other side of the small room. \n Objectives: Do not collide with the walls of the room. \n Conditions: Agent'sn state and obstacle's state. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt":"Mission: Guide the drone through an indoor space to the far side of a small room. \n Objectives: Prevent contact with the people. \n Conditions: Agent'sn state and obstacle's state. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected":"The reward of the drone should be claculated based on the information provided, such as the the coordinates of the agent as well of the information of the obstacle. Takeing the length and width and height of the room should not be ignored. "
        },




        {
            "prompt":"Mission: Drone indoor exploration in a library. \n Objectives: Avoid hitting the surrounding walls. \n Conditions: Agent'sn state and obstacle's state. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected":"The function should take the following inputs: \n - agent's state: a list of integers [x, y, orientation]"
        },
        {
            "prompt":"Mission: Drone indoor exploration in a library. \n Objectives: Navigate without crashing into walls. \n Conditions: Agent'sn state and obstacle's state. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (cables) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect obstacle (chairs) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 10*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (table) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 5*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (bookshelves) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (smoke detectors) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (stackes books) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect dynamic obstacle (shelves) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 9*d_agent_goal"
        },




        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 14*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect static obstacle (trees) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (leaves) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect birds and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collision with stumps. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (rocks) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "MMission: Drone moves in a forest. \n Objectives: Detect thin branches and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 9*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collision with thick tree branches. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_maopped + 14*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (people) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect cliffs and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return -c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },





        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt":"Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid any obstacle with hanging lights. \n Conditions: Agent's information and obstacle's information are available \n Write a reward in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected":"def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error"
        },
        {

            "prompt":"Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid any static obstacle such as food serving counters. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    d_agent_to_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    d_agent_to_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    d_agent_to_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error"
        },

        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with chairs. \n Conditions: Agent's state, obstacle's states are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with table. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_g = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*d_agent_goal - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with trash bins. \n Conditions: Agent's state onstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "Write a reward function in Python for a 2D drone in-Door-Navigation-Problem. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with low-hanging decorations. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },

        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with plants. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = 2 \n    c_2 = 19 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    speed_error = norm(v_agent, v_desired) \n    return 2**d_agent_goal - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with metal rails. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "The reward function is:  ```python"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with cameras. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },






        {
            "prompt":"Mission: Drone in-door navigation in a classroom. \n Objectives: Do not collide with the walls of the room. \n Conditions: Agent's state, obstacle's state are aviable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt":"Mission: Drone in-door navigation in a classroom. \n Objectives: Prevent contact with the people. \n Conditions: Agent's state, obstacle's state are avialable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected":"The reward of the drone should be claculated based on the information provided, such as the the coordinates of the agent as well of the information of the obstacle. Takeing the length and width and height of the room should not be ignored. "
        },
        {
            "prompt":"Mission: Operate the drone in a classroom. \n Objectives: Avoid hitting the ceiling Projector. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected":"The function should take the following inputs: \n - agent's state: a list of integers [x, y, orientation]"
        },
        {
            "prompt":"Mission: Pilot a drone through a classroom. \n Objectives: Navigate without crashing into overhead lights. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect static obstacle (whiteboard) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "Write a reward function in Python for a 2D drone in-Door-Navigation-Problem. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect obstacle (tall bookshelves) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return 10*d_agent_goal - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect obstacle (backpacks) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect dynamic obstacle (student) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "The reward function is:  ```python"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect dynamic obstacle (teachers) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        },





        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "That a simple function."    },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        },

        {
            "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: Maximize unexplored area coverage and avoid collision with chairs and obstacles. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_3*new_area_mapped + d_agent_goal**2 - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "The reward function is:  ```python"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state and obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: Detect dynamic obstacle (cables) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N" 
        },







        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (workers) and avoid collision. \n Conditions: Agent's state, obstacle's state are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*new_area_mapped + c_2*math.exp(d_agent_goal) + speed_error"
        },
        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "That a simple function."    
        },
            {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Avoid collison with high storage racks. \n Conditions: Agent's state and obstacle's information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_tree = calculate_distance(s_agent, s_tree) \n    if d_agent_tree < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Avoid collison with forklifts. \n Conditions: Agent's state and obstacle's information are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "def reward(s_agent, s_branch): \n    c_1 = -2 \n    c_2 = -5 \n    d_agent_branch = calculate_distance(s_agent, s_branch) \n    if d_agent_branch < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },

        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with hanging cables. \n Conditions: Agent's state and obstacle's information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_tree = calculate_distance(s_agent, s_tree) \n    if d_agent_tree < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission:Operate the drone in a warehouse. \n Objectives: Avoid collison with stacks of pallets. \n Conditions: Agent's state and obstacle's information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "def reward(s_agent, s_branch): \n    c_1 = -2 \n    c_2 = -20 \n    d_agent_branch = calculate_distance(s_agent, s_branch) \n    if d_agent_branch < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with stacks of boxes. \n Conditions: Agent's state and obstacle's information are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_tree = calculate_distance(s_agent, s_tree) \n    if d_agent_tree < c1: \n  return 5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with overhead pipes. \n Conditions: Agent's state and obstacle's information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    d_agent_branch = calculate_distance(s_agent, s_branch) \n    if d_agent_branch < c1: \n  return 5.0 \n    return 10*math.exp(d_agent_goal)"
        },


        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Detect people and avoid collison with overhead beams. \n Conditions: Agent's states and moving obstacle information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n     d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },






        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with people. \n Conditions: Agent's states and moving obstacle information are obtainable.. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = -2 \n    c_2 = -5 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },

        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with spotlights. \n Conditions: Agent's states and moving obstacle information are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with balconies. \n Conditions: Agent's states and moving obstacle information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = -2 \n    c_2 = -20 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Detect people and avoid collison with audience seating. \n Conditions: Agent's states and moving obstacle information are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with projectors. \n Conditions: Agent's states and moving obstacle information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = -2 \n    c_2 = -5  \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return 10*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Avoid collision with cabels. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone explores the theater. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state and obstacle's state are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Detect static obstacle (overhead platforms) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 9*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone explores the theater. \n Objectives: Detect static obstacle (drapes) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_maopped + 14*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone explores the theater. \n Objectives: Detect dynamic obstacle (hangings microphones) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 2**d_agent_goal"
        },











        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state and obstacle's state are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return -c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect  dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect static obstacle (basket hoops) and avoid collision. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect static obstacle (scoreboards) and avoid collision. \n Conditions: Agent's state, obstacle's state are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (climbing  ropes) and avoid collision. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with volleyball nets. \n Conditions: Agent's state, obstacle's state are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with hanging flags. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "That a simple function."    
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (moving players) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        },

        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with loose balls. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_3*new_area_mapped + d_agent_goal**2 - speed_error"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (flying balls) and avoid collision. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "The reward function is:  ```python"
        },





        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect dynamic obstacle (patients) and avoid collision. \n Conditions: Agent's state and obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect dynamic obstacle (doctors) and avoid collision. \n Conditions: Agent's state, d obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N" 
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect static obstacle (wires) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone explores the hospital. \n Objectives: Avoid collision with medical equipment stands. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*new_area_mapped + c_2*math.exp(d_agent_goal) + speed_error"
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect wheelchairs and avoid collisions. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "That a simple function."    
        },
        {
            "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect static obstacle (staff) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Avoid collision with defibrillators. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone explores the hospital. \n Objectives: Avoid  any collision with wall-mounted emergency devices. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives:  Avoid collision with lights. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a hospital. \n Objectives: Avoid to collide with static obstacle such as beds. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -c_2 * math.exp(d_agent_goal) \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        },


        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with chairs. \n Conditions: Agent's state and obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goalif d_agent_obstacle < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with table. \n Conditions: Agent's state and obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*d_agent_goal"
        },

        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with ceiling fan blades. \n Conditions: Agent's state and staic obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goalif d_agent_obstacle < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives:Avoid collision with hanging lights. \n Conditions: Agent's state and obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -20 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with curtain. \n Conditions: Agent's state and static obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal if d_agent_obstacle < c1: \n  return 5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with avoid drapes. \n Conditions: Agent's state and obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5  \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 10*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with obstacles. \n Conditions: Agent's state and obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal if d_agent_obstacle < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with static obstacle (wardrobes). \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_g = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt":"Mission: Drone in-door navigation and reach other side of the small room. \n Objectives: Do not collide with the walls of the room. \n Conditions: Agent'sn state and obstacle's state. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt":"Mission: Guide the drone through an indoor space to the far side of a small room. \n Objectives: Prevent contact with the people. \n Conditions: Agent'sn state and obstacle's state. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected":"The reward of the drone should be claculated based on the information provided, such as the the coordinates of the agent as well of the information of the obstacle. Takeing the length and width and height of the room should not be ignored. "
        },




        {
            "prompt":"Mission: Drone indoor exploration in a library. \n Objectives: Avoid hitting the surrounding walls. \n Conditions: Agent'sn state and obstacle's state. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected":"The function should take the following inputs: \n - agent's state: a list of integers [x, y, orientation]"
        },
        {
            "prompt":"Mission: Drone indoor exploration in a library. \n Objectives: Navigate without crashing into walls. \n Conditions: Agent'sn state and obstacle's state. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (cables) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect obstacle (chairs) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 10*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (table) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 5*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (bookshelves) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (smoke detectors) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (stackes books) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect dynamic obstacle (shelves) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 9*d_agent_goal"
        },




        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 14*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect static obstacle (trees) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (leaves) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect birds and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collision with stumps. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (rocks) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "MMission: Drone moves in a forest. \n Objectives: Detect thin branches and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 9*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collision with thick tree branches. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_maopped + 14*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (people) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect cliffs and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return -c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },





        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt":"Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid any obstacle with hanging lights. \n Conditions: Agent's information and obstacle's information are available \n Write a reward in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected":"def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error"
        },
        {

            "prompt":"Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid any static obstacle such as food serving counters. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    d_agent_to_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    d_agent_to_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    d_agent_to_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error"
        },

        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with chairs. \n Conditions: Agent's state, obstacle's states are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with table. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_g = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*d_agent_goal - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with trash bins. \n Conditions: Agent's state onstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "Write a reward function in Python for a 2D drone in-Door-Navigation-Problem. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with low-hanging decorations. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },

        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with plants. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = 2 \n    c_2 = 19 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    speed_error = norm(v_agent, v_desired) \n    return 2**d_agent_goal - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with metal rails. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "The reward function is:  ```python"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with cameras. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },






        {
            "prompt":"Mission: Drone in-door navigation in a classroom. \n Objectives: Do not collide with the walls of the room. \n Conditions: Agent's state, obstacle's state are aviable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt":"Mission: Drone in-door navigation in a classroom. \n Objectives: Prevent contact with the people. \n Conditions: Agent's state, obstacle's state are avialable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected":"The reward of the drone should be claculated based on the information provided, such as the the coordinates of the agent as well of the information of the obstacle. Takeing the length and width and height of the room should not be ignored. "
        },
        {
            "prompt":"Mission: Operate the drone in a classroom. \n Objectives: Avoid hitting the ceiling Projector. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected":"The function should take the following inputs: \n - agent's state: a list of integers [x, y, orientation]"
        },
        {
            "prompt":"Mission: Pilot a drone through a classroom. \n Objectives: Navigate without crashing into overhead lights. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect static obstacle (whiteboard) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "Write a reward function in Python for a 2D drone in-Door-Navigation-Problem. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect obstacle (tall bookshelves) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return 10*d_agent_goal - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect obstacle (backpacks) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect dynamic obstacle (student) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "The reward function is:  ```python"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect dynamic obstacle (teachers) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        },





        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "That a simple function."    },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        },

        {
            "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: Maximize unexplored area coverage and avoid collision with chairs and obstacles. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_3*new_area_mapped + d_agent_goal**2 - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "The reward function is:  ```python"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state and obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: Detect dynamic obstacle (cables) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N" 
        },







        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (workers) and avoid collision. \n Conditions: Agent's state, obstacle's state are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*new_area_mapped + c_2*math.exp(d_agent_goal) + speed_error"
        },
        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "That a simple function."    
        },
            {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Avoid collison with high storage racks. \n Conditions: Agent's state and obstacle's information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_tree = calculate_distance(s_agent, s_tree) \n    if d_agent_tree < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Avoid collison with forklifts. \n Conditions: Agent's state and obstacle's information are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "def reward(s_agent, s_branch): \n    c_1 = -2 \n    c_2 = -5 \n    d_agent_branch = calculate_distance(s_agent, s_branch) \n    if d_agent_branch < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },

        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with hanging cables. \n Conditions: Agent's state and obstacle's information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_tree = calculate_distance(s_agent, s_tree) \n    if d_agent_tree < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission:Operate the drone in a warehouse. \n Objectives: Avoid collison with stacks of pallets. \n Conditions: Agent's state and obstacle's information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "def reward(s_agent, s_branch): \n    c_1 = -2 \n    c_2 = -20 \n    d_agent_branch = calculate_distance(s_agent, s_branch) \n    if d_agent_branch < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with stacks of boxes. \n Conditions: Agent's state and obstacle's information are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_tree = calculate_distance(s_agent, s_tree) \n    if d_agent_tree < c1: \n  return 5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with overhead pipes. \n Conditions: Agent's state and obstacle's information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    d_agent_branch = calculate_distance(s_agent, s_branch) \n    if d_agent_branch < c1: \n  return 5.0 \n    return 10*math.exp(d_agent_goal)"
        },


        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Detect people and avoid collison with overhead beams. \n Conditions: Agent's states and moving obstacle information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n     d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },






        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with people. \n Conditions: Agent's states and moving obstacle information are obtainable.. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = -2 \n    c_2 = -5 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },

        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with spotlights. \n Conditions: Agent's states and moving obstacle information are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with balconies. \n Conditions: Agent's states and moving obstacle information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = -2 \n    c_2 = -20 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Detect people and avoid collison with audience seating. \n Conditions: Agent's states and moving obstacle information are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with projectors. \n Conditions: Agent's states and moving obstacle information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = -2 \n    c_2 = -5  \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return 10*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Avoid collision with cabels. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone explores the theater. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state and obstacle's state are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Detect static obstacle (overhead platforms) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 9*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone explores the theater. \n Objectives: Detect static obstacle (drapes) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_maopped + 14*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone explores the theater. \n Objectives: Detect dynamic obstacle (hangings microphones) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 2**d_agent_goal"
        },











        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state and obstacle's state are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return -c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect  dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect static obstacle (basket hoops) and avoid collision. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect static obstacle (scoreboards) and avoid collision. \n Conditions: Agent's state, obstacle's state are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (climbing  ropes) and avoid collision. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with volleyball nets. \n Conditions: Agent's state, obstacle's state are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with hanging flags. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "That a simple function."    
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (moving players) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        },

        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with loose balls. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_3*new_area_mapped + d_agent_goal**2 - speed_error"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (flying balls) and avoid collision. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "The reward function is:  ```python"
        },





        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect dynamic obstacle (patients) and avoid collision. \n Conditions: Agent's state and obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect dynamic obstacle (doctors) and avoid collision. \n Conditions: Agent's state, d obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N" 
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect static obstacle (wires) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone explores the hospital. \n Objectives: Avoid collision with medical equipment stands. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*new_area_mapped + c_2*math.exp(d_agent_goal) + speed_error"
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect wheelchairs and avoid collisions. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "That a simple function."    
        },
        {
            "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect static obstacle (staff) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Avoid collision with defibrillators. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone explores the hospital. \n Objectives: Avoid  any collision with wall-mounted emergency devices. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives:  Avoid collision with lights. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a hospital. \n Objectives: Avoid to collide with static obstacle such as beds. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        },


        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with chairs. \n Conditions: Agent's state and obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goalif d_agent_obstacle < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with table. \n Conditions: Agent's state and obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*d_agent_goal"
        },

        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with ceiling fan blades. \n Conditions: Agent's state and staic obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goalif d_agent_obstacle < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives:Avoid collision with hanging lights. \n Conditions: Agent's state and obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -20 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with curtain. \n Conditions: Agent's state and static obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal if d_agent_obstacle < c1: \n  return 5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with avoid drapes. \n Conditions: Agent's state and obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5  \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 10*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with obstacles. \n Conditions: Agent's state and obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal if d_agent_obstacle < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with static obstacle (wardrobes). \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_g = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt":"Mission: Drone in-door navigation and reach other side of the small room. \n Objectives: Do not collide with the walls of the room. \n Conditions: Agent'sn state and obstacle's state. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt":"Mission: Guide the drone through an indoor space to the far side of a small room. \n Objectives: Prevent contact with the people. \n Conditions: Agent'sn state and obstacle's state. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected":"The reward of the drone should be claculated based on the information provided, such as the the coordinates of the agent as well of the information of the obstacle. Takeing the length and width and height of the room should not be ignored. "
        },




        {
            "prompt":"Mission: Drone indoor exploration in a library. \n Objectives: Avoid hitting the surrounding walls. \n Conditions: Agent'sn state and obstacle's state. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected":"The function should take the following inputs: \n - agent's state: a list of integers [x, y, orientation]"
        },
        {
            "prompt":"Mission: Drone indoor exploration in a library. \n Objectives: Navigate without crashing into walls. \n Conditions: Agent'sn state and obstacle's state. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (cables) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect obstacle (chairs) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 10*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (table) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 5*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (bookshelves) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (smoke detectors) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (stackes books) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect dynamic obstacle (shelves) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 9*d_agent_goal"
        },




        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 14*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect static obstacle (trees) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (leaves) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect birds and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collision with stumps. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (rocks) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "MMission: Drone moves in a forest. \n Objectives: Detect thin branches and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 9*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collision with thick tree branches. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_maopped + 14*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (people) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect cliffs and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return -c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },





        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt":"Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid any obstacle with hanging lights. \n Conditions: Agent's information and obstacle's information are available \n Write a reward in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected":"def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error"
        },
        {

            "prompt":"Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid any static obstacle such as food serving counters. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    d_agent_to_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    d_agent_to_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    d_agent_to_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error"
        },

        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with chairs. \n Conditions: Agent's state, obstacle's states are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with table. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_g = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*d_agent_goal - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with trash bins. \n Conditions: Agent's state onstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "Write a reward function in Python for a 2D drone in-Door-Navigation-Problem. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with low-hanging decorations. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },

        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with plants. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = 2 \n    c_2 = 19 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    speed_error = norm(v_agent, v_desired) \n    return 2**d_agent_goal - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with metal rails. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "The reward function is:  ```python"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with cameras. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },






        {
            "prompt":"Mission: Drone in-door navigation in a classroom. \n Objectives: Do not collide with the walls of the room. \n Conditions: Agent's state, obstacle's state are aviable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt":"Mission: Drone in-door navigation in a classroom. \n Objectives: Prevent contact with the people. \n Conditions: Agent's state, obstacle's state are avialable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected":"The reward of the drone should be claculated based on the information provided, such as the the coordinates of the agent as well of the information of the obstacle. Takeing the length and width and height of the room should not be ignored. "
        },
        {
            "prompt":"Mission: Operate the drone in a classroom. \n Objectives: Avoid hitting the ceiling Projector. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected":"The function should take the following inputs: \n - agent's state: a list of integers [x, y, orientation]"
        },
        {
            "prompt":"Mission: Pilot a drone through a classroom. \n Objectives: Navigate without crashing into overhead lights. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect static obstacle (whiteboard) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "Write a reward function in Python for a 2D drone in-Door-Navigation-Problem. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect obstacle (tall bookshelves) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return 10*d_agent_goal - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect obstacle (backpacks) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect dynamic obstacle (student) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "The reward function is:  ```python"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect dynamic obstacle (teachers) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        },





        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "That a simple function."    },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        },

        {
            "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: Maximize unexplored area coverage and avoid collision with chairs and obstacles. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_3*new_area_mapped + d_agent_goal**2 - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "The reward function is:  ```python"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state and obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: Detect dynamic obstacle (cables) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N" 
        },







        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (workers) and avoid collision. \n Conditions: Agent's state, obstacle's state are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*new_area_mapped + c_2*math.exp(d_agent_goal) + speed_error"
        },
        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "That a simple function."    
        },
            {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Avoid collison with high storage racks. \n Conditions: Agent's state and obstacle's information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_tree = calculate_distance(s_agent, s_tree) \n    if d_agent_tree < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Avoid collison with forklifts. \n Conditions: Agent's state and obstacle's information are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "def reward(s_agent, s_branch): \n    c_1 = -2 \n    c_2 = -5 \n    d_agent_branch = calculate_distance(s_agent, s_branch) \n    if d_agent_branch < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },

        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with hanging cables. \n Conditions: Agent's state and obstacle's information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_tree = calculate_distance(s_agent, s_tree) \n    if d_agent_tree < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission:Operate the drone in a warehouse. \n Objectives: Avoid collison with stacks of pallets. \n Conditions: Agent's state and obstacle's information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "def reward(s_agent, s_branch): \n    c_1 = -2 \n    c_2 = -20 \n    d_agent_branch = calculate_distance(s_agent, s_branch) \n    if d_agent_branch < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with stacks of boxes. \n Conditions: Agent's state and obstacle's information are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_tree = calculate_distance(s_agent, s_tree) \n    if d_agent_tree < c1: \n  return 5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with overhead pipes. \n Conditions: Agent's state and obstacle's information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    d_agent_branch = calculate_distance(s_agent, s_branch) \n    if d_agent_branch < c1: \n  return 5.0 \n    return 10*math.exp(d_agent_goal)"
        },


        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Detect people and avoid collison with overhead beams. \n Conditions: Agent's states and moving obstacle information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n     d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },






        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with people. \n Conditions: Agent's states and moving obstacle information are obtainable.. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = -2 \n    c_2 = -5 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },

        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with spotlights. \n Conditions: Agent's states and moving obstacle information are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with balconies. \n Conditions: Agent's states and moving obstacle information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = -2 \n    c_2 = -20 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Detect people and avoid collison with audience seating. \n Conditions: Agent's states and moving obstacle information are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with projectors. \n Conditions: Agent's states and moving obstacle information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = -2 \n    c_2 = -5  \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return 10*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Avoid collision with cabels. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone explores the theater. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state and obstacle's state are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Detect static obstacle (overhead platforms) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 9*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone explores the theater. \n Objectives: Detect static obstacle (drapes) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_maopped + 14*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone explores the theater. \n Objectives: Detect dynamic obstacle (hangings microphones) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 2**d_agent_goal"
        },









        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state and obstacle's state are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return -c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect  dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect static obstacle (basket hoops) and avoid collision. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect static obstacle (scoreboards) and avoid collision. \n Conditions: Agent's state, obstacle's state are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (climbing  ropes) and avoid collision. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with volleyball nets. \n Conditions: Agent's state, obstacle's state are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with hanging flags. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "That a simple function."    
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (moving players) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        },

        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with loose balls. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_3*new_area_mapped + d_agent_goal**2 - speed_error"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (flying balls) and avoid collision. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "The reward function is:  ```python"
        },





        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect dynamic obstacle (patients) and avoid collision. \n Conditions: Agent's state and obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect dynamic obstacle (doctors) and avoid collision. \n Conditions: Agent's state, d obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N" 
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect static obstacle (wires) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone explores the hospital. \n Objectives: Avoid collision with medical equipment stands. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*new_area_mapped + c_2*math.exp(d_agent_goal) + speed_error"
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect wheelchairs and avoid collisions. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "That a simple function."    
        },
        {
            "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect static obstacle (staff) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Avoid collision with defibrillators. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone explores the hospital. \n Objectives: Avoid  any collision with wall-mounted emergency devices. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives:  Avoid collision with lights. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a hospital. \n Objectives: Avoid to collide with static obstacle such as beds. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -d_agent_goal**2 \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        },

        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with chairs. \n Conditions: Agent's state and obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goalif d_agent_obstacle < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with table. \n Conditions: Agent's state and obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*d_agent_goal"
        },

        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with ceiling fan blades. \n Conditions: Agent's state and staic obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goalif d_agent_obstacle < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives:Avoid collision with hanging lights. \n Conditions: Agent's state and obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -20 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with curtain. \n Conditions: Agent's state and static obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal if d_agent_obstacle < c1: \n  return 5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with avoid drapes. \n Conditions: Agent's state and obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5  \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 10*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with obstacles. \n Conditions: Agent's state and obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal if d_agent_obstacle < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with static obstacle (wardrobes). \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_g = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt":"Mission: Drone in-door navigation and reach other side of the small room. \n Objectives: Do not collide with the walls of the room. \n Conditions: Agent'sn state and obstacle's state. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt":"Mission: Guide the drone through an indoor space to the far side of a small room. \n Objectives: Prevent contact with the people. \n Conditions: Agent'sn state and obstacle's state. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected":"The reward of the drone should be claculated based on the information provided, such as the the coordinates of the agent as well of the information of the obstacle. Takeing the length and width and height of the room should not be ignored. "
        },




        {
            "prompt":"Mission: Drone indoor exploration in a library. \n Objectives: Avoid hitting the surrounding walls. \n Conditions: Agent'sn state and obstacle's state. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected":"The function should take the following inputs: \n - agent's state: a list of integers [x, y, orientation]"
        },
        {
            "prompt":"Mission: Drone indoor exploration in a library. \n Objectives: Navigate without crashing into walls. \n Conditions: Agent'sn state and obstacle's state. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (cables) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect obstacle (chairs) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 10*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (table) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 5*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (bookshelves) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (smoke detectors) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (stackes books) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect dynamic obstacle (shelves) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 9*d_agent_goal"
        },




        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 14*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect static obstacle (trees) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (leaves) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect birds and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collision with stumps. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (rocks) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "MMission: Drone moves in a forest. \n Objectives: Detect thin branches and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 9*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collision with thick tree branches. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_maopped + 14*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (people) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect cliffs and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return -c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },





        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt":"Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid any obstacle with hanging lights. \n Conditions: Agent's information and obstacle's information are available \n Write a reward in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected":"def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error"
        },
        {

            "prompt":"Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid any static obstacle such as food serving counters. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    d_agent_to_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    d_agent_to_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    d_agent_to_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error"
        },

        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with chairs. \n Conditions: Agent's state, obstacle's states are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with table. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_g = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*d_agent_goal - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with trash bins. \n Conditions: Agent's state onstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "Write a reward function in Python for a 2D drone in-Door-Navigation-Problem. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with low-hanging decorations. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },

        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with plants. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = 2 \n    c_2 = 19 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    speed_error = norm(v_agent, v_desired) \n    return 2**d_agent_goal - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with metal rails. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "The reward function is:  ```python"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with cameras. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },






        {
            "prompt":"Mission: Drone in-door navigation in a classroom. \n Objectives: Do not collide with the walls of the room. \n Conditions: Agent's state, obstacle's state are aviable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt":"Mission: Drone in-door navigation in a classroom. \n Objectives: Prevent contact with the people. \n Conditions: Agent's state, obstacle's state are avialable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected":"The reward of the drone should be claculated based on the information provided, such as the the coordinates of the agent as well of the information of the obstacle. Takeing the length and width and height of the room should not be ignored. "
        },
        {
            "prompt":"Mission: Operate the drone in a classroom. \n Objectives: Avoid hitting the ceiling Projector. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected":"The function should take the following inputs: \n - agent's state: a list of integers [x, y, orientation]"
        },
        {
            "prompt":"Mission: Pilot a drone through a classroom. \n Objectives: Navigate without crashing into overhead lights. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect static obstacle (whiteboard) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "Write a reward function in Python for a 2D drone in-Door-Navigation-Problem. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect obstacle (tall bookshelves) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return 10*d_agent_goal - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect obstacle (backpacks) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect dynamic obstacle (student) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "The reward function is:  ```python"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect dynamic obstacle (teachers) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        },





        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "That a simple function."    },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        },

        {
            "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: Maximize unexplored area coverage and avoid collision with chairs and obstacles. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_3*new_area_mapped + d_agent_goal**2 - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "The reward function is:  ```python"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state and obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: Detect dynamic obstacle (cables) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N" 
        },







        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (workers) and avoid collision. \n Conditions: Agent's state, obstacle's state are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*new_area_mapped + c_2*math.exp(d_agent_goal) + speed_error"
        },
        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "That a simple function."    
        },
            {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Avoid collison with high storage racks. \n Conditions: Agent's state and obstacle's information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_tree = calculate_distance(s_agent, s_tree) \n    if d_agent_tree < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Avoid collison with forklifts. \n Conditions: Agent's state and obstacle's information are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "def reward(s_agent, s_branch): \n    c_1 = -2 \n    c_2 = -5 \n    d_agent_branch = calculate_distance(s_agent, s_branch) \n    if d_agent_branch < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },

        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with hanging cables. \n Conditions: Agent's state and obstacle's information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_tree = calculate_distance(s_agent, s_tree) \n    if d_agent_tree < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission:Operate the drone in a warehouse. \n Objectives: Avoid collison with stacks of pallets. \n Conditions: Agent's state and obstacle's information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "def reward(s_agent, s_branch): \n    c_1 = -2 \n    c_2 = -20 \n    d_agent_branch = calculate_distance(s_agent, s_branch) \n    if d_agent_branch < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with stacks of boxes. \n Conditions: Agent's state and obstacle's information are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_tree = calculate_distance(s_agent, s_tree) \n    if d_agent_tree < c1: \n  return 5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with overhead pipes. \n Conditions: Agent's state and obstacle's information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    d_agent_branch = calculate_distance(s_agent, s_branch) \n    if d_agent_branch < c1: \n  return 5.0 \n    return 10*math.exp(d_agent_goal)"
        },


        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Detect people and avoid collison with overhead beams. \n Conditions: Agent's states and moving obstacle information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n     d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },






        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with people. \n Conditions: Agent's states and moving obstacle information are obtainable.. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = -2 \n    c_2 = -5 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },

        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with spotlights. \n Conditions: Agent's states and moving obstacle information are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with balconies. \n Conditions: Agent's states and moving obstacle information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = -2 \n    c_2 = -20 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Detect people and avoid collison with audience seating. \n Conditions: Agent's states and moving obstacle information are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with projectors. \n Conditions: Agent's states and moving obstacle information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = -2 \n    c_2 = -5  \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return 10*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Avoid collision with cabels. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone explores the theater. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state and obstacle's state are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Detect static obstacle (overhead platforms) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 9*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone explores the theater. \n Objectives: Detect static obstacle (drapes) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_maopped + 14*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone explores the theater. \n Objectives: Detect dynamic obstacle (hangings microphones) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 2**d_agent_goal"
        },









        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state and obstacle's state are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return -c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect  dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect static obstacle (basket hoops) and avoid collision. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect static obstacle (scoreboards) and avoid collision. \n Conditions: Agent's state, obstacle's state are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (climbing  ropes) and avoid collision. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with volleyball nets. \n Conditions: Agent's state, obstacle's state are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with hanging flags. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "That a simple function."    
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (moving players) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        },

        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with loose balls. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_3*new_area_mapped + d_agent_goal**2 - speed_error"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (flying balls) and avoid collision. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "The reward function is:  ```python"
        },





        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect dynamic obstacle (patients) and avoid collision. \n Conditions: Agent's state and obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect dynamic obstacle (doctors) and avoid collision. \n Conditions: Agent's state, d obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N" 
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect static obstacle (wires) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone explores the hospital. \n Objectives: Avoid collision with medical equipment stands. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*new_area_mapped + c_2*math.exp(d_agent_goal) + speed_error"
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect wheelchairs and avoid collisions. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "That a simple function."    
        },
        {
            "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect static obstacle (staff) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Avoid collision with defibrillators. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone explores the hospital. \n Objectives: Avoid  any collision with wall-mounted emergency devices. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives:  Avoid collision with lights. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a hospital. \n Objectives: Avoid to collide with static obstacle such as beds. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -2**d_agent_goal \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        },

        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with chairs. \n Conditions: Agent's state and obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goalif d_agent_obstacle < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with table. \n Conditions: Agent's state and obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*d_agent_goal"
        },

        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with ceiling fan blades. \n Conditions: Agent's state and staic obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goalif d_agent_obstacle < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives:Avoid collision with hanging lights. \n Conditions: Agent's state and obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -20 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with curtain. \n Conditions: Agent's state and static obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal if d_agent_obstacle < c1: \n  return 5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with avoid drapes. \n Conditions: Agent's state and obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5  \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 10*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with obstacles. \n Conditions: Agent's state and obstacle's status are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal if d_agent_obstacle < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with static obstacle (wardrobes). \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_g = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt":"Mission: Drone in-door navigation and reach other side of the small room. \n Objectives: Do not collide with the walls of the room. \n Conditions: Agent'sn state and obstacle's state. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt":"Mission: Guide the drone through an indoor space to the far side of a small room. \n Objectives: Prevent contact with the people. \n Conditions: Agent'sn state and obstacle's state. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected":"The reward of the drone should be claculated based on the information provided, such as the the coordinates of the agent as well of the information of the obstacle. Takeing the length and width and height of the room should not be ignored. "
        },




        {
            "prompt":"Mission: Drone indoor exploration in a library. \n Objectives: Avoid hitting the surrounding walls. \n Conditions: Agent'sn state and obstacle's state. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected":"The function should take the following inputs: \n - agent's state: a list of integers [x, y, orientation]"
        },
        {
            "prompt":"Mission: Drone indoor exploration in a library. \n Objectives: Navigate without crashing into walls. \n Conditions: Agent'sn state and obstacle's state. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (cables) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect obstacle (chairs) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 10*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (table) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 5*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (bookshelves) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (smoke detectors) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (stackes books) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect dynamic obstacle (shelves) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 9*d_agent_goal"
        },




        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 14*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect static obstacle (trees) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (leaves) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect birds and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collision with stumps. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (rocks) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "MMission: Drone moves in a forest. \n Objectives: Detect thin branches and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 9*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collision with thick tree branches. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_maopped + 14*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (people) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect cliffs and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return -c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },





        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt":"Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid any obstacle with hanging lights. \n Conditions: Agent's information and obstacle's information are available \n Write a reward in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected":"def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error"
        },
        {

            "prompt":"Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid any static obstacle such as food serving counters. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    d_agent_to_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    d_agent_to_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    d_agent_to_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error"
        },

        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with chairs. \n Conditions: Agent's state, obstacle's states are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with table. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_g = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*d_agent_goal - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with trash bins. \n Conditions: Agent's state onstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "Write a reward function in Python for a 2D drone in-Door-Navigation-Problem. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with low-hanging decorations. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },

        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with plants. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = 2 \n    c_2 = 19 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    speed_error = norm(v_agent, v_desired) \n    return 2**d_agent_goal - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with metal rails. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "The reward function is:  ```python"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with cameras. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },






        {
            "prompt":"Mission: Drone in-door navigation in a classroom. \n Objectives: Do not collide with the walls of the room. \n Conditions: Agent's state, obstacle's state are aviable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt":"Mission: Drone in-door navigation in a classroom. \n Objectives: Prevent contact with the people. \n Conditions: Agent's state, obstacle's state are avialable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected":"The reward of the drone should be claculated based on the information provided, such as the the coordinates of the agent as well of the information of the obstacle. Takeing the length and width and height of the room should not be ignored. "
        },
        {
            "prompt":"Mission: Operate the drone in a classroom. \n Objectives: Avoid hitting the ceiling Projector. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected":"The function should take the following inputs: \n - agent's state: a list of integers [x, y, orientation]"
        },
        {
            "prompt":"Mission: Pilot a drone through a classroom. \n Objectives: Navigate without crashing into overhead lights. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect static obstacle (whiteboard) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "Write a reward function in Python for a 2D drone in-Door-Navigation-Problem. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect obstacle (tall bookshelves) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return 10*d_agent_goal - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect obstacle (backpacks) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect dynamic obstacle (student) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "The reward function is:  ```python"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect dynamic obstacle (teachers) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        },





        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "That a simple function."    },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        },

        {
            "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: Maximize unexplored area coverage and avoid collision with chairs and obstacles. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_3*new_area_mapped + d_agent_goal**2 - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "The reward function is:  ```python"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state and obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: Detect dynamic obstacle (cables) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N" 
        },







        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (workers) and avoid collision. \n Conditions: Agent's state, obstacle's state are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*new_area_mapped + c_2*math.exp(d_agent_goal) + speed_error"
        },
        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "That a simple function."    
        },
            {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Avoid collison with high storage racks. \n Conditions: Agent's state and obstacle's information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_tree = calculate_distance(s_agent, s_tree) \n    if d_agent_tree < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Avoid collison with forklifts. \n Conditions: Agent's state and obstacle's information are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "def reward(s_agent, s_branch): \n    c_1 = -2 \n    c_2 = -5 \n    d_agent_branch = calculate_distance(s_agent, s_branch) \n    if d_agent_branch < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },

        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with hanging cables. \n Conditions: Agent's state and obstacle's information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_tree = calculate_distance(s_agent, s_tree) \n    if d_agent_tree < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission:Operate the drone in a warehouse. \n Objectives: Avoid collison with stacks of pallets. \n Conditions: Agent's state and obstacle's information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "def reward(s_agent, s_branch): \n    c_1 = -2 \n    c_2 = -20 \n    d_agent_branch = calculate_distance(s_agent, s_branch) \n    if d_agent_branch < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with stacks of boxes. \n Conditions: Agent's state and obstacle's information are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_tree = calculate_distance(s_agent, s_tree) \n    if d_agent_tree < c1: \n  return 5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with overhead pipes. \n Conditions: Agent's state and obstacle's information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    d_agent_branch = calculate_distance(s_agent, s_branch) \n    if d_agent_branch < c1: \n  return 5.0 \n    return 10*math.exp(d_agent_goal)"
        },


        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Detect people and avoid collison with overhead beams. \n Conditions: Agent's states and moving obstacle information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n     d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },






        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with people. \n Conditions: Agent's states and moving obstacle information are obtainable.. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = -2 \n    c_2 = -5 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },

        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with spotlights. \n Conditions: Agent's states and moving obstacle information are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with balconies. \n Conditions: Agent's states and moving obstacle information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = -2 \n    c_2 = -20 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Detect people and avoid collison with audience seating. \n Conditions: Agent's states and moving obstacle information are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with projectors. \n Conditions: Agent's states and moving obstacle information are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = -2 \n    c_2 = -5  \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return 10*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Avoid collision with cabels. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone explores the theater. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state and obstacle's state are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Detect static obstacle (overhead platforms) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 9*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone explores the theater. \n Objectives: Detect static obstacle (drapes) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_maopped + 14*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone explores the theater. \n Objectives: Detect dynamic obstacle (hangings microphones) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 2**d_agent_goal"
        },









        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state and obstacle's state are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return -c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect  dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state and obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect static obstacle (basket hoops) and avoid collision. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect static obstacle (scoreboards) and avoid collision. \n Conditions: Agent's state, obstacle's state are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (climbing  ropes) and avoid collision. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with volleyball nets. \n Conditions: Agent's state, obstacle's state are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with hanging flags. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "That a simple function."    
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (moving players) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        },

        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with loose balls. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_3*new_area_mapped + d_agent_goal**2 - speed_error"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (flying balls) and avoid collision. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "The reward function is:  ```python"
        },





        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect dynamic obstacle (patients) and avoid collision. \n Conditions: Agent's state and obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect dynamic obstacle (doctors) and avoid collision. \n Conditions: Agent's state, d obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N" 
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect static obstacle (wires) and avoid collision. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone explores the hospital. \n Objectives: Avoid collision with medical equipment stands. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*new_area_mapped + c_2*math.exp(d_agent_goal) + speed_error"
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect wheelchairs and avoid collisions. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "That a simple function."    
        },
        {
            "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect static obstacle (staff) and avoid collision. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Avoid collision with defibrillators. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone explores the hospital. \n Objectives: Avoid  any collision with wall-mounted emergency devices. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives:  Avoid collision with lights. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a hospital. \n Objectives: Avoid to collide with static obstacle such as beds. \n Conditions: Agent's state, obstacle's state are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n    c_2 = 15    if d_agent_obstacle < c_1: \n        reward = -10 \n    else: \n        reward = -3*d_agent_goal \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        },
        
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with chairs. \n Conditions: Agent's state and obstacle's status and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goalif d_agent_obstacle < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with table. \n Conditions: Agent's state and obstacle's status and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*d_agent_goal"
        },

        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with ceiling fan blades. \n Conditions: Agent's state and staic obstacle's status and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goalif d_agent_obstacle < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives:Avoid collision with hanging lights. \n Conditions: Agent's state and obstacle's status and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -20 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with curtain. \n Conditions: Agent's state and static obstacle's status and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal if d_agent_obstacle < c1: \n  return 5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with avoid drapes. \n Conditions: Agent's state and obstacle's status and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5  \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 10*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with obstacles. \n Conditions: Agent's state and obstacle's status and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal if d_agent_obstacle < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with static obstacle (wardrobes). \n Conditions: Agent's state, obstacle's state and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_g = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt":"Mission: Drone in-door navigation and reach other side of the small room. \n Objectives: Do not collide with the walls of the room. \n Conditions: Agent'sn state, obstacle's state and desired speed are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt":"Mission: Guide the drone through an indoor space to the far side of a small room. \n Objectives: Prevent contact with the people. \n Conditions: Agent'sn state, obstacle's state and desired speed are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected":"The reward of the drone should be claculated based on the information provided, such as the the coordinates of the agent as well of the information of the obstacle. Takeing the length and width and height of the room should not be ignored. "
        },




        {
            "prompt":"Mission: Drone indoor exploration in a library. \n Objectives: Avoid hitting the surrounding walls. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected":"The function should take the following inputs: \n - agent's state: a list of integers [x, y, orientation]"
        },
        {
            "prompt":"Mission: Drone indoor exploration in a library. \n Objectives: Navigate without crashing into walls. \n Conditions: Agent'sn state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (cables) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are accessible. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect obstacle (chairs) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 10*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (table) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 5*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (bookshelves) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (smoke detectors) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are accessible. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (stackes books) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are accessible. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect dynamic obstacle (shelves) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 9*d_agent_goal"
        },




        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 14*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect static obstacle (trees) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (leaves) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect birds and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collision with stumps. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (rocks) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "MMission: Drone moves in a forest. \n Objectives: Detect thin branches and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 9*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collision with thick tree branches. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_maopped + 14*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (people) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect cliffs and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return -c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },





        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt":"Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid any obstacle with hanging lights. \n Conditions: Agent's information and obstacle's information are available \n Write a reward in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected":"def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error"
        },
        {

            "prompt":"Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid any static obstacle such as food serving counters. \n Conditions: Agent's state, obstacle's state and desired speed are accessible. \n Write a reward in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    d_agent_to_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    d_agent_to_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    d_agent_to_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error"
        },

        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with chairs. \n Conditions: Agent's state, obstacle's states and desired speed are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with table. \n Conditions: Agent's state, obstacle's state and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_g = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*d_agent_goal - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with trash bins. \n Conditions: Agent's state onstacle's state and desired speed are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "Write a reward function in Python for a 2D drone in-Door-Navigation-Problem. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with low-hanging decorations. \n Conditions: Agent's state, obstacle's state and desired speed are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },

        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with plants. \n Conditions: Agent's state, obstacle's state and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = 2 \n    c_2 = 19 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    speed_error = norm(v_agent, v_desired) \n    return 2**d_agent_goal - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with metal rails. \n Conditions: Agent's state, obstacle's state and desired speed are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "The reward function is:  ```python"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with cameras. \n Conditions: Agent's state, obstacle's state and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },






        {
            "prompt":"Mission: Drone in-door navigation in a classroom. \n Objectives: Do not collide with the walls of the room. \n Conditions: Agent's state, obstacle's state and desired speed are avialable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt":"Mission: Drone in-door navigation in a classroom. \n Objectives: Prevent contact with the people. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected":"The reward of the drone should be claculated based on the information provided, such as the the coordinates of the agent as well of the information of the obstacle. Takeing the length and width and height of the room should not be ignored. "
        },
        {
            "prompt":"Mission: Operate the drone in a classroom. \n Objectives: Avoid hitting the ceiling Projector. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected":"The function should take the following inputs: \n - agent's state: a list of integers [x, y, orientation]"
        },
        {
            "prompt":"Mission: Pilot a drone through a classroom. \n Objectives: Navigate without crashing into overhead lights. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect static obstacle (whiteboard) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "Write a reward function in Python for a 2D drone in-Door-Navigation-Problem. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect obstacle (tall bookshelves) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return 10*d_agent_goal - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect obstacle (backpacks) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect dynamic obstacle (student) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "The reward function is:  ```python"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect dynamic obstacle (teachers) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        },





        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "That a simple function."    },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        },

        {
            "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: Maximize unexplored area coverage and avoid collision with chairs and obstacles. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_3*new_area_mapped + d_agent_goal**2 - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "The reward function is:  ```python"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state and obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: Detect dynamic obstacle (cables) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N" 
        },







        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (workers) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*new_area_mapped + c_2*math.exp(d_agent_goal) + speed_error"
        },
        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "That a simple function."    
        },
            {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Avoid collison with high storage racks. \n Conditions: Agent's state and obstacle's information and wanted velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_tree = calculate_distance(s_agent, s_tree) \n    if d_agent_tree < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Avoid collison with forklifts. \n Conditions: Agent's state and obstacle's information and wanted velocity are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(s_agent, s_branch): \n    c_1 = -2 \n    c_2 = -5 \n    d_agent_branch = calculate_distance(s_agent, s_branch) \n    if d_agent_branch < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },

        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with hanging cables. \n Conditions: Agent's state and obstacle's information and wanted velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_tree = calculate_distance(s_agent, s_tree) \n    if d_agent_tree < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission:Operate the drone in a warehouse. \n Objectives: Avoid collison with stacks of pallets. \n Conditions: Agent's state and obstacle's information and wanted velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(s_agent, s_branch): \n    c_1 = -2 \n    c_2 = -20 \n    d_agent_branch = calculate_distance(s_agent, s_branch) \n    if d_agent_branch < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with stacks of boxes. \n Conditions: Agent's state and obstacle's information and wanted velocity are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_tree = calculate_distance(s_agent, s_tree) \n    if d_agent_tree < c1: \n  return 5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with overhead pipes. \n Conditions: Agent's state and obstacle's information and wanted velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    d_agent_branch = calculate_distance(s_agent, s_branch) \n    if d_agent_branch < c1: \n  return 5.0 \n    return 10*math.exp(d_agent_goal)"
        },


        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Detect people and avoid collison with overhead beams. \n Conditions: Agent's states and moving obstacle information and wanted velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n     d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },






        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with people. \n Conditions: Agent's state, obstacle's state and wanted velocity are  obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = -2 \n    c_2 = -5 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },

        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with spotlights. \n Conditions: Agent's states and moving obstacle information and wanted velocity are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with balconies. \n Conditions: Agent's state, obstacle's state and wanted velocity are  obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = -2 \n    c_2 = -20 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Detect people and avoid collison with audience seating. \n Conditions: Agent's states and moving obstacle information and wanted velocity are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with projectors. \n Conditions: Agent's state, obstacle's state and wanted velocity are  obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = -2 \n    c_2 = -5  \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return 10*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Avoid collision with cabels. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone explores the theater. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state and obstacle's state and wanted velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Detect static obstacle (overhead platforms) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 9*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone explores the theater. \n Objectives: Detect static obstacle (drapes) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_maopped + 14*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone explores the theater. \n Objectives: Detect dynamic obstacle (hangings microphones) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 2**d_agent_goal"
        },









        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return -c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect  dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect static obstacle (basket hoops) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired velocity are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect static obstacle (scoreboards) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (climbing  ropes) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired velocity are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with volleyball nets. \n Conditions: Agent's state, obstacle's state and desired velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with hanging flags. \n Conditions: Agent's state, obstacle's state and desired velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "That a simple function."    
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (moving players) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        },

        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with loose balls. \n Conditions: Agent's state, obstacle's state and desired velocity are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_3*new_area_mapped + d_agent_goal**2 - speed_error"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (flying balls) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired velocity are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "The reward function is:  ```python"
        },





        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect dynamic obstacle (patients) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect dynamic obstacle (doctors) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired velocity are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N" 
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect static obstacle (wires) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone explores the hospital. \n Objectives: Avoid collision with medical equipment stands. \n Conditions: Agent's state, obstacle's state and desired velocity are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*new_area_mapped + c_2*math.exp(d_agent_goal) + speed_error"
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect wheelchairs and avoid collisions. \n Conditions: Agent's state, obstacle's state and desired velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "That a simple function."    
        },
        {
            "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect static obstacle (staff) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Avoid collision with defibrillators. \n Conditions: Agent's state, obstacle's state and desired velocity are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone explores the hospital. \n Objectives: Avoid  any collision with wall-mounted emergency devices. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives:  Avoid collision with lights. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a hospital. \n Objectives: Avoid to collide with static obstacle such as beds. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -3*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        },
        
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with chairs. \n Conditions: Agent's state and obstacle's status and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goalif d_agent_obstacle < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with table. \n Conditions: Agent's state and obstacle's status and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*d_agent_goal"
        },

        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with ceiling fan blades. \n Conditions: Agent's state and staic obstacle's status and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goalif d_agent_obstacle < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives:Avoid collision with hanging lights. \n Conditions: Agent's state and obstacle's status and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -20 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with curtain. \n Conditions: Agent's state and static obstacle's status and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal if d_agent_obstacle < c1: \n  return 5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with avoid drapes. \n Conditions: Agent's state and obstacle's status and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5  \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 10*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with obstacles. \n Conditions: Agent's state and obstacle's status and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal if d_agent_obstacle < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with static obstacle (wardrobes). \n Conditions: Agent's state, obstacle's state and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_g = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt":"Mission: Drone in-door navigation and reach other side of the small room. \n Objectives: Do not collide with the walls of the room. \n Conditions: Agent'sn state, obstacle's state and desired speed are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt":"Mission: Guide the drone through an indoor space to the far side of a small room. \n Objectives: Prevent contact with the people. \n Conditions: Agent'sn state, obstacle's state and desired speed are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected":"The reward of the drone should be claculated based on the information provided, such as the the coordinates of the agent as well of the information of the obstacle. Takeing the length and width and height of the room should not be ignored. "
        },




        {
            "prompt":"Mission: Drone indoor exploration in a library. \n Objectives: Avoid hitting the surrounding walls. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected":"The function should take the following inputs: \n - agent's state: a list of integers [x, y, orientation]"
        },
        {
            "prompt":"Mission: Drone indoor exploration in a library. \n Objectives: Navigate without crashing into walls. \n Conditions: Agent'sn state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (cables) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are accessible. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect obstacle (chairs) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 10*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (table) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 5*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (bookshelves) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (smoke detectors) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are accessible. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (stackes books) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are accessible. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect dynamic obstacle (shelves) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 9*d_agent_goal"
        },




        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 14*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect static obstacle (trees) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (leaves) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect birds and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collision with stumps. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (rocks) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "MMission: Drone moves in a forest. \n Objectives: Detect thin branches and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 9*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collision with thick tree branches. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_maopped + 14*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (people) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect cliffs and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return -c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },





        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt":"Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid any obstacle with hanging lights. \n Conditions: Agent's information and obstacle's information are available \n Write a reward in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected":"def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error"
        },
        {

            "prompt":"Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid any static obstacle such as food serving counters. \n Conditions: Agent's state, obstacle's state and desired speed are accessible. \n Write a reward in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    d_agent_to_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    d_agent_to_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    d_agent_to_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error"
        },

        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with chairs. \n Conditions: Agent's state, obstacle's states and desired speed are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with table. \n Conditions: Agent's state, obstacle's state and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_g = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*d_agent_goal - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with trash bins. \n Conditions: Agent's state onstacle's state and desired speed are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "Write a reward function in Python for a 2D drone in-Door-Navigation-Problem. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with low-hanging decorations. \n Conditions: Agent's state, obstacle's state and desired speed are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },

        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with plants. \n Conditions: Agent's state, obstacle's state and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = 2 \n    c_2 = 19 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    speed_error = norm(v_agent, v_desired) \n    return 2**d_agent_goal - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with metal rails. \n Conditions: Agent's state, obstacle's state and desired speed are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "The reward function is:  ```python"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with cameras. \n Conditions: Agent's state, obstacle's state and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },






        {
            "prompt":"Mission: Drone in-door navigation in a classroom. \n Objectives: Do not collide with the walls of the room. \n Conditions: Agent's state, obstacle's state and desired speed are avialable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt":"Mission: Drone in-door navigation in a classroom. \n Objectives: Prevent contact with the people. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected":"The reward of the drone should be claculated based on the information provided, such as the the coordinates of the agent as well of the information of the obstacle. Takeing the length and width and height of the room should not be ignored. "
        },
        {
            "prompt":"Mission: Operate the drone in a classroom. \n Objectives: Avoid hitting the ceiling Projector. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected":"The function should take the following inputs: \n - agent's state: a list of integers [x, y, orientation]"
        },
        {
            "prompt":"Mission: Pilot a drone through a classroom. \n Objectives: Navigate without crashing into overhead lights. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect static obstacle (whiteboard) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "Write a reward function in Python for a 2D drone in-Door-Navigation-Problem. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect obstacle (tall bookshelves) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return 10*d_agent_goal - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect obstacle (backpacks) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect dynamic obstacle (student) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "The reward function is:  ```python"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect dynamic obstacle (teachers) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        },





        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "That a simple function."    },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        },

        {
            "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: Maximize unexplored area coverage and avoid collision with chairs and obstacles. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_3*new_area_mapped + d_agent_goal**2 - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "The reward function is:  ```python"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state and obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: Detect dynamic obstacle (cables) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N" 
        },







        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (workers) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*new_area_mapped + c_2*math.exp(d_agent_goal) + speed_error"
        },
        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "That a simple function."    
        },
            {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Avoid collison with high storage racks. \n Conditions: Agent's state and obstacle's information and wanted velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_tree = calculate_distance(s_agent, s_tree) \n    if d_agent_tree < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Avoid collison with forklifts. \n Conditions: Agent's state and obstacle's information and wanted velocity are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(s_agent, s_branch): \n    c_1 = -2 \n    c_2 = -5 \n    d_agent_branch = calculate_distance(s_agent, s_branch) \n    if d_agent_branch < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },

        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with hanging cables. \n Conditions: Agent's state and obstacle's information and wanted velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_tree = calculate_distance(s_agent, s_tree) \n    if d_agent_tree < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission:Operate the drone in a warehouse. \n Objectives: Avoid collison with stacks of pallets. \n Conditions: Agent's state and obstacle's information and wanted velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(s_agent, s_branch): \n    c_1 = -2 \n    c_2 = -20 \n    d_agent_branch = calculate_distance(s_agent, s_branch) \n    if d_agent_branch < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with stacks of boxes. \n Conditions: Agent's state and obstacle's information and wanted velocity are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_tree = calculate_distance(s_agent, s_tree) \n    if d_agent_tree < c1: \n  return 5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with overhead pipes. \n Conditions: Agent's state and obstacle's information and wanted velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    d_agent_branch = calculate_distance(s_agent, s_branch) \n    if d_agent_branch < c1: \n  return 5.0 \n    return 10*math.exp(d_agent_goal)"
        },


        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Detect people and avoid collison with overhead beams. \n Conditions: Agent's states and moving obstacle information and wanted velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n     d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },






        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with people. \n Conditions: Agent's state, obstacle's state and wanted velocity are  obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = -2 \n    c_2 = -5 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },

        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with spotlights. \n Conditions: Agent's states and moving obstacle information and wanted velocity are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with balconies. \n Conditions: Agent's state, obstacle's state and wanted velocity are  obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = -2 \n    c_2 = -20 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Detect people and avoid collison with audience seating. \n Conditions: Agent's states and moving obstacle information and wanted velocity are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with projectors. \n Conditions: Agent's state, obstacle's state and wanted velocity are  obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = -2 \n    c_2 = -5  \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return 10*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Avoid collision with cabels. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone explores the theater. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state and obstacle's state and wanted velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Detect static obstacle (overhead platforms) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 9*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone explores the theater. \n Objectives: Detect static obstacle (drapes) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_maopped + 14*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone explores the theater. \n Objectives: Detect dynamic obstacle (hangings microphones) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 2**d_agent_goal"
        },









        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return -c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect  dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect static obstacle (basket hoops) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired velocity are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect static obstacle (scoreboards) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (climbing  ropes) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired velocity are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with volleyball nets. \n Conditions: Agent's state, obstacle's state and desired velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with hanging flags. \n Conditions: Agent's state, obstacle's state and desired velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "That a simple function."    
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (moving players) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        },

        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with loose balls. \n Conditions: Agent's state, obstacle's state and desired velocity are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_3*new_area_mapped + d_agent_goal**2 - speed_error"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (flying balls) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired velocity are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "The reward function is:  ```python"
        },





        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect dynamic obstacle (patients) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect dynamic obstacle (doctors) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired velocity are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N" 
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect static obstacle (wires) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone explores the hospital. \n Objectives: Avoid collision with medical equipment stands. \n Conditions: Agent's state, obstacle's state and desired velocity are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*new_area_mapped + c_2*math.exp(d_agent_goal) + speed_error"
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect wheelchairs and avoid collisions. \n Conditions: Agent's state, obstacle's state and desired velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "That a simple function."    
        },
        {
            "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect static obstacle (staff) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Avoid collision with defibrillators. \n Conditions: Agent's state, obstacle's state and desired velocity are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone explores the hospital. \n Objectives: Avoid  any collision with wall-mounted emergency devices. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives:  Avoid collision with lights. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a hospital. \n Objectives: Avoid to collide with static obstacle such as beds. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -d_agent_goal - speed_penealty \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        },
        
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with chairs. \n Conditions: Agent's state and obstacle's status and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goalif d_agent_obstacle < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with table. \n Conditions: Agent's state and obstacle's status and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*d_agent_goal"
        },

        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with ceiling fan blades. \n Conditions: Agent's state and staic obstacle's status and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goalif d_agent_obstacle < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives:Avoid collision with hanging lights. \n Conditions: Agent's state and obstacle's status and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -20 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with curtain. \n Conditions: Agent's state and static obstacle's status and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal if d_agent_obstacle < c1: \n  return 5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with avoid drapes. \n Conditions: Agent's state and obstacle's status and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5  \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 10*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with obstacles. \n Conditions: Agent's state and obstacle's status and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal if d_agent_obstacle < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with static obstacle (wardrobes). \n Conditions: Agent's state, obstacle's state and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_g = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt":"Mission: Drone in-door navigation and reach other side of the small room. \n Objectives: Do not collide with the walls of the room. \n Conditions: Agent'sn state, obstacle's state and desired speed are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt":"Mission: Guide the drone through an indoor space to the far side of a small room. \n Objectives: Prevent contact with the people. \n Conditions: Agent'sn state, obstacle's state and desired speed are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected":"The reward of the drone should be claculated based on the information provided, such as the the coordinates of the agent as well of the information of the obstacle. Takeing the length and width and height of the room should not be ignored. "
        },




        {
            "prompt":"Mission: Drone indoor exploration in a library. \n Objectives: Avoid hitting the surrounding walls. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected":"The function should take the following inputs: \n - agent's state: a list of integers [x, y, orientation]"
        },
        {
            "prompt":"Mission: Drone indoor exploration in a library. \n Objectives: Navigate without crashing into walls. \n Conditions: Agent'sn state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (cables) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are accessible. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect obstacle (chairs) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 10*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (table) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 5*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (bookshelves) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (smoke detectors) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are accessible. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (stackes books) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are accessible. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect dynamic obstacle (shelves) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 9*d_agent_goal"
        },




        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 14*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect static obstacle (trees) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (leaves) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect birds and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collision with stumps. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (rocks) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "MMission: Drone moves in a forest. \n Objectives: Detect thin branches and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 9*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collision with thick tree branches. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_maopped + 14*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (people) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect cliffs and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return -c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },





        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt":"Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid any obstacle with hanging lights. \n Conditions: Agent's information and obstacle's information are available \n Write a reward in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected":"def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error"
        },
        {

            "prompt":"Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid any static obstacle such as food serving counters. \n Conditions: Agent's state, obstacle's state and desired speed are accessible. \n Write a reward in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    d_agent_to_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    d_agent_to_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    d_agent_to_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error"
        },

        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with chairs. \n Conditions: Agent's state, obstacle's states and desired speed are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with table. \n Conditions: Agent's state, obstacle's state and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_g = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*d_agent_goal - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with trash bins. \n Conditions: Agent's state onstacle's state and desired speed are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "Write a reward function in Python for a 2D drone in-Door-Navigation-Problem. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with low-hanging decorations. \n Conditions: Agent's state, obstacle's state and desired speed are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },

        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with plants. \n Conditions: Agent's state, obstacle's state and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = 2 \n    c_2 = 19 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    speed_error = norm(v_agent, v_desired) \n    return 2**d_agent_goal - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with metal rails. \n Conditions: Agent's state, obstacle's state and desired speed are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "The reward function is:  ```python"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with cameras. \n Conditions: Agent's state, obstacle's state and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },






        {
            "prompt":"Mission: Drone in-door navigation in a classroom. \n Objectives: Do not collide with the walls of the room. \n Conditions: Agent's state, obstacle's state and desired speed are avialable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt":"Mission: Drone in-door navigation in a classroom. \n Objectives: Prevent contact with the people. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected":"The reward of the drone should be claculated based on the information provided, such as the the coordinates of the agent as well of the information of the obstacle. Takeing the length and width and height of the room should not be ignored. "
        },
        {
            "prompt":"Mission: Operate the drone in a classroom. \n Objectives: Avoid hitting the ceiling Projector. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected":"The function should take the following inputs: \n - agent's state: a list of integers [x, y, orientation]"
        },
        {
            "prompt":"Mission: Pilot a drone through a classroom. \n Objectives: Navigate without crashing into overhead lights. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect static obstacle (whiteboard) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "Write a reward function in Python for a 2D drone in-Door-Navigation-Problem. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect obstacle (tall bookshelves) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return 10*d_agent_goal - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect obstacle (backpacks) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect dynamic obstacle (student) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "The reward function is:  ```python"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect dynamic obstacle (teachers) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        },





        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "That a simple function."    },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        },

        {
            "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: Maximize unexplored area coverage and avoid collision with chairs and obstacles. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_3*new_area_mapped + d_agent_goal**2 - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "The reward function is:  ```python"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state and obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: Detect dynamic obstacle (cables) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N" 
        },







        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (workers) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*new_area_mapped + c_2*math.exp(d_agent_goal) + speed_error"
        },
        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "That a simple function."    
        },
            {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Avoid collison with high storage racks. \n Conditions: Agent's state and obstacle's information and wanted velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_tree = calculate_distance(s_agent, s_tree) \n    if d_agent_tree < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Avoid collison with forklifts. \n Conditions: Agent's state and obstacle's information and wanted velocity are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "def reward(s_agent, s_branch): \n    c_1 = -2 \n    c_2 = -5 \n    d_agent_branch = calculate_distance(s_agent, s_branch) \n    if d_agent_branch < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },

        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with hanging cables. \n Conditions: Agent's state and obstacle's information and wanted velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_tree = calculate_distance(s_agent, s_tree) \n    if d_agent_tree < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission:Operate the drone in a warehouse. \n Objectives: Avoid collison with stacks of pallets. \n Conditions: Agent's state and obstacle's information and wanted velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "def reward(s_agent, s_branch): \n    c_1 = -2 \n    c_2 = -20 \n    d_agent_branch = calculate_distance(s_agent, s_branch) \n    if d_agent_branch < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with stacks of boxes. \n Conditions: Agent's state and obstacle's information and wanted velocity are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_tree = calculate_distance(s_agent, s_tree) \n    if d_agent_tree < c1: \n  return 5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with overhead pipes. \n Conditions: Agent's state and obstacle's information and wanted velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    d_agent_branch = calculate_distance(s_agent, s_branch) \n    if d_agent_branch < c1: \n  return 5.0 \n    return 10*math.exp(d_agent_goal)"
        },


        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Detect people and avoid collison with overhead beams. \n Conditions: Agent's states and moving obstacle information and wanted velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n     d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },






        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with people. \n Conditions: Agent's state, obstacle's state and wanted velocity are  obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = -2 \n    c_2 = -5 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },

        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with spotlights. \n Conditions: Agent's states and moving obstacle information and wanted velocity are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with balconies. \n Conditions: Agent's state, obstacle's state and wanted velocity are  obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = -2 \n    c_2 = -20 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Detect people and avoid collison with audience seating. \n Conditions: Agent's states and moving obstacle information and wanted velocity are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with projectors. \n Conditions: Agent's state, obstacle's state and wanted velocity are  obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = -2 \n    c_2 = -5  \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return 10*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Avoid collision with cabels. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone explores the theater. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state and obstacle's state and wanted velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Detect static obstacle (overhead platforms) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 9*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone explores the theater. \n Objectives: Detect static obstacle (drapes) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_maopped + 14*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone explores the theater. \n Objectives: Detect dynamic obstacle (hangings microphones) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 2**d_agent_goal"
        },









        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return -c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect  dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect static obstacle (basket hoops) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired velocity are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect static obstacle (scoreboards) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (climbing  ropes) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired velocity are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with volleyball nets. \n Conditions: Agent's state, obstacle's state and desired velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with hanging flags. \n Conditions: Agent's state, obstacle's state and desired velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "That a simple function."    
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (moving players) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        },

        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with loose balls. \n Conditions: Agent's state, obstacle's state and desired velocity are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_3*new_area_mapped + d_agent_goal**2 - speed_error"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (flying balls) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired velocity are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "The reward function is:  ```python"
        },





        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect dynamic obstacle (patients) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect dynamic obstacle (doctors) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired velocity are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N" 
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect static obstacle (wires) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone explores the hospital. \n Objectives: Avoid collision with medical equipment stands. \n Conditions: Agent's state, obstacle's state and desired velocity are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*new_area_mapped + c_2*math.exp(d_agent_goal) + speed_error"
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect wheelchairs and avoid collisions. \n Conditions: Agent's state, obstacle's state and desired velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "That a simple function."    
        },
        {
            "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect static obstacle (staff) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Avoid collision with defibrillators. \n Conditions: Agent's state, obstacle's state and desired velocity are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone explores the hospital. \n Objectives: Avoid  any collision with wall-mounted emergency devices. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives:  Avoid collision with lights. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a hospital. \n Objectives: Avoid to collide with static obstacle such as beds. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -math.exp(d_agent_goal) - speed_penealty \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        },
        
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with chairs. \n Conditions: Agent's state and obstacle's status and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goalif d_agent_obstacle < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with table. \n Conditions: Agent's state and obstacle's status and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*d_agent_goal"
        },

        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with ceiling fan blades. \n Conditions: Agent's state and staic obstacle's status and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goalif d_agent_obstacle < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives:Avoid collision with hanging lights. \n Conditions: Agent's state and obstacle's status and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -20 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with curtain. \n Conditions: Agent's state and static obstacle's status and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal if d_agent_obstacle < c1: \n  return 5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with avoid drapes. \n Conditions: Agent's state and obstacle's status and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5  \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 10*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with obstacles. \n Conditions: Agent's state and obstacle's status and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal if d_agent_obstacle < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with static obstacle (wardrobes). \n Conditions: Agent's state, obstacle's state and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_g = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt":"Mission: Drone in-door navigation and reach other side of the small room. \n Objectives: Do not collide with the walls of the room. \n Conditions: Agent'sn state, obstacle's state and desired speed are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt":"Mission: Guide the drone through an indoor space to the far side of a small room. \n Objectives: Prevent contact with the people. \n Conditions: Agent'sn state, obstacle's state and desired speed are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected":"The reward of the drone should be claculated based on the information provided, such as the the coordinates of the agent as well of the information of the obstacle. Takeing the length and width and height of the room should not be ignored. "
        },




        {
            "prompt":"Mission: Drone indoor exploration in a library. \n Objectives: Avoid hitting the surrounding walls. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected":"The function should take the following inputs: \n - agent's state: a list of integers [x, y, orientation]"
        },
        {
            "prompt":"Mission: Drone indoor exploration in a library. \n Objectives: Navigate without crashing into walls. \n Conditions: Agent'sn state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (cables) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are accessible. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect obstacle (chairs) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 10*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (table) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 5*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (bookshelves) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (smoke detectors) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are accessible. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (stackes books) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are accessible. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect dynamic obstacle (shelves) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 9*d_agent_goal"
        },




        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 14*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect static obstacle (trees) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (leaves) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
           "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect birds and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collision with stumps. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (rocks) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "MMission: Drone moves in a forest. \n Objectives: Detect thin branches and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 9*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collision with thick tree branches. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_maopped + 14*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (people) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect cliffs and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return -c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },





        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt":"Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid any obstacle with hanging lights. \n Conditions: Agent's information and obstacle's information are available \n Write a reward in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected":"def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error"
        },
        {

            "prompt":"Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid any static obstacle such as food serving counters. \n Conditions: Agent's state, obstacle's state and desired speed are accessible. \n Write a reward in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    d_agent_to_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    d_agent_to_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error \n def reward(s_agent, s_obstacle): \n    d_agent_to_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_to_obstacle < c_1: \n  return -1.0 \n    if d_agent_to_goal < c_2: \n  return 1.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*d_agent_to_goal - speed_error"
        },

        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with chairs. \n Conditions: Agent's state, obstacle's states and desired speed are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with table. \n Conditions: Agent's state, obstacle's state and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_goal_g = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*d_agent_goal - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with trash bins. \n Conditions: Agent's state onstacle's state and desired speed are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "Write a reward function in Python for a 2D drone in-Door-Navigation-Problem. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with low-hanging decorations. \n Conditions: Agent's state, obstacle's state and desired speed are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },

        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with plants. \n Conditions: Agent's state, obstacle's state and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = 2 \n    c_2 = 19 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    speed_error = norm(v_agent, v_desired) \n    return 2**d_agent_goal - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with metal rails. \n Conditions: Agent's state, obstacle's state and desired speed are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "The reward function is:  ```python"
        },
        {
            "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with cameras. \n Conditions: Agent's state, obstacle's state and desired speed are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },






        {
            "prompt":"Mission: Drone in-door navigation in a classroom. \n Objectives: Do not collide with the walls of the room. \n Conditions: Agent's state, obstacle's state and desired speed are avialable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt":"Mission: Drone in-door navigation in a classroom. \n Objectives: Prevent contact with the people. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected":"The reward of the drone should be claculated based on the information provided, such as the the coordinates of the agent as well of the information of the obstacle. Takeing the length and width and height of the room should not be ignored. "
        },
        {
            "prompt":"Mission: Operate the drone in a classroom. \n Objectives: Avoid hitting the ceiling Projector. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected":"The function should take the following inputs: \n - agent's state: a list of integers [x, y, orientation]"
        },
        {
            "prompt":"Mission: Pilot a drone through a classroom. \n Objectives: Navigate without crashing into overhead lights. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected":"- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect static obstacle (whiteboard) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "Write a reward function in Python for a 2D drone in-Door-Navigation-Problem. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect obstacle (tall bookshelves) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return 10*d_agent_goal - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect obstacle (backpacks) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect dynamic obstacle (student) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "The reward function is:  ```python"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect dynamic obstacle (teachers) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        },





        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "That a simple function."    },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        },

        {
            "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: Maximize unexplored area coverage and avoid collision with chairs and obstacles. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_3*new_area_mapped + d_agent_goal**2 - speed_error"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "The reward function is:  ```python"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state and obstacle's state are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: Detect dynamic obstacle (cables) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N" 
        },







        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (workers) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*new_area_mapped + c_2*math.exp(d_agent_goal) + speed_error"
        },
        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "That a simple function."    
        },
            {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Avoid collison with high storage racks. \n Conditions: Agent's state and obstacle's information and wanted velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_tree = calculate_distance(s_agent, s_tree) \n    if d_agent_tree < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },
        {
            "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Avoid collison with forklifts. \n Conditions: Agent's state and obstacle's information and wanted velocity are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(s_agent, s_branch): \n    c_1 = -2 \n    c_2 = -5 \n    d_agent_branch = calculate_distance(s_agent, s_branch) \n    if d_agent_branch < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },

        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with hanging cables. \n Conditions: Agent's state and obstacle's information and wanted velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_tree = calculate_distance(s_agent, s_tree) \n    if d_agent_tree < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission:Operate the drone in a warehouse. \n Objectives: Avoid collison with stacks of pallets. \n Conditions: Agent's state and obstacle's information and wanted velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(s_agent, s_branch): \n    c_1 = -2 \n    c_2 = -20 \n    d_agent_branch = calculate_distance(s_agent, s_branch) \n    if d_agent_branch < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with stacks of boxes. \n Conditions: Agent's state and obstacle's information and wanted velocity are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_tree = calculate_distance(s_agent, s_tree) \n    if d_agent_tree < c1: \n  return 5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with overhead pipes. \n Conditions: Agent's state and obstacle's information and wanted velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    d_agent_branch = calculate_distance(s_agent, s_branch) \n    if d_agent_branch < c1: \n  return 5.0 \n    return 10*math.exp(d_agent_goal)"
        },


        {
            "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Detect people and avoid collison with overhead beams. \n Conditions: Agent's states and moving obstacle information and wanted velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n     d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },






        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with people. \n Conditions: Agent's state, obstacle's state and wanted velocity are  obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = -2 \n    c_2 = -5 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*d_agent_goal"
        },

        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with spotlights. \n Conditions: Agent's states and moving obstacle information and wanted velocity are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with balconies. \n Conditions: Agent's state, obstacle's state and wanted velocity are  obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = -2 \n    c_2 = -20 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Detect people and avoid collison with audience seating. \n Conditions: Agent's states and moving obstacle information and wanted velocity are available. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = 2 \n    c_2 = 10 \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return 2**d_agent_goal"
        },
        {
            "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with projectors. \n Conditions: Agent's state, obstacle's state and wanted velocity are  obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): \n    c_1 = -2 \n    c_2 = -5  \n    d_agent_people = calculate_distance(s_agent, s_peple) \n    if d_agent_people < c1: \n  return 5.0 \n    return 10*math.exp(d_agent_goal)"
        },

        {
            "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Avoid collision with cabels. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone explores the theater. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state and obstacle's state and wanted velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Detect static obstacle (overhead platforms) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 9*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone explores the theater. \n Objectives: Detect static obstacle (drapes) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_maopped + 14*d_agent_goal"
        },
        {
            "prompt": "Mission: Drone explores the theater. \n Objectives: Detect dynamic obstacle (hangings microphones) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return 10*new_area_mapped + 2**d_agent_goal"
        },









        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return -c_3*new_area_mapped + c_2*math.exp(d_agent_goal)"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect  dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return 5.0 \n    return c_3*new_area_mapped + d_agent_goal**2"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect static obstacle (basket hoops) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired velocity are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect static obstacle (scoreboards) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (climbing  ropes) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired velocity are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with volleyball nets. \n Conditions: Agent's state, obstacle's state and desired velocity are obtainable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with hanging flags. \n Conditions: Agent's state, obstacle's state and desired velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "That a simple function."    
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (moving players) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        },

        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with loose balls. \n Conditions: Agent's state, obstacle's state and desired velocity are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():    c_1 = -2 \n    c_2 = -5 \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_3*new_area_mapped + d_agent_goal**2 - speed_error"
        },
        {
            "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (flying balls) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired velocity are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "The reward function is:  ```python"
        },





        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect dynamic obstacle (patients) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect dynamic obstacle (doctors) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired velocity are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N" 
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect static obstacle (wires) and avoid collision. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone explores the hospital. \n Objectives: Avoid collision with medical equipment stands. \n Conditions: Agent's state, obstacle's state and desired velocity are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return -c_3*new_area_mapped + c_2*math.exp(d_agent_goal) + speed_error"
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect wheelchairs and avoid collisions. \n Conditions: Agent's state, obstacle's state and desired velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "That a simple function."    
        },
        {
            "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect static obstacle (staff) and avoid collision. \n Conditions: Agent's state, obstacle's state and desired velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
           "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    speed_error = norm(v_agent, v_desired) \n    return c_2*math.exp(d_agent_goal) - speed_error"
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Avoid collision with defibrillators. \n Conditions: Agent's state, obstacle's state and desired velocity are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flight. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "prompt": "Mission: Drone explores the hospital. \n Objectives: Avoid  any collision with wall-mounted emergency devices. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
        },
        {
            "prompt": "Mission: Operate the drone in a hospital. \n Objectives:  Avoid collision with lights. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "1 + 2 + 3 + 4 + 5 + 10 + 7 + 8 + 2 + 10 + 11 + 12 + ...+ N"
        },
        {
            "prompt": "Mission: Drone indoor exploration in a hospital. \n Objectives: Avoid to collide with static obstacle such as beds. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
            "chosen": "def reward(s_agent, s_obstacle, s_goal): \n    d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    d_agent_goal = calculate_distance(s_agent, s_goal) \n    reward = 0 \n    c_1 = 2 \n        c_2 = 15 \n        if d_agent_obstacle < c_1: \n               reward = -10 \n    else: \n        speed_penealty = norm(v_agent - v_desired) \n        reward = -2*d_agent_goal - speed_penealty \n    return reward",
            "rejected": "The reward functionis complex and calculated based on differen component"
        }


















]