[
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with chairs. \n Conditions: Agent's state and chair's status and desired speed are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel): \n    dist_chair = np.linalg.norm(agent_pos - chair_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_chair < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def reward(agent_pos, chairs_pos, goal_pos): \n    distance = agent_pos - chairs_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with chair. \n Conditions: Agent's state and chair's status and desired speed are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def reward(agent_pos, chair_pos, goal_pos): \n    distance = agent_pos - chair_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with ceiling fan blades. \n Conditions: Agent's state and staic fan blades's status and desired speed are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def reward(pos, vel, obs, goal, target_vel): \n    baldes = 'clear'\n    if blades == 'clear': \n        return 1\n    return -1"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives:Avoid collision such as hanging lamps. \n Conditions: Agent's state and lamp's status and desired speed are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def reward(agent_pos, hanging_lamps_pos, goal_pos): \n    distance = agent_pos - hanging_lamps_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with curtain. \n Conditions: Agent's state and static curtain's status and desired speed are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, curtain_pos, goal_pos, desired_vel): \n    dist_curtain = np.linalg.norm(agent_pos - curtain_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_chair < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def reward(agent_pos, curtain_pos, goal_pos): \n    distance = agent_pos - curtain_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with avoid drapes. \n Conditions: Agent's state and drape's status and desired speed are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "import math \n    nvel = int((max_vel * v0) / (math.sqrt(dt ** 2 + v0**2))) \n    return nvel"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with obstacles. \n Conditions: Agent's state and obstacle's status and desired speed are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def apply_squadron(robber): \n        if robber.state == \"idle\" and robbed: \n            print(\"You have been caught by the squadrons.\") \n            return False \n        elif robber.state != \"idle\": \n            print(\"The squadrons detected you as an intruder!\") \n            return True"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with static obstacle (wardrobes). \n Conditions: Agent's state, wardrobe's state and desired speed are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "import math \n    nvel = int((max_vel * v0) / (math.sqrt(dt ** 2 + v0**2))) \n    return nvel"
  },
  {
    "prompt": "Mission: Drone in-door navigation and reach other side of the small room. \n Objectives: Do not collide with the walls of the room. \n Conditions: Agent's state, obstacle's state and desired speed are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, wall_pos, goal_pos, desired_vel): \n    dist_wall = np.linalg.norm(agent_pos - wall_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_wall < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Guide the drone through an indoor space to the far side of a small room. \n Objectives: Prevent contact with the people. \n Conditions: Agent's state, obstacle's state and desired speed are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Avoid hitting the surrounding walls. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def apply_rule(agents, obstacles):\n        Applies a rule that will allow an In-Depth Exploration of the indoors using drones."
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Navigate without crashing into walls. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect obstacle (chairs) and avoid collision. \n Conditions: Agent's state, chair's state and desired speed are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel): \n    dist_chair = np.linalg.norm(agent_pos - chair_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_chair < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def check_collision(): \n        pass"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (table) and avoid collision. \n Conditions: Agent's state, table's state and desired speed are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (bookshelves) and avoid collision. \n Conditions: Agent's state, bookshelve's state and desired speed are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def apply_rule(agents, obstacles):\n        Applies a rule that will allow an In-Depth Exploration of the indoors using drones."
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (smoke detectors) and avoid collision. \n Conditions: Agent's state, smoke detector's state and desired speed are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def check_collision(): \n        pass"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (stackes books) and avoid collision. \n Conditions: Agent's state, book's state and desired speed are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, book_pos, goal_pos, desired_vel): \n    dist_book = np.linalg.norm(agent_pos - book_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_book < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, human's state and desired speed are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def apply_rule(agents, obstacles):\n        Applies a rule that will allow an In-Depth Exploration of the indoors using drones."
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, human's state and desired speed are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect static obstacle (trees) and avoid collision. \n Conditions: Agent's state, tree's state and desired speed are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def apply_rule(agents, obstacles):\n        Applies a rule that will allow an In-Depth Exploration of the indoors using drones."
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (leaves) and avoid collision. \n Conditions: Agent's state, leaves's state and desired speed are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, leaves_pos, goal_pos, desired_vel): \n    dist_leaves = np.linalg.norm(agent_pos - leaves_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_leaves < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "import math \n    nvel = int((max_vel * v0) / (math.sqrt(dt ** 2 + v0**2))) \n    return nvel"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect birds and avoid collision. \n Conditions: Agent's state, bird's state and desired speed are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collision with stumps. \n Conditions: Agent's state, stump's state and desired speed are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def reward(agent_pos, stumps_pos, goal_pos): \n    distance = agent_pos - stumps_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (rocks) and avoid collision. \n Conditions: Agent's state, rock's state and desired speed are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def find_closest_point(agents, obstacles, agent_state, agent_speed, target, max_dist=50, min_dist=-max_dist, speed_factor=3):\n    closest_points = []\n    for i, point in enumerate(obstacles): \n        dist_from_agent = calculate_distance(agent_state, np.array(point)) \n        if dist_from_agent < min_dist: \n            continue \n         prev_point = np.array(np.where((np.linalg.norm(points) > 4)))"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect thin branches and avoid collision. \n Conditions: Agent's state, branche's state and desired speed are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, branches_pos, goal_pos, desired_vel): \n    dist_branches = np.linalg.norm(agent_pos - branches_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_branches < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collision with thick tree branches. \n Conditions: Agent's state, tree branche's state and desired speed are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def apply_rule(agents, obstacles):\n        Applies a rule that will allow an In-Depth Exploration of the indoors using drones."
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (people) and avoid collision. \n Conditions: Agent's state, people's state and desired speed are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect cliffs and avoid collision. \n Conditions: Agent's state, cliff's state and desired speed are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def check_collision(): \n        pass"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, human's state and desired speed are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, human_pos, goal_pos, desired_vel): \n    dist_human = np.linalg.norm(agent_pos - human_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_human < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def apply_rule(agents, obstacles):\n        Applies a rule that will allow an In-Depth Exploration of the indoors using drones."
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid any static obstacle such as food serving counters. \n Conditions: Agent's state, food counter's state and desired speed are accessible. \n Write a reward in Python.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with chairs. \n Conditions: Agent's state, chair's states and desired speed are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def check_collision(): \n        pass"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with table. \n Conditions: Agent's state, table's state and desired speed are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with trash bins. \n Conditions: Agent's state trash bin's state and desired speed are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, bin_pos, goal_pos, desired_vel): \n    dist_bin = np.linalg.norm(agent_pos - bin_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_bin < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def apply_rule(agents, obstacles):\n        Applies a rule that will allow an In-Depth Exploration of the indoors using drones."
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with low-hanging decorations. \n Conditions: Agent's state, hanging decoration's state and desired speed are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with plants. \n Conditions: Agent's state, plant's state and desired speed are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "drones = [] \n obs = [] \n    for i, obj in enumerate(mesa['objects']): \n        if 'drone' in str(obj) or 'droner' in str(obj): \n            drones.append(i) \n        elif 'obstacle' in str(obj) or 'object' in str(obj): \n            obs.append(i)"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with metal rails. \n Conditions: Agent's state, metal rail's state and desired speed are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def check_collision(): \n        pass"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with cameras. \n Conditions: Agent's state, camera's state and desired speed are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, camera_pos, goal_pos, desired_vel): \n    dist_camera = np.linalg.norm(agent_pos - camera_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_camera < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone in-door navigation in a classroom. \n Objectives: Prevent contact with the people. \n Conditions: Agent's state, people's state and desired speed are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "import math \n    nvel = int((max_vel * v0) / (math.sqrt(dt ** 2 + v0**2))) \n    return nvel"
  },
  {
    "prompt": "Mission: Pilot a drone through a classroom. \n Objectives: Navigate without crashing into overhead lamps. \n Conditions: Agent's state, overhead lamp's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect obstacle (tall bookshelves) and avoid collision. \n Conditions: Agent's state, bookshelve's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect obstacle (backpacks) and avoid collision. \n Conditions: Agent's state, backpack's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, backpack_pos, goal_pos, desired_vel): \n    dist_backpack = np.linalg.norm(agent_pos - backpack_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_backpack < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state, people's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def check_collision(): \n        pass"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Avoid collision with desk. \n Conditions: Agent's state, desk's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "Core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect sidboards and do not crash in it \n Conditions: Agent's state, sidboard's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect bench and avoid collision. \n Conditions: Agent's state, bench's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, bench_pos, goal_pos, desired_vel): \n    dist_bench = np.linalg.norm(agent_pos - bench_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_bench < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "drones = [] \n obs = [] \n    for i, obj in enumerate(mesa['objects']): \n        if 'drone' in str(obj) or 'droner' in str(obj): \n            drones.append(i) \n        elif 'obstacle' in str(obj) or 'object' in str(obj): \n            obs.append(i)"
  },
  {
    "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: There are hanging equipments do not collied with it. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Avoid collision with laptops. \n Conditions: Agent's state and laptop's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def find_closest_point(agents, obstacles, agent_state, agent_speed, target, max_dist=50, min_dist=-max_dist, speed_factor=3):\n    closest_points = []\n    for i, point in enumerate(obstacles): \n        dist_from_agent = calculate_distance(agent_state, np.array(point)) \n        if dist_from_agent < min_dist: \n            continue \n         prev_point = np.array(np.where((np.linalg.norm(points) > 4)))"
  },
  {
    "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: There a tablets in the room. Do not collide with them \n Conditions: Agent's state, tablet's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state, people's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, people_pos, goal_pos, desired_vel): \n    dist_people = np.linalg.norm(agent_pos - people_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_people < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "drones = [] \n obs = [] \n    for i, obj in enumerate(mesa['objects']): \n        if 'drone' in str(obj) or 'droner' in str(obj): \n            drones.append(i) \n        elif 'obstacle' in str(obj) or 'object' in str(obj): \n            obs.append(i)"
  },
  {
    "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (workers) and avoid collision. \n Conditions: Agent's state, workers's state and wanted velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def find_closest_point(agents, obstacles, agent_state, agent_speed, target, max_dist=50, min_dist=-max_dist, speed_factor=3):\n    closest_points = []\n    for i, point in enumerate(obstacles): \n        dist_from_agent = calculate_distance(agent_state, np.array(point)) \n        if dist_from_agent < min_dist: \n            continue \n         prev_point = np.array(np.where((np.linalg.norm(points) > 4)))"
  },
  {
    "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Avoid collison with high storage racks. \n Conditions: Agent's state and storage rack's information and wanted velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Avoid collison with forklifts. \n Conditions: Agent's state and forklift's information and wanted velocity are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison such as hanging cables. \n Conditions: Agent's state and cable's information and wanted velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, cable_pos, goal_pos, desired_vel): \n    dist_cable = np.linalg.norm(agent_pos - cable_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_cable < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def find_closest_point(agents, obstacles, agent_state, agent_speed, target, max_dist=50, min_dist=-max_dist, speed_factor=3):\n    closest_points = []\n    for i, point in enumerate(obstacles): \n        dist_from_agent = calculate_distance(agent_state, np.array(point)) \n        if dist_from_agent < min_dist: \n            continue \n         prev_point = np.array(np.where((np.linalg.norm(points) > 4)))"
  },
  {
    "prompt": "Mission:Operate the drone in a warehouse. \n Objectives: Avoid collison with stacks of pallets. \n Conditions: Agent's state and pallet's information and wanted velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "import math \n    nvel = int((max_vel * v0) / (math.sqrt(dt ** 2 + v0**2))) \n    return nvel"
  },
  {
    "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with overhead pipes. \n Conditions: Agent's state and pipe's information and wanted velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Detect people and avoid collison with overhead beams. \n Conditions: Agent's states and overhead beam information and wanted velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def apply_rule (state, action):\n        if 'Collision' in state:\n            print(\"You collided with\", end=\" \")\n            print(action)\n        else:\n            print('In your turn!')"
  },
  {
    "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with people. \n Conditions: Agent's state, people's state and wanted velocity are  obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, people_pos, goal_pos, desired_vel): \n    dist_people = np.linalg.norm(agent_pos - people_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_people < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with spotlamps. \n Conditions: Agent's state and spotlamp information and wanted velocity are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def find_closest_point(agents, obstacles, agent_state, agent_speed, target, max_dist=50, min_dist=-max_dist, speed_factor=3):\n    closest_points = []\n    for i, point in enumerate(obstacles): \n        dist_from_agent = calculate_distance(agent_state, np.array(point)) \n        if dist_from_agent < min_dist: \n            continue \n         prev_point = np.array(np.where((np.linalg.norm(points) > 4)))"
  },
  {
    "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with balconies. \n Conditions: Agent's state, balcony's state and wanted velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Detect people and avoid collison with audience seating. \n Conditions: Agent's states and audience seating information and wanted velocity are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with projectors. \n Conditions: Agent's state, projectors's state and wanted velocity are  obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, projector_pos, goal_pos, desired_vel): \n    dist_projector = np.linalg.norm(agent_pos - projector_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_projector < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def apply_rule (state, action):\n        if 'Collision' in state:\n            print(\"You collided with\", end=\" \")\n            print(action)\n        else:\n            print('In your turn!')"
  },
  {
    "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Avoid collision with cabels. \n Conditions: Agent's state, cabel's state and desired speed are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def reward(agent_pos, cabels_pos, goal_pos): \n    distance = agent_pos - cabels_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Drone explores the theater. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state and people's state and wanted velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def find_closest_point(agents, obstacles, agent_state, agent_speed, target, max_dist=50, min_dist=-max_dist, speed_factor=3):\n    closest_points = []\n    for i, point in enumerate(obstacles): \n        dist_from_agent = calculate_distance(agent_state, np.array(point)) \n        if dist_from_agent < min_dist: \n            continue \n         prev_point = np.array(np.where((np.linalg.norm(points) > 4)))"
  },
  {
    "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Detect static obstacle (overhead platforms) and avoid collision. \n Conditions: Agent's state, platform's state and desired speed are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def find_closest_point(agents, obstacles, agent_state, agent_speed, target, max_dist=50, min_dist=-max_dist, speed_factor=3):\n    closest_points = []\n    for i, point in enumerate(obstacles): \n        dist_from_agent = calculate_distance(agent_state, np.array(point)) \n        if dist_from_agent < min_dist: \n            continue \n         prev_point = np.array(np.where((np.linalg.norm(points) > 4)))"
  },
  {
    "prompt": "Mission: Drone explores the theater. \n Objectives: Detect static obstacle (drapes) and avoid collision. \n Conditions: Agent's state, drape's state and desired speed are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, drape_pos, goal_pos, desired_vel): \n    dist_drape = np.linalg.norm(agent_pos - drape_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_drape < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone explores the theater. \n Objectives: Detect dynamic obstacle (hangings microphones) and avoid collision. \n Conditions: Agent's state, microphone's state and desired speed are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "drones = [] \n obs = [] \n    for i, obj in enumerate(mesa['objects']): \n        if 'drone' in str(obj) or 'droner' in str(obj): \n            drones.append(i) \n        elif 'obstacle' in str(obj) or 'object' in str(obj): \n            obs.append(i)"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, human's state and wanted velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "import math \n    nvel = int((max_vel * v0) / (math.sqrt(dt ** 2 + v0**2))) \n    return nvel"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect  dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state, people's state and desired speed are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect static obstacle (basket hoops) and avoid collision. \n Conditions: Agent's state, basket hoop's state and desired velocity are usable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, basket_hoop_pos, goal_pos, desired_vel): \n    dist_basket_hoop = np.linalg.norm(agent_pos - basket_hoop_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_basket_hoop < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "drones = [] \n obs = [] \n    for i, obj in enumerate(mesa['objects']): \n        if 'drone' in str(obj) or 'droner' in str(obj): \n            drones.append(i) \n        elif 'obstacle' in str(obj) or 'object' in str(obj): \n            obs.append(i)"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with volleyball nets. \n Conditions: Agent's state, volleyball net's state and desired velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def find_drones(mesa): \n    drones = [] \n    for i, obj in enumerate(np.random.choice(10, size=(len(mesa), 2))): \n        if (np.linalg.norm(obj[0] - mesa) < np.linalg.norm(obj[1]) + np.linalg.norm(drones[-1][0])): \n            drones.append((i, np.array([obj[0], obj[1]])))"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with loose balls. \n Conditions: Agent's state, ball's state and desired velocity are usable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def reward(agent_pos, loose_balls_pos, goal_pos): \n    distance = agent_pos - loose_balls_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect dynamic obstacle (doctors) and avoid collision. \n Conditions: Agent's state, doctor's state and desired velocity are usable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect static obstacle (wires) and avoid collision. \n Conditions: Agent's state, v's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, wire_pos, goal_pos, desired_vel): \n    dist_wire = np.linalg.norm(agent_pos - wire_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_wire < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone explores the hospital. \n Objectives: Avoid collision with medical equipment stands. \n Conditions: Agent's state, medical equipment's state and desired velocity are usable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "drones = [] \n obs = [] \n    for i, obj in enumerate(mesa['objects']): \n        if 'drone' in str(obj) or 'droner' in str(obj): \n            drones.append(i) \n        elif 'obstacle' in str(obj) or 'object' in str(obj): \n            obs.append(i)"
  },
  {
    "prompt": "Mission: Drone explores the hospital. \n Objectives: Avoid  any collision with wall-mounted emergency devices. \n Conditions: Agent's state, emergency device's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Operate the drone in a hospital. \n Objectives:  Avoid collision with lamps. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def reward(agent_pos, lamps_pos, goal_pos): \n    distance = agent_pos - lamps_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with chairs. \n Conditions: Agent's state and chair's status. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel): \n    dist_chair = np.linalg.norm(agent_pos - chair_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_chair < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def apply_rule (state, action):\n        if 'Collision' in state:\n            print(\"You collided with\", end=\" \")\n            print(action)\n        else:\n            print('In your turn!')"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with chair. \n Conditions: Agent's state and chair's status are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def reward(agent_pos, chair_pos, goal_pos): \n    distance = agent_pos - chair_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with ceiling fan blades. \n Conditions: Agent's state and staic fan blades's status are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def reward(pos, vel, obs, goal, target_vel): \n    baldes = 'clear'\n    if blades == 'clear': \n        return 1\n    return -1"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives:Avoid collision such as hanging lamps. \n Conditions: Agent's state and lamp's status are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def reward(agent_pos, hanging_lamps_pos, goal_pos): \n    distance = agent_pos - hanging_lamps_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with curtain. \n Conditions: Agent's state and static curtain's status are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, curtain_pos, goal_pos, desired_vel): \n    dist_curtain = np.linalg.norm(agent_pos - curtain_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_curtain < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "class MazeNode (`object`): \n    def __init__ (self, x, y, parent, action, visited=False, dist=-1, cost=0, heuristic=0, **kwargs ): \n        super().__init__(*kwargs) \n        self.x = x \n        self.y = y \n        self.parent = parent \n        self.action = action"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with avoid drapes. \n Conditions: Agent's state and drape's status are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def find_drones(mesa): \n    drones = [] \n    for i, obj in enumerate(np.random.choice(10, size=(len(mesa), 2))): \n        if (np.linalg.norm(obj[0] - mesa) < np.linalg.norm(obj[1]) + np.linalg.norm(drones[-1][0])): \n            drones.append((i, np.array([obj[0], obj[1]])))"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with obstacles. \n Conditions: Agent's state and obstacle's status are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def apply_squadron(robber): \n        if robber.state == \"idle\" and robbed: \n            print(\"You have been caught by the squadrons.\") \n            return False \n        elif robber.state != \"idle\": \n            print(\"The squadrons detected you as an intruder!\") \n            return True"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with static obstacle (wardrobes). \n Conditions: Agent's state, wardrobe's state are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone in-door navigation and reach other side of the small room. \n Objectives: Do not collide with the walls of the room. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, wall_pos, goal_pos, desired_vel): \n    dist_wall = np.linalg.norm(agent_pos - wall_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_wall < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def apply_squadron(robber): \n        if robber.state == \"idle\" and robbed: \n            print(\"You have been caught by the squadrons.\") \n            return False \n        elif robber.state != \"idle\": \n            print(\"The squadrons detected you as an intruder!\") \n            return True"
  },
  {
    "prompt": "Mission: Guide the drone through an indoor space to the far side of a small room. \n Objectives: Prevent contact with the people. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Avoid hitting the surrounding walls. \n Conditions: Agent's state, obstacle's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def apply_squadron(robber): \n        if robber.state == \"idle\" and robbed: \n            print(\"You have been caught by the squadrons.\") \n            return False \n        elif robber.state != \"idle\": \n            print(\"The squadrons detected you as an intruder!\") \n            return True"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Navigate without crashing into walls. \n Conditions: Agent's state, obstacle's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def apply_squadron(robber): \n        if robber.state == \"idle\" and robbed: \n            print(\"You have been caught by the squadrons.\") \n            return False \n        elif robber.state != \"idle\": \n            print(\"The squadrons detected you as an intruder!\") \n            return True"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (cables) and avoid collision. \n Conditions: Agent's state, cable's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, cable_pos, goal_pos, desired_vel): \n    dist_cable = np.linalg.norm(agent_pos - cable_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_cable < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect obstacle (chairs) and avoid collision. \n Conditions: Agent's state, chair's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def apply_rule (state, action):\n        if 'Collision' in state:\n            print(\"You collided with\", end=\" \")\n            print(action)\n        else:\n            print('In your turn!')"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (table) and avoid collision. \n Conditions: Agent's state, table's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (bookshelves) and avoid collision. \n Conditions: Agent's state, bookshelve's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (smoke cables) and avoid collision. \n Conditions: Agent's state, smoke detector's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, cable_pos, goal_pos, desired_vel): \n    dist_cable = np.linalg.norm(agent_pos - cable_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_cable < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def apply_rule (state, action):\n        if 'Collision' in state:\n            print(\"You collided with\", end=\" \")\n            print(action)\n        else:\n            print('In your turn!')"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (stackes books) and avoid collision. \n Conditions: Agent's state, book's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, human's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def rules(): \n   return 0"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect dynamic obstacle (shelves) and avoid collision. \n Conditions: Agent's state, shelve's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect static obstacle (trees) and avoid collision. \n Conditions: Agent's state, tree's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, tree_pos, goal_pos, desired_vel): \n    dist_tree = np.linalg.norm(agent_pos - tree_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_tree < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def find_drones(mesa): \n    drones = [] \n    for i, obj in enumerate(np.random.choice(10, size=(len(mesa), 2))): \n        if (np.linalg.norm(obj[0] - mesa) < np.linalg.norm(obj[1]) + np.linalg.norm(drones[-1][0])): \n            drones.append((i, np.array([obj[0], obj[1]])))"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect birds and avoid collision. \n Conditions: Agent's state, bird's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def rules(): \n   return 0"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collision with stumps. \n Conditions: Agent's state, stump's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def reward(agent_pos, stumps_pos, goal_pos): \n    distance = agent_pos - stumps_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (rocks) and avoid collision. \n Conditions: Agent's state, rock's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "self.grid[self.current_node.x][self.current_node.y-1][current_node.action] = self.current_node \n self.grid[self.current_node.x+1][self.current_node.y-1][current_node.action] = self.current_node \n self.grid[self.current_node.x-1][self.current_node.y-1][current_node.action] = self.current_node \n self.grid[self.current_node.x][self.current_node.y][current_node.action] = self.current_node"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect thin branches and avoid collision. \n Conditions: Agent's state, branche's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, branche_pos, goal_pos, desired_vel): \n    dist_branche = np.linalg.norm(agent_pos - branche_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_branche < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "import math \n    nvel = int((max_vel * v0) / (math.sqrt(dt ** 2 + v0**2))) \n    return nvel"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (people) and avoid collision. \n Conditions: Agent's state, people's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect cliffs and avoid collision. \n Conditions: Agent's state, cliff's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "class MazeNode (`object`): \n    def __init__ (self, x, y, parent, action, visited=False, dist=-1, cost=0, heuristic=0, **kwargs ): \n        super().__init__(*kwargs) \n        self.x = x \n        self.y = y \n        self.parent = parent \n        self.action = action"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, human's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid any obstacle such as hanging lamps. \n Conditions: Agent's information and lamp's information are available \n Write a reward in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, lamp_pos, goal_pos, desired_vel): \n    dist_lamp = np.linalg.norm(agent_pos - lamp_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_lamp < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid any static obstacle such as food serving counters. \n Conditions: Agent's state, food counter's state are accessible. \n Write a reward in Python.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def apply_rule (state, action):\n        if 'Collision' in state:\n            print(\"You collided with\", end=\" \")\n            print(action)\n        else:\n            print('In your turn!')"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with chairs. \n Conditions: Agent's state, chair's states are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "self.grid[self.current_node.x][self.current_node.y-1][current_node.action] = self.current_node \n self.grid[self.current_node.x+1][self.current_node.y-1][current_node.action] = self.current_node \n self.grid[self.current_node.x-1][self.current_node.y-1][current_node.action] = self.current_node \n self.grid[self.current_node.x][self.current_node.y][current_node.action] = self.current_node"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with low-hanging decorations. \n Conditions: Agent's state, hanging decoration's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with plants. \n Conditions: Agent's state, plant's state are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, plant_pos, goal_pos, desired_vel): \n    dist_plant = np.linalg.norm(agent_pos - plant_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_plant < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with metal rails. \n Conditions: Agent's state, metal rail's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "import math \n    nvel = int((max_vel * v0) / (math.sqrt(dt ** 2 + v0**2))) \n    return nvel"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with cameras. \n Conditions: Agent's state, camera's state are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone in-door navigation in a classroom. \n Objectives: Prevent contact with the people. \n Conditions: Agent's state, people's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "class MazeNode (`object`): \n    def __init__ (self, x, y, parent, action, visited=False, dist=-1, cost=0, heuristic=0, **kwargs ): \n        super().__init__(*kwargs) \n        self.x = x \n        self.y = y \n        self.parent = parent \n        self.action = action"
  },
  {
    "prompt": "Mission: Pilot a drone through a classroom. \n Objectives: Navigate without crashing into overhead lamps. \n Conditions: Agent's state, overhead lamp's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, lamp_pos, goal_pos, desired_vel): \n    dist_lamp = np.linalg.norm(agent_pos - lamp_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_lamp < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect obstacle (tall bookshelves) and avoid collision. \n Conditions: Agent's state, bookshelve's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def apply_rule (state, action):\n        if 'Collision' in state:\n            print(\"You collided with\", end=\" \")\n            print(action)\n        else:\n            print('In your turn!')"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect obstacle (backpacks) and avoid collision. \n Conditions: Agent's state, backpack's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state, people's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Avoid collision with desk. \n Conditions: Agent's state, desk's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, desk_pos, goal_pos, desired_vel): \n    dist_desk = np.linalg.norm(agent_pos - desk_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_desk < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "Core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect sidboards and do not crash in it \n Conditions: Agent's state, sidboard's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect bench and avoid collision. \n Conditions: Agent's state, bench's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "self.grid[self.current_node.x][self.current_node.y-1][current_node.action] = self.current_node \n self.grid[self.current_node.x+1][self.current_node.y-1][current_node.action] = self.current_node \n self.grid[self.current_node.x-1][self.current_node.y-1][current_node.action] = self.current_node \n self.grid[self.current_node.x][self.current_node.y][current_node.action] = self.current_node"
  },
  {
    "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: There are hanging equipments do not collied with it. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def apply_rule (state, action):\n        if 'Collision' in state:\n            print(\"You collided with\", end=\" \")\n            print(action)\n        else:\n            print('In your turn!')"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Avoid collision with laptops. \n Conditions: Agent's state and laptop's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, laptop_pos, goal_pos, desired_vel): \n    dist_laptop = np.linalg.norm(agent_pos - laptop_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_laptop < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def reward(agent_pos, laptops_pos, goal_pos): \n    distance = agent_pos - laptops_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: There a tablets in the room. Do not collide with them \n Conditions: Agent's state, tablet's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (workers) and avoid collision. \n Conditions: Agent's state, workers's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def rules(): \n   return 0"
  },
  {
    "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Avoid collison with forklifts. \n Conditions: Agent's state and forklift's information are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with overhead pipes. \n Conditions: Agent's state and pipe's information are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, pipe_pos, goal_pos, desired_vel): \n    dist_pipe = np.linalg.norm(agent_pos - pipe_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_pipe < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def find_drones(mesa):\n    drones = []\n    obs = []\n    for i, (x, y) in enumerate(np.where(mesa)):\n        if mesa[x][y] < 5:\n            continue"
  },
  {
    "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Detect people and avoid collison with overhead beams. \n Conditions: Agent's states and overhead beam information are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with people. \n Conditions: Agent's state, people's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "class MazeNode (`object`): \n    def __init__ (self, x, y, parent, action, visited=False, dist=-1, cost=0, heuristic=0, **kwargs ): \n        super().__init__(*kwargs) \n        self.x = x \n        self.y = y \n        self.parent = parent \n        self.action = action"
  },
  {
    "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Detect people and avoid collison with audience seating. \n Conditions: Agent's states and audience seating information are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with projectors. \n Conditions: Agent's state, projectors's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, projector_pos, goal_pos, desired_vel): \n    dist_projector = np.linalg.norm(agent_pos - projector_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_projector < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def find_drones(mesa):\n    drones = []\n    obs = []\n    for i, (x, y) in enumerate(np.where(mesa)):\n        if mesa[x][y] < 5:\n            continue"
  },
  {
    "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Avoid collision with cabels. \n Conditions: Agent's state, cabel's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def reward(agent_pos, cabels_pos, goal_pos): \n    distance = agent_pos - cabels_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Drone explores the theater. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state and people's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Detect static obstacle (overhead platforms) and avoid collision. \n Conditions: Agent's state, platform's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def find_drones(mesa):\n    drones = []\n    obs = []\n    for i, (x, y) in enumerate(np.where(mesa)):\n        if mesa[x][y] < 5:\n            continue"
  },
  {
    "prompt": "Mission: Drone explores the theater. \n Objectives: Detect static obstacle (drapes) and avoid collision. \n Conditions: Agent's state, drape's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, drape_pos, goal_pos, desired_vel): \n    dist_drape = np.linalg.norm(agent_pos - drape_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_drape < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone explores the theater. \n Objectives: Detect dynamic obstacle (hangings microphones) and avoid collision. \n Conditions: Agent's state, microphone's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def apply_rule (state, action):\n        if 'Collision' in state:\n            print(\"You collided with\", end=\" \")\n            print(action)\n        else:\n            print('In your turn!')"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, human's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "class MazeNode (`object`): \n    def __init__ (self, x, y, parent, action, visited=False, dist=-1, cost=0, heuristic=0, **kwargs ): \n        super().__init__(*kwargs) \n        self.x = x \n        self.y = y \n        self.parent = parent \n        self.action = action"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect  dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state, people's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (climbing ropes) and avoid collision. \n Conditions: Agent's state, climbing rope's state are usable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, climbing_rope_pos, goal_pos, desired_vel): \n    dist_climbing_rope = np.linalg.norm(agent_pos - climbing_rope_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_climbing_rope < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision such as hanging flags. \n Conditions: Agent's state, flag's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def find_drones(mesa):\n    drones = []\n    obs = []\n    for i, (x, y) in enumerate(np.where(mesa)):\n        if mesa[x][y] < 5:\n            continue"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with loose balls. \n Conditions: Agent's state, ball's state are usable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def check_collision(current_state, action, target, max_speed, min_speed, dt): \n        pass\n    return check_collision"
  },
  {
    "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect dynamic obstacle (patients) and avoid collision. \n Conditions: Agent's state, patient's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "self.grid[self.current_node.x][self.current_node.y-1][current_node.action] = self.current_node \n self.grid[self.current_node.x+1][self.current_node.y-1][current_node.action] = self.current_node \n self.grid[self.current_node.x-1][self.current_node.y-1][current_node.action] = self.current_node \n self.grid[self.current_node.x][self.current_node.y][current_node.action] = self.current_node"
  },
  {
    "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect dynamic obstacle (doctors) and avoid collision. \n Conditions: Agent's state, doctor's state are usable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, doctor_pos, goal_pos, desired_vel): \n    dist_doctor = np.linalg.norm(agent_pos - doctor_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_doctor < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect static obstacle (wires) and avoid collision. \n Conditions: Agent's state, wire's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def rules(): \n   return 0"
  },
  {
    "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect static obstacle (staff) and avoid collision. \n Conditions: Agent's state, staff's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Avoid collision with defibrillators. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def check_collision(current_state, action, target, max_speed, min_speed, dt): \n        pass\n    return check_collision"
  },
  {
    "prompt": "Mission: Drone explores the hospital. \n Objectives: Avoid  any collision with wall-mounted emergency devices. \n Conditions: Agent's state, emergency device's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, emergency_device_pos, goal_pos, desired_vel): \n    dist_emergency_device = np.linalg.norm(agent_pos - emergency_device_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_emergency_device < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def apply_rule (state, action):\n        if 'Collision' in state:\n            print(\"You collided with\", end=\" \")\n            print(action)\n        else:\n            print('In your turn!')"
  },
  {
    "prompt": "Mission: Operate the drone in a hospital. \n Objectives:  Avoid collision with lamps. \n Conditions: Agent's state, lamp's state are usable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def reward(agent_pos, lamps_pos, goal_pos): \n    distance = agent_pos - lamps_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a hospital. \n Objectives: Avoid to collide with static obstacle such as beds. \n Conditions: Agent's state, bed's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def find_drones(mesa):\n    drones = []\n    obs = []\n    for i, (x, y) in enumerate(np.where(mesa)):\n        if mesa[x][y] < 5:\n            continue"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with chairs. \n Conditions: Agent's state and chair's status. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def reward(agent_pos, chairs_pos, goal_pos): \n    distance = agent_pos - chairs_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with chair. \n Conditions: Agent's state and chair's status are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel): \n    dist_chair = np.linalg.norm(agent_pos - chair_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_chair < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "class MazeNode (`object`): \n    def __init__ (self, x, y, parent, action, visited=False, dist=-1, cost=0, heuristic=0, **kwargs ): \n        super().__init__(*kwargs) \n        self.x = x \n        self.y = y \n        self.parent = parent \n        self.action = action"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with ceiling fan blades. \n Conditions: Agent's state and staic fan blades's status are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def reward(pos, vel, obs, goal, target_vel): \n    baldes = 'clear'\n    if blades == 'clear': \n        return 1\n    return -1"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives:Avoid collision such as hanging lamps. \n Conditions: Agent's state and lamp's status are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def reward(agent_pos, hanging_lamps_pos, goal_pos): \n    distance = agent_pos - hanging_lamps_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with curtain. \n Conditions: Agent's state and static curtain's status are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def reward(agent_pos, curtain_pos, goal_pos): \n    distance = agent_pos - curtain_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with avoid drapes. \n Conditions: Agent's state and drape's status are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, drape_pos, goal_pos, desired_vel): \n    dist_drape = np.linalg.norm(agent_pos - drape_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_drape < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def reward(agent_pos, avoid_drapes_pos, goal_pos): \n    distance = agent_pos - avoid_drapes_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with obstacles. \n Conditions: Agent's state and obstacle's status are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def apply_squadron(robber): \n        if robber.state == \"idle\" and robbed: \n            print(\"You have been caught by the squadrons.\") \n            return False \n        elif robber.state != \"idle\": \n            print(\"The squadrons detected you as an intruder!\") \n            return True"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with static obstacle (wardrobes). \n Conditions: Agent's state, wardrobe's state are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "self.grid[self.current_node.x][self.current_node.y-1][current_node.action] = self.current_node \n self.grid[self.current_node.x+1][self.current_node.y-1][current_node.action] = self.current_node \n self.grid[self.current_node.x-1][self.current_node.y-1][current_node.action] = self.current_node \n self.grid[self.current_node.x][self.current_node.y][current_node.action] = self.current_node"
  },
  {
    "prompt": "Mission: Drone in-door navigation and reach other side of the small room. \n Objectives: Do not collide with the walls of the room. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Guide the drone through an indoor space to the far side of a small room. \n Objectives: Prevent contact with the people. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, people_pos, goal_pos, desired_vel): \n    dist_people = np.linalg.norm(agent_pos - people_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_people < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def apply_rule (state, action):\n        if 'Collision' in state:\n            print(\"You collided with\", end=\" \")\n            print(action)\n        else:\n            print('In your turn!')"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Avoid hitting the surrounding walls. \n Conditions: Agent's state, obstacle's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Navigate without crashing into walls. \n Conditions: Agent's state, obstacle's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def check_collision(current_state, action, target, max_speed, min_speed, dt): \n        pass\n    return check_collision"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (cables) and avoid collision. \n Conditions: Agent's state, cable's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def rules(): \n   return 0"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect obstacle (chairs) and avoid collision. \n Conditions: Agent's state, chair's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel): \n    dist_chair = np.linalg.norm(agent_pos - chair_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_chair < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (table) and avoid collision. \n Conditions: Agent's state, table's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def find_drones(mesa):\n    drones = []\n    obs = []\n    for i, (x, y) in enumerate(np.where(mesa)):\n        if mesa[x][y] < 5:\n            continue"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (bookshelves) and avoid collision. \n Conditions: Agent's state, bookshelve's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "class MazeNode (`object`): \n    def __init__ (self, x, y, parent, action, visited=False, dist=-1, cost=0, heuristic=0, **kwargs ): \n        super().__init__(*kwargs) \n        self.x = x \n        self.y = y \n        self.parent = parent \n        self.action = action"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (smoke detectors) and avoid collision. \n Conditions: Agent's state, smoke detector's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (stackes books) and avoid collision. \n Conditions: Agent's state, book's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, book_pos, goal_pos, desired_vel): \n    dist_book = np.linalg.norm(agent_pos - book_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_book < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def find_drones(mesa):\n    drones = []\n    obs = []\n    for i, (x, y) in enumerate(np.where(mesa)):\n        if mesa[x][y] < 5:\n            continue"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, human's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect dynamic obstacle (shelves) and avoid collision. \n Conditions: Agent's state, shelve's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "class MazeNode (`object`): \n    def __init__ (self, x, y, parent, action, visited=False, dist=-1, cost=0, heuristic=0, **kwargs ): \n        super().__init__(*kwargs) \n        self.x = x \n        self.y = y \n        self.parent = parent \n        self.action = action"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect static obstacle (trees) and avoid collision. \n Conditions: Agent's state, tree's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def find_drones(mesa):\n    drones = []\n    obs = []\n    for i, (x, y) in enumerate(np.where(mesa)):\n        if mesa[x][y] < 5:\n            continue"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect birds and avoid collision. \n Conditions: Agent's state, bird's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, birds_pos, goal_pos, desired_vel): \n    dist_birds = np.linalg.norm(agent_pos - birds_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_birds < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collision with stumps. \n Conditions: Agent's state, stump's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def rules(): \n   return 0"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (rocks) and avoid collision. \n Conditions: Agent's state, rock's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect thin branches and avoid collision. \n Conditions: Agent's state, branche's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def check_collision(current_state, action, target, max_speed, min_speed, dt): \n        pass\n    return check_collision"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (people) and avoid collision. \n Conditions: Agent's state, people's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, people_pos, goal_pos, desired_vel): \n    dist_people = np.linalg.norm(agent_pos - people_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_people < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect cliffs and avoid collision. \n Conditions: Agent's state, cliff's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, human's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def apply_rule (state, action):\n        if 'Collision' in state:\n            print(\"You collided with\", end=\" \")\n            print(action)\n        else:\n            print('In your turn!')"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid any obstacle such as hanging lamps. \n Conditions: Agent's information and lamp's information are available \n Write a reward in Python.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "self.grid[self.current_node.x][self.current_node.y-1][current_node.action] = self.current_node \n self.grid[self.current_node.x+1][self.current_node.y-1][current_node.action] = self.current_node \n self.grid[self.current_node.x-1][self.current_node.y-1][current_node.action] = self.current_node \n self.grid[self.current_node.x][self.current_node.y][current_node.action] = self.current_node"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid any static obstacle such as food serving counters. \n Conditions: Agent's state, food counter's state are accessible. \n Write a reward in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, counter_pos, goal_pos, desired_vel): \n    dist_counter = np.linalg.norm(agent_pos - counter_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_counter < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with chairs. \n Conditions: Agent's state, chair's states are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def rules(): \n   return 0"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with low-hanging decorations. \n Conditions: Agent's state, hanging decoration's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with plants. \n Conditions: Agent's state, plant's state are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def find_drones(mesa):\n    drones = []\n    obs = []\n    for i, (x, y) in enumerate(np.where(mesa)):\n        if mesa[x][y] < 5:\n            continue"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with metal rails. \n Conditions: Agent's state, metal rail's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, rails_pos, goal_pos, desired_vel): \n    dist_rails = np.linalg.norm(agent_pos - rails_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_rails < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "class MazeNode (`object`): \n    def __init__ (self, x, y, parent, action, visited=False, dist=-1, cost=0, heuristic=0, **kwargs ): \n        super().__init__(*kwargs) \n        self.x = x \n        self.y = y \n        self.parent = parent \n        self.action = action"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with cameras. \n Conditions: Agent's state, camera's state are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone in-door navigation in a classroom. \n Objectives: Prevent contact with the people. \n Conditions: Agent's state, people's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def check_collision(current_state, action, target, max_speed, min_speed, dt): \n        pass\n    return check_collision"
  },
  {
    "prompt": "Mission: Pilot a drone through a classroom. \n Objectives: Navigate without crashing into overhead lamps. \n Conditions: Agent's state, overhead lamp's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def find_drones(mesa):\n    drones = []\n    obs = []\n    for i, (x, y) in enumerate(np.where(mesa)):\n        if mesa[x][y] < 5:\n            continue"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect obstacle (tall bookshelves) and avoid collision. \n Conditions: Agent's state, bookshelve's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, bookshelve_pos, goal_pos, desired_vel): \n    dist_bookshelve = np.linalg.norm(agent_pos - bookshelve_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_bookshelve < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect obstacle (backpacks) and avoid collision. \n Conditions: Agent's state, backpack's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state, people's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def check_collision(): \n        pass"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Avoid collision with desk. \n Conditions: Agent's state, desk's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "Core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect sidboards and do not crash in it \n Conditions: Agent's state, sidboard's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, sidboard_pos, goal_pos, desired_vel): \n    dist_sidboard = np.linalg.norm(agent_pos - sidboard_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_sidboard < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect bench and avoid collision. \n Conditions: Agent's state, bench's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def check_collision(): \n        pass"
  },
  {
    "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: There are hanging equipments do not collied with it. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Avoid collision with laptops. \n Conditions: Agent's state and laptop's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "class MazeNode (`object`): \n    def __init__ (self, x, y, parent, action, visited=False, dist=-1, cost=0, heuristic=0, **kwargs ): \n        super().__init__(*kwargs) \n        self.x = x \n        self.y = y \n        self.parent = parent \n        self.action = action"
  },
  {
    "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: There a tablets in the room. Do not collide with them \n Conditions: Agent's state, tablet's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, tablet_pos, goal_pos, desired_vel): \n    dist_tablet = np.linalg.norm(agent_pos - tablet_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_tablet < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (workers) and avoid collision. \n Conditions: Agent's state, workers's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def find_drones(mesa):\n    drones = []\n    obs = []\n    for i, (x, y) in enumerate(np.where(mesa)):\n        if mesa[x][y] < 5:\n            continue"
  },
  {
    "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Avoid collison with forklifts. \n Conditions: Agent's state and forklift's information are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with overhead pipes. \n Conditions: Agent's state and pipe's information are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "class MazeNode (`object`): \n    def __init__ (self, x, y, parent, action, visited=False, dist=-1, cost=0, heuristic=0, **kwargs ): \n        super().__init__(*kwargs) \n        self.x = x \n        self.y = y \n        self.parent = parent \n        self.action = action"
  },
  {
    "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Detect people and avoid collison with overhead beams. \n Conditions: Agent's states and overhead beam information are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, overhead_beam_pos, goal_pos, desired_vel): \n    dist_overhead_beam = np.linalg.norm(agent_pos - overhead_beam_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_overhead_beam < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with people. \n Conditions: Agent's state, people's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def check_collision(): \n        pass"
  },
  {
    "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Detect people and avoid collison with audience seating. \n Conditions: Agent's states and audience seating information are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def find_drones(mesa):\n    drones = []\n    obs = []\n    for i, (x, y) in enumerate(np.where(mesa)):\n        if mesa[x][y] < 5:\n            continue"
  },
  {
    "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with projectors. \n Conditions: Agent's state, projectors's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Avoid collision with cabels. \n Conditions: Agent's state, cabel's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, cabel_pos, goal_pos, desired_vel): \n    dist_cabel = np.linalg.norm(agent_pos - cabel_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_cabel < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def check_collision(current_state, action, target, max_speed, min_speed, dt): \n        pass\n    return check_collision"
  },
  {
    "prompt": "Mission: Drone explores the theater. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state and people's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def check_collision(): \n        pass"
  },
  {
    "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Detect static obstacle (overhead platforms) and avoid collision. \n Conditions: Agent's state, platform's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone explores the theater. \n Objectives: Detect static obstacle (drapes) and avoid collision. \n Conditions: Agent's state, drape's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone explores the theater. \n Objectives: Detect dynamic obstacle (hangings microphones) and avoid collision. \n Conditions: Agent's state, microphone's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, microphone_pos, goal_pos, desired_vel): \n    dist_microphone = np.linalg.norm(agent_pos - microphone_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_microphone < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def rules(): \n   return 0"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, human's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect  dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state, people's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def rules(): \n   return 0"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (climbing ropes) and avoid collision. \n Conditions: Agent's state, climbing rope's state are usable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "class MazeNode (`object`): \n    def __init__ (self, x, y, parent, action, visited=False, dist=-1, cost=0, heuristic=0, **kwargs ): \n        super().__init__(*kwargs) \n        self.x = x \n        self.y = y \n        self.parent = parent \n        self.action = action"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision such as hanging flags. \n Conditions: Agent's state, flag's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, flag_pos, goal_pos, desired_vel): \n    dist_flag = np.linalg.norm(agent_pos - flag_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_flag < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def rules(): \n   return 0"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with loose balls. \n Conditions: Agent's state, ball's state are usable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def reward(agent_pos, loose_balls_pos, goal_pos): \n    distance = agent_pos - loose_balls_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect dynamic obstacle (patients) and avoid collision. \n Conditions: Agent's state, patient's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def check_collision(current_state, action, target, max_speed, min_speed, dt): \n        pass\n    return check_collision"
  },
  {
    "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect dynamic obstacle (doctors) and avoid collision. \n Conditions: Agent's state, doctor's state are usable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect static obstacle (wires) and avoid collision. \n Conditions: Agent's state, wire's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, wire_pos, goal_pos, desired_vel): \n    dist_wire = np.linalg.norm(agent_pos - wire_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_wire < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def check_collision(current_state, action, target, max_speed, min_speed, dt): \n        pass\n    return check_collision"
  },
  {
    "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect static obstacle (staff) and avoid collision. \n Conditions: Agent's state, staff's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Avoid collision with defibrillators. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "self.grid[self.current_node.x][self.current_node.y-1][current_node.action] = self.current_node \n self.grid[self.current_node.x+1][self.current_node.y-1][current_node.action] = self.current_node \n self.grid[self.current_node.x-1][self.current_node.y-1][current_node.action] = self.current_node \n self.grid[self.current_node.x][self.current_node.y][current_node.action] = self.current_node"
  },
  {
    "prompt": "Mission: Drone explores the hospital. \n Objectives: Avoid  any collision with wall-mounted emergency devices. \n Conditions: Agent's state, emergency device's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Operate the drone in a hospital. \n Objectives:  Avoid collision with lamps. \n Conditions: Agent's state, lamp's state are usable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, lamp_pos, goal_pos, desired_vel): \n    dist_lamp = np.linalg.norm(agent_pos - lamp_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_lamp < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def check_collision(): \n        pass"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a hospital. \n Objectives: Avoid to collide with static obstacle such as beds. \n Conditions: Agent's state, bed's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with chairs. \n Conditions: Agent's state and chair's status. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def find_drones(mesa):\n    drones = []\n    obs = []\n    for i, (x, y) in enumerate(np.where(mesa)):\n        if mesa[x][y] < 5:\n            continue"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with chair. \n Conditions: Agent's state and chair's status are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "self.grid[self.current_node.x][self.current_node.y-1][current_node.action] = self.current_node \n self.grid[self.current_node.x+1][self.current_node.y-1][current_node.action] = self.current_node \n self.grid[self.current_node.x-1][self.current_node.y-1][current_node.action] = self.current_node \n self.grid[self.current_node.x][self.current_node.y][current_node.action] = self.current_node"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with ceiling fan blades. \n Conditions: Agent's state and staic fan blades's status are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, fan_blade_pos, goal_pos, desired_vel): \n    dist_fan_blade = np.linalg.norm(agent_pos - fan_blade_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_fan_blade < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def reward(pos, vel, obs, goal, target_vel): \n    baldes = 'clear'\n    if blades == 'clear': \n        return 1\n    return -1"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives:Avoid collision such as hanging lamps. \n Conditions: Agent's state and lamp's status are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def reward(agent_pos, hanging_lamps_pos, goal_pos): \n    distance = agent_pos - hanging_lamps_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with curtain. \n Conditions: Agent's state and static curtain's status are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "class MazeNode (`object`): \n    def __init__ (self, x, y, parent, action, visited=False, dist=-1, cost=0, heuristic=0, **kwargs ): \n        super().__init__(*kwargs) \n        self.x = x \n        self.y = y \n        self.parent = parent \n        self.action = action"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with avoid drapes. \n Conditions: Agent's state and drape's status are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def find_drones(agents, obstacles): \n    drone_states = [d[2] for d in agents] \n    found_items = [] \n    return found items"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with obstacles. \n Conditions: Agent's state and obstacle's status are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, obstacle_pos, goal_pos, desired_vel): \n    dist_obstacle = np.linalg.norm(agent_pos - obstacle_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_obstacle < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def apply_squadron(robber): \n        if robber.state == \"idle\" and robbed: \n            print(\"You have been caught by the squadrons.\") \n            return False \n        elif robber.state != \"idle\": \n            print(\"The squadrons detected you as an intruder!\") \n            return True"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with static obstacle (wardrobes). \n Conditions: Agent's state, wardrobe's state are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone in-door navigation and reach other side of the small room. \n Objectives: Do not collide with the walls of the room. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def find_drones(agents, obstacles): \n    drone_states = [d[2] for d in agents] \n    found_items = [] \n    return found items"
  },
  {
    "prompt": "Mission: Guide the drone through an indoor space to the far side of a small room. \n Objectives: Prevent contact with the people. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Avoid hitting the surrounding walls. \n Conditions: Agent's state, obstacle's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, wall_pos, goal_pos, desired_vel): \n    dist_wall = np.linalg.norm(agent_pos - wall_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_wall < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def rules(): \n   return 0"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Navigate without crashing into walls. \n Conditions: Agent's state, obstacle's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def find_drones(agents, obstacles): \n    drone_states = [d[2] for d in agents] \n    found_items = [] \n    return found items"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (cables) and avoid collision. \n Conditions: Agent's state, cable's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def rules(): \n   return 0"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect obstacle (chairs) and avoid collision. \n Conditions: Agent's state, chair's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (table) and avoid collision. \n Conditions: Agent's state, table's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, table_pos, goal_pos, desired_vel): \n    dist_table = np.linalg.norm(agent_pos - table_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_table < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def check_collision(): \n        pass"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (bookshelves) and avoid collision. \n Conditions: Agent's state, bookshelve's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (smoke detectors) and avoid collision. \n Conditions: Agent's state, smoke detector's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def check_collision(): \n        pass"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (stackes books) and avoid collision. \n Conditions: Agent's state, book's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect dynamic obstacle (table) and avoid collision. \n Conditions: Agent's state, human's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, table_pos, goal_pos, desired_vel): \n    dist_table = np.linalg.norm(agent_pos - table_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_table < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect dynamic obstacle (shelves) and avoid collision. \n Conditions: Agent's state, shelve's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect static obstacle (trees) and avoid collision. \n Conditions: Agent's state, tree's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def find_drones(agents, obstacles): \n    drone_states = [d[2] for d in agents] \n    found_items = [] \n    return found items"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect birds and avoid collision. \n Conditions: Agent's state, bird's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collision with stumps. \n Conditions: Agent's state, stump's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, stumps_pos, goal_pos, desired_vel): \n    dist_stumps = np.linalg.norm(agent_pos - stumps_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_stumps < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def np_norm(a): \n    return np.linalg.norm(np.array(a))n"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (rocks) and avoid collision. \n Conditions: Agent's state, rock's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "class MazeNode (`object`): \n    def __init__ (self, x, y, parent, action, visited=False, dist=-1, cost=0, heuristic=0, **kwargs ): \n        super().__init__(*kwargs) \n        self.x = x \n        self.y = y \n        self.parent = parent \n        self.action = action"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect thin branches and avoid collision. \n Conditions: Agent's state, branche's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def find_drones(mesa):\n    drones = []\n    obs = []\n    for i, (x, y) in enumerate(np.where(mesa)):\n        if mesa[x][y] < 5:\n            continue"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (people) and avoid collision. \n Conditions: Agent's state, people's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def np_norm(a): \n    return np.linalg.norm(np.array(a))"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect cliffs and avoid collision. \n Conditions: Agent's state, cliff's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, cliff_pos, goal_pos, desired_vel): \n    dist_cliff = np.linalg.norm(agent_pos - cliff_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_cliff < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, human's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def find_drones(agents, obstacles): \n    drone_states = [d[2] for d in agents] \n    found_items = [] \n    return found items"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid any obstacle such as hanging lamps. \n Conditions: Agent's information and lamp's information are available \n Write a reward in Python.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid any static obstacle such as food serving counters. \n Conditions: Agent's state, food counter's state are accessible. \n Write a reward in Python.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def np_norm(a): \n    return np.linalg.norm(np.array(a))"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with chairs. \n Conditions: Agent's state, chair's states are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel): \n    dist_chair = np.linalg.norm(agent_pos - chair_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_chair < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with low-hanging decorations. \n Conditions: Agent's state, hanging decoration's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def check_collision(): \n        pass"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with plants. \n Conditions: Agent's state, plant's state are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with metal rails. \n Conditions: Agent's state, metal rail's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def find_drones(agents, obstacles): \n    drone_states = [d[2] for d in agents] \n    found_items = [] \n    return found items"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with cameras. \n Conditions: Agent's state, camera's state are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, cameras_pos, goal_pos, desired_vel): \n    dist_cameras = np.linalg.norm(agent_pos - cameras_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_cameras < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def np_norm(a): \n    return np.linalg.norm(np.array(a))"
  },
  {
    "prompt": "Mission: Drone in-door navigation in a classroom. \n Objectives: Prevent contact with the people. \n Conditions: Agent's state, people's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def find_drones(agents, obstacles): \n    drone_states = [d[2] for d in agents] \n    found_items = [] \n    return found items"
  },
  {
    "prompt": "Mission: Pilot a drone through a classroom. \n Objectives: Navigate without crashing into overhead lamps. \n Conditions: Agent's state, overhead lamp's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect obstacle (tall bookshelves) and avoid collision. \n Conditions: Agent's state, bookshelve's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def np_norm(a): \n    return np.linalg.norm(np.array(a))"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect obstacle (backpacks) and avoid collision. \n Conditions: Agent's state, backpack's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, backpacks_pos, goal_pos, desired_vel): \n    dist_backpacks = np.linalg.norm(agent_pos - backpacks_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_backpacks < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "find_drones([[0, 0, 1]], [[-1, 0, 0], [-1, 0, 0], [-1, 0, 0], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1]"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state, people's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Avoid collision with desk. \n Conditions: Agent's state, desk's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def reward(agent_pos, desk_pos, goal_pos): \n    distance = agent_pos - desk_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect sidboards and do not crash in it \n Conditions: Agent's state, sidboard's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "Core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect bench and avoid collision. \n Conditions: Agent's state, bench's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, bench_pos, goal_pos, desired_vel): \n    dist_bench = np.linalg.norm(agent_pos - bench_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_bench < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def find_drones(agents, obstacles): \n    min_distance = np.inf \n    closest_drone = None \n    for i, (agent, target) in enumerate(zip(agents, targets)): \n        dists = np.linalg.norm(np.array(agent.position) - np.array(target), axis=1)"
  },
  {
    "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: There are hanging equipments do not collied with it. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "Core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Avoid collision with laptops. \n Conditions: Agent's state and laptop's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def find_drones(agents, obstacles): \n    min_distance = np.inf \n    closest_drone = None \n    for i, (agent, target) in enumerate(zip(agents, targets)): \n        dists = np.linalg.norm(np.array(agent.position) - np.array(target), axis=1)"
  },
  {
    "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: There a tablets in the room. Do not collide with them \n Conditions: Agent's state, tablet's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def find_drones(agents, obstacles): \n    drone_states = [d[2] for d in agents] \n    found_items = [] \n    return found items"
  },
  {
    "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (workers) and avoid collision. \n Conditions: Agent's state, workers's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, workers_pos, goal_pos, desired_vel): \n    dist_workers = np.linalg.norm(agent_pos - workers_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_workers < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward"
  },
  {
    "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Avoid collison with forklifts. \n Conditions: Agent's state and forklift's information are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def find_drones(agents, obstacles): \n    min_distance = np.inf \n    closest_drone = None \n    for i, (agent, target) in enumerate(zip(agents, targets)): \n        dists = np.linalg.norm(np.array(agent.position) - np.array(target), axis=1)"
  },
  {
    "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with overhead pipes. \n Conditions: Agent's state and pipe's information are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward"
  },
  {
    "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Detect people and avoid collison with overhead beams. \n Conditions: Agent's states and overhead beam information are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with people. \n Conditions: Agent's state, people's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, people_pos, goal_pos, desired_vel): \n    dist_people = np.linalg.norm(agent_pos - people_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_people < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward reward"
  },
  {
    "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Detect people and avoid collison with audience seating. \n Conditions: Agent's states and audience seating information are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with projectors. \n Conditions: Agent's state, projectors's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def find_drones(mesa):\n    drones = []\n    obs = []\n    for i, (x, y) in enumerate(np.where(mesa)):\n        if mesa[x][y] < 5:\n            continue"
  },
  {
    "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Avoid collision with cabels. \n Conditions: Agent's state, cabel's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def reward(agent_pos, cabels_pos, goal_pos): \n    distance = agent_pos - cabels_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Drone explores the theater. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state and people's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, people_pos, goal_pos, desired_vel): \n    dist_people = np.linalg.norm(agent_pos - people_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_people < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def find_drones(agents, obstacles): \n    drone_states = [d[2] for d in agents] \n    found_items = [] \n    return found items"
  },
  {
    "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Detect static obstacle (overhead platforms) and avoid collision. \n Conditions: Agent's state, platform's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def find_drones(mesa):\n    drones = []\n    obs = []\n    for i, (x, y) in enumerate(np.where(mesa)):\n        if mesa[x][y] < 5:\n            continue"
  },
  {
    "prompt": "Mission: Drone explores the theater. \n Objectives: Detect static obstacle (drapes) and avoid collision. \n Conditions: Agent's state, drape's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "find_drones([[0, 0, 1]], [[-1, 0, 0], [-1, 0, 0], [-1, 0, 0], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1])"
  },
  {
    "prompt": "Mission: Drone explores the theater. \n Objectives: Detect dynamic obstacle (hangings microphones) and avoid collision. \n Conditions: Agent's state, microphone's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, human's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, human_pos, goal_pos, desired_vel): \n    dist_human = np.linalg.norm(agent_pos - human_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_human < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "class MazeNode (`object`): \n    def __init__ (self, x, y, parent, action, visited=False, dist=-1, cost=0, heuristic=0, **kwargs ): \n        super().__init__(*kwargs) \n        self.x = x \n        self.y = y \n        self.parent = parent \n        self.action = action"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect  dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state, people's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def find_drones(mesa):\n    drones = []\n    obs = []\n    for i, (x, y) in enumerate(np.where(mesa)):\n        if mesa[x][y] < 5:\n            continue"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (climbing ropes) and avoid collision. \n Conditions: Agent's state, climbing rope's state are usable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision such as hanging flags. \n Conditions: Agent's state, flag's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def reward(agent_pos, hanging_flags_pos, goal_pos): \n    distance = agent_pos - hanging_flags_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with loose balls. \n Conditions: Agent's state, ball's state are usable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, ball_pos, goal_pos, desired_vel): \n    dist_ball = np.linalg.norm(agent_pos - ball_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_ball < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "find_drones([[0, 0, 1]], [[-1, 0, 0], [-1, 0, 0], [-1, 0, 0], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1])"
  },
  {
    "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect dynamic obstacle (patients) and avoid collision. \n Conditions: Agent's state, patient's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect dynamic obstacle (doctors) and avoid collision. \n Conditions: Agent's state, doctor's state are usable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def find_drones(mesa):\n    drones = []\n    obs = []\n    for i, (x, y) in enumerate(np.where(mesa)):\n        if mesa[x][y] < 5:\n            continue"
  },
  {
    "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect static obstacle (wires) and avoid collision. \n Conditions: Agent's state, wire's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "find_drones([[0, 0, 1]], [[-1, 0, 0], [-1, 0, 0], [-1, 0, 0], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1]"
  },
  {
    "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect static obstacle (staff) and avoid collision. \n Conditions: Agent's state, staff's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, staff_pos, goal_pos, desired_vel): \n    dist_staff = np.linalg.norm(agent_pos - staff_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_staff < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Avoid collision with defibrillators. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, v_target): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    v_penalty = np.linalg.norm(vel - v_target) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - 0.3 * v_penalty - penalty \n    return reward",
    "rejected": "def check_collision(): \n        pass"
  },
  {
    "prompt": "Mission: Drone explores the hospital. \n Objectives: Avoid  any collision with wall-mounted emergency devices. \n Conditions: Agent's state, emergency device's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, v, obstacle, goal, v_desired): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    velocity_mismatch = np.linalg.norm(v - v_desired)\n    reward = goal_reward - 0.2 * velocity_mismatch + collision_risk \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Operate the drone in a hospital. \n Objectives:  Avoid collision with lamps. \n Conditions: Agent's state, lamp's state are usable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, vel, obs, goal, target_vel): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    speed_diff = np.linalg.norm(vel - target_vel) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    reward -= 0.1 * speed_diff\n    return reward",
    "rejected": "def reward(agent_pos, lamps_pos, goal_pos): \n    distance = agent_pos - lamps_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a hospital. \n Objectives: Avoid to collide with static obstacle such as beds. \n Conditions: Agent's state, bed's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, bed_pos, goal_pos, desired_vel): \n    dist_bed = np.linalg.norm(agent_pos - beds_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    speed_penalty = np.linalg.norm(agent_vel - desired_vel) \n    if dist_beds < 1.0: \n        return -25\n    reward = -dist_goal - 0.5 * speed_penalty \n    return reward",
    "rejected": "def find_drones(agents, obstacles): \n    min_distance = np.inf \n    closest_drone = None \n    for i, (agent, target) in enumerate(zip(agents, targets)): \n        dists = np.linalg.norm(np.array(agent.position) - np.array(target), axis=1)"
  },

  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with chairs. \n Conditions: Agent's state and chair's status are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, chair_pos, goal_pos): \n    dist_chair = np.linalg.norm(agent_pos - chair_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_chair < 1.0: \n        return -25\n    reward = -dist_goal \n    return reward",
    "rejected": "def reward(agent_pos, chairs_pos, goal_pos): \n    distance = agent_pos - chairs_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with chair. \n Conditions: Agent's state and chair's status are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - penalty \n    return reward",
    "rejected": "def reward(agent_pos, chair_pos, goal_pos): \n    distance = agent_pos - chair_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with ceiling fan blades. \n Conditions: Agent's state and staic fan blades's status are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def reward(pos, obs, goal): \n    baldes = 'clear'\n    if blades == 'clear': \n        return 1\n    return -1"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives:Avoid collision such as hanging lamps. \n Conditions: Agent's state and lamp's status are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    return reward",
    "rejected": "def reward(agent_pos, hanging_lamps_pos, goal_pos): \n    distance = agent_pos - hanging_lamps_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with curtain. \n Conditions: Agent's state and static curtain's status are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, curtain_pos, goal_pos): \n    dist_curtain = np.linalg.norm(agent_pos - curtain_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_chair < 1.0: \n        return -25\n    reward = -dist_goal \n    return reward",
    "rejected": "def reward(agent_pos, curtain_pos, goal_pos): \n    distance = agent_pos - curtain_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with avoid drapes. \n Conditions: Agent's state and drape's status are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - penalty \n    return reward",
    "rejected": "import math \n    nvel = int((max_vel * v0) / (math.sqrt(dt ** 2 + v0**2))) \n    return nvel"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with obstacles. \n Conditions: Agent's state and obstacle's status are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def apply_squadron(robber): \n        if robber.state == \"idle\" and robbed: \n            print(\"You have been caught by the squadrons.\") \n            return False \n        elif robber.state != \"idle\": \n            print(\"The squadrons detected you as an intruder!\") \n            return True"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with static obstacle (wardrobes). \n Conditions: Agent's state, wardrobe's state are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    return reward",
    "rejected": "import math \n    nvel = int((max_vel * v0) / (math.sqrt(dt ** 2 + v0**2))) \n    return nvel"
  },
  {
    "prompt": "Mission: Drone in-door navigation and reach other side of the small room. \n Objectives: Do not collide with the walls of the room. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, wall_pos, goal_pos): \n    dist_wall = np.linalg.norm(agent_pos - wall_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_wall < 1.0: \n        return -25\n    reward = -dist_goal \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Guide the drone through an indoor space to the far side of a small room. \n Objectives: Prevent contact with the people. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Avoid hitting the surrounding walls. \n Conditions: Agent's state, obstacle's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def apply_rule(agents, obstacles):\n        Applies a rule that will allow an In-Depth Exploration of the indoors using drones."
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Navigate without crashing into walls. \n Conditions: Agent's state, obstacle's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect obstacle (chairs) and avoid collision. \n Conditions: Agent's state, chair's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, chair_pos, goal_pos): \n    dist_chair = np.linalg.norm(agent_pos - chair_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_chair < 1.0: \n        return -25\n    reward = -dist_goal \n    return reward",
    "rejected": "def check_collision(): \n        pass"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (table) and avoid collision. \n Conditions: Agent's state, table's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (bookshelves) and avoid collision. \n Conditions: Agent's state, bookshelve's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def apply_rule(agents, obstacles):\n        Applies a rule that will allow an In-Depth Exploration of the indoors using drones."
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (smoke detectors) and avoid collision. \n Conditions: Agent's state, smoke detector's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    return reward",
    "rejected": "def check_collision(): \n        pass"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (stackes books) and avoid collision. \n Conditions: Agent's state, book's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, book_pos, goal_pos): \n    dist_book = np.linalg.norm(agent_pos - book_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_book < 1.0: \n        return -25\n    reward = -dist_goal \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, human's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - penalty \n    return reward",
    "rejected": "def apply_rule(agents, obstacles):\n        Applies a rule that will allow an In-Depth Exploration of the indoors using drones."
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, human's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect static obstacle (trees) and avoid collision. \n Conditions: Agent's state, tree's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    return reward",
    "rejected": "def apply_rule(agents, obstacles):\n        Applies a rule that will allow an In-Depth Exploration of the indoors using drones."
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (leaves) and avoid collision. \n Conditions: Agent's state, leaves's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, leaves_pos, goal_pos): \n    dist_leaves = np.linalg.norm(agent_pos - leaves_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_leaves < 1.0: \n        return -25\n    reward = -dist_goal \n    return reward",
    "rejected": "import math \n    nvel = int((max_vel * v0) / (math.sqrt(dt ** 2 + v0**2))) \n    return nvel"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect birds and avoid collision. \n Conditions: Agent's state, bird's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collision with stumps. \n Conditions: Agent's state, stump's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "The challenge here isnt about finding the shortest route; rather, its about ensuring that no two robots ever meet due to collisions. This may seem like a simple problem, but applying rules such as avoiding collisions could lead to very complex scenarios when applied across multiple groups of robots. "
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (rocks) and avoid collision. \n Conditions: Agent's state, rock's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    return reward",
    "rejected": "def find_closest_point(agents, obstacles, agent_state, agent_speed, target, max_dist=50, min_dist=-max_dist, speed_factor=3):\n    closest_points = []\n    for i, point in enumerate(obstacles): \n        dist_from_agent = calculate_distance(agent_state, np.array(point)) \n        if dist_from_agent < min_dist: \n            continue \n         prev_point = np.array(np.where((np.linalg.norm(points) > 4)))"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect thin branches and avoid collision. \n Conditions: Agent's state, branche's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, branches_pos, goal_pos): \n    dist_branches = np.linalg.norm(agent_pos - branches_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_branches < 1.0: \n        return -25\n    reward = -dist_goal \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collision with thick tree branches. \n Conditions: Agent's state, tree branche's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - penalty \n    return reward",
    "rejected": "def apply_rule(agents, obstacles):\n        Applies a rule that will allow an In-Depth Exploration of the indoors using drones."
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (people) and avoid collision. \n Conditions: Agent's state, people's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect cliffs and avoid collision. \n Conditions: Agent's state, cliff's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    return reward",
    "rejected": "def check_collision(): \n        pass"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, human's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, human_pos, goal_pos): \n    dist_human = np.linalg.norm(agent_pos - human_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_human < 1.0: \n        return -25\n    reward = -dist_goal \n    return reward",
    "rejected": "def apply_rule(agents, obstacles):\n        Applies a rule that will allow an In-Depth Exploration of the indoors using drones."
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid any static obstacle such as food serving counters. \n Conditions: Agent's state, food counter's state are accessible. \n Write a reward in Python.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with chairs. \n Conditions: Agent's state, chair's states are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def check_collision(): \n        pass"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with table. \n Conditions: Agent's state, table's state are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with trash bins. \n Conditions: Agent's state trash bin's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, bin_pos, goal_pos): \n    dist_bin = np.linalg.norm(agent_pos - bin_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_bin < 1.0: \n        return -25\n    reward = -dist_goal \n    return reward",
    "rejected": "def apply_rule(agents, obstacles):\n        Applies a rule that will allow an In-Depth Exploration of the indoors using drones."
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with low-hanging decorations. \n Conditions: Agent's state, hanging decoration's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with plants. \n Conditions: Agent's state, plant's state are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "drones = [] \n obs = [] \n    for i, obj in enumerate(mesa['objects']): \n        if 'drone' in str(obj) or 'droner' in str(obj): \n            drones.append(i) \n        elif 'obstacle' in str(obj) or 'object' in str(obj): \n            obs.append(i)"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with metal rails. \n Conditions: Agent's state, metal rail's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    return reward",
    "rejected": "def check_collision(): \n        pass"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with cameras. \n Conditions: Agent's state, camera's state are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, camera_pos, goal_pos): \n    dist_camera = np.linalg.norm(agent_pos - camera_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_camera < 1.0: \n        return -25\n    reward = -dist_goal \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone in-door navigation in a classroom. \n Objectives: Prevent contact with the people. \n Conditions: Agent's state, people's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - penalty \n    return reward",
    "rejected": "import math \n    nvel = int((max_vel * v0) / (math.sqrt(dt ** 2 + v0**2))) \n    return nvel"
  },
  {
    "prompt": "Mission: Pilot a drone through a classroom. \n Objectives: Navigate without crashing into overhead lamps. \n Conditions: Agent's state, overhead lamp's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect obstacle (tall bookshelves) and avoid collision. \n Conditions: Agent's state, bookshelve's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect obstacle (backpacks) and avoid collision. \n Conditions: Agent's state, backpack's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, backpack_pos, goal_pos): \n    dist_backpack = np.linalg.norm(agent_pos - backpack_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_backpack < 1.0: \n        return -25\n    reward = -dist_goal \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state, people's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - penalty \n    return reward",
    "rejected": "def check_collision(): \n        pass"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Avoid collision with desk. \n Conditions: Agent's state, desk's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "Core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect sidboards and do not crash in it \n Conditions: Agent's state, sidboard's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    return reward",
    "rejected": "The challenge here isnt about finding the shortest route; rather, its about ensuring that no two robots ever meet due to collisions. This may seem like a simple problem, but applying rules such as avoiding collisions could lead to very complex scenarios when applied across multiple groups of robots. "
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect bench and avoid collision. \n Conditions: Agent's state, bench's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, bench_pos, goal_pos): \n    dist_bench = np.linalg.norm(agent_pos - bench_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_bench < 1.0: \n        return -25\n    reward = -dist_goal \n    return reward",
    "rejected": "drones = [] \n obs = [] \n    for i, obj in enumerate(mesa['objects']): \n        if 'drone' in str(obj) or 'droner' in str(obj): \n            drones.append(i) \n        elif 'obstacle' in str(obj) or 'object' in str(obj): \n            obs.append(i)"
  },
  {
    "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: There are hanging equipments do not collied with it. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Avoid collision with laptops. \n Conditions: Agent's state and laptop's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def find_closest_point(agents, obstacles, agent_state, agent_speed, target, max_dist=50, min_dist=-max_dist, speed_factor=3):\n    closest_points = []\n    for i, point in enumerate(obstacles): \n        dist_from_agent = calculate_distance(agent_state, np.array(point)) \n        if dist_from_agent < min_dist: \n            continue \n         prev_point = np.array(np.where((np.linalg.norm(points) > 4)))"
  },
  {
    "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: There a tablets in the room. Do not collide with them \n Conditions: Agent's state, tablet's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state, people's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, people_pos, goal_pos): \n    dist_people = np.linalg.norm(agent_pos - people_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_people < 1.0: \n        return -25\n    reward = -dist_goal \n    return reward",
    "rejected": "drones = [] \n obs = [] \n    for i, obj in enumerate(mesa['objects']): \n        if 'drone' in str(obj) or 'droner' in str(obj): \n            drones.append(i) \n        elif 'obstacle' in str(obj) or 'object' in str(obj): \n            obs.append(i)"
  },
  {
    "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (workers) and avoid collision. \n Conditions: Agent's state, workers's state and wanted velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - penalty \n    return reward",
    "rejected": "def find_closest_point(agents, obstacles, agent_state, agent_speed, target, max_dist=50, min_dist=-max_dist, speed_factor=3):\n    closest_points = []\n    for i, point in enumerate(obstacles): \n        dist_from_agent = calculate_distance(agent_state, np.array(point)) \n        if dist_from_agent < min_dist: \n            continue \n         prev_point = np.array(np.where((np.linalg.norm(points) > 4)))"
  },
  {
    "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Avoid collison with high storage racks. \n Conditions: Agent's state and storage rack's information and wanted velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Avoid collison with forklifts. \n Conditions: Agent's state and forklift's information and wanted velocity are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison such as hanging cables. \n Conditions: Agent's state and cable's information and wanted velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, cable_pos, goal_pos): \n    dist_cable = np.linalg.norm(agent_pos - cable_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_cable < 1.0: \n        return -25\n    reward = -dist_goal \n    return reward",
    "rejected": "def find_closest_point(agents, obstacles, agent_state, agent_speed, target, max_dist=50, min_dist=-max_dist, speed_factor=3):\n    closest_points = []\n    for i, point in enumerate(obstacles): \n        dist_from_agent = calculate_distance(agent_state, np.array(point)) \n        if dist_from_agent < min_dist: \n            continue \n         prev_point = np.array(np.where((np.linalg.norm(points) > 4)))"
  },
  {
    "prompt": "Mission:Operate the drone in a warehouse. \n Objectives: Avoid collison with stacks of pallets. \n Conditions: Agent's state and pallet's information and wanted velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - penalty \n    return reward",
    "rejected": "import math \n    nvel = int((max_vel * v0) / (math.sqrt(dt ** 2 + v0**2))) \n    return nvel"
  },
  {
    "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with overhead pipes. \n Conditions: Agent's state and pipe's information and wanted velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Detect people and avoid collison with overhead beams. \n Conditions: Agent's states and overhead beam information and wanted velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    return reward",
    "rejected": "def apply_rule (state, action):\n        if 'Collision' in state:\n            print(\"You collided with\", end=\" \")\n            print(action)\n        else:\n            print('In your turn!')"
  },
  {
    "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with people. \n Conditions: Agent's state, people's state and wanted velocity are  obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, people_pos, goal_pos): \n    dist_people = np.linalg.norm(agent_pos - people_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_people < 1.0: \n        return -25\n    reward = -dist_goal \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with spotlamps. \n Conditions: Agent's state and spotlamp information and wanted velocity are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - penalty \n    return reward",
    "rejected": "def find_closest_point(agents, obstacles, agent_state, agent_speed, target, max_dist=50, min_dist=-max_dist, speed_factor=3):\n    closest_points = []\n    for i, point in enumerate(obstacles): \n        dist_from_agent = calculate_distance(agent_state, np.array(point)) \n        if dist_from_agent < min_dist: \n            continue \n         prev_point = np.array(np.where((np.linalg.norm(points) > 4)))"
  },
  {
    "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with balconies. \n Conditions: Agent's state, balcony's state and wanted velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Detect people and avoid collison with audience seating. \n Conditions: Agent's states and audience seating information and wanted velocity are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with projectors. \n Conditions: Agent's state, projectors's state and wanted velocity are  obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, projector_pos, goal_pos): \n    dist_projector = np.linalg.norm(agent_pos - projector_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_projector < 1.0: \n        return -25\n    reward = -dist_goal \n    return reward",
    "rejected": "def apply_rule (state, action):\n        if 'Collision' in state:\n            print(\"You collided with\", end=\" \")\n            print(action)\n        else:\n            print('In your turn!')"
  },
  {
    "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Avoid collision with cabels. \n Conditions: Agent's state, cabel's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - penalty \n    return reward",
    "rejected": "def reward(agent_pos, cabels_pos, goal_pos): \n    distance = agent_pos - cabels_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Drone explores the theater. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state and people's state and wanted velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def find_closest_point(agents, obstacles, agent_state, agent_speed, target, max_dist=50, min_dist=-max_dist, speed_factor=3):\n    closest_points = []\n    for i, point in enumerate(obstacles): \n        dist_from_agent = calculate_distance(agent_state, np.array(point)) \n        if dist_from_agent < min_dist: \n            continue \n         prev_point = np.array(np.where((np.linalg.norm(points) > 4)))"
  },
  {
    "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Detect static obstacle (overhead platforms) and avoid collision. \n Conditions: Agent's state, platform's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    return reward",
    "rejected": "def find_closest_point(agents, obstacles, agent_state, agent_speed, target, max_dist=50, min_dist=-max_dist, speed_factor=3):\n    closest_points = []\n    for i, point in enumerate(obstacles): \n        dist_from_agent = calculate_distance(agent_state, np.array(point)) \n        if dist_from_agent < min_dist: \n            continue \n         prev_point = np.array(np.where((np.linalg.norm(points) > 4)))"
  },
  {
    "prompt": "Mission: Drone explores the theater. \n Objectives: Detect static obstacle (drapes) and avoid collision. \n Conditions: Agent's state, drape's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, drape_pos, goal_pos): \n    dist_drape = np.linalg.norm(agent_pos - drape_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_drape < 1.0: \n        return -25\n    reward = -dist_goal \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone explores the theater. \n Objectives: Detect dynamic obstacle (hangings microphones) and avoid collision. \n Conditions: Agent's state, microphone's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - penalty \n    return reward",
    "rejected": "drones = [] \n obs = [] \n    for i, obj in enumerate(mesa['objects']): \n        if 'drone' in str(obj) or 'droner' in str(obj): \n            drones.append(i) \n        elif 'obstacle' in str(obj) or 'object' in str(obj): \n            obs.append(i)"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, human's state and wanted velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "import math \n    nvel = int((max_vel * v0) / (math.sqrt(dt ** 2 + v0**2))) \n    return nvel"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect  dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state, people's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    return reward",
    "rejected": "The challenge here isnt about finding the shortest route; rather, its about ensuring that no two robots ever meet due to collisions. This may seem like a simple problem, but applying rules such as avoiding collisions could lead to very complex scenarios when applied across multiple groups of robots. "
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect static obstacle (basket hoops) and avoid collision. \n Conditions: Agent's state, basket hoop's state and desired velocity are usable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, basket_hoop_pos, goal_pos): \n    dist_basket_hoop = np.linalg.norm(agent_pos - basket_hoop_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_basket_hoop < 1.0: \n        return -25\n    reward = -dist_goal \n    return reward",
    "rejected": "drones = [] \n obs = [] \n    for i, obj in enumerate(mesa['objects']): \n        if 'drone' in str(obj) or 'droner' in str(obj): \n            drones.append(i) \n        elif 'obstacle' in str(obj) or 'object' in str(obj): \n            obs.append(i)"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with volleyball nets. \n Conditions: Agent's state, volleyball net's state and desired velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - penalty \n    return reward",
    "rejected": "def find_drones(mesa): \n    drones = [] \n    for i, obj in enumerate(np.random.choice(10, size=(len(mesa), 2))): \n        if (np.linalg.norm(obj[0] - mesa) < np.linalg.norm(obj[1]) + np.linalg.norm(drones[-1][0])): \n            drones.append((i, np.array([obj[0], obj[1]])))"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with loose balls. \n Conditions: Agent's state, ball's state and desired velocity are usable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def reward(agent_pos, loose_balls_pos, goal_pos): \n    distance = agent_pos - loose_balls_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect dynamic obstacle (doctors) and avoid collision. \n Conditions: Agent's state, doctor's state and desired velocity are usable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect static obstacle (wires) and avoid collision. \n Conditions: Agent's state, v's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, wire_pos, goal_pos): \n    dist_wire = np.linalg.norm(agent_pos - wire_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_wire < 1.0: \n        return -25\n    reward = -dist_goal \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone explores the hospital. \n Objectives: Avoid collision with medical equipment stands. \n Conditions: Agent's state, medical equipment's state and desired velocity are usable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - penalty \n    return reward",
    "rejected": "drones = [] \n obs = [] \n    for i, obj in enumerate(mesa['objects']): \n        if 'drone' in str(obj) or 'droner' in str(obj): \n            drones.append(i) \n        elif 'obstacle' in str(obj) or 'object' in str(obj): \n            obs.append(i)"
  },
  {
    "prompt": "Mission: Drone explores the hospital. \n Objectives: Avoid  any collision with wall-mounted emergency devices. \n Conditions: Agent's state, emergency device's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Operate the drone in a hospital. \n Objectives:  Avoid collision with lamps. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 15 \n    return reward",
    "rejected": "def reward(agent_pos, lamps_pos, goal_pos): \n    distance = agent_pos - lamps_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with chairs. \n Conditions: Agent's state and chair's status are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, chair_pos, goal_pos): \n    dist_chair = np.linalg.norm(agent_pos - chair_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_chair < 1.0: \n        return -10 \n    reward = -dist_goal \n    return reward",
    "rejected": "def reward(agent_pos, chairs_pos, goal_pos): \n    distance = agent_pos - chairs_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with chair. \n Conditions: Agent's state and chair's status are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - penalty \n    return reward",
    "rejected": "def reward(agent_pos, chair_pos, goal_pos): \n    distance = agent_pos - chair_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with ceiling fan blades. \n Conditions: Agent's state and staic fan blades's status are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def reward(pos, obs, goal): \n    baldes = 'clear'\n    if blades == 'clear': \n        return 1\n    return -1"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives:Avoid collision such as hanging lamps. \n Conditions: Agent's state and lamp's status are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 5 \n    return reward",
    "rejected": "def reward(agent_pos, hanging_lamps_pos, goal_pos): \n    distance = agent_pos - hanging_lamps_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with curtain. \n Conditions: Agent's state and static curtain's status are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, curtain_pos, goal_pos): \n    dist_curtain = np.linalg.norm(agent_pos - curtain_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_chair < 1.0: \n        return -10 \n    reward = -dist_goal \n    return reward",
    "rejected": "def reward(agent_pos, curtain_pos, goal_pos): \n    distance = agent_pos - curtain_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with avoid drapes. \n Conditions: Agent's state and drape's status are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - penalty \n    return reward",
    "rejected": "The challenge here isnt about finding the shortest route; rather, its about ensuring that no two robots ever meet due to collisions. This may seem like a simple problem, but applying rules such as avoiding collisions could lead to very complex scenarios when applied across multiple groups of robots. "
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with obstacles. \n Conditions: Agent's state and obstacle's status are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def apply_squadron(robber): \n        if robber.state == \"idle\" and robbed: \n            print(\"You have been caught by the squadrons.\") \n            return False \n        elif robber.state != \"idle\": \n            print(\"The squadrons detected you as an intruder!\") \n            return True"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with static obstacle (wardrobes). \n Conditions: Agent's state, wardrobe's state are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 5 \n    return reward",
    "rejected": "import math \n    nvel = int((max_vel * v0) / (math.sqrt(dt ** 2 + v0**2))) \n    return nvel"
  },
  {
    "prompt": "Mission: Drone in-door navigation and reach other side of the small room. \n Objectives: Do not collide with the walls of the room. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, wall_pos, goal_pos): \n    dist_wall = np.linalg.norm(agent_pos - wall_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_wall < 1.0: \n        return -10 \n    reward = -dist_goal \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Guide the drone through an indoor space to the far side of a small room. \n Objectives: Prevent contact with the people. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Avoid hitting the surrounding walls. \n Conditions: Agent's state, obstacle's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def apply_rule(agents, obstacles):\n        Applies a rule that will allow an In-Depth Exploration of the indoors using drones."
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Navigate without crashing into walls. \n Conditions: Agent's state, obstacle's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 5 \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect obstacle (chairs) and avoid collision. \n Conditions: Agent's state, chair's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, chair_pos, goal_pos): \n    dist_chair = np.linalg.norm(agent_pos - chair_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_chair < 1.0: \n        return -10 \n    reward = -dist_goal \n    return reward",
    "rejected": "def check_collision(): \n        pass"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (table) and avoid collision. \n Conditions: Agent's state, table's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (bookshelves) and avoid collision. \n Conditions: Agent's state, bookshelve's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def apply_rule(agents, obstacles):\n        Applies a rule that will allow an In-Depth Exploration of the indoors using drones."
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (smoke detectors) and avoid collision. \n Conditions: Agent's state, smoke detector's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 5 \n    return reward",
    "rejected": "The challenge here isnt about finding the shortest route; rather, its about ensuring that no two robots ever meet due to collisions. This may seem like a simple problem, but applying rules such as avoiding collisions could lead to very complex scenarios when applied across multiple groups of robots. "
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (stackes books) and avoid collision. \n Conditions: Agent's state, book's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, book_pos, goal_pos): \n    dist_book = np.linalg.norm(agent_pos - book_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_book < 1.0: \n        return -10 \n    reward = -dist_goal \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, human's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - penalty \n    return reward",
    "rejected": "def apply_rule(agents, obstacles):\n        Applies a rule that will allow an In-Depth Exploration of the indoors using drones."
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, human's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect static obstacle (trees) and avoid collision. \n Conditions: Agent's state, tree's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 5 \n    return reward",
    "rejected": "def apply_rule(agents, obstacles):\n        Applies a rule that will allow an In-Depth Exploration of the indoors using drones."
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (leaves) and avoid collision. \n Conditions: Agent's state, leaves's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, leaves_pos, goal_pos): \n    dist_leaves = np.linalg.norm(agent_pos - leaves_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_leaves < 1.0: \n        return -10 \n    reward = -dist_goal \n    return reward",
    "rejected": "import math \n    nvel = int((max_vel * v0) / (math.sqrt(dt ** 2 + v0**2))) \n    return nvel"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect birds and avoid collision. \n Conditions: Agent's state, bird's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collision with stumps. \n Conditions: Agent's state, stump's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def reward(agent_pos, stumps_pos, goal_pos): \n    distance = agent_pos - stumps_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (rocks) and avoid collision. \n Conditions: Agent's state, rock's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 5 \n    return reward",
    "rejected": "def find_closest_point(agents, obstacles, agent_state, agent_speed, target, max_dist=50, min_dist=-max_dist, speed_factor=3):\n    closest_points = []\n    for i, point in enumerate(obstacles): \n        dist_from_agent = calculate_distance(agent_state, np.array(point)) \n        if dist_from_agent < min_dist: \n            continue \n         prev_point = np.array(np.where((np.linalg.norm(points) > 4)))"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect thin branches and avoid collision. \n Conditions: Agent's state, branche's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, branches_pos, goal_pos): \n    dist_branches = np.linalg.norm(agent_pos - branches_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_branches < 1.0: \n        return -10 \n    reward = -dist_goal \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collision with thick tree branches. \n Conditions: Agent's state, tree branche's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - penalty \n    return reward",
    "rejected": "def apply_rule(agents, obstacles):\n        Applies a rule that will allow an In-Depth Exploration of the indoors using drones."
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (people) and avoid collision. \n Conditions: Agent's state, people's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect cliffs and avoid collision. \n Conditions: Agent's state, cliff's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 5 \n    return reward",
    "rejected": "def check_collision(): \n        pass"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, human's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, human_pos, goal_pos): \n    dist_human = np.linalg.norm(agent_pos - human_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_human < 1.0: \n        return -10 \n    reward = -dist_goal \n    return reward",
    "rejected": "def apply_rule(agents, obstacles):\n        Applies a rule that will allow an In-Depth Exploration of the indoors using drones."
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid any static obstacle such as food serving counters. \n Conditions: Agent's state, food counter's state are accessible. \n Write a reward in Python.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with chairs. \n Conditions: Agent's state, chair's states are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def check_collision(): \n        pass"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with table. \n Conditions: Agent's state, table's state are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 5 \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with trash bins. \n Conditions: Agent's state trash bin's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, bin_pos, goal_pos): \n    dist_bin = np.linalg.norm(agent_pos - bin_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_bin < 1.0: \n        return -10 \n    reward = -dist_goal \n    return reward",
    "rejected": "def apply_rule(agents, obstacles):\n        Applies a rule that will allow an In-Depth Exploration of the indoors using drones."
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with low-hanging decorations. \n Conditions: Agent's state, hanging decoration's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with plants. \n Conditions: Agent's state, plant's state are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "drones = [] \n obs = [] \n    for i, obj in enumerate(mesa['objects']): \n        if 'drone' in str(obj) or 'droner' in str(obj): \n            drones.append(i) \n        elif 'obstacle' in str(obj) or 'object' in str(obj): \n            obs.append(i)"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with metal rails. \n Conditions: Agent's state, metal rail's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 5 \n    return reward",
    "rejected": "def check_collision(): \n        pass"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with cameras. \n Conditions: Agent's state, camera's state are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, camera_pos, goal_pos): \n    dist_camera = np.linalg.norm(agent_pos - camera_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_camera < 1.0: \n        return -10 \n    reward = -dist_goal \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone in-door navigation in a classroom. \n Objectives: Prevent contact with the people. \n Conditions: Agent's state, people's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - penalty \n    return reward",
    "rejected": "The challenge here isnt about finding the shortest route; rather, its about ensuring that no two robots ever meet due to collisions. This may seem like a simple problem, but applying rules such as avoiding collisions could lead to very complex scenarios when applied across multiple groups of robots. "
  },
  {
    "prompt": "Mission: Pilot a drone through a classroom. \n Objectives: Navigate without crashing into overhead lamps. \n Conditions: Agent's state, overhead lamp's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect obstacle (tall bookshelves) and avoid collision. \n Conditions: Agent's state, bookshelve's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 5 \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect obstacle (backpacks) and avoid collision. \n Conditions: Agent's state, backpack's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, backpack_pos, goal_pos): \n    dist_backpack = np.linalg.norm(agent_pos - backpack_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_backpack < 1.0: \n        return -10 \n    reward = -dist_goal \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state, people's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - penalty \n    return reward",
    "rejected": "def check_collision(): \n        pass"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Avoid collision with desk. \n Conditions: Agent's state, desk's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "Core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect sidboards and do not crash in it \n Conditions: Agent's state, sidboard's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 5 \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect bench and avoid collision. \n Conditions: Agent's state, bench's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, bench_pos, goal_pos): \n    dist_bench = np.linalg.norm(agent_pos - bench_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_bench < 1.0: \n        return -10 \n    reward = -dist_goal \n    return reward",
    "rejected": "drones = [] \n obs = [] \n    for i, obj in enumerate(mesa['objects']): \n        if 'drone' in str(obj) or 'droner' in str(obj): \n            drones.append(i) \n        elif 'obstacle' in str(obj) or 'object' in str(obj): \n            obs.append(i)"
  },
  {
    "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: There are hanging equipments do not collied with it. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - penalty \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Avoid collision with laptops. \n Conditions: Agent's state and laptop's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def find_closest_point(agents, obstacles, agent_state, agent_speed, target, max_dist=50, min_dist=-max_dist, speed_factor=3):\n    closest_points = []\n    for i, point in enumerate(obstacles): \n        dist_from_agent = calculate_distance(agent_state, np.array(point)) \n        if dist_from_agent < min_dist: \n            continue \n         prev_point = np.array(np.where((np.linalg.norm(points) > 4)))"
  },
  {
    "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: There a tablets in the room. Do not collide with them \n Conditions: Agent's state, tablet's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 5 \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state, people's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, people_pos, goal_pos): \n    dist_people = np.linalg.norm(agent_pos - people_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_people < 1.0: \n        return -10 \n    reward = -dist_goal \n    return reward",
    "rejected": "drones = [] \n obs = [] \n    for i, obj in enumerate(mesa['objects']): \n        if 'drone' in str(obj) or 'droner' in str(obj): \n            drones.append(i) \n        elif 'obstacle' in str(obj) or 'object' in str(obj): \n            obs.append(i)"
  },
  {
    "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (workers) and avoid collision. \n Conditions: Agent's state, workers's state and wanted velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - penalty \n    return reward",
    "rejected": "def find_closest_point(agents, obstacles, agent_state, agent_speed, target, max_dist=50, min_dist=-max_dist, speed_factor=3):\n    closest_points = []\n    for i, point in enumerate(obstacles): \n        dist_from_agent = calculate_distance(agent_state, np.array(point)) \n        if dist_from_agent < min_dist: \n            continue \n         prev_point = np.array(np.where((np.linalg.norm(points) > 4)))"
  },
  {
    "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Avoid collison with high storage racks. \n Conditions: Agent's state and storage rack's information and wanted velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Avoid collison with forklifts. \n Conditions: Agent's state and forklift's information and wanted velocity are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 5 \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison such as hanging cables. \n Conditions: Agent's state and cable's information and wanted velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, cable_pos, goal_pos): \n    dist_cable = np.linalg.norm(agent_pos - cable_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_cable < 1.0: \n        return -10 \n    reward = -dist_goal \n    return reward",
    "rejected": "def find_closest_point(agents, obstacles, agent_state, agent_speed, target, max_dist=50, min_dist=-max_dist, speed_factor=3):\n    closest_points = []\n    for i, point in enumerate(obstacles): \n        dist_from_agent = calculate_distance(agent_state, np.array(point)) \n        if dist_from_agent < min_dist: \n            continue \n         prev_point = np.array(np.where((np.linalg.norm(points) > 4)))"
  },
  {
    "prompt": "Mission:Operate the drone in a warehouse. \n Objectives: Avoid collison with stacks of pallets. \n Conditions: Agent's state and pallet's information and wanted velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - penalty \n    return reward",
    "rejected": "The challenge here isnt about finding the shortest route; rather, its about ensuring that no two robots ever meet due to collisions. This may seem like a simple problem, but applying rules such as avoiding collisions could lead to very complex scenarios when applied across multiple groups of robots. "
  },
  {
    "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with overhead pipes. \n Conditions: Agent's state and pipe's information and wanted velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Detect people and avoid collison with overhead beams. \n Conditions: Agent's states and overhead beam information and wanted velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 5 \n    return reward",
    "rejected": "def apply_rule (state, action):\n        if 'Collision' in state:\n            print(\"You collided with\", end=\" \")\n            print(action)\n        else:\n            print('In your turn!')"
  },
  {
    "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with people. \n Conditions: Agent's state, people's state and wanted velocity are  obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, people_pos, goal_pos): \n    dist_people = np.linalg.norm(agent_pos - people_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_people < 1.0: \n        return -10 \n    reward = -dist_goal \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with spotlamps. \n Conditions: Agent's state and spotlamp information and wanted velocity are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - penalty \n    return reward",
    "rejected": "def find_closest_point(agents, obstacles, agent_state, agent_speed, target, max_dist=50, min_dist=-max_dist, speed_factor=3):\n    closest_points = []\n    for i, point in enumerate(obstacles): \n        dist_from_agent = calculate_distance(agent_state, np.array(point)) \n        if dist_from_agent < min_dist: \n            continue \n         prev_point = np.array(np.where((np.linalg.norm(points) > 4)))"
  },
  {
    "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with balconies. \n Conditions: Agent's state, balcony's state and wanted velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Detect people and avoid collison with audience seating. \n Conditions: Agent's states and audience seating information and wanted velocity are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 5 \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with projectors. \n Conditions: Agent's state, projectors's state and wanted velocity are  obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, projector_pos, goal_pos): \n    dist_projector = np.linalg.norm(agent_pos - projector_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_projector < 1.0: \n        return -10 \n    reward = -dist_goal \n    return reward",
    "rejected": "def apply_rule (state, action):\n        if 'Collision' in state:\n            print(\"You collided with\", end=\" \")\n            print(action)\n        else:\n            print('In your turn!')"
  },
  {
    "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Avoid collision with cabels. \n Conditions: Agent's state, cabel's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - penalty \n    return reward",
    "rejected": "def reward(agent_pos, cabels_pos, goal_pos): \n    distance = agent_pos - cabels_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Drone explores the theater. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state and people's state and wanted velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def find_closest_point(agents, obstacles, agent_state, agent_speed, target, max_dist=50, min_dist=-max_dist, speed_factor=3):\n    closest_points = []\n    for i, point in enumerate(obstacles): \n        dist_from_agent = calculate_distance(agent_state, np.array(point)) \n        if dist_from_agent < min_dist: \n            continue \n         prev_point = np.array(np.where((np.linalg.norm(points) > 4)))"
  },
  {
    "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Detect static obstacle (overhead platforms) and avoid collision. \n Conditions: Agent's state, platform's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 5 \n    return reward",
    "rejected": "def find_closest_point(agents, obstacles, agent_state, agent_speed, target, max_dist=50, min_dist=-max_dist, speed_factor=3):\n    closest_points = []\n    for i, point in enumerate(obstacles): \n        dist_from_agent = calculate_distance(agent_state, np.array(point)) \n        if dist_from_agent < min_dist: \n            continue \n         prev_point = np.array(np.where((np.linalg.norm(points) > 4)))"
  },
  {
    "prompt": "Mission: Drone explores the theater. \n Objectives: Detect static obstacle (drapes) and avoid collision. \n Conditions: Agent's state, drape's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, drape_pos, goal_pos): \n    dist_drape = np.linalg.norm(agent_pos - drape_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_drape < 1.0: \n        return -10 \n    reward = -dist_goal \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone explores the theater. \n Objectives: Detect dynamic obstacle (hangings microphones) and avoid collision. \n Conditions: Agent's state, microphone's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - penalty \n    return reward",
    "rejected": "drones = [] \n obs = [] \n    for i, obj in enumerate(mesa['objects']): \n        if 'drone' in str(obj) or 'droner' in str(obj): \n            drones.append(i) \n        elif 'obstacle' in str(obj) or 'object' in str(obj): \n            obs.append(i)"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, human's state and wanted velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "The challenge here isnt about finding the shortest route; rather, its about ensuring that no two robots ever meet due to collisions. This may seem like a simple problem, but applying rules such as avoiding collisions could lead to very complex scenarios when applied across multiple groups of robots. "
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect  dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state, people's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 5 \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect static obstacle (basket hoops) and avoid collision. \n Conditions: Agent's state, basket hoop's state and desired velocity are usable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, basket_hoop_pos, goal_pos): \n    dist_basket_hoop = np.linalg.norm(agent_pos - basket_hoop_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_basket_hoop < 1.0: \n        return -10 \n    reward = -dist_goal \n    return reward",
    "rejected": "drones = [] \n obs = [] \n    for i, obj in enumerate(mesa['objects']): \n        if 'drone' in str(obj) or 'droner' in str(obj): \n            drones.append(i) \n        elif 'obstacle' in str(obj) or 'object' in str(obj): \n            obs.append(i)"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with volleyball nets. \n Conditions: Agent's state, volleyball net's state and desired velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - penalty \n    return reward",
    "rejected": "def find_drones(mesa): \n    drones = [] \n    for i, obj in enumerate(np.random.choice(10, size=(len(mesa), 2))): \n        if (np.linalg.norm(obj[0] - mesa) < np.linalg.norm(obj[1]) + np.linalg.norm(drones[-1][0])): \n            drones.append((i, np.array([obj[0], obj[1]])))"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with loose balls. \n Conditions: Agent's state, ball's state and desired velocity are usable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def reward(agent_pos, loose_balls_pos, goal_pos): \n    distance = agent_pos - loose_balls_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect dynamic obstacle (doctors) and avoid collision. \n Conditions: Agent's state, doctor's state and desired velocity are usable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 5 \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect static obstacle (wires) and avoid collision. \n Conditions: Agent's state, v's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, wire_pos, goal_pos): \n    dist_wire = np.linalg.norm(agent_pos - wire_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_wire < 1.0: \n        return -10 \n    reward = -dist_goal \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone explores the hospital. \n Objectives: Avoid collision with medical equipment stands. \n Conditions: Agent's state, medical equipment's state and desired velocity are usable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -d_goal - penalty \n    return reward",
    "rejected": "drones = [] \n obs = [] \n    for i, obj in enumerate(mesa['objects']): \n        if 'drone' in str(obj) or 'droner' in str(obj): \n            drones.append(i) \n        elif 'obstacle' in str(obj) or 'object' in str(obj): \n            obs.append(i)"
  },
  {
    "prompt": "Mission: Drone explores the hospital. \n Objectives: Avoid  any collision with wall-mounted emergency devices. \n Conditions: Agent's state, emergency device's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def find_drone(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Operate the drone in a hospital. \n Objectives:  Avoid collision with lamps. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * to_goal\n    if to_obs < 1.1: \n        reward -= 5 \n    return reward",
    "rejected": "def reward(agent_pos, lamps_pos, goal_pos): \n    distance = agent_pos - lamps_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },


  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with chairs. \n Conditions: Agent's state and chair's status are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, chair_pos, goal_pos): \n    dist_chair = np.linalg.norm(agent_pos - chair_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_chair < 1.0: \n        return -10 \n    reward = -math.exp(dist_goal) \n    return reward",
    "rejected": "The challenge here isnt about finding the shortest route; rather, its about ensuring that no two robots ever meet due to collisions. This may seem like a simple problem, but applying rules such as avoiding collisions could lead to very complex scenarios when applied across multiple groups of robots. "
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with chair. \n Conditions: Agent's state and chair's status are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -math.exp(d_goal) - penalty \n    return reward",
    "rejected": "def reward(agent_pos, chair_pos, goal_pos): \n    distance = agent_pos - chair_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with ceiling fan blades. \n Conditions: Agent's state and staic fan blades's status are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def reward(pos, obs, goal): \n    baldes = 'clear'\n    if blades == 'clear': \n        return 1\n    return -1"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives:Avoid collision such as hanging lamps. \n Conditions: Agent's state and lamp's status are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * math.exp(to_goal)\n    if to_obs < 1.1: \n        reward -= 5 \n    return reward",
    "rejected": "def reward(agent_pos, hanging_lamps_pos, goal_pos): \n    distance = agent_pos - hanging_lamps_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with curtain. \n Conditions: Agent's state and static curtain's status are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, curtain_pos, goal_pos): \n    dist_curtain = np.linalg.norm(agent_pos - curtain_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_chair < 1.0: \n        return -10 \n    reward = -math.exp(dist_goal) \n    return reward",
    "rejected": "def reward(agent_pos, curtain_pos, goal_pos): \n    distance = agent_pos - curtain_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with avoid drapes. \n Conditions: Agent's state and drape's status are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -math.exp(d_goal) - penalty \n    return reward",
    "rejected": "import math \n    nvel = int((max_vel * v0) / (math.sqrt(dt ** 2 + v0**2))) \n    return nvel"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with obstacles. \n Conditions: Agent's state and obstacle's status are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def apply_squadron(robber): \n        if robber.state == \"idle\" and robbed: \n            print(\"You have been caught by the squadrons.\") \n            return False \n        elif robber.state != \"idle\": \n            print(\"The squadrons detected you as an intruder!\") \n            return True"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with static obstacle (wardrobes). \n Conditions: Agent's state, wardrobe's state are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * math.exp(to_goal)\n    if to_obs < 1.1: \n        reward -= 5 \n    return reward",
    "rejected": "import math \n    nvel = int((max_vel * v0) / (math.sqrt(dt ** 2 + v0**2))) \n    return nvel"
  },
  {
    "prompt": "Mission: Drone in-door navigation and reach other side of the small room. \n Objectives: Do not collide with the walls of the room. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, wall_pos, goal_pos): \n    dist_wall = np.linalg.norm(agent_pos - wall_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_wall < 1.0: \n        return -10 \n    reward = -math.exp(dist_goal) \n    return reward",
    "rejected": "The challenge here isnt about finding the shortest route; rather, its about ensuring that no two robots ever meet due to collisions. This may seem like a simple problem, but applying rules such as avoiding collisions could lead to very complex scenarios when applied across multiple groups of robots. "
  },
  {
    "prompt": "Mission: Guide the drone through an indoor space to the far side of a small room. \n Objectives: Prevent contact with the people. \n Conditions: Agent's state, obstacle's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -math.exp(d_goal) - penalty \n    return reward",
    "rejected": "def enumerate(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Avoid hitting the surrounding walls. \n Conditions: Agent's state, obstacle's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def apply_rule(agents, obstacles):\n        Applies a rule that will allow an In-Depth Exploration of the indoors using drones."
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Navigate without crashing into walls. \n Conditions: Agent's state, obstacle's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * math.exp(to_goal)\n    if to_obs < 1.1: \n        reward -= 5 \n    return reward",
    "rejected": "def enumerate(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect obstacle (chairs) and avoid collision. \n Conditions: Agent's state, chair's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, chair_pos, goal_pos): \n    dist_chair = np.linalg.norm(agent_pos - chair_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_chair < 1.0: \n        return -10 \n    reward = -math.exp(dist_goal) \n    return reward",
    "rejected": "def check_collision(): \n        pass"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (table) and avoid collision. \n Conditions: Agent's state, table's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -math.exp(d_goal) - penalty \n    return reward",
    "rejected": "def enumerate(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (bookshelves) and avoid collision. \n Conditions: Agent's state, bookshelve's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def apply_rule(agents, obstacles):\n        Applies a rule that will allow an In-Depth Exploration of the indoors using drones."
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (smoke detectors) and avoid collision. \n Conditions: Agent's state, smoke detector's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * math.exp(to_goal)\n    if to_obs < 1.1: \n        reward -= 5 \n    return reward",
    "rejected": "def check_collision(): \n        pass"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (stackes books) and avoid collision. \n Conditions: Agent's state, book's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, book_pos, goal_pos): \n    dist_book = np.linalg.norm(agent_pos - book_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_book < 1.0: \n        return -10 \n    reward = -math.exp(dist_goal) \n    return reward",
    "rejected": "def enumerate(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, human's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -math.exp(d_goal) - penalty \n    return reward",
    "rejected": "The challenge here isnt about finding the shortest route; rather, its about ensuring that no two robots ever meet due to collisions. This may seem like a simple problem, but applying rules such as avoiding collisions could lead to very complex scenarios when applied across multiple groups of robots. "
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, human's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def enumerate(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect static obstacle (trees) and avoid collision. \n Conditions: Agent's state, tree's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * math.exp(to_goal)\n    if to_obs < 1.1: \n        reward -= 5 \n    return reward",
    "rejected": "def apply_rule(agents, obstacles):\n        Applies a rule that will allow an In-Depth Exploration of the indoors using drones."
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (leaves) and avoid collision. \n Conditions: Agent's state, leaves's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, leaves_pos, goal_pos): \n    dist_leaves = np.linalg.norm(agent_pos - leaves_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_leaves < 1.0: \n        return -10 \n    reward = -math.exp(dist_goal) \n    return reward",
    "rejected": "import math \n    nvel = int((max_vel * v0) / (math.sqrt(dt ** 2 + v0**2))) \n    return nvel"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect birds and avoid collision. \n Conditions: Agent's state, bird's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -math.exp(d_goal) - penalty \n    return reward",
    "rejected": "def enumerate(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collision with stumps. \n Conditions: Agent's state, stump's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def reward(agent_pos, stumps_pos, goal_pos): \n    distance = agent_pos - stumps_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (rocks) and avoid collision. \n Conditions: Agent's state, rock's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * math.exp(to_goal)\n    if to_obs < 1.1: \n        reward -= 5 \n    return reward",
    "rejected": "def find_closest_point(agents, obstacles, agent_state, agent_speed, target, max_dist=50, min_dist=-max_dist, speed_factor=3):\n    closest_points = []\n    for i, point in enumerate(obstacles): \n        dist_from_agent = calculate_distance(agent_state, np.array(point)) \n        if dist_from_agent < min_dist: \n            continue \n         prev_point = np.array(np.where((np.linalg.norm(points) > 4)))"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect thin branches and avoid collision. \n Conditions: Agent's state, branche's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, branches_pos, goal_pos): \n    dist_branches = np.linalg.norm(agent_pos - branches_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_branches < 1.0: \n        return -10 \n    reward = -math.exp(dist_goal) \n    return reward",
    "rejected": "def enumerate(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collision with thick tree branches. \n Conditions: Agent's state, tree branche's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -math.exp(d_goal) - penalty \n    return reward",
    "rejected": "def apply_rule(agents, obstacles):\n        Applies a rule that will allow an In-Depth Exploration of the indoors using drones."
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (people) and avoid collision. \n Conditions: Agent's state, people's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def enumerate(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect cliffs and avoid collision. \n Conditions: Agent's state, cliff's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * math.exp(to_goal)\n    if to_obs < 1.1: \n        reward -= 5 \n    return reward",
    "rejected": "def check_collision(): \n        pass"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, human's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, human_pos, goal_pos): \n    dist_human = np.linalg.norm(agent_pos - human_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_human < 1.0: \n        return -10 \n    reward = -math.exp(dist_goal) \n    return reward",
    "rejected": "def apply_rule(agents, obstacles):\n        Applies a rule that will allow an In-Depth Exploration of the indoors using drones."
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid any static obstacle such as food serving counters. \n Conditions: Agent's state, food counter's state are accessible. \n Write a reward in Python.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -math.exp(d_goal) - penalty \n    return reward",
    "rejected": "The challenge here isnt about finding the shortest route; rather, its about ensuring that no two robots ever meet due to collisions. This may seem like a simple problem, but applying rules such as avoiding collisions could lead to very complex scenarios when applied across multiple groups of robots. "
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with chairs. \n Conditions: Agent's state, chair's states are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def check_collision(): \n        pass"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with table. \n Conditions: Agent's state, table's state are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * math.exp(to_goal)\n    if to_obs < 1.1: \n        reward -= 5 \n    return reward",
    "rejected": "def enumerate(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with trash bins. \n Conditions: Agent's state trash bin's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, bin_pos, goal_pos): \n    dist_bin = np.linalg.norm(agent_pos - bin_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_bin < 1.0: \n        return -10 \n    reward = -math.exp(dist_goal) \n    return reward",
    "rejected": "def apply_rule(agents, obstacles):\n        Applies a rule that will allow an In-Depth Exploration of the indoors using drones."
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with low-hanging decorations. \n Conditions: Agent's state, hanging decoration's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -math.exp(d_goal) - penalty \n    return reward",
    "rejected": "def enumerate(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with plants. \n Conditions: Agent's state, plant's state are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "drones = [] \n obs = [] \n    for i, obj in enumerate(mesa['objects']): \n        if 'drone' in str(obj) or 'droner' in str(obj): \n            drones.append(i) \n        elif 'obstacle' in str(obj) or 'object' in str(obj): \n            obs.append(i)"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with metal rails. \n Conditions: Agent's state, metal rail's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * math.exp(to_goal)\n    if to_obs < 1.1: \n        reward -= 5 \n    return reward",
    "rejected": "def check_collision(): \n        pass"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with cameras. \n Conditions: Agent's state, camera's state are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, camera_pos, goal_pos): \n    dist_camera = np.linalg.norm(agent_pos - camera_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_camera < 1.0: \n        return -10 \n    reward = -math.exp(dist_goal) \n    return reward",
    "rejected": "def enumerate(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone in-door navigation in a classroom. \n Objectives: Prevent contact with the people. \n Conditions: Agent's state, people's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -math.exp(d_goal) - penalty \n    return reward",
    "rejected": "import math \n    nvel = int((max_vel * v0) / (math.sqrt(dt ** 2 + v0**2))) \n    return nvel"
  },
  {
    "prompt": "Mission: Pilot a drone through a classroom. \n Objectives: Navigate without crashing into overhead lamps. \n Conditions: Agent's state, overhead lamp's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def enumerate(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect obstacle (tall bookshelves) and avoid collision. \n Conditions: Agent's state, bookshelve's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * math.exp(to_goal)\n    if to_obs < 1.1: \n        reward -= 5 \n    return reward",
    "rejected": "def enumerate(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect obstacle (backpacks) and avoid collision. \n Conditions: Agent's state, backpack's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, backpack_pos, goal_pos): \n    dist_backpack = np.linalg.norm(agent_pos - backpack_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_backpack < 1.0: \n        return -10 \n    reward = -math.exp(dist_goal) \n    return reward",
    "rejected": "def enumerate(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state, people's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -math.exp(d_goal) - penalty \n    return reward",
    "rejected": "The challenge here isnt about finding the shortest route; rather, its about ensuring that no two robots ever meet due to collisions. This may seem like a simple problem, but applying rules such as avoiding collisions could lead to very complex scenarios when applied across multiple groups of robots. "
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Avoid collision with desk. \n Conditions: Agent's state, desk's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "Core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core core"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect sidboards and do not crash in it \n Conditions: Agent's state, sidboard's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * math.exp(to_goal)\n    if to_obs < 1.1: \n        reward -= 5 \n    return reward",
    "rejected": "def enumerate(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect bench and avoid collision. \n Conditions: Agent's state, bench's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, bench_pos, goal_pos): \n    dist_bench = np.linalg.norm(agent_pos - bench_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_bench < 1.0: \n        return -10 \n    reward = -math.exp(dist_goal) \n    return reward",
    "rejected": "drones = [] \n obs = [] \n    for i, obj in enumerate(mesa['objects']): \n        if 'drone' in str(obj) or 'droner' in str(obj): \n            drones.append(i) \n        elif 'obstacle' in str(obj) or 'object' in str(obj): \n            obs.append(i)"
  },
  {
    "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: There are hanging equipments do not collied with it. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -math.exp(d_goal) - penalty \n    return reward",
    "rejected": "def enumerate(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Avoid collision with laptops. \n Conditions: Agent's state and laptop's state are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def find_closest_point(agents, obstacles, agent_state, agent_speed, target, max_dist=50, min_dist=-max_dist, speed_factor=3):\n    closest_points = []\n    for i, point in enumerate(obstacles): \n        dist_from_agent = calculate_distance(agent_state, np.array(point)) \n        if dist_from_agent < min_dist: \n            continue \n         prev_point = np.array(np.where((np.linalg.norm(points) > 4)))"
  },
  {
    "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: There a tablets in the room. Do not collide with them \n Conditions: Agent's state, tablet's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * math.exp(to_goal)\n    if to_obs < 1.1: \n        reward -= 5 \n    return reward",
    "rejected": "def enumerate(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state, people's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, people_pos, goal_pos): \n    dist_people = np.linalg.norm(agent_pos - people_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_people < 1.0: \n        return -10 \n    reward = -math.exp(dist_goal) \n    return reward",
    "rejected": "drones = [] \n obs = [] \n    for i, obj in enumerate(mesa['objects']): \n        if 'drone' in str(obj) or 'droner' in str(obj): \n            drones.append(i) \n        elif 'obstacle' in str(obj) or 'object' in str(obj): \n            obs.append(i)"
  },
  {
    "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (workers) and avoid collision. \n Conditions: Agent's state, workers's state and wanted velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -math.exp(d_goal) - penalty \n    return reward",
    "rejected": "def find_closest_point(agents, obstacles, agent_state, agent_speed, target, max_dist=50, min_dist=-max_dist, speed_factor=3):\n    closest_points = []\n    for i, point in enumerate(obstacles): \n        dist_from_agent = calculate_distance(agent_state, np.array(point)) \n        if dist_from_agent < min_dist: \n            continue \n         prev_point = np.array(np.where((np.linalg.norm(points) > 4)))"
  },
  {
    "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Avoid collison with high storage racks. \n Conditions: Agent's state and storage rack's information and wanted velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def enumerate(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Avoid collison with forklifts. \n Conditions: Agent's state and forklift's information and wanted velocity are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * math.exp(to_goal)\n    if to_obs < 1.1: \n        reward -= 5 \n    return reward",
    "rejected": "def enumerate(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison such as hanging cables. \n Conditions: Agent's state and cable's information and wanted velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, cable_pos, goal_pos): \n    dist_cable = np.linalg.norm(agent_pos - cable_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_cable < 1.0: \n        return -10 \n    reward = -math.exp(dist_goal) \n    return reward",
    "rejected": "def find_closest_point(agents, obstacles, agent_state, agent_speed, target, max_dist=50, min_dist=-max_dist, speed_factor=3):\n    closest_points = []\n    for i, point in enumerate(obstacles): \n        dist_from_agent = calculate_distance(agent_state, np.array(point)) \n        if dist_from_agent < min_dist: \n            continue \n         prev_point = np.array(np.where((np.linalg.norm(points) > 4)))"
  },
  {
    "prompt": "Mission:Operate the drone in a warehouse. \n Objectives: Avoid collison with stacks of pallets. \n Conditions: Agent's state and pallet's information and wanted velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -math.exp(d_goal) - penalty \n    return reward",
    "rejected": "import math \n    nvel = int((max_vel * v0) / (math.sqrt(dt ** 2 + v0**2))) \n    return nvel"
  },
  {
    "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with overhead pipes. \n Conditions: Agent's state and pipe's information and wanted velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def enumerate(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Detect people and avoid collison with overhead beams. \n Conditions: Agent's states and overhead beam information and wanted velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * math.exp(to_goal)\n    if to_obs < 1.1: \n        reward -= 5 \n    return reward",
    "rejected": "def apply_rule (state, action):\n        if 'Collision' in state:\n            print(\"You collided with\", end=\" \")\n            print(action)\n        else:\n            print('In your turn!')"
  },
  {
    "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with people. \n Conditions: Agent's state, people's state and wanted velocity are  obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, people_pos, goal_pos): \n    dist_people = np.linalg.norm(agent_pos - people_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_people < 1.0: \n        return -10 \n    reward = -math.exp(dist_goal) \n    return reward",
    "rejected": "The challenge here isnt about finding the shortest route; rather, its about ensuring that no two robots ever meet due to collisions. This may seem like a simple problem, but applying rules such as avoiding collisions could lead to very complex scenarios when applied across multiple groups of robots. "
  },
  {
    "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with spotlamps. \n Conditions: Agent's state and spotlamp information and wanted velocity are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -math.exp(d_goal) - penalty \n    return reward",
    "rejected": "def find_closest_point(agents, obstacles, agent_state, agent_speed, target, max_dist=50, min_dist=-max_dist, speed_factor=3):\n    closest_points = []\n    for i, point in enumerate(obstacles): \n        dist_from_agent = calculate_distance(agent_state, np.array(point)) \n        if dist_from_agent < min_dist: \n            continue \n         prev_point = np.array(np.where((np.linalg.norm(points) > 4)))"
  },
  {
    "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with balconies. \n Conditions: Agent's state, balcony's state and wanted velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def enumerate(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Detect people and avoid collison with audience seating. \n Conditions: Agent's states and audience seating information and wanted velocity are available. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * math.exp(to_goal)\n    if to_obs < 1.1: \n        reward -= 5 \n    return reward",
    "rejected": "def enumerate(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with projectors. \n Conditions: Agent's state, projectors's state and wanted velocity are  obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, projector_pos, goal_pos): \n    dist_projector = np.linalg.norm(agent_pos - projector_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_projector < 1.0: \n        return -10 \n    reward = -math.exp(dist_goal) \n    return reward",
    "rejected": "def apply_rule (state, action):\n        if 'Collision' in state:\n            print(\"You collided with\", end=\" \")\n            print(action)\n        else:\n            print('In your turn!')"
  },
  {
    "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Avoid collision with cabels. \n Conditions: Agent's state, cabel's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -math.exp(d_goal) - penalty \n    return reward",
    "rejected": "def reward(agent_pos, cabels_pos, goal_pos): \n    distance = agent_pos - cabels_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Drone explores the theater. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state and people's state and wanted velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def find_closest_point(agents, obstacles, agent_state, agent_speed, target, max_dist=50, min_dist=-max_dist, speed_factor=3):\n    closest_points = []\n    for i, point in enumerate(obstacles): \n        dist_from_agent = calculate_distance(agent_state, np.array(point)) \n        if dist_from_agent < min_dist: \n            continue \n         prev_point = np.array(np.where((np.linalg.norm(points) > 4)))"
  },
  {
    "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Detect static obstacle (overhead platforms) and avoid collision. \n Conditions: Agent's state, platform's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * math.exp(to_goal)\n    if to_obs < 1.1: \n        reward -= 5 \n    return reward",
    "rejected": "def find_closest_point(agents, obstacles, agent_state, agent_speed, target, max_dist=50, min_dist=-max_dist, speed_factor=3):\n    closest_points = []\n    for i, point in enumerate(obstacles): \n        dist_from_agent = calculate_distance(agent_state, np.array(point)) \n        if dist_from_agent < min_dist: \n            continue \n         prev_point = np.array(np.where((np.linalg.norm(points) > 4)))"
  },
  {
    "prompt": "Mission: Drone explores the theater. \n Objectives: Detect static obstacle (drapes) and avoid collision. \n Conditions: Agent's state, drape's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, drape_pos, goal_pos): \n    dist_drape = np.linalg.norm(agent_pos - drape_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_drape < 1.0: \n        return -10 \n    reward = -math.exp(dist_goal) \n    return reward",
    "rejected": "def enumerate(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone explores the theater. \n Objectives: Detect dynamic obstacle (hangings microphones) and avoid collision. \n Conditions: Agent's state, microphone's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -math.exp(d_goal) - penalty \n    return reward",
    "rejected": "drones = [] \n obs = [] \n    for i, obj in enumerate(mesa['objects']): \n        if 'drone' in str(obj) or 'droner' in str(obj): \n            drones.append(i) \n        elif 'obstacle' in str(obj) or 'object' in str(obj): \n            obs.append(i)"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, human's state and wanted velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "import math \n    nvel = int((max_vel * v0) / (math.sqrt(dt ** 2 + v0**2))) \n    return nvel"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect  dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state, people's state are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * math.exp(to_goal)\n    if to_obs < 1.1: \n        reward -= 5 \n    return reward",
    "rejected": "The challenge here isnt about finding the shortest route; rather, its about ensuring that no two robots ever meet due to collisions. This may seem like a simple problem, but applying rules such as avoiding collisions could lead to very complex scenarios when applied across multiple groups of robots. "
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect static obstacle (basket hoops) and avoid collision. \n Conditions: Agent's state, basket hoop's state and desired velocity are usable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, basket_hoop_pos, goal_pos): \n    dist_basket_hoop = np.linalg.norm(agent_pos - basket_hoop_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_basket_hoop < 1.0: \n        return -10 \n    reward = -math.exp(dist_goal) \n    return reward",
    "rejected": "drones = [] \n obs = [] \n    for i, obj in enumerate(mesa['objects']): \n        if 'drone' in str(obj) or 'droner' in str(obj): \n            drones.append(i) \n        elif 'obstacle' in str(obj) or 'object' in str(obj): \n            obs.append(i)"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with volleyball nets. \n Conditions: Agent's state, volleyball net's state and desired velocity are obtainable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -math.exp(d_goal) - penalty \n    return reward",
    "rejected": "def enumerates(mesa): \n    drones = [] \n    for i, obj in enumerate(np.random.choice(10, size=(len(mesa), 2))): \n        if (np.linalg.norm(obj[0] - mesa) < np.linalg.norm(obj[1]) + np.linalg.norm(drones[-1][0])): \n            drones.append((i, np.array([obj[0], obj[1]])))"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with loose balls. \n Conditions: Agent's state, ball's state and desired velocity are usable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def reward(agent_pos, loose_balls_pos, goal_pos): \n    distance = agent_pos - loose_balls_pos\n    if distance.all() == 0: \n        reward = 100\n    else: \n        reward = -1\n    return reward"
  },
  {
    "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect dynamic obstacle (doctors) and avoid collision. \n Conditions: Agent's state, doctor's state and desired velocity are usable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * math.exp(to_goal)\n    if to_obs < 1.1: \n        reward -= 5 \n    return reward",
    "rejected": "def enumerate(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect static obstacle (wires) and avoid collision. \n Conditions: Agent's state, v's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(agent_pos, wire_pos, goal_pos): \n    dist_wire = np.linalg.norm(agent_pos - wire_pos) \n    dist_goal = np.linalg.norm(agent_pos - goal_pos) \n    if dist_wire < 1.0: \n        return -10 \n    reward = -math.exp(dist_goal) \n    return reward",
    "rejected": "def enumerate(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Drone explores the hospital. \n Objectives: Avoid collision with medical equipment stands. \n Conditions: Agent's state, medical equipment's state and desired velocity are usable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    d_obs = np.linalg.norm(pos - obs)\n    d_goal = np.linalg.norm(pos - goal) \n    penalty = np.exp(-d_obs) * 10 \n    reward = -math.exp(d_goal) - penalty \n    return reward",
    "rejected": "drones = [] \n obs = [] \n    for i, obj in enumerate(mesa['objects']): \n        if 'drone' in str(obj) or 'droner' in str(obj): \n            drones.append(i) \n        elif 'obstacle' in str(obj) or 'object' in str(obj): \n            obs.append(i)"
  },
  {
    "prompt": "Mission: Drone explores the hospital. \n Objectives: Avoid  any collision with wall-mounted emergency devices. \n Conditions: Agent's state, emergency device's state and wanted velocity are accessible. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(p, obstacle, goal): \n    goal_reward = -np.linalg.norm(p - goal) \n    collision_risk = -50 if np.linalg.norm(p - obstacle) < 0.8 else 0\n    reward = goal_reward  + collision_risk \n    return reward",
    "rejected": "def enumerate(): \n    return best_drone \n\n    def find_obstacle(): \n        return best_obstacle"
  },
  {
    "prompt": "Mission: Operate the drone in a hospital. \n Objectives:  Avoid collision with lamps. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python. Only return the code. Do not explain.",
    "chosen": "import numpy as np\n    def reward(pos, obs, goal): \n    to_obs = np.linalg.norm(pos - obs)\n    to_goal = np.linalg.norm(pos - goal) \n    reward = -1.5 * math.exp(to_goal)\n    if to_obs < 1.1: \n        reward -= 5 \n    return reward",
    "rejected": "The challenge here isnt about finding the shortest route; rather, its about ensuring that no two robots ever meet due to collisions. This may seem like a simple problem, but applying rules such as avoiding collisions could lead to very complex scenarios when applied across multiple groups of robots. "
  }
]
