[
    {
        "prompt": "Scenario: Drone indoor exploration in a room. \n Aim: Avoid collision with ceiling fan blades. \n Assumption: Agent's state and static fan blades's status are available. \n Implement a reward function in Python",
        "chosen": "def reward(s_agent, s_blade, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_blade = calculate_distance_min(s_agent, s_blade) \n    penalty_collision = -15 if (d_agent_blade < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Drone indoor exploration in a room. \n Aim: Avoid collision with chair. \n Assumption: Agent's state and chair's status are available. \n Implement a reward function in Python",
        "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -15 if (d_agent_chair < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Drone indoor exploration in a room. \n Aim: Avoid collision with hanging lamps. \n Assumption: Agent's state and lamp's status are available. \n Implement a reward function in Python",
        "chosen": "def reward(s_agent, s_lamp, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lamp = calculate_distance_min(s_agent, s_lamp) \n    penalty_collision = -15 if (d_agent_lamp < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Drone indoor exploration in a room. \n Aim: Avoid collision with curtain. \n Assumption: Agent's state and static curtain's status available. \n Implement a reward function in Python",
        "chosen": "def reward(s_agent, s_curtain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_curtain = calculate_distance_min(s_agent, s_curtain) \n    penalty_collision = -15 if (d_agent_curtain < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Drone indoor exploration in a room. \n Aim: Avoid collision with obstacles. \n Assumption: Agent's state and obstacles's status are available. \n Implement a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Drone indoor exploration in a room. \n Aim: Avoid collision with drapes. \n Assumption: Agent's state and drape's status are available. \n Implement a reward function in Python",
        "chosen": "def reward(s_agent, s_drape, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_drape = calculate_distance_min(s_agent, s_drape) \n    penalty_collision = -15 if (d_agent_drape < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Drone indoor exploration in a room. \n Aim: Avoid collision with static obstacle (wardrobe). \n Assumption: Agent's state, wardrobe's state are available. \n Implement a reward function in Python",
        "chosen": "def reward(s_agent, s_wardrobe, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wardrobe = calculate_distance_min(s_agent, s_wardrobe) \n    penalty_collision = -15 if (d_agent_wardrobe < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Drone in-door navigation and reach other side of the small room. \n Aim: Do not collide with the walls of the room. \n Assumption: Agent's state, wall's information accessible. \n Implement a reward function in Python",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -15 if (d_agent_wall < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Guide the drone through an indoor space to the far side of a small room. \n Aim: Prevent contact with the people. \n Assumption: Agent's state, people's state accessible. \n Implement a reward function in Python",
        "chosen": "def reward(s_agent, s_people, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_people = calculate_distance_min(s_agent, s_people) \n    penalty_collision = -15 if (d_agent_people < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Guide the drone through an indoor space such as a small room. \n Aim: Prevent contact with chairs and tables. \n Assumption: Agent's state, obstacles' state accessible. \n Implement a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Guide the drone through an indoor space such as a small room. \n Aim: Prevent contact with fan blades and curtains. \n Assumption: Agent's state, obstacles' state accessible. \n Implement a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Guide the drone through an indoor space such as a small room. \n Aim: Prevent contact with drapes and people. \n Assumption: Agent's state, obstacles' state accessible. \n Implement a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Guide the drone through an indoor space such as a small room. \n Aim: Prevent contact with lamps and projectors. \n Assumption: Agent's state, obstacles' state accessible. \n Implement a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Guide the drone through an indoor space such as a small room. \n Aim: Prevent contact with boxes and chairs. \n Assumption: Agent's state, obstacles' state accessible. \n Implement a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Guide the drone through an indoor space such as a small room. \n Aim: Avoid collision with boxes and chairs. \n Assumption: Agent's state, obstacles' state accessible. \n Implement a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Guide the drone through an indoor space such as a small room. \n Aim: Prevent contact with humans and chairs. \n Assumption: Agent's state, obstacles' state accessible. \n Implement a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Guide the drone through an indoor space such as a small room. \n Aim: Prevent contact with humans, chairs and table. \n Assumption: Agent's state, obstacles' state accessible. \n Implement a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Guide the drone through an indoor space such as a small room. \n Aim: Prevent contact with humans, projector and bench. \n Assumption: Agent's state, obstacles' state accessible. \n Implement a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Guide the drone through an indoor space such as a small room. \n Aim: Prevent contact with people, boxes and bench. \n Assumption: Agent's state, obstacles' state accessible. \n Implement a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Guide the drone through an indoor space such as a small room. \n Aim: Prevent contact with shelf, plants and curtains. \n Assumption: Agent's state, obstacles' state accessible. \n Implement a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Avoid hitting the surrounding walls. \n Conditions: Agent's state, wall's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -15 if (d_agent_wall < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Navigate without crashing into walls. \n Conditions: Agent's state, obstacle's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -15 if (d_agent_wall < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect obstacle (chairs) and avoid collision. \n Conditions: Agent's state, chair's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -15 if (d_agent_chair < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect obstacle (table) and avoid collision. \n Conditions: Agent's state, table's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_table, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_table_min = calculate_distance_min(s_agent, s_table) \n    penalty_collision = -15 if (d_agent_table_min < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect bookshelves and avoid collision. \n Conditions: Agent's state, bookshelve's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bookshelve, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bookshelve = calculate_distance_min(s_agent, s_bookshelve) \n    penalty_collision = -15 if (d_agent_bookshelve < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect smoke detectors and avoid collision. \n Conditions: Agent's state, smoke detector's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_smoke_detector, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_smoke_detectore = calculate_distance_min(s_agent, s_smoke_detector) \n    penalty_collision = -15 if (d_agent_smoke_detector < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect obstacle (stacked books) and avoid collision. \n Conditions: Agent's state, book's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_book, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_book = calculate_distance_min(s_agent, s_book) \n    penalty_collision = -15 if (d_agent_book < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, human's state are accessible. \n Write a reward function in Python",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -15 if (d_agent_human < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect chairs and tabels and avoid collision. \n Conditions: Agent's state, obstacles' states are accessible. \n Write a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect students and shelves and avoid collision. \n Conditions: Agent's state, obstacles' states are accessible. \n Write a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Avoid collisions with lamps and plants. \n Conditions: Agent's state, obstacles' states are accessible. \n Write a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Avoid collisions with decorations and tall shelves. \n Conditions: Agent's state, obstacles' states are accessible. \n Write a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Avoid collisions with stack books and lamps. \n Conditions: Agent's state, obstacles' states are accessible. \n Write a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Avoid collisions with books and lights. \n Conditions: Agent's state, obstacles' states are accessible. \n Write a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Avoid collisions with books, lights and humans. \n Conditions: Agent's state, obstacles' states are accessible. \n Write a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Avoid collisions with chair, table and humans. \n Conditions: Agent's state, obstacles' states are accessible. \n Write a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Avoid collisions with desks, lamps and students. \n Conditions: Agent's state, obstacles' states are accessible. \n Write a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Avoid collisions with stacked books, computers and staffs. \n Conditions: Agent's state, obstacles' states are accessible. \n Write a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Avoid collisions with stacked books, laptops and book carts. \n Conditions: Agent's state, obstacles' states are accessible. \n Write a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Avoid collisions with  curtains, plants and staffs. \n Conditions: Agent's state, obstacles' states are accessible. \n Write a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, human's state obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -15 if (d_agent_human < 1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect static obstacle such as a tree and avoid collision. \n Conditions: Agent's state, tree's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_tree, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tree = calculate_distance_min(s_agent, s_tree) \n    penalty_collision = -15 if (d_agent_tree < 1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle such as a leaf and avoid collision. \n Conditions: Agent's state, leaf's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_leaf, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_leaf = calculate_distance_min(s_agent, s_leaf) \n    penalty_collision = -15 if (d_agent_leaf < 1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect birds and avoid collision. \n Conditions: Agent's state, bird's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bird, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bird = calculate_distance_min(s_agent, s_bird) \n    penalty_collision = -15 if (d_agent_bird < 1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collision with stumps. \n Conditions: Agent's state and stump's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_stump, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_stump = calculate_distance_min(s_agent, s_stump) \n    penalty_collision = -15 if (d_agent_stump < 1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle such as a rock and avoid collision. \n Conditions: Agent's state, rock's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_rock, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rock = calculate_distance_min(s_agent, s_rock) \n    penalty_collision = -15 if (d_agent_rock < 1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect thin branches and avoid collision. \n Conditions: Agent's state, branche's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_branch, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_branch = calculate_distance_min(s_agent, s_branch) \n    penalty_collision = -15 if (d_agent_branch < 1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collision with thick tree branches. \n Conditions: Agent's state, tree branche's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_tree_branch, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tree_branch = calculate_distance_min(s_agent, s_tree_branch) \n    penalty_collision = -15 if (d_agent_tree_branch < 1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect people and avoid collision. \n Conditions: Agent's state, people's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_people, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_people = calculate_distance_min(s_agent, s_people) \n    penalty_collision = -15 if (d_agent_people < 1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect stones and avoid collision. \n Conditions: Agent's state and stone's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_stone, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_stone = calculate_distance_min(s_agent, s_stone) \n    penalty_collision = -15 if (d_agent_stone < 1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect stones and trees and avoid collision. \n Conditions: Agent's state and obstacles' states are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect branches and trees and avoid collision. \n Conditions: Agent's state and obstacles' states are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect humans and animals and avoid collision. \n Conditions: Agent's state and obstacles' states are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect leaves and trees avoid collision. \n Conditions: Agent's state and obstacles' states are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collisions with the many trees. \n Conditions: Agent's state and obstacles' states are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collisions with trees, branches and humans. \n Conditions: Agent's state and obstacles' states are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collisions with stumps, birds and humans. \n Conditions: Agent's state and obstacles' states are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collisions with trees, animals and stones. \n Conditions: Agent's state and obstacles' states are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collisions with maple trees, cliffs and signs. \n Conditions: Agent's state and obstacles' states are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collisions with maple trees, cliffs and signs. \n Conditions: Agent's state and obstacles' states are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor mensa environment using a drone.  \n targets: Successfully reach the designated target state, Detect humans as dynamic obstacles, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of nearby humans, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -15 if (d_agent_human < 1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor mensa environment using a drone.  \n targets: Successfully reach the designated target state, Detect lamp, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of lamp, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_lamp, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lamp = calculate_distance_min(s_agent, s_lamp) \n    penalty_collision = -15 if (d_agent_lamp < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor mensa environment using a drone.  \n targets: Successfully reach the designated target state, Detect counter, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of counter, and the target state can all be accessed.  \n Write a python code for reward function",
        "chosen": "def reward(s_agent, s_counter, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_counter = calculate_distance_min(s_agent, s_counter) \n    penalty_collision = -15 if (d_agent_counter < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor mensa environment using a drone.  \n targets: Successfully reach the designated target state, Detect chair, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of chair, and the target state can all be accessed.  \n Write a python code for reward function",
        "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -15 if (d_agent_chair < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor mensa environment using a drone.  \n targets: Successfully reach the designated target state, Detect table, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of table, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_table, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_table = calculate_distance_min(s_agent, s_table) \n    penalty_collision = -15 if (d_agent_table < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor mensa environment using a drone.  \n targets: Successfully reach the designated target state, Detect trash bin and Avoid collisions.  \n Assumptions: The drone’s own state, the states of trash bin, and the target state can all be accessed.  \n Write a python code for reward function",
        "chosen": "def reward(s_agent, s_trash_bin, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_trash_bin = calculate_distance_min(s_agent, s_trash_bin) \n    penalty_collision = -15 if (d_agent_trash_bin < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor mensa environment using a drone.  \n targets: Successfully reach the designated target state, Detect decoration and Avoid collisions.  \n Assumptions: The drone’s own state, the states of decoration, and the target state can all be accessed.  \n Write a python code for reward function",
        "chosen": "def reward(s_agent, s_decoration, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_decoration = calculate_distance_min(s_agent, s_decoration) \n    penalty_collision = -15 if (d_agent_decoration < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor mensa environment using a drone.  \n targets: Successfully reach the designated target state, Detect plant and Avoid collisions.  \n Assumptions: The drone’s own state, the states of plant, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_plant, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_plant = calculate_distance_min(s_agent, s_plant) \n    penalty_collision = -15 if (d_agent_plant < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor mensa environment using a drone.  \n targets: Successfully reach the designated target state, Detect metal rail and Avoid collisions.  \n Assumptions: The drone’s own state, the states of rail, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_rail, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rail = calculate_distance_min(s_agent, s_rail) \n    penalty_collision = -15 if (d_agent_rail < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor mensa environment using a drone.  \n targets: Successfully reach the designated target state, Detect camera, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of camera, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_cameras, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cameras = calculate_distance_min(s_agent, s_cameras) \n    penalty_collision = -15 if (d_agent_cameras < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor mensa environment using a drone.  \n targets: Successfully reach the designated target state, Detect people and table, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of multiple obstacles, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor mensa environment using a drone.  \n targets: Successfully reach the designated target state, Detect students and chair, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of multiple obstacles, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor mensa environment using a drone.  \n targets: Successfully reach the designated target state, Detect plants and chair, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of multiple obstacles, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor mensa environment using a drone.  \n targets: Successfully reach the designated target state, Detect plant and desk, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of multiple obstacles, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor mensa environment using a drone.  \n targets: Successfully reach the designated target state, Detect decoration and plants, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of multiple obstacles, and the target state can all be accessed.  \n Write a python code for reward function",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor mensa environment using a drone.  \n targets: Successfully reach the designated target state, Detect students and table, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of multiple obstacles, and the target state can all be accessed.  \n Write a python code for reward function",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor mensa environment using a drone.  \n targets: Successfully reach the designated target state, Detect chair, plant and table, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of multiple obstacles, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor mensa environment using a drone.  \n targets: Successfully reach the designated target state, Detect staff, student and table, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of multiple obstacles, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor mensa environment using a drone.  \n targets: Successfully reach the designated target state, Detect counter, rails and camera, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of multiple obstacles, and the target state can all be accessed.  \n Write a python code for reward function",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor mensa environment using a drone.  \n targets: Successfully reach the designated target state, Detect food counter, trash bin and staff, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of multiple obstacles, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone in-door navigation in a classroom. \n Objectives: Do not collide with the a wall of the room. \n Conditions: Agent's state, wall's location are avialable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -15 if (d_agent_wall < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone in-door navigation in a classroom. \n Objectives: Prevent contact with the people. \n Conditions: Agent's state and people's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_people, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_people = calculate_distance_min(s_agent, s_people) \n    penalty_collision = -15 if (d_agent_people < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Operate the drone in a classroom. \n Objectives: Avoid hitting the ceiling projector. \n Conditions: Agent's state and projector's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_projector, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_projector = calculate_distance_min(s_agent, s_projector) \n    penalty_collision = -15 if (d_agent_projector < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Pilot a drone through a classroom. \n Objectives: Navigate without crashing into overhead lamps. \n Conditions: Agent's state, overhead lamp's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_lamp, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lamp = calculate_distance_min(s_agent, s_lamp) \n    penalty_collision = -15 if (d_agent_lamp < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect static obstacle (blackboard) and avoid collision. \n Conditions: Agent's state, blackboard's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_blackboard, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_blackboard = calculate_distance_min(s_agent, s_blackboard) \n    penalty_collision = -15 if (d_agent_blackboard < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect obstacle (tall bookshelves) and avoid collision. \n Conditions: Agent's state, bookshelve's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bookshelve, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bookshelve = calculate_distance_min(s_agent, s_bookshelve) \n    penalty_collision = -15 if (d_agent_bookshelve < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect obstacle (backpack) and avoid collision. \n Conditions: Agent's state and backpack's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_backpack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_backpack = calculate_distance_min(s_agent, s_backpack) \n    penalty_collision = -15 if (d_agent_backpack < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Do not collide with human. \n Conditions: Agent's state and human's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -15 if (d_agent_human < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect dynamic obstacle (a teacher) and avoid collision. \n Conditions: Agent's state and teacher's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_teacher, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_teacher = calculate_distance_min(s_agent, s_teacher) \n    penalty_collision = -15 if (d_agent_teacher < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect dynamic obstacle (a student) and avoid collision. \n Conditions: Agent's state and student's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_student, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_student = calculate_distance_min(s_agent, s_student) \n    penalty_collision = -15 if (d_agent_student < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Avoid collision with student and teacher. \n Conditions: Agent's state and student's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Avoid collision with chair and desk. \n Conditions: Agent's state and student's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Avoid collision with student and backpack. \n Conditions: Agent's state and student's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Avoid collision with shelves and backpack. \n Conditions: Agent's state and student's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Avoid collision with blackboards and whiteboards. \n Conditions: Agent's state and student's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Avoid collision with teacher, blackboards and whiteboards. \n Conditions: Agent's state and student's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Avoid collision with teacher, pupils and desk. \n Conditions: Agent's state and student's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Avoid collision with shelves, chair and table. \n Conditions: Agent's state and student's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Avoid collision with shelves, chair and table. \n Conditions: Agent's state and student's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Avoid collision with three boxes. \n Conditions: Agent's state and student's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a conference room. The drone must detect and avoid moving obstacles (people) while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of people in the room.",
        "chosen": "def reward(s_agent, s_people, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_people = calculate_distance_min(s_agent, s_people) \n    penalty_collision = -15 if (d_agent_people < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a conference room. The drone must detect and avoid desk while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of desk in the room.",
        "chosen": "def reward(s_agent, s_desk, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_desk = calculate_distance_min(s_agent, s_desk) \n    penalty_collision = -15 if (d_agent_desk < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a conference room. The drone must detect and avoid sideboard while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of sideboard in the room.",
        "chosen": "def reward(s_agent, s_sideboard, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sideboard = calculate_distance_min(s_agent, s_sideboard) \n    penalty_collision = -15 if (d_agent_sideboard < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a conference room. The drone must detect and avoid bench while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of bench in the room.",
        "chosen": "def reward(s_agent, s_bench, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bench = calculate_distance_min(s_agent, s_bench) \n    penalty_collision = -15 if (d_agent_bench < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a conference room. The drone must detect and avoid whiteboard while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of whiteboard in the room.",
        "chosen": "def reward(s_agent, s_whiteboard, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_whiteboard = calculate_distance_min(s_agent, s_whiteboard) \n    penalty_collision = -15 if (d_agent_sideboard < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a conference room. The drone must detect and avoid projector while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of projector in the room.",
        "chosen": "def reward(s_agent, s_projector, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_projector = calculate_distance_min(s_agent, s_projector) \n    penalty_collision = -15 if (d_agent_projector < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a conference room. The drone must detect and avoid equipment while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of equipment in the room.",
        "chosen": "def reward(s_agent, s_equipment, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_equipment = calculate_distance_min(s_agent, s_equipment) \n    penalty_collision = -15 if (d_agent_equipment < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a conference room. The drone must detect and avoid speaker while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of speaker in the room.",
        "chosen": "def reward(s_agent, s_speaker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_speaker = calculate_distance_min(s_agent, s_speaker) \n    penalty_collision = -15 if (d_agent_speaker < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a conference room. The drone must detect and avoid laptop while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of laptop in the room.",
        "chosen": "def reward(s_agent, s_laptop, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_laptop = calculate_distance_min(s_agent, s_laptop) \n    penalty_collision = -15 if (d_agent_laptop < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a conference room. The drone must detect and avoid screen while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of screen in the room.",
        "chosen": "def reward(s_agent, s_screen, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_screen = calculate_distance_min(s_agent, s_screen) \n    penalty_collision = -15 if (d_agent_screen < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a conference room. The drone must detect and avoid screen and desk while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of multiple obstacles in the room.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a conference room. The drone must detect and avoid table and chair while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of multiple obstacles in the room.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked which piloting inside a conference room. The drone must detect and avoid computer and person while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of multiple obstacles in the room.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked which piloting inside a conference room. The drone must detect and avoid plant and shelf while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of multiple obstacles in the room.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked which piloting inside a conference room. The drone must detect and avoid two boxes while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of multiple obstacles in the room.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked which piloting inside a conference room. The drone must detect and avoid table, chair and table while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of multiple obstacles in the room.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked which piloting inside a conference room. The drone must detect and avoid laptop, shelf and plant while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of multiple obstacles in the room.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked which piloting inside a conference room. The drone must detect and avoid shelf, box and person while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of multiple obstacles in the room",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked which piloting inside a conference room. The drone must detect and avoid whiteboard, screen and plant while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of multiple obstacles in the room.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked which piloting inside a conference room. The drone must detect and avoid multiple boxes while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of multiple obstacles in the room.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "ission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (one staff) and avoid collision. \n Conditions: Agent's state, staff's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_staff = calculate_distance_min(s_agent, s_staff) \n    penalty_collision = -15 if (d_agent_staff < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (a worker) and avoid collision. \n Conditions: Agent's state and worker's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_worker = calculate_distance_min(s_agent, s_worker) \n    penalty_collision = -15 if (d_agent_worker < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (a human) and avoid collision. \n Conditions: Agent's state, human's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -15 if (d_agent_human < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (a worker) and avoid collision. \n Conditions: Agent's state and worker's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_worker = calculate_distance_min(s_agent, s_worker) \n    penalty_collision = -15 if (d_agent_worker < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Avoid collison with high storage racks. \n Conditions: Agent's state and storage rack's information are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rack = calculate_distance_min(s_agent, s_rack) \n    penalty_collision = -15 if (d_agent_rack < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Avoid collison with a forklift . \n Conditions: Agent's state and forklift's information are available. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_forklift, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_forklift = calculate_distance_min(s_agent, s_forklift) \n    penalty_collision = -15 if (d_agent_forklift < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with a hanging cable. \n Conditions: Agent's state and cable's information are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_cable, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cable = calculate_distance_min(s_agent, s_cable) \n    penalty_collision = -15 if (d_agent_cable < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission:Operate the drone in a warehouse. \n Objectives: Avoid collison with a stack of pallets. \n Conditions: Agent's state and pallet's information are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_pallet, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_pallet = calculate_distance_min(s_agent, s_pallet) \n    penalty_collision = -15 if (d_agent_pallet < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with a package. \n Conditions: Agent's state and package's information are available. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_package, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_package = calculate_distance_min(s_agent, s_package) \n    penalty_collision = -15 if (d_agent_package < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with overhead pipes. \n Conditions: Agent's state and pipe's information are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_pipe, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_pipe = calculate_distance_min(s_agent, s_pipe) \n    penalty_collision = -15 if (d_agent_pipe < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Detect the overhead beam and do not collide with it. \n Conditions: Agent's states and overhead beam information are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_beam, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_pipe = calculate_distance_min(s_agent, s_beam) \n    penalty_collision = -15 if (d_agent_beam < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Detect the shelves and human and do not collide with them. \n Conditions: Agent's states and overhead beam information are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Detect the pakets and staffs and do not collide with them. \n Conditions: Agent's states and overhead beam information are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Detect the stackes boxes and do not collide with them. \n Conditions: Agent's states and overhead beam information are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Detect the maschines and staffs and do not collide with them. \n Conditions: Agent's states and overhead beam information are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Detect the beams and hanging lamps and do not collide with them. \n Conditions: Agent's states and overhead beam information are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Detect the cable, rack and people and do not collide with them. \n Conditions: Agent's states and overhead beam information are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Detect the desk, chair and computer and do not collide with them. \n Conditions: Agent's states and overhead beam information are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Detect the beam, pipes and lamps and do not collide with them. \n Conditions: Agent's states and overhead beam information are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Detect the forklift, stairs and rails and do not collide with them. \n Conditions: Agent's states and overhead beam information are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a theater. The drone must detect people, avoid collisions with them, and use the available agent and people states.",
        "chosen": "def reward(s_agent, s_people, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_people = calculate_distance_min(s_agent, s_people) \n    penalty_collision = -15 if (d_agent_people < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a theater. The drone must detect spotlamp, avoid collisions with them and reach target, and use the available agent and spotlmap and agent states.",
        "chosen": "def reward(s_agent, s_spotlamp, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_spotlamp = calculate_distance_min(s_agent, s_spotlamp) \n    penalty_collision = -15 if (d_agent_spotlamp < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a theater. The drone must detect balcony, avoid collisions with them and reach target, and use the available agent and balcony and agent states.",
        "chosen": "def reward(s_agent, s_balcony, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_balcony = calculate_distance_min(s_agent, s_balcony) \n    penalty_collision = -15 if (d_agent_balcony < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a theater. The drone must detect seat, avoid collisions with them and reach target, and use the available agent and seat and agent states.",
        "chosen": "def reward(s_agent, s_seat, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_seat = calculate_distance_min(s_agent, s_seat) \n    penalty_collision = -15 if (d_agent_seat < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a theater. The drone must detect projector, avoid collisions with them and reach target, and use the available agent and projector and agent states.",
        "chosen": "def reward(s_agent, s_projector, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_projector = calculate_distance_min(s_agent, s_projector) \n    penalty_collision = -15 if (d_agent_projector < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a theater. The drone must detect cable, avoid collisions with them and reach target, and use the available agent and cable and agent states.",
        "chosen": "def reward(s_agent, s_cable, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cable = calculate_distance_min(s_agent, s_cable) \n    penalty_collision = -15 if (d_agent_cable < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a theater. The drone must detect human, avoid collisions with them and reach target, and use the available agent and human and agent states.",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -15 if (d_agent_human < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a theater. The drone must detect platform, avoid collisions with them and reach target, and use the available agent and platform and agent states.",
        "chosen": "def reward(s_agent, s_platform, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_platform = calculate_distance_min(s_agent, s_platform) \n    penalty_collision = -15 if (d_agent_platform < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a theater. The drone must detect drape, avoid collisions with them and reach target, and use the available agent and drape and agent states.",
        "chosen": "def reward(s_agent, s_drape, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_drape = calculate_distance_min(s_agent, s_drape) \n    penalty_collision = -15 if (d_agent_drape < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a theater. The drone must detect microphone, avoid collisions with them and reach target, and use the available agent and microphone and agent states.",
        "chosen": "def reward(s_agent, s_microphone, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_microphone = calculate_distance_min(s_agent, s_microphone) \n    penalty_collision = -15 if (d_agent_microphone < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a theater. The drone must detect spotlamp and projector, avoid collisions with them and reach target, and use the available agent and multiple obstacles and agent states.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a theater. The drone must detect chair and actor, avoid collisions with them and reach target, and use the available agent and multiple obstacles and agent states.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a theater. The drone must detect stage and drape, avoid collisions with them and reach target, and use the available agent and multiple obstacles and agent states.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a theater. The drone must detect microphone and music box, avoid collisions with them and reach target, and use the available agent and multiple obstacles and agent states.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a theater. The drone must detect staff and visitor, avoid collisions with them and reach target, and use the available agent and multiple obstacles and agent states.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a theater. The drone must detect chandler, rope and stage, avoid collisions with them and reach target, and use the available agent and multiple obstacles and agent states.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a theater. The drone must detect table, chair and human, avoid collisions with them and reach target, and use the available agent and multiple obstacles and agent states.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a theater. The drone must detect three boxes and projector, avoid collisions with them and reach target, and use the available agent and multiple obstacles and agent states.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a theater. The drone must detect beam, balcony railing and spotlight, avoid collisions with them and reach target, and use the available agent and multiple obstacles and agent states.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a theater. The drone must detect platform, microphone and drape, avoid collisions with them and reach target, and use the available agent and multiple obstacles and agent states.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a theater. The drone must detect visitor seat and box, avoid collisions with them and reach target, and use the available agent and multiple obstacles and agent states.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, human's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -15 if (d_agent_human < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect  dynamic obstacle (person) and avoid collision. \n Conditions: Agent's state, person's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_person, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_person = calculate_distance_min(s_agent, s_person) \n    penalty_collision = -15 if (d_agent_person < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect the static obstacle basket hoop and avoid collision. \n Conditions: Agent's state, basket hoop's state are usable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_basket_hoop, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_basket_hoop = calculate_distance_min(s_agent, s_basket_hoop) \n    penalty_collision = -15 if (d_agent_basket_hoop < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect the scoreboard and avoid collision. \n Conditions: Agent's state, scoreboard's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_scoreboard, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_scoreboard = calculate_distance_min(s_agent, s_scoreboard) \n    penalty_collision = -15 if (d_agent_scoreboard < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect the climbing rope and avoid collision. \n Conditions: Agent's state and climbing rope's state are usable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_rope, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rope = calculate_distance_min(s_agent, s_rope) \n    penalty_collision = -15 if (d_agent_rope < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with volleyball nets. \n Conditions: Agent's state and net's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_net, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_net = calculate_distance_min(s_agent, s_net) \n    penalty_collision = -15 if (d_agent_net < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with hanging flags. \n Conditions: Agent's state and flag's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_flag, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_flag = calculate_distance_min(s_agent, s_flag) \n    penalty_collision = -15 if (d_agent_flag < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (moving players) and avoid collision. \n Conditions: Agent's state and player's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_player, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_player = calculate_distance_min(s_agent, s_player) \n    penalty_collision = -15 if (d_agent_player < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with a ball. \n Conditions: Agent's state, ball's state are usable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ball, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ball = calculate_distance_min(s_agent, s_ball) \n    penalty_collision = -15 if (d_agent_ball < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle flying ball and avoid collision. \n Conditions: Agent's state, ball's state are usable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ball, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ball = calculate_distance_min(s_agent, s_ball) \n    penalty_collision = -15 if (d_agent_ball < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (player) and avoid collision. \n Conditions: Agent's state and player's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_player, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_player = calculate_distance_min(s_agent, s_player) \n    penalty_collision = -15 if (d_agent_player < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with ball and player. \n Conditions: Agent's state and obstacles state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with rope and net. \n Conditions: Agent's state and obstacles state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with coach and flag. \n Conditions: Agent's state and obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with basketball and rope. \n Conditions: Agent's state and obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with basketball and net. \n Conditions: Agent's state and obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with basketball and net. \n Conditions: Agent's state and obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with coach, basketball and net. \n Conditions: Agent's state and obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with player, ball and rope. \n Conditions: Agent's state and obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with flag, student and net. \n Conditions: Agent's state and obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect the patient and avoid collision. \n Conditions: Agent's state and patient's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_patient, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_patient = calculate_distance_min(s_agent, s_patient) \n    penalty_collision = -15 if (d_agent_patient < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect dynamic obstacle such as a doctor and avoid collision. \n Conditions: Agent's state and doctor's state are usable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_doctor, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_doctor = calculate_distance_min(s_agent, s_doctor) \n    penalty_collision = -15 if (d_agent_doctor < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect the wire and avoid collision. \n Conditions: Agent's state and wire's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_wire, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wire = calculate_distance_min(s_agent, s_wire) \n    penalty_collision = -15 if (d_agent_wire < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone explores the hospital. \n Objectives: Avoid collision with a medical equipment. \n Conditions: Agent's state and equipment's state are usable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_equipment, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_equipment = calculate_distance_min(s_agent, s_equipment) \n    penalty_collision = -15 if (d_agent_equipment < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect the wheelchair and avoid collisions. \n Conditions: Agent's state and wheelchair's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_wheelchair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wheelchair = calculate_distance_min(s_agent, s_wheelchair) \n    penalty_collision = -15 if (d_agent_wheelchair < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect the staff and avoid collision. \n Conditions: Agent's state, staff's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_staff = calculate_distance_min(s_agent, s_staffr) \n    penalty_collision = -15 if (d_agent_staff < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Avoid collision with the defibrillator. \n Conditions: Agent's state and defibrillator's state are usable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_defibrillator, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_defibrillator = calculate_distance_min(s_agent, s_defibrillator) \n    penalty_collision = -15 if (d_agent_defibrillator < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone explores the hospital. \n Objectives: Avoid  any collision with wall-mounted emergency devices. \n Conditions: Agent's state and device's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_device, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_device = calculate_distance_min(s_agent, s_device) \n    penalty_collision = -15 if (d_agent_device < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Operate the drone in a hospital. \n Objectives:  Avoid collision with the lamp. \n Conditions: Agent's state and lamp's state are usable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_lamp, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lamp = calculate_distance_min(s_agent, s_lamp) \n    penalty_collision = -15 if (d_agent_lamp < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect the staff and patient avoid collision. \n Conditions: Agent's statea and obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect the defibrillator and equipment avoid collision. \n Conditions: Agent's statea and obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect the wire and wheelchair avoid collision. \n Conditions: Agent's statea and obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect the patient and wheelchair avoid collision. \n Conditions: Agent's statea and obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect the staff and lamp avoid collision. \n Conditions: Agent's statea and obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect the three packages and avoid collision. \n Conditions: Agent's statea and obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect the doctor, the patient and the wheelchair and avoid collision. \n Conditions: Agent's statea and obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect the defibrillator, the equipment and the wheelchair and avoid collision. \n Conditions: Agent's statea and obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect the lamp, the bed and the screen and avoid collision. \n Conditions: Agent's statea and obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect the counter, the nurse and the patient and avoid collision. \n Conditions: Agent's statea and obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect  are three boxes and avoid collision. \n Conditions: Agent's statea and obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a room, avoids ceiling fan blades, and aims to reach a target using its state and avoid hitting blade with using blade's status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_blade, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_blade = calculate_distance_min(s_agent, s_blade) \n    penalty_collision = -15 if (d_agent_blade < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a room, avoids chair, and aims to reach a target using its state and avoid hitting clair with using chair status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -15 if (d_agent_chair < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a room, avoids lamp, and aims to reach a target using its state and avoid hitting lamp with using lamp status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_lamp, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lamp = calculate_distance_min(s_agent, s_lamp) \n    penalty_collision = -15 if (d_agent_lamp < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a room, avoids curtain, and aims to reach a target using its state and avoid hitting curtain with using curtain status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_curtain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_curtain = calculate_distance_min(s_agent, s_curtain) \n    penalty_collision = -15 if (d_agent_curtain < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a room, avoids obstacles, and aims to reach a target using its state and avoid hitting obstacles with using obstacles status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a room, avoids drape, and aims to reach a target using its state and avoid hitting drape with using drape status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_drape, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_drape = calculate_distance_min(s_agent, s_drape) \n    penalty_collision = -15 if (d_agent_drape < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a room, avoids wardrobe, and aims to reach a target using its state and avoid hitting wardrobe with using wardrobe status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_wardrobe, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wardrobe = calculate_distance_min(s_agent, s_wardrobe) \n    penalty_collision = -15 if (d_agent_wardrobe < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a room, avoids wall, and aims to reach a target using its state and avoid hitting wall with using wall status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -15 if (d_agent_wall < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a room, avoids ceiling people, and aims to reach a target using its state and avoid hitting people with using people status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_people, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_people = calculate_distance_min(s_agent, s_people) \n    penalty_collision = -15 if (d_agent_people < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigate inside a small room, avoids obstacles, and aims to reach a target using its state and avoid hitting obstcales with using obstacles status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigate inside a big room, avoids obstacles, and aims to reach a target using its state and avoid hitting obstcales with using obstacles status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Guide the drone through an indoor space such as a small room. \n Objectives: Prevent contact with drapes and people. \n Conditions: Agent's state, obstacles' state accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a small room, aims to reach a target, and avoids hitting obstacles such as lamps and projectors using its state and multiple obstacle status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a small room, aims to reach a target, and avoids hitting obstacles such as boxes and chairs using its state and multiple obstacle status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a small room, aims to reach a target, and avoids hitting obstacles such as boxes and table using its state and multiple obstacle status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a small room, aims to reach a target, and avoids hitting obstacles such as human and chair using its state and multiple obstacle status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a small room, aims to reach a target, and avoids hitting obstacles such as human and chair, table using its state and multiple obstacle status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a small room, aims to reach a target, and avoids hitting obstacles such as human, bench and projectors using its state and multiple obstacle status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a small room, aims to reach a target, and avoids hitting obstacles such as people, box and bench using its state and multiple obstacle status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a small room, aims to reach a target, and avoids hitting obstacles such as shelf, plant and curtain using its state and multiple obstacle status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a library, attempts to reach a target, and must prevent collisions with surrounding walls using drone state, the wall status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -15 if (d_agent_wall < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a library, attempts to reach a target, and must prevent collisions with surrounding opposite walls using drone state, the wall status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -15 if (d_agent_wall < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a library, attempts to reach a target, and must prevent collisions with chair using drone state, the chair state, and target state. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -15 if (d_agent_chair < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a library, attempts to reach a target, and must prevent collisions with table using table state, the chair state, and target state. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_table, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_table_min = calculate_distance_min(s_agent, s_table) \n    penalty_collision = -15 if (d_agent_table_min < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a library, attempts to reach a target, and must prevent collisions with bookshelve using table state, the bookshelf state, and target state. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_bookshelve, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bookshelve = calculate_distance_min(s_agent, s_bookshelve) \n    penalty_collision = -15 if (d_agent_bookshelve < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a library, attempts to reach a target, and must prevent collisions with smoke detector using table state, the smoke detector state, and target state. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_smoke_detector, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_smoke_detectore = calculate_distance_min(s_agent, s_smoke_detector) \n    penalty_collision = -15 if (d_agent_smoke_detector < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a library, attempts to reach a target, and must prevent collisions with stacked book using table state, the book state, and target state. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_book, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_book = calculate_distance_min(s_agent, s_book) \n    penalty_collision = -15 if (d_agent_book < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a library, attempts to reach a target, and must prevent collisions with human using table state, the human state, and target state. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -15 if (d_agent_human < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a library, tries to reach a target, and must avoid crashing into multiple obstacles such as chair, tables using its own state, the multiple obstacles’ status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a library, tries to reach a target, and must avoid crashing into multiple obstacles such as student and shelf using its own state, the multiple obstacles’ status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a library, tries to reach a target, and must avoid crashing into multiple obstacles such as student and lamp using its own state, the multiple obstacles’ status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a library, tries to reach a target, and must avoid crashing into multiple obstacles such as decoration and tall shelfves using its own state, the multiple obstacles’ status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a library, tries to reach a target, and must avoid crashing into multiple obstacles such as stack book and lamp using its own state, the multiple obstacles’ status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a library, tries to reach a target, and must avoid crashing into multiple obstacles such as book and light using its own state, the multiple obstacles’ status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a library, tries to reach a target, and must avoid crashing into multiple obstacles such as student, book and light using its own state, the multiple obstacles’ status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a library, tries to reach a target, and must avoid crashing into multiple obstacles such as chair, table and human using its own state, the multiple obstacles’ status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a library, tries to reach a target, and must avoid crashing into multiple obstacles such as desks, students and book using its own state, the multiple obstacles’ status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a library, tries to reach a target, and must avoid crashing into multiple obstacles such as book, computers and staff using its own state, the multiple obstacles’ status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a library, tries to reach a target, and must avoid crashing into multiple obstacles such as book, laptop and lamp using its own state, the multiple obstacles’ status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a library, tries to reach a target, and must avoid crashing into multiple obstacles such as curtain, book and decor using its own state, the multiple obstacles’ status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a forest. \n Duties: Detect dynamic obstacle (human) and avoid collision. \n Inputs: Agent's state, human's state obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -15 if (d_agent_human < 1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a forest. \n Duties: Detect static obstacle such as a tree and avoid collision. \n Inputs: Agent's state, tree's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_tree, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tree = calculate_distance_min(s_agent, s_tree) \n    penalty_collision = -15 if (d_agent_tree < 1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a forest. \n Duties: Detect obstacle such as a leaf and avoid collision. \n Inputs: Agent's state, leaf's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_leaf, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_leaf = calculate_distance_min(s_agent, s_leaf) \n    penalty_collision = -15 if (d_agent_leaf < 1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a forest. \n Duties: Detect birds and avoid collision. \n Inputs: Agent's state, bird's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bird, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bird = calculate_distance_min(s_agent, s_bird) \n    penalty_collision = -15 if (d_agent_bird < 1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a forest. \n Duties: Avoid collision with stumps. \n Inputs: Agent's state and stump's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_stump, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_stump = calculate_distance_min(s_agent, s_stump) \n    penalty_collision = -15 if (d_agent_stump < 1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a forest. \n Duties: Detect obstacle such as a rock and avoid collision. \n Inputs: Agent's state, rock's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_rock, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rock = calculate_distance_min(s_agent, s_rock) \n    penalty_collision = -15 if (d_agent_rock < 1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a forest. \n Duties: Detect thin branches and avoid collision. \n Inputs: Agent's state, branche's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_branch, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_branch = calculate_distance_min(s_agent, s_branch) \n    penalty_collision = -15 if (d_agent_branch < 1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a forest. \n Duties: Avoid collision with thick tree branches. \n Inputs: Agent's state, tree branche's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_tree_branch, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tree_branch = calculate_distance_min(s_agent, s_tree_branch) \n    penalty_collision = -15 if (d_agent_tree_branch < 1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a forest. \n Duties: Detect people and avoid collision. \n Inputs: Agent's state, people's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_people, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_people = calculate_distance_min(s_agent, s_people) \n    penalty_collision = -15 if (d_agent_people < 1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a forest. \n Duties: Detect stones and avoid collision. \n Inputs: Agent's state and stone's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_stone, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_stone = calculate_distance_min(s_agent, s_stone) \n    penalty_collision = -15 if (d_agent_stone < 1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a forest. \n Duties: Detect stones and trees and avoid collision. \n Inputs: Agent's state and obstacles' states are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a forest. \n Duties: Detect branches and trees and avoid collision. \n Inputs: Agent's state and obstacles' states are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a forest. \n Duties: Detect humans and animals and avoid collision. \n Inputs: Agent's state and obstacles' states are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a forest. \n Duties: Detect leaves and trees avoid collision. \n Inputs: Agent's state and obstacles' states are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a forest. \n Duties: Avoid collisions with the many trees. \n Inputs: Agent's state and obstacles' states are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a forest. \n Duties: Avoid collisions with trees, branches and humans. \n Inputs: Agent's state and obstacles' states are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a forest. \n Duties: Avoid collisions with stumps, birds and humans. \n Inputs: Agent's state and obstacles' states are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a forest. \n Duties: Avoid collisions with trees, animals and stones. \n Inputs: Agent's state and obstacles' states are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a forest. \n Duties: Avoid collisions with maple trees, cliffs and signs. \n Inputs: Agent's state and obstacles' states are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a forest. \n Duties: Avoid collisions with maple trees, cliffs and signs. \n Inputs: Agent's state and obstacles' states are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor mensa with a drone, where the drone must reach the target while detecting humans as dynamic obstacles and avoiding collisions, assuming the agent’s state, human states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -15 if (d_agent_human < 1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor mensa with a drone, where the drone must reach the target while detecting lamps as obstacles and avoiding collisions, assuming the agent’s state, lamp states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_lamp, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lamp = calculate_distance_min(s_agent, s_lamp) \n    penalty_collision = -15 if (d_agent_lamp < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor mensa with a drone, where the drone must reach the target while detecting counter as obstacles and avoiding collisions, assuming the agent’s state, counter states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_counter, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_counter = calculate_distance_min(s_agent, s_counter) \n    penalty_collision = -15 if (d_agent_counter < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor mensa with a drone, where the drone must reach the target while detecting chairs as obstacles and avoiding collisions, assuming the agent’s state, chair states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -15 if (d_agent_chair < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor mensa with a drone, where the drone must reach the target while detecting tables as obstacles and avoiding collisions, assuming the agent’s state, table states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_table, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_table = calculate_distance_min(s_agent, s_table) \n    penalty_collision = -15 if (d_agent_table < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor mensa with a drone, where the drone must reach the target while detecting trash bin as obstacles and avoiding collisions, assuming the agent’s state, trash bin states, and the target state are observable. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_trash_bin, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_trash_bin = calculate_distance_min(s_agent, s_trash_bin) \n    penalty_collision = -15 if (d_agent_trash_bin < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor mensa with a drone, where the drone must reach the target while detecting decoration as obstacles and avoiding collisions, assuming the agent’s state, decoration states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_decoration, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_decoration = calculate_distance_min(s_agent, s_decoration) \n    penalty_collision = -15 if (d_agent_decoration < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor mensa with a drone, where the drone must reach the target while detecting plants as obstacles and avoiding collisions, assuming the agent’s state, plant states, and the target state are observable. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_plant, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_plant = calculate_distance_min(s_agent, s_plant) \n    penalty_collision = -15 if (d_agent_plant < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor mensa with a drone, where the drone must reach the target while detecting metal rail as obstacles and avoiding collisions, assuming the agent’s state, rail states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_rail, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rail = calculate_distance_min(s_agent, s_rail) \n    penalty_collision = -15 if (d_agent_rail < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor mensa with a drone, where the drone must reach the target while detecting cameras as obstacles and avoiding collisions, assuming the agent’s state, camera states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_cameras, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cameras = calculate_distance_min(s_agent, s_cameras) \n    penalty_collision = -15 if (d_agent_cameras < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor mensa with a drone, where the drone must reach the target while detecting obstacles as obstacles and avoiding collisions, assuming the agent’s state, multiple obstacles states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor mensa with a drone, where the drone must reach the target while detecting students and chairs as obstacles and avoiding collisions, assuming the agent’s state, multiple obstacles states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor mensa with a drone, where the drone must reach the target while detecting platns and chairs as obstacles and avoiding collisions, assuming the agent’s state, multiple obstacles states, and the target state are observable. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor mensa with a drone, where the drone must reach the target while detecting platns and desks as obstacles and avoiding collisions, assuming the agent’s state, multiple obstacles states, and the target state are observable. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor mensa with a drone, where the drone must reach the target while detecting decoration and plants as obstacles and avoiding collisions, assuming the agent’s state, multiple obstacles states, and the target state are observable. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor mensa with a drone, where the drone must reach the target while detecting camera, students and chairs as obstacles and avoiding collisions, assuming the agent’s state, multiple obstacles states, and the target state are observable. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor mensa with a drone, where the drone must reach the target while detecting chair, plant and table as obstacles and avoiding collisions, assuming the agent’s state, multiple obstacles states, and the target state are observable. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor mensa with a drone, where the drone must reach the target while detecting students and staff, student and chair as obstacles and avoiding collisions, assuming the agent’s state, multiple obstacles states, and the target state are observable. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor mensa with a drone, where the drone must reach the target while detecting food counter, rails and camera as obstacles and avoiding collisions, assuming the agent’s state, multiple obstacles states, and the target state are observable. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor mensa with a drone, where the drone must reach the target while detecting food counter, trash bin and student as obstacles and avoiding collisions, assuming the agent’s state, multiple obstacles states, and the target state are observable. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a classroom, where it must avoid colliding with the walls and reach the target. The drone’s state, target's state and the information of the walls are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -15 if (d_agent_wall < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a classroom, where it must avoid colliding with the peoples and reach the target. The drone’s state, target's state and the information of the people are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_people, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_people = calculate_distance_min(s_agent, s_people) \n    penalty_collision = -15 if (d_agent_people < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a classroom, where it must avoid colliding with the projector and reach the target. The drone’s state, target's state and the information of the projector are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_projector, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_projector = calculate_distance_min(s_agent, s_projector) \n    penalty_collision = -15 if (d_agent_projector < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a classroom, where it must avoid colliding with the lamp and reach the target. The drone’s state, target's state and the information of the lamp are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_lamp, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lamp = calculate_distance_min(s_agent, s_lamp) \n    penalty_collision = -15 if (d_agent_lamp < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a classroom, where it must avoid colliding with the blackboard and reach the target. The drone’s state, target's state and the information of the blackboard are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_blackboard, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_blackboard = calculate_distance_min(s_agent, s_blackboard) \n    penalty_collision = -15 if (d_agent_blackboard < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a classroom, where it must avoid colliding with the bookshelve and reach the target. The drone’s state, target's state and the information of the bookshelve are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_bookshelve, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bookshelve = calculate_distance_min(s_agent, s_bookshelve) \n    penalty_collision = -15 if (d_agent_bookshelve < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a classroom, where it must avoid colliding with the backpack and reach the target. The drone’s state, target's state and the information of the backpack are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_backpack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_backpack = calculate_distance_min(s_agent, s_backpack) \n    penalty_collision = -15 if (d_agent_backpack < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a classroom, where it must avoid colliding with the human and reach the target. The drone’s state, target's state and the information of the human are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -15 if (d_agent_human < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a classroom, where it must avoid colliding with the teacher and reach the target. The drone’s state, target's state and the information of the teacher are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_teacher, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_teacher = calculate_distance_min(s_agent, s_teacher) \n    penalty_collision = -15 if (d_agent_teacher < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a classroom, where it must avoid colliding with the student and reach the target. The drone’s state, target's state and the information of the student are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_student, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_student = calculate_distance_min(s_agent, s_student) \n    penalty_collision = -15 if (d_agent_student < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a classroom, where it must avoid colliding with the student and teacher and reach the target. The drone’s state, target's state and the information of the multiple obstacles are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a classroom, where it must avoid colliding with the chair and desk and reach the target. The drone’s state, target's state and the information of the multiple obstacles are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a classroom, where it must avoid colliding with the student and backpack and reach the target. The drone’s state, target's state and the information of the multiple obstacles are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a classroom, where it must avoid colliding with the shelf and backpack and reach the target. The drone’s state, target's state and the information of the multiple obstacles are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a classroom, where it must avoid colliding with the blackboard and whiteboard and reach the target. The drone’s state, target's state and the information of the multiple obstacles are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a classroom, where it must avoid colliding with the blackboard, student and teacher and reach the target. The drone’s state, target's state and the information of the multiple obstacles are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a classroom, where it must avoid colliding with the pupil, desk and teacher and reach the target. The drone’s state, target's state and the information of the multiple obstacles are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a classroom, where it must avoid colliding with the shelf, chair and table and reach the target. The drone’s state, target's state and the information of the multiple obstacles are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a classroom, where it must avoid colliding with the cupboard, frame and teacher and reach the target. The drone’s state, target's state and the information of the multiple obstacles are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a classroom, where it must avoid colliding with the multiple boxes and reach the target. The drone’s state, target's state and the information of the multiple obstacles are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable for reinforcement learning. \n Scenario: an indoor drone explores a conference room. \n Requirements: the drone should identify and avoid dynamic obstacles (humans) while progressing toward the target. \n The environment provides access to the agent state, target state, and human states.",
        "chosen": "def reward(s_agent, s_people, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_people = calculate_distance_min(s_agent, s_people) \n    penalty_collision = -15 if (d_agent_people < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable for reinforcement learning. \n Scenario: an indoor drone explores a conference room. \n Requirements: the drone should identify and avoid obstacles (desk) while progressing toward the target. \n The environment provides access to the agent state, target state, and desk states.",
        "chosen": "def reward(s_agent, s_desk, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_desk = calculate_distance_min(s_agent, s_desk) \n    penalty_collision = -15 if (d_agent_desk < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable for reinforcement learning. \n Scenario: an indoor drone explores a conference room. \n Requirements: the drone should identify and avoid obstacle (sideboard) while progressing toward the target. \n The environment provides access to the agent state, target state, and sideboard states.",
        "chosen": "def reward(s_agent, s_sideboard, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sideboard = calculate_distance_min(s_agent, s_sideboard) \n    penalty_collision = -15 if (d_agent_sideboard < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable for reinforcement learning. \n Scenario: an indoor drone explores a conference room. \n Requirements: the drone should identify and avoid obstacle (bench) while progressing toward the target. \n The environment provides access to the agent state, target state, and bench states.",
        "chosen": "def reward(s_agent, s_bench, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bench = calculate_distance_min(s_agent, s_bench) \n    penalty_collision = -15 if (d_agent_bench < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable for reinforcement learning. \n Scenario: an indoor drone explores a conference room. \n Requirements: the drone should identify and avoid obstacles(whiteboard) while progressing toward the target. \n The environment provides access to the agent state, target state, and whiteboard states.",
        "chosen": "def reward(s_agent, s_whiteboard, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_whiteboard = calculate_distance_min(s_agent, s_whiteboard) \n    penalty_collision = -15 if (d_agent_sideboard < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable for reinforcement learning. \n Scenario: an indoor drone explores a conference room. \n Requirements: the drone should identify and avoid obstacle (projector) while progressing toward the target. \n The environment provides access to the agent state, target state, and projector states.",
        "chosen": "def reward(s_agent, s_projector, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_projector = calculate_distance_min(s_agent, s_projector) \n    penalty_collision = -15 if (d_agent_projector < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable for reinforcement learning. \n Scenario: an indoor drone explores a conference room. \n Requirements: the drone should identify and avoid obstacle (equipment) while progressing toward the target. \n The environment provides access to the agent state, target state, and equipment states.",
        "chosen": "def reward(s_agent, s_equipment, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_equipment = calculate_distance_min(s_agent, s_equipment) \n    penalty_collision = -15 if (d_agent_equipment < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable for reinforcement learning. \n Scenario: an indoor drone explores a conference room. \n Requirements: the drone should identify and avoid obstacle (speaker) while progressing toward the target. \n The environment provides access to the agent state, target state, and speaker states.",
        "chosen": "def reward(s_agent, s_speaker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_speaker = calculate_distance_min(s_agent, s_speaker) \n    penalty_collision = -15 if (d_agent_speaker < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable for reinforcement learning. \n Scenario: an indoor drone explores a conference room. \n Requirements: the drone should identify and avoid obstacle (laptop) while progressing toward the target. \n The environment provides access to the agent state, target state, and laptop states.",
        "chosen": "def reward(s_agent, s_laptop, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_laptop = calculate_distance_min(s_agent, s_laptop) \n    penalty_collision = -15 if (d_agent_laptop < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable for reinforcement learning. \n Scenario: an indoor drone explores a conference room. \n Requirements: the drone should identify and avoid obstacle (screen) while progressing toward the target. \n The environment provides access to the agent state, target state, and screen states.",
        "chosen": "def reward(s_agent, s_screen, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_screen = calculate_distance_min(s_agent, s_screen) \n    penalty_collision = -15 if (d_agent_screen < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable for reinforcement learning. \n Scenario: an indoor drone explores a conference room. \n Requirements: the drone should identify and avoid obstacles (screen, laptop) while progressing toward the target. \n The environment provides access to the agent state, target state, and multiple obstacles states.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "MImplement in Python a reward function suitable for reinforcement learning. \n Scenario: an indoor drone explores a conference room. \n Requirements: the drone should identify and avoid obstacles (table, chair) while progressing toward the target. \n The environment provides access to the agent state, target state, and multiple obstacles states.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable for reinforcement learning. \n Scenario: an indoor drone explores a conference room. \n Requirements: the drone should identify and avoid obstacles (computer, screen) while progressing toward the target. \n The environment provides access to the agent state, target state, and multiple obstacles states.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable for reinforcement learning. \n Scenario: an indoor drone explores a conference room. \n Requirements: the drone should identify and avoid obstacles (plant, shelf) while progressing toward the target. \n The environment provides access to the agent state, target state, and multiple obstacles states.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable for reinforcement learning. \n Scenario: an indoor drone explores a conference room. \n Requirements: the drone should identify and avoid obstacles (box, plant) while progressing toward the target. \n The environment provides access to the agent state, target state, and multiple obstacles states.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable for reinforcement learning. \n Scenario: an indoor drone explores a conference room. \n Requirements: the drone should identify and avoid obstacles (screen, laptop) while progressing toward the target. \n The environment provides access to the agent state, target state, and multiple obstacles states",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable for reinforcement learning. \n Scenario: an indoor drone explores a conference room. \n Requirements: the drone should identify and avoid obstacles (plant, shelf, laptop) while progressing toward the target. \n The environment provides access to the agent state, target state, and multiple obstacles states.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable for reinforcement learning. \n Scenario: an indoor drone explores a conference room. \n Requirements: the drone should identify and avoid obstacles (shelf, box, laptop) while progressing toward the target. \n The environment provides access to the agent state, target state, and multiple obstacles states.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable for reinforcement learning. \n Scenario: an indoor drone explores a conference room. \n Requirements: the drone should identify and avoid obstacles (screen, whiteboard, box) while progressing toward the target. \n The environment provides access to the agent state, target state, and multiple obstacles states.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable for reinforcement learning. \n Scenario: an indoor drone explores a conference room. \n Requirements: the drone should identify and avoid obstacles (multiple box) while progressing toward the target. \n The environment provides access to the agent state, target state, and multiple obstacles states.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function for reinforcement learning. \n Scenario: a drone operates in a warehouse environment. \n It must identify a staff member as an obstacle, prevent collisions, and move toward the designated target. \n The agent, target, and staff states are provided.",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_staff = calculate_distance_min(s_agent, s_staff) \n    penalty_collision = -15 if (d_agent_staff < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function for reinforcement learning. \n Scenario: a drone operates in a warehouse environment. \n It must identify a worker as an obstacle, prevent collisions, and move toward the designated target. \n The agent, target, and worker states are provided.",
        "chosen": "def reward(s_agent, s_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_worker = calculate_distance_min(s_agent, s_worker) \n    penalty_collision = -15 if (d_agent_worker < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function for reinforcement learning. \n Scenario: a drone operates in a warehouse environment. \n It must identify a human as an obstacle, prevent collisions, and move toward the designated target. \n The agent, target, and human states are provided.",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -15 if (d_agent_human < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function for reinforcement learning. \n Scenario: a drone operates in a warehouse environment. \n It must identify a worker as an obstacle, prevent collisions, and move toward the designated target. \n The agent, target, and worker states are provided.",
        "chosen": "def reward(s_agent, s_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_worker = calculate_distance_min(s_agent, s_worker) \n    penalty_collision = -15 if (d_agent_worker < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function for reinforcement learning. \n Scenario: a drone operates in a warehouse environment. \n It must identify a rack as an obstacle, prevent collisions, and move toward the designated target. \n The agent, target, and rack states are provided.",
        "chosen": "def reward(s_agent, s_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rack = calculate_distance_min(s_agent, s_rack) \n    penalty_collision = -15 if (d_agent_rack < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function for reinforcement learning. \n Scenario: a drone operates in a warehouse environment. \n It must identify a forklift as an obstacle, prevent collisions, and move toward the designated target. \n The agent, target, and forklift states are provided",
        "chosen": "def reward(s_agent, s_forklift, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_forklift = calculate_distance_min(s_agent, s_forklift) \n    penalty_collision = -15 if (d_agent_forklift < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function for reinforcement learning. \n Scenario: a drone operates in a warehouse environment. \n It must identify a cable as an obstacle, prevent collisions, and move toward the designated target. \n The agent, target, and cable states are provided.",
        "chosen": "def reward(s_agent, s_cable, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cable = calculate_distance_min(s_agent, s_cable) \n    penalty_collision = -15 if (d_agent_cable < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function for reinforcement learning. \n Scenario: a drone operates in a warehouse environment. \n It must identify a stacked of pallet as an obstacle, prevent collisions, and move toward the designated target. \n The agent, target, and pallet states are provided.",
        "chosen": "def reward(s_agent, s_pallet, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_pallet = calculate_distance_min(s_agent, s_pallet) \n    penalty_collision = -15 if (d_agent_pallet < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function for reinforcement learning. \n Scenario: a drone operates in a warehouse environment. \n It must identify a package as an obstacle, prevent collisions, and move toward the designated target. \n The agent, target, and package states are provided.",
        "chosen": "def reward(s_agent, s_package, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_package = calculate_distance_min(s_agent, s_package) \n    penalty_collision = -15 if (d_agent_package < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function for reinforcement learning. \n Scenario: a drone operates in a warehouse environment. \n It must identify a overhead pipe as an obstacle, prevent collisions, and move toward the designated target. \n The agent, target, and pipe states are provided.",
        "chosen": "def reward(s_agent, s_pipe, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_pipe = calculate_distance_min(s_agent, s_pipe) \n    penalty_collision = -15 if (d_agent_pipe < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function for reinforcement learning. \n Scenario: a drone operates in a warehouse environment. \n It must identify a beam as an obstacle, prevent collisions, and move toward the designated target. \n The agent, target, and beam states are provided.",
        "chosen": "def reward(s_agent, s_beam, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_pipe = calculate_distance_min(s_agent, s_beam) \n    penalty_collision = -15 if (d_agent_beam < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function for reinforcement learning. \n Scenario: a drone operates in a warehouse environment. \n It must identify a shelves and human as an obstacle, prevent collisions, and move toward the designated target. \n The agent, target, and multiple obstacles states are provided.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function for reinforcement learning. \n Scenario: a drone operates in a warehouse environment. \n It must identify a packet and staff, prevent collisions, and move toward the designated target. \n The agent, target, and multiple obstacles states are provided.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function for reinforcement learning. \n Scenario: a drone operates in a warehouse environment. \n It must identify the stacked boxes, prevent collisions, and move toward the designated target. \n The agent, target, and multiple obstacles states are provided.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function for reinforcement learning. \n Scenario: a drone operates in a warehouse environment. \n It must identify the maschine and staffs, prevent collisions, and move toward the designated target. \n The agent, target, and multiple obstacles states are provided.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function for reinforcement learning. \n Scenario: a drone operates in a warehouse environment. \n It must identify the beam and lamps, prevent collisions, and move toward the designated target. \n The agent, target, and multiple obstacles states are provided.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function for reinforcement learning. \n Scenario: a drone operates in a warehouse environment. \n It must identify the cable, rack and table, prevent collisions, and move toward the designated target. \n The agent, target, and multiple obstacles states are provided.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function for reinforcement learning. \n Scenario: a drone operates in a warehouse environment. \n It must identify the desk, chair and computer, prevent collisions, and move toward the designated target. \n The agent, target, and multiple obstacles states are provided",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function for reinforcement learning. \n Scenario: a drone operates in a warehouse environment. \n It must identify the beam, pipe and lamo, prevent collisions, and move toward the designated target. \n The agent, target, and multiple obstacles states are provided",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function for reinforcement learning. \n Scenario: a drone operates in a warehouse environment. \n It must identify the forklift, stair and lift, prevent collisions, and move toward the designated target. \n The agent, target, and multiple obstacles states are provided.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with people. \n Conditions: Agent's state, people's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_people, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_people = calculate_distance_min(s_agent, s_people) \n    penalty_collision = -15 if (d_agent_people < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect spotlamp and avoid collison with the spotlamp. \n Conditions: Agent's state and spotlamp information are available. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_spotlamp, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_spotlamp = calculate_distance_min(s_agent, s_spotlamp) \n    penalty_collision = -15 if (d_agent_spotlamp < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a theater. \n Objectives: Avoid collison with the balcony. \n Conditions: Agent's state and balcony's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_balcony, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_balcony = calculate_distance_min(s_agent, s_balcony) \n    penalty_collision = -15 if (d_agent_balcony < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Avoid collison with the visitor's seat. \n Conditions: Agent's states and seat's information are available. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_seat, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_seat = calculate_distance_min(s_agent, s_seat) \n    penalty_collision = -15 if (d_agent_seat < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a theater. \n Objectives: Avoid collison with the projector. \n Conditions: Agent's state, projectors's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_projector, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_projector = calculate_distance_min(s_agent, s_projector) \n    penalty_collision = -15 if (d_agent_projector < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Avoid collision with a cable. \n Conditions: Agent's state, cable's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_cable, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cable = calculate_distance_min(s_agent, s_cable) \n    penalty_collision = -15 if (d_agent_cable < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone explores the theater. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state and human's state obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -15 if (d_agent_human < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Detect static obstacle (overhead platform) and avoid collision. \n Conditions: Agent's state, platform's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_platform, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_platform = calculate_distance_min(s_agent, s_platform) \n    penalty_collision = -15 if (d_agent_platform < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone explores the theater. \n Objectives: Detect static obstacle (drape) and avoid collision. \n Conditions: Agent's state, drape's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_drape, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_drape = calculate_distance_min(s_agent, s_drape) \n    penalty_collision = -15 if (d_agent_drape < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone explores the theater. \n Objectives: Detect dynamic obstacle (hangings microphone) and avoid collision. \n Conditions: Agent's state, microphone's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_microphone, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_microphone = calculate_distance_min(s_agent, s_microphone) \n    penalty_collision = -15 if (d_agent_microphone < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Operate the drone in a theater. \n Objectives: Detect the spotlamp and projectors and do not collide with them. \n Conditions: Agent's states and overhead beam information are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Operate the drone in a theater. \n Objectives: Detect the chair and people and do not collide with them. \n Conditions: Agent's states and overhead beam information are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Operate the drone in a theater. \n Objectives: Detect the stage and drape and do not collide with them. \n Conditions: Agent's states and overhead beam information are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Operate the drone in a v. \n Objectives: Detect the microphone and music box do not collide with them. \n Conditions: Agent's states and overhead beam information are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Operate the drone in a theater. \n Objectives: Detect the staff and visitor do not collide with them. \n Conditions: Agent's states and overhead beam information are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Operate the drone in a theater. \n Objectives: Detect the chandelier, catwalk and rigging ropes and do not collide with it. \n Conditions: Agent's states and obstacles information are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Operate the drone in a theater. \n Objectives: Detect the table, chair and human and do not collide with it. \n Conditions: Agent's states and obstacles information are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Operate the drone in a theater. \n Objectives: Detect the three boxes and do not collide with it. \n Conditions: Agent's states and obstacles information are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Operate the drone in a theater. \n Objectives: Detect the ceiling beam, balcony railing and spotlight and do not collide with it. \n Conditions: Agent's states and obstacles information are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Operate the drone in a theater. \n Objectives: Detect the platform, microphone and drapes and do not collide with it. \n Conditions: Agent's states and obstacles information are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Operate the drone in a theater. \n Objectives: Detect the visitorn seat and box and do not collide with it. \n Conditions: Agent's states and obstacles information are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a Sports hall, where it must avoid colliding with the human and reach the target. The drone’s state, target's state and the information of the human are available.  Create a Python code for reward function..",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -15 if (d_agent_human < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a Sports hall, where it must avoid colliding with the person and reach the target. The drone’s state, target's state and the information of the person are available.  Create a Python code for reward function..",
        "chosen": "def reward(s_agent, s_person, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_person = calculate_distance_min(s_agent, s_person) \n    penalty_collision = -15 if (d_agent_person < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a Sports hall, where it must avoid colliding with the basket hoop and reach the target. The drone’s state, target's state and the information of the basket hoop are available.  Create a Python code for reward function..",
        "chosen": "def reward(s_agent, s_basket_hoop, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_basket_hoop = calculate_distance_min(s_agent, s_basket_hoop) \n    penalty_collision = -15 if (d_agent_basket_hoop < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a Sports hall, where it must avoid colliding with the scoreboard and reach the target. The drone’s state, target's state and the information of the scoreboard are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_scoreboard, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_scoreboard = calculate_distance_min(s_agent, s_scoreboard) \n    penalty_collision = -15 if (d_agent_scoreboard < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a Sports hall, where it must avoid colliding with the rope and reach the target. The drone’s state, target's state and the information of the rope are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_rope, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rope = calculate_distance_min(s_agent, s_rope) \n    penalty_collision = -15 if (d_agent_rope < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a Sports hall, where it must avoid colliding with the volleyball net and reach the target. The drone’s state, target's state and the information of the net are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_net, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_net = calculate_distance_min(s_agent, s_net) \n    penalty_collision = -15 if (d_agent_net < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a Sports hall, where it must avoid colliding with the flag and reach the target. The drone’s state, target's state and the information of the flag are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_flag, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_flag = calculate_distance_min(s_agent, s_flag) \n    penalty_collision = -15 if (d_agent_flag < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a Sports hall, where it must avoid colliding with the player and reach the target. The drone’s state, target's state and the information of the player are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_player, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_player = calculate_distance_min(s_agent, s_player) \n    penalty_collision = -15 if (d_agent_player < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a Sports hall, where it must avoid colliding with the ball and reach the target. The drone’s state, target's state and the information of the ball are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_ball, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ball = calculate_distance_min(s_agent, s_ball) \n    penalty_collision = -15 if (d_agent_ball < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a Sports hall, where it must avoid colliding with the small ball and reach the target. The drone’s state, target's state and the information of the ball are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_ball, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ball = calculate_distance_min(s_agent, s_ball) \n    penalty_collision = -15 if (d_agent_ball < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a Sports hall, where it must avoid colliding with the player and reach the target. The drone’s state, target's state and the information of the player are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_player, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_player = calculate_distance_min(s_agent, s_player) \n    penalty_collision = -15 if (d_agent_player < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a Sports hall, where it must avoid colliding with the player and ball and reach the target. The drone’s state, target's state and the information of the multiple obstacles are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a Sports hall, where it must avoid colliding with the rope and net and reach the target. The drone’s state, target's state and the information of the multiple obstacles are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a Sports hall, where it must avoid colliding with the coach and flag and reach the target. The drone’s state, target's state and the information of the multiple obstacles are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a Sports hall, where it must avoid colliding with the basketball and rope and reach the target. The drone’s state, target's state and the information of the multiple obstacles are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a Sports hall, where it must avoid colliding with the basketball and net and reach the target. The drone’s state, target's state and the information of the multiple obstacles are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a Sports hall, where it must avoid colliding with the net and flag and reach the target. The drone’s state, target's state and the information of the multiple obstacles are available.  Create a Python code for reward function",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a Sports hall, where it must avoid colliding with the basketball, coach and net and reach the target. The drone’s state, target's state and the information of the multiple obstacles are available.  Create a Python code for reward function",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a Sports hall, where it must avoid colliding with the player, rope and ball and reach the target. The drone’s state, target's state and the information of the multiple obstacles are available.  Create a Python code for reward function",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a Sports hall, where it must avoid colliding with the flag, student and net and reach the target. The drone’s state, target's state and the information of the multiple obstacles are available.  Create a Python code for reward function",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a hospital, aims to reach a target, and must avoid collisions with patient using its own state, the patient status, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_patient, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_patient = calculate_distance_min(s_agent, s_patient) \n    penalty_collision = -15 if (d_agent_patient < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a hospital, aims to reach a target, and must avoid collisions with doctor using its own state, the doctor status, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_doctor, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_doctor = calculate_distance_min(s_agent, s_doctor) \n    penalty_collision = -15 if (d_agent_doctor < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a hospital, aims to reach a target, and must avoid collisions with wire using its own state, the wire status, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_wire, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wire = calculate_distance_min(s_agent, s_wire) \n    penalty_collision = -15 if (d_agent_wire < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a hospital, aims to reach a target, and must avoid collisions with equipment using its own state, the equipment status, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_equipment, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_equipment = calculate_distance_min(s_agent, s_equipment) \n    penalty_collision = -15 if (d_agent_equipment < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a hospital, aims to reach a target, and must avoid collisions with wheelchair using its own state, the wheelchair status, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_wheelchair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wheelchair = calculate_distance_min(s_agent, s_wheelchair) \n    penalty_collision = -15 if (d_agent_wheelchair < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a hospital, aims to reach a target, and must avoid collisions with staff using its own state, the staff status, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_staff = calculate_distance_min(s_agent, s_staffr) \n    penalty_collision = -15 if (d_agent_staff < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a hospital, aims to reach a target, and must avoid collisions with defibrillator using its own state, the defibrillator status, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_defibrillator, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_defibrillator = calculate_distance_min(s_agent, s_defibrillator) \n    penalty_collision = -15 if (d_agent_defibrillator < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a hospital, aims to reach a target, and must avoid collisions with device using its own state, the device status, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_device, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_device = calculate_distance_min(s_agent, s_device) \n    penalty_collision = -15 if (d_agent_device < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a hospital, aims to reach a target, and must avoid collisions with lamp using its own state, the lamp status, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_lamp, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lamp = calculate_distance_min(s_agent, s_lamp) \n    penalty_collision = -15 if (d_agent_lamp < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a hospital, aims to reach a target, and must avoid collisions with staff and patient using its own state, the multiple obstacales status, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a hospital, aims to reach a target, and must avoid collisions with defibrillator and equipment using its own state, the multiple obstacales status, and the target state. Code a reward function in python..",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a hospital, aims to reach a target, and must avoid collisions with wire and wheelchair using its own state, the multiple obstacales status, and the target state. Code a reward function in python..",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a hospital, aims to reach a target, and must avoid collisions with wheelchair and patient using its own state, the multiple obstacales status, and the target state. Code a reward function in python..",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a hospital, aims to reach a target, and must avoid collisions with staff and lamp using its own state, the multiple obstacales status, and the target state. Code a reward function in python..",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a hospital, aims to reach a target, and must avoid collisions with three package using its own state, the multiple obstacales status, and the target state. Code a reward function in python..",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a hospital, aims to reach a target, and must avoid collisions with doctor, wheelchair and patient using its own state, the multiple obstacales status, and the target state. Code a reward function in python..",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a hospital, aims to reach a target, and must avoid collisions with defibrillator, equipment and wheelchair and patient using its own state, the multiple obstacales status, and the target state. Code a reward function in python..",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a hospital, aims to reach a target, and must avoid collisions with lamp, bed and screen using its own state, the multiple obstacales status, and the target state. Code a reward function in python..",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a hospital, aims to reach a target, and must avoid collisions with counter, nurse and patient using its own state, the multiple obstacales status, and the target state. Code a reward function in python..",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a hospital, aims to reach a target, and must avoid collisions with three boxes using its own state, the multiple obstacales status, and the target state. Code a reward function in python..",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a hall, aims to reach a target, and must avoid collisions with ceiling fan blades using its own state, the blade status, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_blade, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_blade = calculate_distance_min(s_agent, s_blade) \n    penalty_collision = -20 if (d_agent_blade < 0.7) else 5 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a hall, aims to reach a target, and must avoid collisions with chair using its own state, the chair status, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -20 if (d_agent_chair < 0.7) else 5 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a hall, aims to reach a target, and must avoid collisions with lmap using its own state, the lamp state, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_lamp, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lamp = calculate_distance_min(s_agent, s_lamp) \n    penalty_collision = -20 if (d_agent_lamp < 0.7) else 5 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a hall, aims to reach a target, and must avoid collisions with curtain using its own state, the curtain state, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_curtain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_curtain = calculate_distance_min(s_agent, s_curtain) \n    penalty_collision = -20 if (d_agent_curtain < 0.7) else 5 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a hall, aims to reach a target, and must avoid collisions with multiple obstcales using its own state, the obstacles state, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -20 if (d_agent_obstacles_min < 0.7) else 5 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a hall, aims to reach a target, and must avoid collisions with drape using its own state, the drape state, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_drape, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_drape = calculate_distance_min(s_agent, s_drape) \n    penalty_collision = -20 if (d_agent_drape < 0.7) else 5 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a hall, aims to reach a target, and must avoid collisions with wardrobe using its own state, the wardrobe state, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_wardrobe, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wardrobe = calculate_distance_min(s_agent, s_wardrobe) \n    penalty_collision = -20 if (d_agent_wardrobe < 0.7) else 5 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a hall, aims to reach a target, and must avoid collisions with wall using its own state, the wall state, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -20 if (d_agent_wall < 0.7) else 5 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a hall, aims to reach a target, and must avoid collisions with people using its own state, the people state, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_people, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_people = calculate_distance_min(s_agent, s_people) \n    penalty_collision = -20 if (d_agent_people < 0.7) else 5 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flies inside a hall, aims to reach a target, and must avoid collisions with multiple obstacles such as chair and table using its own state, the multiple obstacle status, and the target state. Code a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -20 if (d_agent_obstacles_min < c_1) else 5 \n    reward_reaching_target = 15 if (d_agent_target < c_2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flies inside a hall, aims to reach a target, and must avoid collisions with multiple obstacles such as fan blade and table using its own state, the multiple obstacle status, and the target state. Code a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < c_1) else 5 \n    reward_reaching_target = 15 if (d_agent_target < c_2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flies inside a hall, aims to reach a target, and must avoid collisions with multiple obstacles such as drape and people using its own state, the multiple obstacle status, and the target state. Code a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -20 if (d_agent_obstacles_min < c_1) else 5 \n    reward_reaching_target = 15 if (d_agent_target < c_2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flies inside a room, aims to reach a target, and must avoid collisions with multiple obstacles such as lamp and projector using its own state, the multiple obstacle status, and the target state. Code a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -20 if (d_agent_obstacles_min < c_1) else 5 \n    reward_reaching_target = 15 if (d_agent_target < c_2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flies inside a room, aims to reach a target, and must avoid collisions with multiple obstacles such as boxes and chair using its own state, the multiple obstacle status, and the target state. Code a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -20 if (d_agent_obstacles_min < c_1) else 5 \n    reward_reaching_target = 15 if (d_agent_target < c_2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flies inside a room, aims to reach a target, and must avoid collisions with multiple obstacles such as table and chair using its own state, the multiple obstacle status, and the target state. Code a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -20 if (d_agent_obstacles_min < c_1) else 5 \n    reward_reaching_target = 15 if (d_agent_target < c_2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flies inside a room, aims to reach a target, and must avoid collisions with multiple obstacles such as human, chair and table using its own state, the multiple obstacle status, and the target state. Code a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -20 if (d_agent_obstacles_min < c_1) else 5 \n    reward_reaching_target = 15 if (d_agent_target < c_2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flies inside a room, aims to reach a target, and must avoid collisions with multiple obstacles such as human, lamp and projector using its own state, the multiple obstacle status, and the target state. Code a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -20 if (d_agent_obstacles_min < c_1) else 5 \n    reward_reaching_target = 15 if (d_agent_target < c_2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flies inside a room, aims to reach a target, and must avoid collisions with multiple obstacles such as people, curtain and projector using its own state, the multiple obstacle status, and the target state. Code a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -20 if (d_agent_obstacles_min < c_1) else 5 \n    reward_reaching_target = 15 if (d_agent_target < c_2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flies inside a room, aims to reach a target, and must avoid collisions with multiple obstacles such as people, bench and projector using its own state, the multiple obstacle status, and the target state. Code a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -20 if (d_agent_obstacles_min < c_1) else 5 \n    reward_reaching_target = 15 if (d_agent_target < c_2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flies inside a room, aims to reach a target, and must avoid collisions with multiple obstacles such as shelf, plant and decor using its own state, the multiple obstacle status, and the target state. Code a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -20 if (d_agent_obstacles_min < c_1) else 5 \n    reward_reaching_target = 15 if (d_agent_target < c_2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone must navigate toward a target while avoiding crashes into the walls, relying on agent state, the walls’ positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -10 if (d_agent_wall < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone must navigate toward a target while avoiding crashes into the surrounding walls, relying on agent state, the walls’ positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -10 if (d_agent_wall < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone must navigate toward a target while avoiding crashes into the chairs, relying on agent state, the chair positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -10 if (d_agent_chair < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone must navigate toward a target while avoiding crashes into the table, relying on agent state, the table positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_table, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_table_min = calculate_distance_min(s_agent, s_table) \n    penalty_collision = -10 if (d_agent_table_min < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone must navigate toward a target while avoiding crashes into the bookshelves, relying on agent state, the bookshelves positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_bookshelve, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bookshelve = calculate_distance_min(s_agent, s_bookshelve) \n    penalty_collision = -10 if (d_agent_bookshelve < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone must navigate toward a target while avoiding crashes into the smoke detector, relying on agent state, the smoke detecor positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_smoke_detector, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_smoke_detectore = calculate_distance_min(s_agent, s_smoke_detector) \n    penalty_collision = -10 if (d_agent_smoke_detector < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone must navigate toward a target while avoiding crashes into the stacked books, relying on agent state, the books positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_book, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_book = calculate_distance_min(s_agent, s_book) \n    penalty_collision = -10 if (d_agent_book < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone must navigate toward a target while avoiding crashes into the human, relying on agent state, the human's positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -10 if (d_agent_human < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone is tasked with navigating toward a target target while steering clear of obstacles like chair and table. It has access to its state, the multiple obstacles’ conditions, and target data. Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone is tasked with navigating toward a target target while steering clear of obstacles like shelves. It has access to its state, the multiple obstacles’ conditions, and target data. Design a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone is tasked with navigating toward a target target while steering clear of obstacles like lamps and table. It has access to its state, the multiple obstacles’ conditions, and target data. Design a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone is tasked with navigating toward a target target while steering clear of obstacles like decoration and shelf. It has access to its state, the multiple obstacles’ conditions, and target data. Design a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone is tasked with navigating toward a target target while steering clear of obstacles like book and shelf. It has access to its state, the multiple obstacles’ conditions, and target data. Design a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone is tasked with navigating toward a target target while steering clear of obstacles like books and lights. It has access to its state, the multiple obstacles’ conditions, and target data. Design a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone is tasked with navigating toward a target target while steering clear of obstacles like book, lights and table. It has access to its state, the multiple obstacles’ conditions, and target data. Design a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone is tasked with navigating toward a target target while steering clear of obstacles like chair, human and table. It has access to its state, the multiple obstacles’ conditions, and target data. Design a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone is tasked with navigating toward a target target while steering clear of obstacles like desk, table and students. It has access to its state, the multiple obstacles’ conditions, and target data. Design a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone is tasked with navigating toward a target target while steering clear of obstacles like book, computer and staff. It has access to its state, the multiple obstacles’ conditions, and target data. Design a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone is tasked with navigating toward a target target while steering clear of obstacles like stacked book, laptop and book carts. It has access to its state, the multiple obstacles’ conditions, and target data. Design a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone is tasked with navigating toward a target target while steering clear of obstacles like curtain, plant and staff. It has access to its state, the multiple obstacles’ conditions, and target data. Design a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a forest, aims to detect a dynamic obstacle (a human), and must avoid collisions by using its own state and the human’s state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -12 if (d_agent_human < 1) else 0 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a forest, aims to detect a static obstacle (a tree), and must avoid collisions by using its own state and the tree's state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_tree, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tree = calculate_distance_min(s_agent, s_tree) \n    penalty_collision = -12 if (d_agent_tree < 1) else 0 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a forest, aims to detect a stic obstacle (a leaf), and must avoid collisions by using its own state and the leaf’s state. Create a reward function in Python..",
        "chosen": "def reward(s_agent, s_leaf, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_leaf = calculate_distance_min(s_agent, s_leaf) \n    penalty_collision = -12 if (d_agent_leaf < 1) else 0 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a forest, aims to detect a obstacle (a bird), and must avoid collisions by using its own state and the bird’s state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_bird, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bird = calculate_distance_min(s_agent, s_bird) \n    penalty_collision = -12 if (d_agent_bird < 1) else 0 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a forest, aims to detect a obstacle (a stump), and must avoid collisions by using its own state and the stump’s state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_stump, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_stump = calculate_distance_min(s_agent, s_stump) \n    penalty_collision = -12 if (d_agent_stump < 1) else 0 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a forest, aims to detect a obstacle (a rock), and must avoid collisions by using its own state and the rock’s state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_rock, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rock = calculate_distance_min(s_agent, s_rock) \n    penalty_collision = -12 if (d_agent_rock < 1) else 0 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a forest, aims to detect a obstacle (a thin branches), and must avoid collisions by using its own state and the branch’s state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_branch, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_branch = calculate_distance_min(s_agent, s_branch) \n    penalty_collision = -12 if (d_agent_branch < 1) else 0 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a forest, aims to detect a obstacle (a tree branch), and must avoid collisions by using its own state and the tree branch’s state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_tree_branch, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tree_branch = calculate_distance_min(s_agent, s_tree_branch) \n    penalty_collision = -12 if (d_agent_tree_branch < 1) else 0 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a forest, aims to detect a obstacle (a people), and must avoid collisions by using its own state and the people’s state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_people, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_people = calculate_distance_min(s_agent, s_people) \n    penalty_collision = -12 if (d_agent_people < 1) else 0 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a forest, aims to detect a obstacle (a stone), and must avoid collisions by using its own state and the stone’s state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_stone, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_stone = calculate_distance_min(s_agent, s_stone) \n    penalty_collision = -12 if (d_agent_stone < 1) else 0 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a forest, aims to detect a obstacle, and must avoid collisions by using its own state and the obstacle’s state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -12 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 16 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates a forest with the aim of reaching a target while avoiding collisions with branches and trees. Available Information: The agent’s state and the obstacles’ states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -12 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 16 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates a forest with the aim of reaching a target while avoiding collisions with humans and animals. Available Information: The agent’s state and the obstacles’ states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -12 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 16 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates a forest with the aim of reaching a target while avoiding collisions with leafes and trees. Available Information: The agent’s state and the obstacles’ states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -12 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 16 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates a forest with the aim of reaching a target while avoiding collisions with multiple trees. Available Information: The agent’s state and the obstacles’ states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -12 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 16 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates a forest with the aim of reaching a target while avoiding collisions with human, branches and trees. Available Information: The agent’s state and the obstacles’ states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -12 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 16 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates a forest with the aim of reaching a target while avoiding collisions with stump, bird and human. Available Information: The agent’s state and the obstacles’ states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -12 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 16 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates a forest with the aim of reaching a target while avoiding collisions with trees, animal and stone. Available Information: The agent’s state and the obstacles’ states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -12 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 16 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates a forest with the aim of reaching a target while avoiding collisions with tree, cliff and signs. Available Information: The agent’s state and the obstacles’ states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -12 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 16 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates a forest with the aim of reaching a target while avoiding collisions with animals, human and signs. Available Information: The agent’s state and the obstacles’ states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -12 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 16 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, human's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -20 if (d_agent_human < 1) else 3 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid any collision with hanging lamps. \n Conditions: Agent's information and lamp's information are available \n Write a reward in Python.",
        "chosen": "def reward(s_agent, s_lamp, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lamp = calculate_distance_min(s_agent, s_lamp) \n    penalty_collision = -20 if (d_agent_lamp < 0.5) else 3 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid any static obstacle such as food serving counters. \n Conditions: Agent's state, food counter's state are accessible. \n Write a reward in Python.",
        "chosen": "def reward(s_agent, s_counter, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_counter = calculate_distance_min(s_agent, s_counter) \n    penalty_collision = -20 if (d_agent_counter < 0.5) else 3 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with a chair. \n Conditions: Agent's state, chair's states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -20 if (d_agent_chair < 0.5) else 3 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with a table. \n Conditions: Agent's state, table's state are available. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_table, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_table = calculate_distance_min(s_agent, s_table) \n    penalty_collision = -20 if (d_agent_table < 0.5) else 3 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with trash bins. \n Conditions: Agent's state and trash bin's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_trash_bin, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_trash_bin = calculate_distance_min(s_agent, s_trash_bin) \n    penalty_collision = -20 if (d_agent_trash_bin < 0.5) else 3 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with a low-hanging decoration. \n Conditions: Agent's state, hanging decoration's state and desired speed are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_decoration, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_decoration = calculate_distance_min(s_agent, s_decoration) \n    penalty_collision = -20 if (d_agent_decoration < 0.5) else 3 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with plants. \n Conditions: Agent's state and plant's state are available. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_plant, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_plant = calculate_distance_min(s_agent, s_plant) \n    penalty_collision = -20 if (d_agent_plant < 0.5) else 3 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Do not collide with metal rails. \n Conditions: Agent's state, metal rail's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_rail, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rail = calculate_distance_min(s_agent, s_rail) \n    penalty_collision = -20 if (d_agent_rail < 0.5) else 3 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Do not collide with cameras. \n Conditions: Agent's state and camera's state are available. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_cameras, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cameras = calculate_distance_min(s_agent, s_cameras) \n    penalty_collision = -20 if (d_agent_cameras < 0.5) else 3 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a mensa. \n Objectives: Avoid collisions with people and table. \n Conditions: Agent's state and obstacles' states are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -20 if (d_agent_obstacles_min < 0.8) else 3 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a mensa. \n Objectives: Avoid collisions with students and chairs. \n Conditions: Agent's state and obstacles' states are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -20 if (d_agent_obstacles_min < 0.8) else 3 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a mensa. \n Objectives: Avoid collisions with plants and chairs. \n Conditions: Agent's state and obstacles' states are available. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -20 if (d_agent_obstacles_min < 0.8) else 3 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a mensa. \n Objectives: Avoid collisions with plants and desk. \n Conditions: Agent's state and obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -20 if (d_agent_obstacles_min < 0.8) else 3 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a mensa. \n Objectives: Avoid collisions with decoration and plant. \n Conditions: Agent's state and obstacles' states are available. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -20 if (d_agent_obstacles_min < 0.8) else 3 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a mensa. \n Objectives: Avoid collisions with cameras, student and table. \n Conditions: Agent's state and obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -20 if (d_agent_obstacles_min < 0.8) else 3 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a mensa. \n Objectives: Avoid collisions with chair, plant and table. \n Conditions: Agent's state and obstacles' states are available. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -20 if (d_agent_obstacles_min < 0.8) else 3 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a mensa. \n Objectives: Avoid collisions with staff, student and table. \n Conditions: Agent's state and obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -20 if (d_agent_obstacles_min < 0.8) else 3 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a mensa. \n Objectives: Avoid collisions with food counter, rails and camera. \n Conditions: Agent's state and obstacles' states are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -20 if (d_agent_obstacles_min < 0.8) else 3 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a mensa. \n Objectives: Avoid collisions with food counter, trash bin and staff. \n Conditions: Agent's state and obstacles' states are available. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -20 if (d_agent_obstacles_min < 0.8) else 3 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a classroom environment while ensuring it does not crash into the walls and reaches target. State about the drone’s state, target’s state and the states of the walls can be accessed.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -10 if (d_agent_wall < 0.6) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a classroom environment while ensuring it does not crash into the people and reaches target. State about the drone’s state, target’s state and the states of the people can be accessed.",
        "chosen": "def reward(s_agent, s_people, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_people = calculate_distance_min(s_agent, s_people) \n    penalty_collision = -10 if (d_agent_people < 0.6) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a classroom environment while ensuring it does not crash into the projector and reaches target. State about the drone’s state, target’s state and the states of the projector can be accessed..",
        "chosen": "def reward(s_agent, s_projector, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_projector = calculate_distance_min(s_agent, s_projector) \n    penalty_collision = -10 if (d_agent_projector < 0.6) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a classroom environment while ensuring it does not crash into the lamp and reaches target. State about the drone’s state, target’s state and the states of the lamp can be accessed.",
        "chosen": "def reward(s_agent, s_lamp, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lamp = calculate_distance_min(s_agent, s_lamp) \n    penalty_collision = -10 if (d_agent_lamp < 0.6) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a classroom environment while ensuring it does not crash into the blackboard and reaches target. State about the drone’s state, target’s state and the states of the blackboard can be accessed.",
        "chosen": "def reward(s_agent, s_blackboard, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_blackboard = calculate_distance_min(s_agent, s_blackboard) \n    penalty_collision = -10 if (d_agent_blackboard < 0.6) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a classroom environment while ensuring it does not crash into the bookshelves and reaches target. State about the drone’s state, target’s state and the states of the bookshelf can be accessed.",
        "chosen": "def reward(s_agent, s_bookshelve, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bookshelve = calculate_distance_min(s_agent, s_bookshelve) \n    penalty_collision = -10 if (d_agent_bookshelve < 0.6) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a classroom environment while ensuring it does not crash into the backpack and reaches target. State about the drone’s state, target’s state and the states of the backpack can be accessed.",
        "chosen": "def reward(s_agent, s_backpack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_backpack = calculate_distance_min(s_agent, s_backpack) \n    penalty_collision = -10 if (d_agent_backpack < 0.6) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a classroom environment while ensuring it does not crash into the human and reaches target. State about the drone’s state, target’s state and the states of the human can be accessed.",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -10 if (d_agent_human < 0.6) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a classroom environment while ensuring it does not crash into the teacher and reaches target. State about the drone’s state, target’s state and the states of the teacher can be accessed.",
        "chosen": "def reward(s_agent, s_teacher, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_teacher = calculate_distance_min(s_agent, s_teacher) \n    penalty_collision = -10 if (d_agent_teacher < 0.6) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a classroom environment while ensuring it does not crash into the student and reaches target. State about the drone’s state, target’s state and the states of the student can be accessed.",
        "chosen": "def reward(s_agent, s_student, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_student = calculate_distance_min(s_agent, s_student) \n    penalty_collision = -10 if (d_agent_student < 0.6) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a classroom environment while ensuring it does not crash into the obstacles and reaches target. State about the drone’s state, target’s state and the states of the multiple obstacles can be accessed.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a classroom environment while ensuring it does not crash into the chair and table and reaches target. State about the drone’s state, target’s state and the states of the multiple obstacles can be accessed.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a classroom environment while ensuring it does not crash into the student and backppack and reaches target. State about the drone’s state, target’s state and the states of the multiple obstacles can be accessed.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a classroom environment while ensuring it does not crash into the shelves and backppack and reaches target. State about the drone’s state, target’s state and the states of the multiple obstacles can be accessed",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a classroom environment while ensuring it does not crash into the backboard and whiteboard and reaches target. State about the drone’s state, target’s state and the states of the multiple obstacles can be accessed",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a classroom environment while ensuring it does not crash into the student and teacher, backppack and reaches target. State about the drone’s state, target’s state and the states of the multiple obstacles can be accessed",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a classroom environment while ensuring it does not crash into the teacher, pupil and desk and reaches target. State about the drone’s state, target’s state and the states of the multiple obstacles can be accessed",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a classroom environment while ensuring it does not crash into the shelf, chair and table and reaches target. State about the drone’s state, target’s state and the states of the multiple obstacles can be accessed",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a classroom environment while ensuring it does not crash into the student, teacher and backppack and reaches target. State about the drone’s state, target’s state and the states of the multiple obstacles can be accessed.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a classroom environment while ensuring it does not crash into the multiple boxes and reaches target. State about the drone’s state, target’s state and the states of the multiple obstacles can be accessed",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a conference room, a drone must navigate to a target. \n It has access to its own state, the target’s state, and the states of people present. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_people, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_people = calculate_distance_min(s_agent, s_people) \n    penalty_collision = -12 if (d_agent_people < 0.5) else 0 \n    reward_reaching_target = 14 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a conference room, a drone must navigate to a target and do not bump desk. \n It has access to its own state, the target’s state, and the states of desk present. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_desk, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_desk = calculate_distance_min(s_agent, s_desk) \n    penalty_collision = -12 if (d_agent_desk < 0.5) else 0 \n    reward_reaching_target = 14 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a conference room, a drone must navigate to a target and do not bump sideboard. \n It has access to its own state, the target’s state, and the states of sideboard present. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_sideboard, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sideboard = calculate_distance_min(s_agent, s_sideboard) \n    penalty_collision = -12 if (d_agent_sideboard < 0.5) else 0 \n    reward_reaching_target = 14 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a conference room, a drone must navigate to a target and do not bump bench. \n It has access to its own state, the target’s state, and the states of bench present. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_bench, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bench = calculate_distance_min(s_agent, s_bench) \n    penalty_collision = -12 if (d_agent_bench < 0.5) else 0 \n    reward_reaching_target = 14 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a conference room, a drone must navigate to a target and do not bump whiteboard. \n It has access to its own state, the target’s state, and the states of whiteboard present. \n Write a reward function in Python that rewards reaching the target",
        "chosen": "def reward(s_agent, s_whiteboard, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_whiteboard = calculate_distance_min(s_agent, s_whiteboard) \n    penalty_collision = -12 if (d_agent_sideboard < 0.5) else 0 \n    reward_reaching_target = 14 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a conference room, a drone must navigate to a target and do not bump projector. \n It has access to its own state, the target’s state, and the states of projector present. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_projector, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_projector = calculate_distance_min(s_agent, s_projector) \n    penalty_collision = -12 if (d_agent_projector < 0.5) else 0 \n    reward_reaching_target = 14 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a conference room, a drone must navigate to a target and do not bump equipment. \n It has access to its own state, the target’s state, and the states of equipment present. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_equipment, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_equipment = calculate_distance_min(s_agent, s_equipment) \n    penalty_collision = -12 if (d_agent_equipment < 0.5) else 0 \n    reward_reaching_target = 14 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a conference room, a drone must navigate to a target and do not bump speaker. \n It has access to its own state, the target’s state, and the states of speaker present. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_speaker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_speaker = calculate_distance_min(s_agent, s_speaker) \n    penalty_collision = -12 if (d_agent_speaker < 0.5) else 0 \n    reward_reaching_target = 14 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a conference room, a drone must navigate to a target and do not bump laptop. \n It has access to its own state, the target’s state, and the states of laptop present. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_laptop, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_laptop = calculate_distance_min(s_agent, s_laptop) \n    penalty_collision = -12 if (d_agent_laptop < 0.5) else 0 \n    reward_reaching_target = 14 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a conference room, a drone must navigate to a target and do not screen desk. \n It has access to its own state, the target’s state, and the states of screen present. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_screen, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_screen = calculate_distance_min(s_agent, s_screen) \n    penalty_collision = -12 if (d_agent_screen < 0.5) else 0 \n    reward_reaching_target = 14 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a conference room, a drone must navigate to a target and do not bump screen and projector. \n It has access to its own state, the target’s state, and the states of multiple obstacles present. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -12 if (d_agent_obstacles_min < 0.6) else 0 \n    reward_reaching_target = 14 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a conference room, a drone must navigate to a target and do not bump table and chair. \n It has access to its own state, the target’s state, and the states of multiple obstacles present. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -12 if (d_agent_obstacles_min < 0.6) else 0 \n    reward_reaching_target = 14 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a conference room, a drone must navigate to a target and do not bump computer and screen. \n It has access to its own state, the target’s state, and the states of multiple obstacles present. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -12 if (d_agent_obstacles_min < 0.6) else 0 \n    reward_reaching_target = 14 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a conference room, a drone must navigate to a target and do not bump plant and shelf. \n It has access to its own state, the target’s state, and the states of multiple obstacles present. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -12 if (d_agent_obstacles_min < 0.6) else 0 \n    reward_reaching_target = 14 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a conference room, a drone must navigate to a target and do not bump box and plant. \n It has access to its own state, the target’s state, and the states of multiple obstacles present. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -12 if (d_agent_obstacles_min < 0.6) else 0 \n    reward_reaching_target = 14 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a conference room, a drone must navigate to a target and do not bump chair, table and person. \n It has access to its own state, the target’s state, and the states of multiple obstacles present. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -12 if (d_agent_obstacles_min < 0.6) else 0 \n    reward_reaching_target = 14 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a conference room, a drone must navigate to a target and do not bump laptop, shelf and plant. \n It has access to its own state, the target’s state, and the states of multiple obstacles present. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -12 if (d_agent_obstacles_min < 0.6) else 0 \n    reward_reaching_target = 14 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a conference room, a drone must navigate to a target and do not bump shelf, box and projector. \n It has access to its own state, the target’s state, and the states of multiple obstacles present. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -12 if (d_agent_obstacles_min < 0.6) else 0 \n    reward_reaching_target = 14 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a conference room, a drone must navigate to a target and do not bump whitescreen, screen and plant. \n It has access to its own state, the target’s state, and the states of multiple obstacles present. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -12 if (d_agent_obstacles_min < 0.6) else 0 \n    reward_reaching_target = 14 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a conference room, a drone must navigate to a target and do not bump multiple box. \n It has access to its own state, the target’s state, and the states of multiple obstacles present. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -12 if (d_agent_obstacles_min < 0.6) else 0 \n    reward_reaching_target = 14 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a warehouse. The drone must detect a staff member as a moving obstacle, avoid collisions, and reach its target. The function can use the agent state, target state, and staff state.",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_staff = calculate_distance_min(s_agent, s_staff) \n    penalty_collision = -11 if (d_agent_staff < 0.5) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a warehouse. The drone must detect a worker as a obstacle, avoid collisions, and reach its target. The function can use the agent state, target state, and worker state.",
        "chosen": "def reward(s_agent, s_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_worker = calculate_distance_min(s_agent, s_worker) \n    penalty_collision = -11 if (d_agent_worker < 0.5) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a warehouse. The drone must detect a human as a obstacle, avoid collisions, and reach its target. The function can use the agent state, target state, and human state.",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -11 if (d_agent_human < 0.5) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a warehouse. The drone must detect a human worker as a obstacle, avoid collisions, and reach its target. The function can use the agent state, target state, and worker state.",
        "chosen": "def reward(s_agent, s_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_worker = calculate_distance_min(s_agent, s_worker) \n    penalty_collision = -11 if (d_agent_worker < 0.5) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a warehouse. The drone must detect a rack as a obstacle, avoid collisions, and reach its target. The function can use the agent state, target state, and rack state.",
        "chosen": "def reward(s_agent, s_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rack = calculate_distance_min(s_agent, s_rack) \n    penalty_collision = -11 if (d_agent_rack < 0.5) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a warehouse. The drone must detect a forklift as a obstacle, avoid collisions, and reach its target. The function can use the agent state, target state, and forklift state.",
        "chosen": "def reward(s_agent, s_forklift, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_forklift = calculate_distance_min(s_agent, s_forklift) \n    penalty_collision = -11 if (d_agent_forklift < 0.5) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a warehouse. The drone must detect a cable as a obstacle, avoid collisions, and reach its target. The function can use the agent state, target state, and cable state.",
        "chosen": "def reward(s_agent, s_cable, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cable = calculate_distance_min(s_agent, s_cable) \n    penalty_collision = -11 if (d_agent_cable < 0.5) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a warehouse. The drone must detect a pallet as a obstacle, avoid collisions, and reach its target. The function can use the agent state, target state, and pallat state.",
        "chosen": "def reward(s_agent, s_pallet, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_pallet = calculate_distance_min(s_agent, s_pallet) \n    penalty_collision = -11 if (d_agent_pallet < 0.5) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a warehouse. The drone must detect a package as a obstacle, avoid collisions, and reach its target. The function can use the agent state, target state, and package state.",
        "chosen": "def reward(s_agent, s_package, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_package = calculate_distance_min(s_agent, s_package) \n    penalty_collision = -11 if (d_agent_package < 0.5) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a warehouse. The drone must detect a pipe as a obstacle, avoid collisions, and reach its target. The function can use the agent state, target state, and pipe state.",
        "chosen": "def reward(s_agent, s_pipe, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_pipe = calculate_distance_min(s_agent, s_pipe) \n    penalty_collision = -11 if (d_agent_pipe < 0.5) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a warehouse. The drone must detect a beam as a obstacle, avoid collisions, and reach its target. The function can use the agent state, target state, and beam state.",
        "chosen": "def reward(s_agent, s_beam, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_pipe = calculate_distance_min(s_agent, s_beam) \n    penalty_collision = -11 if (d_agent_beam < 0.5) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a warehouse. The drone must detect a beam and worker as a obstacle, avoid collisions, and reach its target. The function can use the agent state, target state, and multiple obstacles state.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -11 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a warehouse. The drone must detect a packet and staff as a obstacle, avoid collisions, and reach its target. The function can use the agent state, target state, and multiple obstacles state.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -11 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a warehouse. The drone must detect a many boxes as a obstacle, avoid collisions, and reach its target. The function can use the agent state, target state, and multiple obstacles state.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -11 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a warehouse. The drone must detect a maschine and staff as a obstacle, avoid collisions, and reach its target. The function can use the agent state, target state, and multiple obstacles state.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -11 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a warehouse. The drone must detect a pipe and lamp as a obstacle, avoid collisions, and reach its target. The function can use the agent state, target state, and multiple obstacles state.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -11 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a warehouse. The drone must detect a people, rack and packet as a obstacle, avoid collisions, and reach its target. The function can use the agent state, target state, and multiple obstacles state.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -11 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a warehouse. The drone must detect a desk, chair and computer as a obstacle, avoid collisions, and reach its target. The function can use the agent state, target state, and multiple obstacles state.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -11 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a warehouse. The drone must detect a beam, pipe and lamp as a obstacle, avoid collisions, and reach its target. The function can use the agent state, target state, and multiple obstacles state.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -11 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a warehouse. The drone must detect a forklift, stair and rails as a obstacle, avoid collisions, and reach its target. The function can use the agent state, target state, and multiple obstacles state.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -11 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a theater environment. It should identify people as obstacles, prevent collisions, reach target and rely on the Agent, target and people states for decision-making. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_people, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_people = calculate_distance_min(s_agent, s_people) \n    penalty_collision = -14 if (d_agent_people < 0.5) else 5 \n    reward_reaching_target = 25 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a theater environment. It should identify spotlamp as obstacles, prevent collisions, reach target and rely on the Agent, target and spotlamp states for decision-making. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_spotlamp, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_spotlamp = calculate_distance_min(s_agent, s_spotlamp) \n    penalty_collision = -14 if (d_agent_spotlamp < 0.5) else 5 \n    reward_reaching_target = 25 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a theater environment. It should identify balconny as obstacles, prevent collisions, reach target and rely on the Agent, target and balcony states for decision-making. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_balcony, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_balcony = calculate_distance_min(s_agent, s_balcony) \n    penalty_collision = -14 if (d_agent_balcony < 0.5) else 5 \n    reward_reaching_target = 25 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a theater environment. It should identify seat as obstacles, prevent collisions, reach target and rely on the Agent, target and seat states for decision-making. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_seat, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_seat = calculate_distance_min(s_agent, s_seat) \n    penalty_collision = -14 if (d_agent_seat < 0.5) else 5 \n    reward_reaching_target = 25 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a theater environment. It should identify projector as obstacles, prevent collisions, reach target and rely on the Agent, target and projector states for decision-making. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_projector, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_projector = calculate_distance_min(s_agent, s_projector) \n    penalty_collision = -14 if (d_agent_projector < 0.5) else 5 \n    reward_reaching_target = 25 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a theater environment. It should identify cable as obstacles, prevent collisions, reach target and rely on the Agent, target and cable states for decision-making. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_cable, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cable = calculate_distance_min(s_agent, s_cable) \n    penalty_collision = -14 if (d_agent_cable < 0.5) else 5 \n    reward_reaching_target = 25 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a theater environment. It should identify human as obstacles, prevent collisions, reach target and rely on the Agent, target and human states for decision-making. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -14 if (d_agent_human < 0.8) else 5 \n    reward_reaching_target = 25 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a theater environment. It should identify platform as obstacles, prevent collisions, reach target and rely on the Agent, target and platform states for decision-making. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_platform, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_platform = calculate_distance_min(s_agent, s_platform) \n    penalty_collision = -14 if (d_agent_platform < 0.8) else 5 \n    reward_reaching_target = 25 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a theater environment. It should identify drape as obstacles, prevent collisions, reach target and rely on the Agent, target and drape states for decision-making. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_drape, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_drape = calculate_distance_min(s_agent, s_drape) \n    penalty_collision = -14 if (d_agent_drape < 0.8) else 5 \n    reward_reaching_target = 25 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a theater environment. It should identify microphone as obstacles, prevent collisions, reach target and rely on the Agent, target and microphone states for decision-making. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_microphone, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_microphone = calculate_distance_min(s_agent, s_microphone) \n    penalty_collision = -14 if (d_agent_microphone < 0.8) else 5 \n    reward_reaching_target = 25 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a theater environment. It should identify projector and spotlamp as obstacles, prevent collisions, reach target and rely on the Agent, target and multiple obstacles states for decision-making. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -14 if (d_agent_obstacles_min < 0.8) else 5 \n    reward_reaching_target = 25 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a theater environment. It should identify chair and people as obstacles, prevent collisions, reach target and rely on the Agent, target and multiple obstacles states for decision-making. Develop a reward function in Python..",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -14 if (d_agent_obstacles_min < 0.8) else 5 \n    reward_reaching_target = 25 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a theater environment. It should identify stage and drape as obstacles, prevent collisions, reach target and rely on the Agent, target and multiple obstacles states for decision-making. Develop a reward function in Python..",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -14 if (d_agent_obstacles_min < 0.8) else 5 \n    reward_reaching_target = 25 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a theater environment. It should identify microphone and music box as obstacles, prevent collisions, reach target and rely on the Agent, target and multiple obstacles states for decision-making. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -14 if (d_agent_obstacles_min < 0.8) else 5 \n    reward_reaching_target = 25 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a theater environment. It should identify staff and visitor as obstacles, prevent collisions, reach target and rely on the Agent, target and multiple obstacles states for decision-making. Develop a reward function in Python..",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -14 if (d_agent_obstacles_min < 0.8) else 5 \n    reward_reaching_target = 25 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a theater environment. It should identify chandlier, catwork and rope as obstacles, prevent collisions, reach target and rely on the Agent, target and multiple obstacles states for decision-making. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -14 if (d_agent_obstacles_min < 0.8) else 5 \n    reward_reaching_target = 25 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a theater environment. It should identify table, chair and human as obstacles, prevent collisions, reach target and rely on the Agent, target and multiple obstacles states for decision-making. Develop a reward function in Python..",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -14 if (d_agent_obstacles_min < 0.8) else 5 \n    reward_reaching_target = 25 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a theater environment. It should identifystacked boxesp as obstacles, prevent collisions, reach target and rely on the Agent, target and multiple obstacles states for decision-making. Develop a reward function in Python..",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -14 if (d_agent_obstacles_min < 0.8) else 5 \n    reward_reaching_target = 25 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a theater environment. It should identify balcony railing, spotlmap and ceiling beam as obstacles, prevent collisions, reach target and rely on the Agent, target and multiple obstacles states for decision-making. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -14 if (d_agent_obstacles_min < 0.8) else 5 \n    reward_reaching_target = 25 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a theater environment. It should identify platform, microphone and drape as obstacles, prevent collisions, reach target and rely on the Agent, target and multiple obstacles states for decision-making. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -14 if (d_agent_obstacles_min < 0.8) else 5 \n    reward_reaching_target = 25 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a theater environment. It should identify visitors and stacked box as obstacles, prevent collisions, reach target and rely on the Agent, target and multiple obstacles states for decision-making. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -14 if (d_agent_obstacles_min < 0.8) else 5 \n    reward_reaching_target = 25 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor Sports hall environment using a drone.  \n targets: Successfully reach the designated target state, Detect humans as obstacles, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of nearby humans, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -19 if (d_agent_human < 0.6) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor Sports hall environment using a drone.  \n targets: Successfully reach the designated target state, Detect person as obstacles, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of nearby persons, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_person, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_person = calculate_distance_min(s_agent, s_person) \n    penalty_collision = -19 if (d_agent_person < 0.6) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor Sports hall environment using a drone.  \n targets: Successfully reach the designated target state, Detect basket hoop as obstacles, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of nearby basket hoop, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_basket_hoop, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_basket_hoop = calculate_distance_min(s_agent, s_basket_hoop) \n    penalty_collision = -19 if (d_agent_basket_hoop < 0.6) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor Sports hall environment using a drone.  \n targets: Successfully reach the designated target state, Detect scoreboard as obstacles, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of nearby scoreboard, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_scoreboard, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_scoreboard = calculate_distance_min(s_agent, s_scoreboard) \n    penalty_collision = -19 if (d_agent_scoreboard < 0.6) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor Sports hall environment using a drone.  \n targets: Successfully reach the designated target state, Detect rope as obstacles, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of nearby ropes, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_rope, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rope = calculate_distance_min(s_agent, s_rope) \n    penalty_collision = -19 if (d_agent_rope < 0.6) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor Sports hall environment using a drone.  \n targets: Successfully reach the designated target state, Detect nets as obstacles, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of nearby nets, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_net, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_net = calculate_distance_min(s_agent, s_net) \n    penalty_collision = -19 if (d_agent_net < 0.6) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor Sports hall environment using a drone.  \n targets: Successfully reach the designated target state, Detect flag as obstacles, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of flag, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_flag, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_flag = calculate_distance_min(s_agent, s_flag) \n    penalty_collision = -19 if (d_agent_flag < 0.6) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor Sports hall environment using a drone.  \n targets: Successfully reach the designated target state, Detect player as obstacles, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of player, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_player, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_player = calculate_distance_min(s_agent, s_player) \n    penalty_collision = -19 if (d_agent_player < 0.6) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor Sports hall environment using a drone.  \n targets: Successfully reach the designated target state, Detect ball as obstacles, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of ball, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_ball, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ball = calculate_distance_min(s_agent, s_ball) \n    penalty_collision = -19 if (d_agent_ball < 0.6) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor Sports hall environment using a drone.  \n targets: Successfully reach the designated target state, Detect small ball as obstacles, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of ball, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_ball, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ball = calculate_distance_min(s_agent, s_ball) \n    penalty_collision = -19 if (d_agent_ball < 0.6) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor Sports hall environment using a drone.  \n targets: Successfully reach the designated target state, Detect basketball player as obstacles, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of nearby player, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_player, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_player = calculate_distance_min(s_agent, s_player) \n    penalty_collision = -19 if (d_agent_player < 0.6) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor Sports hall environment using a drone.  \n targets: Successfully reach the designated target state, Detect ball and player as obstacles, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of multiple obstacles, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 1) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor Sports hall environment using a drone.  \n targets: Successfully reach the designated target state, Detect ball and rope as net, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of multiple obstacles, and the target state can all be accessed.  \n Write a python code for reward function",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 1) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor Sports hall environment using a drone.  \n targets: Successfully reach the designated target state, Detect coach and mats as obstacles, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of multiple obstacles, and the target state can all be accessed.  \n Write a python code for reward function",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 1) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor Sports hall environment using a drone.  \n targets: Successfully reach the designated target state, Detect basketball ans rope as obstacles, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of multiple obstacles, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 1) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with basketball and net. \n Conditions: Agent's state and obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 1) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor Sports hall environment using a drone.  \n targets: Successfully reach the designated target state, Detect basketball ans net as obstacles, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of multiple obstacles, and the target state can all be accessed.  \n Write a python code for reward function",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 1) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor Sports hall environment using a drone.  \n targets: Successfully reach the designated target state, Detect ball ans mats as obstacles, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of multiple obstacles, and the target state can all be accessed.  \n Write a python code for reward function",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 1) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor Sports hall environment using a drone.  \n targets: Successfully reach the designated target state, Detect ball, rope ans player as obstacles, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of multiple obstacles, and the target state can all be accessed.  \n Write a python code for reward function",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 1) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor Sports hall environment using a drone.  \n targets: Successfully reach the designated target state, Detect flag, student ans net as obstacles, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of multiple obstacles, and the target state can all be accessed.  \n Write a python code for reward function",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 1) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a hospital, where it must avoid colliding with the patient and reach the target. The drone’s state, target's state and the information of the patient are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_patient, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_patient = calculate_distance_min(s_agent, s_patient) \n    penalty_collision = -19 if (d_agent_patient < 0.6) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a hospital, where it must avoid colliding with the doctor and reach the target. The drone’s state, target's state and the information of the doctor are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_doctor, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_doctor = calculate_distance_min(s_agent, s_doctor) \n    penalty_collision = -19 if (d_agent_doctor < 0.6) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a hospital, where it must avoid colliding with the wire and reach the target. The drone’s state, target's state and the information of the wire are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_wire, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wire = calculate_distance_min(s_agent, s_wire) \n    penalty_collision = -19 if (d_agent_wire < 0.6) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a hospital, where it must avoid colliding with the equipment and reach the target. The drone’s state, target's state and the information of the equipment are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_equipment, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_equipment = calculate_distance_min(s_agent, s_equipment) \n    penalty_collision = -19 if (d_agent_equipment < 0.6) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a hospital, where it must avoid colliding with the wheelchair and reach the target. The drone’s state, target's state and the information of the wheelchair are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_wheelchair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wheelchair = calculate_distance_min(s_agent, s_wheelchair) \n    penalty_collision = -19 if (d_agent_wheelchair < 0.6) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a hospital, where it must avoid colliding with the staff and reach the target. The drone’s state, target's state and the information of the staff are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_staff = calculate_distance_min(s_agent, s_staffr) \n    penalty_collision = -19 if (d_agent_staff < 0.6) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a hospital, where it must avoid colliding with the defibrillator and reach the target. The drone’s state, target's state and the information of the defibrillator are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_defibrillator, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_defibrillator = calculate_distance_min(s_agent, s_defibrillator) \n    penalty_collision = -19 if (d_agent_defibrillator < 0.6) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a hospital, where it must avoid colliding with the device and reach the target. The drone’s state, target's state and the information of the device are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_device, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_device = calculate_distance_min(s_agent, s_device) \n    penalty_collision = -19 if (d_agent_device < 0.6) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a hospital, where it must avoid colliding with the lamp and reach the target. The drone’s state, target's state and the information of the lamp are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_lamp, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lamp = calculate_distance_min(s_agent, s_lamp) \n    penalty_collision = -19 if (d_agent_lamp < 0.6) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a hospital, where it must avoid colliding with the patient and doctor and reach the target. The drone’s state, target's state and the information of the multiple obstacles are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 1) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a hospital, where it must avoid colliding with the defibrillator and equipment and reach the target. The drone’s state, target's state and the information of the multiple obstacles are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 1) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a hospital, where it must avoid colliding with the wire and wheelchair  and reach the target. The drone’s state, target's state and the information of the multiple obstacles are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 1) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a hospital, where it must avoid colliding with the patient and wheelchair and reach the target. The drone’s state, target's state and the information of the multiple obstacles are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 1) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a hospital, where it must avoid colliding with the doctor and lamp and reach the target. The drone’s state, target's state and the information of the multiple obstacles are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 1) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a hospital, where it must avoid colliding with the three packets and reach the target. The drone’s state, target's state and the information of the multiple obstacles are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 1) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a hospital, where it must avoid colliding with the patient, wheelchair and doctor  and reach the target. The drone’s state, target's state and the information of the multiple obstacles are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 1) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a hospital, where it must avoid colliding with the defibrillator, equipment and patient and reach the target. The drone’s state, target's state and the information of the multiple obstacles are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 1) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a hospital, where it must avoid colliding with the lamp, screen and bed and reach the target. The drone’s state, target's state and the information of the multiple obstacles are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 1) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a hospital, where it must avoid colliding with the counter, nurse and bed and reach the target. The drone’s state, target's state and the information of the multiple obstacles are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 1) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a hospital, where it must avoid colliding with the three boxes and reach the target. The drone’s state, target's state and the information of the multiple obstacles are available.  Create a Python code for reward function..",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 1) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores an indoor room, avoids collisions with ceiling fan blades, and uses its state, blade status, and target information to not bump into blades and reach the target. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_blade, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_blade = calculate_distance_min(s_agent, s_blade) \n    penalty_collision = -10 if (d_agent_blade < 0.5) else 0 \n    reward_reaching_target = 5 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent explores an indoor room, avoids collisions with chairs, and uses its state, chair status, and target information to not bump into chair and reach the target. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -10 if (d_agent_chair < 0.5) else 0 \n    reward_reaching_target = 5 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent explores an indoor room, avoids collisions with lamp, and uses its state, lamp status, and target information to not bump into lamp and reach the target. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_lamp, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lamp = calculate_distance_min(s_agent, s_lamp) \n    penalty_collision = -10 if (d_agent_lamp < 0.5) else 0 \n    reward_reaching_target = 5 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent explores an indoor room, avoids collisions with curtains, and uses its state, curtain status, and target information to not bump into curtain and reach the target. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_curtain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_curtain = calculate_distance_min(s_agent, s_curtain) \n    penalty_collision = -10 if (d_agent_curtain < 0.5) else 0 \n    reward_reaching_target = 5 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with obstacles. \n Conditions: Agent's state and obstacles's status are available. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.5) else 0 \n    reward_reaching_target = 5 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent explores an indoor room, avoids collisions with drape, and uses its state, drape status, and target information to not bump into drape and reach the target. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_drape, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_drape = calculate_distance_min(s_agent, s_drape) \n    penalty_collision = -10 if (d_agent_drape < 0.5) else 0 \n    reward_reaching_target = 5 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent explores an indoor room, avoids collisions with wardrobe, and uses its state, wardrobe status, and target information to not bump into wardrobe and reach the target. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_wardrobe, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wardrobe = calculate_distance_min(s_agent, s_wardrobe) \n    penalty_collision = -10 if (d_agent_wardrobe < 0.5) else 0 \n    reward_reaching_target = 5 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent explores an indoor room, avoids collisions with wall, and uses its state, wall information, and target information to not bump into wall and reach the target. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -10 if (d_agent_wall < 0.5) else 0 \n    reward_reaching_target = 5 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent explores an indoor room, avoids collisions with people, and uses its state, people information, and target information to not bump into people and reach the target. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_people, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_people = calculate_distance_min(s_agent, s_people) \n    penalty_collision = -10 if (d_agent_people < 0.5) else 0 \n    reward_reaching_target = 5 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent explores an indoor room, avoids collisions with obstacles, and uses its state, multiplr obstacles information, and target information to not bump into obstacles and reach the target. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.7) else 0 \n    reward_reaching_target = 5 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent explores an indoor room, avoids collisions with blades and projector, and uses its state, multiple obstacles information, and target information to not bump into these obstacles and reach the target. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.7) else 0 \n    reward_reaching_target = 5 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent explores an indoor room, avoids collisions with poeple and projector, and uses its state, multiple obstacles information, and target information to not bump into these obstacles and reach the target. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.7) else 0 \n    reward_reaching_target = 5 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent explores an indoor room, avoids collisions with lamp and projector, and uses its state, multiple obstacles information, and target information to not bump into these obstacles and reach the target. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.7) else 0 \n    reward_reaching_target = 5 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent explores an indoor room, avoids collisions with boxes and chair, and uses its state, multiple obstacles information, and target information to not bump into these obstacles and reach the target. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.7) else 0 \n    reward_reaching_target = 5 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent explores an indoor room, avoids collisions with box and table, and uses its state, multiple obstacles information, and target information to not bump into these obstacles and reach the target. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.7) else 0 \n    reward_reaching_target = 5 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent explores an indoor room, avoids collisions with human and chair, and uses its state, multiple obstacles information, and target information to not bump into these obstacles and reach the target. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.7) else 0 \n    reward_reaching_target = 5 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent explores an indoor room, avoids collisions with human, chair and table, and uses its state, multiple obstacles information, and target information to not bump into these obstacles and reach the target. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.7) else 0 \n    reward_reaching_target = 5 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent explores an indoor room, avoids collisions with human, projector and bench, and uses its state, multiple obstacles information, and target information to not bump into these obstacles and reach the target. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.7) else 0 \n    reward_reaching_target = 5 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent explores an indoor room, avoids collisions with people, bench and box, and uses its state, multiple obstacles information, and target information to not bump into these obstacles and reach the target. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.7) else 0 \n    reward_reaching_target = 5 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent explores an indoor room, avoids collisions with shelf, plant and decor and uses its state, multiple obstacles information, and target information to not bump into these obstacles and reach the target. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.7) else 0 \n    reward_reaching_target = 5 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Drone indoor exploration in a library. \n Goal: Avoid hitting the surrounding walls. \n Given: Agent's state, wall's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -15 if (d_agent_wall < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Drone indoor exploration in a library. \n Goal: Navigate without crashing into walls. \n Given: Agent's state, obstacle's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -15 if (d_agent_wall < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Drone indoor exploration in a library. \n Goal: Detect obstacle (chairs) and avoid collision. \n Given: Agent's state, chair's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -15 if (d_agent_chair < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Drone indoor exploration in a library. \n Goal: Detect obstacle (table) and avoid collision. \n Given: Agent's state, table's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_table, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_table_min = calculate_distance_min(s_agent, s_table) \n    penalty_collision = -15 if (d_agent_table_min < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Drone indoor exploration in a library. \n Goal: Detect bookshelves and avoid collision. \n Given: Agent's state, bookshelve's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bookshelve, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bookshelve = calculate_distance_min(s_agent, s_bookshelve) \n    penalty_collision = -15 if (d_agent_bookshelve < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Drone indoor exploration in a library. \n Goal: Detect smoke detectors and avoid collision. \n Given: Agent's state, smoke detector's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_smoke_detector, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_smoke_detectore = calculate_distance_min(s_agent, s_smoke_detector) \n    penalty_collision = -15 if (d_agent_smoke_detector < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Drone indoor exploration in a library. \n Goal: Detect obstacle (stacked books) and avoid collision. \n Given: Agent's state, book's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_book, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_book = calculate_distance_min(s_agent, s_book) \n    penalty_collision = -15 if (d_agent_book < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Drone indoor exploration in a library. \n Goal: Detect dynamic obstacle (human) and avoid collision. \n Given: Agent's state, human's state are accessible. \n Write a reward function in Python",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -15 if (d_agent_human < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Drone indoor exploration in a library. \n Goal: Detect chairs and tabels and avoid collision. \n Given: Agent's state, obstacles' states are accessible. \n Write a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Drone indoor exploration in a library. \n Goal: Detect students and shelves and avoid collision. \n Given: Agent's state, obstacles' states are accessible. \n Write a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Drone indoor exploration in a library. \n Goal: Avoid collisions with lamps and plants. \n Given: Agent's state, obstacles' states are accessible. \n Write a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Drone indoor exploration in a library. \n Goal: Avoid collisions with decorations and tall shelves. \n Given: Agent's state, obstacles' states are accessible. \n Write a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Drone indoor exploration in a library. \n Goal: Avoid collisions with stack books and lamps. \n Given: Agent's state, obstacles' states are accessible. \n Write a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Drone indoor exploration in a library. \n Goal: Avoid collisions with books and lights. \n Given: Agent's state, obstacles' states are accessible. \n Write a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Drone indoor exploration in a library. \n Goal: Avoid collisions with books, lights and humans. \n Given: Agent's state, obstacles' states are accessible. \n Write a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Drone indoor exploration in a library. \n Goal: Avoid collisions with chair, table and humans. \n Given: Agent's state, obstacles' states are accessible. \n Write a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Drone indoor exploration in a library. \n Goal: Avoid collisions with desks, lamps and students. \n Given: Agent's state, obstacles' states are accessible. \n Write a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Drone indoor exploration in a library. \n Goal: Avoid collisions with stacked books, computers and staffs. \n Given: Agent's state, obstacles' states are accessible. \n Write a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Drone indoor exploration in a library. \n Goal: Avoid collisions with stacked books, laptops and book carts. \n Given: Agent's state, obstacles' states are accessible. \n Write a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Drone indoor exploration in a library. \n Goal: Avoid collisions with  curtains, plants and staffs. \n Given: Agent's state, obstacles' states are accessible. \n Write a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a forest setting, a drone navigates while tracking the human and avoiding collisions. The agent can access drone's state, target's state and the human’s state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -15 if (d_agent_human < 1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a forest setting, a drone navigates while tracking the tree and avoiding collisions. The agent can access drone's state, target's state and the tree state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_tree, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tree = calculate_distance_min(s_agent, s_tree) \n    penalty_collision = -15 if (d_agent_tree < 1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a forest setting, a drone navigates while tracking the leaf and avoiding collisions. The agent can access drone's state, target's state and the leaf state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_leaf, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_leaf = calculate_distance_min(s_agent, s_leaf) \n    penalty_collision = -15 if (d_agent_leaf < 1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a forest setting, a drone navigates while tracking the bird and avoiding collisions. The agent can access drone's state, target's state and the bird state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bird, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bird = calculate_distance_min(s_agent, s_bird) \n    penalty_collision = -15 if (d_agent_bird < 1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a forest setting, a drone navigates while tracking the stump and avoiding collisions. The agent can access drone's state, target's state and the stump's state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_stump, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_stump = calculate_distance_min(s_agent, s_stump) \n    penalty_collision = -15 if (d_agent_stump < 1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a forest setting, a drone navigates while tracking the rock and avoiding collisions. The agent can access drone's state, target's state and the rock’s state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_rock, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rock = calculate_distance_min(s_agent, s_rock) \n    penalty_collision = -15 if (d_agent_rock < 1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a forest setting, a drone navigates while tracking the branch and avoiding collisions. The agent can access drone's state, target's state and the branch’s state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_branch, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_branch = calculate_distance_min(s_agent, s_branch) \n    penalty_collision = -15 if (d_agent_branch < 1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a forest setting, a drone navigates while tracking the tree branch and avoiding collisions. The agent can access drone's state, target's state and the tree branch’s state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_tree_branch, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tree_branch = calculate_distance_min(s_agent, s_tree_branch) \n    penalty_collision = -15 if (d_agent_tree_branch < 1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a forest setting, a drone navigates while tracking the people and avoiding collisions. The agent can access drone's state, target's state and the people’s state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_people, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_people = calculate_distance_min(s_agent, s_people) \n    penalty_collision = -15 if (d_agent_people < 1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a forest setting, a drone navigates while tracking the stone and avoiding collisions. The agent can access drone's state, target's state and the stone’s state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_stone, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_stone = calculate_distance_min(s_agent, s_stone) \n    penalty_collision = -15 if (d_agent_stone < 1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone moves through a forest, targeting a target and steering clear of trees and stones. Inputs: The drone’s state and the states of obstacles are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone moves through a forest, targeting a target and steering clear of trees and branches. Inputs: The drone’s state and the states of obstacles are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone moves through a forest, targeting a target and steering clear of human and animal. Inputs: The drone’s state and the states of obstacles are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone moves through a forest, targeting a target and steering clear of trees and leaves. Inputs: The drone’s state and the states of obstacles are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone moves through a forest, targeting a target and steering clear of the multiple trees. Inputs: The drone’s state and the states of obstacles are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone moves through a forest, targeting a target and steering clear of trees, human and branches. Inputs: The drone’s state and the states of obstacles are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone moves through a forest, targeting a target and steering clear of human, birds and stump. Inputs: The drone’s state and the states of obstacles are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone moves through a forest, targeting a target and steering clear of trees, animals and stones. Inputs: The drone’s state and the states of obstacles are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone moves through a forest, targeting a target and steering clear of maple trees, signs and cliffs. Inputs: The drone’s state and the states of obstacles are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone moves through a forest, targeting a target and steering clear of trees, signs and animals. Inputs: The drone’s state and the states of obstacles are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, human's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -15 if (d_agent_human < 1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid any collision with hanging lamps. \n Conditions: Agent's information and lamp's information are available \n Write a reward in Python.",
        "chosen": "def reward(s_agent, s_lamp, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lamp = calculate_distance_min(s_agent, s_lamp) \n    penalty_collision = -15 if (d_agent_lamp < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid any static obstacle such as food serving counters. \n Conditions: Agent's state, food counter's state are accessible. \n Write a reward in Python.",
        "chosen": "def reward(s_agent, s_counter, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_counter = calculate_distance_min(s_agent, s_counter) \n    penalty_collision = -15 if (d_agent_counter < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with a chair. \n Conditions: Agent's state, chair's states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -15 if (d_agent_chair < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with a table. \n Conditions: Agent's state, table's state are available. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_table, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_table = calculate_distance_min(s_agent, s_table) \n    penalty_collision = -15 if (d_agent_table < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with trash bins. \n Conditions: Agent's state and trash bin's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_trash_bin, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_trash_bin = calculate_distance_min(s_agent, s_trash_bin) \n    penalty_collision = -15 if (d_agent_trash_bin < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with a low-hanging decoration. \n Conditions: Agent's state, hanging decoration's state and desired speed are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_decoration, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_decoration = calculate_distance_min(s_agent, s_decoration) \n    penalty_collision = -15 if (d_agent_decoration < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with plants. \n Conditions: Agent's state and plant's state are available. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_plant, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_plant = calculate_distance_min(s_agent, s_plant) \n    penalty_collision = -15 if (d_agent_plant < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Do not collide with metal rails. \n Conditions: Agent's state, metal rail's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_rail, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rail = calculate_distance_min(s_agent, s_rail) \n    penalty_collision = -15 if (d_agent_rail < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Do not collide with cameras. \n Conditions: Agent's state and camera's state are available. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_cameras, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cameras = calculate_distance_min(s_agent, s_cameras) \n    penalty_collision = -15 if (d_agent_cameras < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a mensa. \n Objectives: Avoid collisions with people and table. \n Conditions: Agent's state and obstacles' states are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a mensa. \n Objectives: Avoid collisions with students and chairs. \n Conditions: Agent's state and obstacles' states are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a mensa. \n Objectives: Avoid collisions with plants and chairs. \n Conditions: Agent's state and obstacles' states are available. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a mensa. \n Objectives: Avoid collisions with plants and desk. \n Conditions: Agent's state and obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a mensa. \n Objectives: Avoid collisions with decoration and plant. \n Conditions: Agent's state and obstacles' states are available. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a mensa. \n Objectives: Avoid collisions with cameras, student and table. \n Conditions: Agent's state and obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a mensa. \n Objectives: Avoid collisions with chair, plant and table. \n Conditions: Agent's state and obstacles' states are available. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a mensa. \n Objectives: Avoid collisions with staff, student and table. \n Conditions: Agent's state and obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a mensa. \n Objectives: Avoid collisions with food counter, rails and camera. \n Conditions: Agent's state and obstacles' states are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a mensa. \n Objectives: Avoid collisions with food counter, trash bin and staff. \n Conditions: Agent's state and obstacles' states are available. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to navigate a drone in-door a classroom so that it avoids collisions with the walls while reaching a specified target. The drone’s state, the walls’ positions, and the target’s state are all available. Implement a reward function.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -15 if (d_agent_wall < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to navigate a drone in-door a classroom so that it avoids collisions with the people while reaching a specified target. The drone’s state, the people's positions, and the target’s state are all available. Implement a reward function.",
        "chosen": "def reward(s_agent, s_people, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_people = calculate_distance_min(s_agent, s_people) \n    penalty_collision = -15 if (d_agent_people < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to navigate a drone in-door a classroom so that it avoids collisions with the projector while reaching a specified target. The drone’s state, the projectors’ positions, and the target’s state are all available. Implement a reward function.",
        "chosen": "def reward(s_agent, s_projector, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_projector = calculate_distance_min(s_agent, s_projector) \n    penalty_collision = -15 if (d_agent_projector < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to navigate a drone in-door a classroom so that it avoids collisions with the lamps while reaching a specified target. The drone’s state, the lampls’ positions, and the target’s state are all available. Implement a reward function",
        "chosen": "def reward(s_agent, s_lamp, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lamp = calculate_distance_min(s_agent, s_lamp) \n    penalty_collision = -15 if (d_agent_lamp < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to navigate a drone in-door a classroom so that it avoids collisions with the blackboard while reaching a specified target. The drone’s state, the blackboards’ positions, and the target’s state are all available. Implement a reward function",
        "chosen": "def reward(s_agent, s_blackboard, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_blackboard = calculate_distance_min(s_agent, s_blackboard) \n    penalty_collision = -15 if (d_agent_blackboard < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to navigate a drone in-door a classroom so that it avoids collisions with the tall booksshelf while reaching a specified target. The drone’s state, the bookshelves’ positions, and the target’s state are all available. Implement a reward function.",
        "chosen": "def reward(s_agent, s_bookshelve, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bookshelve = calculate_distance_min(s_agent, s_bookshelve) \n    penalty_collision = -15 if (d_agent_bookshelve < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to navigate a drone in-door a classroom so that it avoids collisions with the backpacks while reaching a specified target. The drone’s state, the backpacks’ positions, and the target’s state are all available. Implement a reward function.",
        "chosen": "def reward(s_agent, s_backpack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_backpack = calculate_distance_min(s_agent, s_backpack) \n    penalty_collision = -15 if (d_agent_backpack < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to navigate a drone in-door a classroom so that it avoids collisions with the humans while reaching a specified target. The drone’s state, the human’ positions, and the target’s state are all available. Implement a reward function.",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -15 if (d_agent_human < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to navigate a drone in-door a classroom so that it avoids collisions with the teacher while reaching a specified target. The drone’s state, the teachers’ positions, and the target’s state are all available. Implement a reward function.",
        "chosen": "def reward(s_agent, s_teacher, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_teacher = calculate_distance_min(s_agent, s_teacher) \n    penalty_collision = -15 if (d_agent_teacher < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to navigate a drone in-door a classroom so that it avoids collisions with the student while reaching a specified target. The drone’s state, the student's positions, and the target’s state are all available. Implement a reward function.",
        "chosen": "def reward(s_agent, s_student, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_student = calculate_distance_min(s_agent, s_student) \n    penalty_collision = -15 if (d_agent_student < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to navigate a drone in-door a classroom so that it avoids collisions with the student and teacher while reaching a specified target. The drone’s state, the multiple obstacles’ positions, and the target’s state are all available. Implement a reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to navigate a drone in-door a classroom so that it avoids collisions with the chair and desk while reaching a specified target. The drone’s state, the multiple obstacles’ positions, and the target’s state are all available. Implement a reward function",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to navigate a drone in-door a classroom so that it avoids collisions with the student and backpack while reaching a specified target. The drone’s state, the multiple obstacles’ positions, and the target’s state are all available. Implement a reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to navigate a drone in-door a classroom so that it avoids collisions with the shelf and backpack while reaching a specified target. The drone’s state, the multiple obstacles’ positions, and the target’s state are all available. Implement a reward function",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to navigate a drone in-door a classroom so that it avoids collisions with the blackboard and whiteboard while reaching a specified target. The drone’s state, the multiple obstacles’ positions, and the target’s state are all available. Implement a reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to navigate a drone in-door a classroom so that it avoids collisions with the blackboard, whiteboard and teacher while reaching a specified target. The drone’s state, the multiple obstacles’ positions, and the target’s state are all available. Implement a reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to navigate a drone in-door a classroom so that it avoids collisions with the pupil, desk and teacher while reaching a specified target. The drone’s state, the multiple obstacles’ positions, and the target’s state are all available. Implement a reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to navigate a drone in-door a classroom so that it avoids collisions with the shelves, student and teacher while reaching a specified target. The drone’s state, the multiple obstacles’ positions, and the target’s state are all available. Implement a reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to navigate a drone in-door a classroom so that it avoids collisions with the shelf, chair and table while reaching a specified target. The drone’s state, the multiple obstacles’ positions, and the target’s state are all available. Implement a reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to navigate a drone in-door a classroom so that it avoids collisions with the many boxes while reaching a specified target. The drone’s state, the multiple obstacles’ positions, and the target’s state are all available. Implement a reward function",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating a conference room. The function should encourage target-reaching, penalize collisions with moving people, and use the available agent, target, and people states.",
        "chosen": "def reward(s_agent, s_people, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_people = calculate_distance_min(s_agent, s_people) \n    penalty_collision = -15 if (d_agent_people < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating a conference room. The function should encourage target-reaching, penalize collisions with desk, and use the available agent, target, and desk states.",
        "chosen": "def reward(s_agent, s_desk, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_desk = calculate_distance_min(s_agent, s_desk) \n    penalty_collision = -15 if (d_agent_desk < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating a conference room. The function should encourage target-reaching, penalize collisions with sideboard, and use the available agent, target, and sideboard states.",
        "chosen": "def reward(s_agent, s_sideboard, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sideboard = calculate_distance_min(s_agent, s_sideboard) \n    penalty_collision = -15 if (d_agent_sideboard < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating a conference room. The function should encourage target-reaching, penalize collisions with bench, and use the available agent, target, and bench states.",
        "chosen": "def reward(s_agent, s_bench, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bench = calculate_distance_min(s_agent, s_bench) \n    penalty_collision = -15 if (d_agent_bench < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating a conference room. The function should encourage target-reaching, penalize collisions with whiteboard, and use the available agent, target, and whiteboard states.",
        "chosen": "def reward(s_agent, s_whiteboard, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_whiteboard = calculate_distance_min(s_agent, s_whiteboard) \n    penalty_collision = -15 if (d_agent_sideboard < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating a conference room. The function should encourage target-reaching, penalize collisions with projector, and use the available agent, target, and projector states.",
        "chosen": "def reward(s_agent, s_projector, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_projector = calculate_distance_min(s_agent, s_projector) \n    penalty_collision = -15 if (d_agent_projector < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating a conference room. The function should encourage target-reaching, penalize collisions with equipment, and use the available agent, target, and equipment states.",
        "chosen": "def reward(s_agent, s_equipment, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_equipment = calculate_distance_min(s_agent, s_equipment) \n    penalty_collision = -15 if (d_agent_equipment < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating a conference room. The function should encourage target-reaching, penalize collisions with speaker, and use the available agent, target, and speaker states.",
        "chosen": "def reward(s_agent, s_speaker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_speaker = calculate_distance_min(s_agent, s_speaker) \n    penalty_collision = -15 if (d_agent_speaker < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating a conference room. The function should encourage target-reaching, penalize collisions with laptop, and use the available agent, target, and laptop states.",
        "chosen": "def reward(s_agent, s_laptop, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_laptop = calculate_distance_min(s_agent, s_laptop) \n    penalty_collision = -15 if (d_agent_laptop < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating a conference room. The function should encourage target-reaching, penalize collisions with screen, and use the available agent, target, and screen states.",
        "chosen": "def reward(s_agent, s_screen, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_screen = calculate_distance_min(s_agent, s_screen) \n    penalty_collision = -15 if (d_agent_screen < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating a conference room. The function should encourage target-reaching, penalize collisions with screen and projector, and use the available agent, target, and multiple obstacle states.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating a conference room. The function should encourage target-reaching, penalize collisions with chair and table, and use the available agent, target, and multiple obstacle states.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating a conference room. The function should encourage target-reaching, penalize collisions with computer and person, and use the available agent, target, and multiple obstacle states.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating a conference room. The function should encourage target-reaching, penalize collisions with plant and shelf, and use the available agent, target, and multiple obstacle states.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating a conference room. The function should encourage target-reaching, penalize collisions with multiple box, and use the available agent, target, and multiple obstacle states.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating a conference room. The function should encourage target-reaching, penalize collisions with person, chair and table, and use the available agent, target, and multiple obstacle states.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating a conference room. The function should encourage target-reaching, penalize collisions with laptop, shelf and plant, and use the available agent, target, and multiple obstacle states.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating a conference room. The function should encourage target-reaching, penalize collisions with shelf, box and plant, and use the available agent, target, and multiple obstacle states.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating a conference room. The function should encourage target-reaching, penalize collisions with whiteboard, screen and lamp, and use the available agent, target, and multiple obstacle states.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating a conference room. The function should encourage target-reaching, penalize collisions with multiple boxes and use the available agent, target, and multiple obstacle states.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Create a reward function in Python where a warehouse drone navigates to a target. The drone should avoid hitting the staff. You have access to agent, target, and staff states.",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_staff = calculate_distance_min(s_agent, s_staff) \n    penalty_collision = -15 if (d_agent_staff < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Create a reward function in Python where a warehouse drone navigates to a target. The drone should avoid hitting the work. You have access to agent, target, and worker states.",
        "chosen": "def reward(s_agent, s_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_worker = calculate_distance_min(s_agent, s_worker) \n    penalty_collision = -15 if (d_agent_worker < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Create a reward function in Python where a warehouse drone navigates to a target. The drone should avoid hitting the human. You have access to agent, target, and human states.",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -15 if (d_agent_human < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Create a reward function in Python where a warehouse drone navigates to a target. The drone should avoid hitting the human workers. You have access to agent, target, and worker states.",
        "chosen": "def reward(s_agent, s_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_worker = calculate_distance_min(s_agent, s_worker) \n    penalty_collision = -15 if (d_agent_worker < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Create a reward function in Python where a warehouse drone navigates to a target. The drone should avoid hitting the rack. You have access to agent, target, and rack states.",
        "chosen": "def reward(s_agent, s_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rack = calculate_distance_min(s_agent, s_rack) \n    penalty_collision = -15 if (d_agent_rack < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Create a reward function in Python where a warehouse drone navigates to a target. The drone should avoid hitting the forklift. You have access to agent, target, and forklift states.",
        "chosen": "def reward(s_agent, s_forklift, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_forklift = calculate_distance_min(s_agent, s_forklift) \n    penalty_collision = -15 if (d_agent_forklift < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Create a reward function in Python where a warehouse drone navigates to a target. The drone should avoid hitting the cable. You have access to agent, target, and cable states.",
        "chosen": "def reward(s_agent, s_cable, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cable = calculate_distance_min(s_agent, s_cable) \n    penalty_collision = -15 if (d_agent_cable < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Create a reward function in Python where a warehouse drone navigates to a target. The drone should avoid hitting the pallet. You have access to agent, target, and pallet states.",
        "chosen": "def reward(s_agent, s_pallet, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_pallet = calculate_distance_min(s_agent, s_pallet) \n    penalty_collision = -15 if (d_agent_pallet < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Create a reward function in Python where a warehouse drone navigates to a target. The drone should avoid hitting the package. You have access to agent, target, and package states..",
        "chosen": "def reward(s_agent, s_package, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_package = calculate_distance_min(s_agent, s_package) \n    penalty_collision = -15 if (d_agent_package < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Create a reward function in Python where a warehouse drone navigates to a target. The drone should avoid hitting the pipe. You have access to agent, target, and pipe states.",
        "chosen": "def reward(s_agent, s_pipe, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_pipe = calculate_distance_min(s_agent, s_pipe) \n    penalty_collision = -15 if (d_agent_pipe < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Create a reward function in Python where a warehouse drone navigates to a target. The drone should avoid hitting the beam. You have access to agent, target, and beam states.",
        "chosen": "def reward(s_agent, s_beam, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_pipe = calculate_distance_min(s_agent, s_beam) \n    penalty_collision = -15 if (d_agent_beam < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Create a reward function in Python where a warehouse drone navigates to a target. The drone should avoid hitting the beam and pipe. You have access to agent, target, and multiple obstacles states.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Create a reward function in Python where a warehouse drone navigates to a target. The drone should avoid hitting the packet and staff. You have access to agent, target, and multiple obstacles states.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Create a reward function in Python where a warehouse drone navigates to a target. The drone should avoid hitting the stacked boxes. You have access to agent, target, and multiple obstacles states.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Create a reward function in Python where a warehouse drone navigates to a target. The drone should avoid hitting the maschine and staff. You have access to agent, target, and multiple obstacles states.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Create a reward function in Python where a warehouse drone navigates to a target. The drone should avoid hitting the beam and hanging lamp. You have access to agent, target, and multiple obstacles states.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Create a reward function in Python where a warehouse drone navigates to a target. The drone should avoid hitting the cable, rack and people. You have access to agent, target, and multiple obstacles states.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Create a reward function in Python where a warehouse drone navigates to a target. The drone should avoid hitting the computer, desk and chair. You have access to agent, target, and multiple obstacles states.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Create a reward function in Python where a warehouse drone navigates to a target. The drone should avoid hitting the beam, pipe and lamp. You have access to agent, target, and multiple obstacles states.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Create a reward function in Python where a warehouse drone navigates to a target. The drone should avoid hitting the forklift, chair and rail. You have access to agent, target, and multiple obstacles states.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a theater environment, a drone is required to navigate safely and reach target. It can access its own state and the states of people in the theater and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_people, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_people = calculate_distance_min(s_agent, s_people) \n    penalty_collision = -15 if (d_agent_people < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a theater environment, a drone is required to navigate safely and reach target. It can access its own state and the states of spotlight in the theater and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_spotlamp, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_spotlamp = calculate_distance_min(s_agent, s_spotlamp) \n    penalty_collision = -15 if (d_agent_spotlamp < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a theater environment, a drone is required to navigate safely and reach target. It can access its own state and the states of balcony in the theater and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_balcony, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_balcony = calculate_distance_min(s_agent, s_balcony) \n    penalty_collision = -15 if (d_agent_balcony < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a theater environment, a drone is required to navigate safely and reach target. It can access its own state and the states of seat in the theater and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_seat, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_seat = calculate_distance_min(s_agent, s_seat) \n    penalty_collision = -15 if (d_agent_seat < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a theater environment, a drone is required to navigate safely and reach target. It can access its own state and the states of projector in the theater and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_projector, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_projector = calculate_distance_min(s_agent, s_projector) \n    penalty_collision = -15 if (d_agent_projector < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a theater environment, a drone is required to navigate safely and reach target. It can access its own state and the states of cable in the theater and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_cable, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cable = calculate_distance_min(s_agent, s_cable) \n    penalty_collision = -15 if (d_agent_cable < 0.5) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a theater environment, a drone is required to navigate safely and reach target. It can access its own state and the states of human in the theater and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -15 if (d_agent_human < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a theater environment, a drone is required to navigate safely and reach target. It can access its own state and the states of platform in the theater and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_platform, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_platform = calculate_distance_min(s_agent, s_platform) \n    penalty_collision = -15 if (d_agent_platform < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a theater environment, a drone is required to navigate safely and reach target. It can access its own state and the states of people in the drape and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_drape, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_drape = calculate_distance_min(s_agent, s_drape) \n    penalty_collision = -15 if (d_agent_drape < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a theater environment, a drone is required to navigate safely and reach target. It can access its own state and the states of microphone in the theater and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_microphone, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_microphone = calculate_distance_min(s_agent, s_microphone) \n    penalty_collision = -15 if (d_agent_microphone < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a theater environment, a drone is required to navigate safely around spotlight and projector and reach target.  It can access its own state and the states of multiple obstacles in the theater and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a theater environment, a drone is required to navigate safely around chair and staff and reach target.  It can access its own state and the states of multiple obstacles in the theater and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a theater environment, a drone is required to navigate safely around stage and drape and reach target.  It can access its own state and the states of multiple obstacles in the theater and target’s state. Write a reward function in Python supporting safe movement",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a theater environment, a drone is required to navigate safely around microphone and music box and reach target.  It can access its own state and the states of multiple obstacles in the theater and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a theater environment, a drone is required to navigate safely around staff and visitor and reach target.  It can access its own state and the states of multiple obstacles in the theater and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a theater environment, a drone is required to navigate safely around chandlier, catwork and rope and reach target.  It can access its own state and the states of multiple obstacles in the theater and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a theater environment, a drone is required to navigate safely around table, chair and human and reach target.  It can access its own state and the states of multiple obstacles in the theater and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a theater environment, a drone is required to navigate safely around stacked boxes and reach target.  It can access its own state and the states of multiple obstacles in the theater and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a theater environment, a drone is required to navigate safely around beam, spotlight and rope and reach target.  It can access its own state and the states of multiple obstacles in the theater and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a theater environment, a drone is required to navigate safely around platform, microphone and drape and reach target.  It can access its own state and the states of multiple obstacles in the theater and target’s state. Write a reward function in Python supporting safe movement",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a theater environment, a drone is required to navigate safely around visitor seat and box and reach target.  It can access its own state and the states of multiple obstacles in the theater and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, avoids humans, and aims to reach a target using its state and avoid hitting  human with using human's status. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -15 if (d_agent_human < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, avoids person, and aims to reach a target using its state and avoid hitting person with using person's status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_person, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_person = calculate_distance_min(s_agent, s_person) \n    penalty_collision = -15 if (d_agent_person < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, avoids basketball hoop, and aims to reach a target using its state and avoid hitting basketball hoop with using basketball hoop's status. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_basket_hoop, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_basket_hoop = calculate_distance_min(s_agent, s_basket_hoop) \n    penalty_collision = -15 if (d_agent_basket_hoop < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, avoids scorboard, and aims to reach a target using its state and avoid hitting scoreboard with using scoreboard's status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_scoreboard, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_scoreboard = calculate_distance_min(s_agent, s_scoreboard) \n    penalty_collision = -15 if (d_agent_scoreboard < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, avoids rope, and aims to reach a target using its state and avoid hitting rope with using rope's status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_rope, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rope = calculate_distance_min(s_agent, s_rope) \n    penalty_collision = -15 if (d_agent_rope < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, avoids net, and aims to reach a target using its state and avoid hitting net with using net's status. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_net, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_net = calculate_distance_min(s_agent, s_net) \n    penalty_collision = -15 if (d_agent_net < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, avoids flag, and aims to reach a target using its state and avoid hitting flag with using flag's status. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_flag, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_flag = calculate_distance_min(s_agent, s_flag) \n    penalty_collision = -15 if (d_agent_flag < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, avoids player, and aims to reach a target using its state and avoid hitting player with using player's status. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_player, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_player = calculate_distance_min(s_agent, s_player) \n    penalty_collision = -15 if (d_agent_player < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, avoids ball, and aims to reach a target using its state and avoid hitting ball with using ball's status. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_ball, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ball = calculate_distance_min(s_agent, s_ball) \n    penalty_collision = -15 if (d_agent_ball < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, avoids basketball, and aims to reach a target using its state and avoid hitting ball with using ball's status. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_ball, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ball = calculate_distance_min(s_agent, s_ball) \n    penalty_collision = -15 if (d_agent_ball < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, avoids player, and aims to reach a target using its state and avoid hitting player with using player's status. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_player, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_player = calculate_distance_min(s_agent, s_player) \n    penalty_collision = -15 if (d_agent_player < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, avoids player and ball, and aims to reach a target using its state and avoid hitting player and ball with using multiple obstcale's status. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, avoids rope and net, and aims to reach a target using its state and avoid hitting rope and net with using multiple obstcale's status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, avoids player and ball, and aims to reach a target using its state and avoid hitting player and ball with using multiple obstcale's status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, avoids coach and net, and aims to reach a target using its state and avoid hitting coach and net with using multiple obstcale's status. Write a reward function in Python..",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, avoids net and basketball, and aims to reach a target using its state and avoid hitting net and basketball with using multiple obstcale's status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, avoids volleyball and small ball, and aims to reach a target using its state and avoid hitting plvolleyball ayer and ball with using multiple obstcale's status. Write a reward function in Python..",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, avoids coach, basketball and net, and aims to reach a target using its state and avoid hitting coach, basketball and net with using multiple obstcale's status. Write a reward function in Python..",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, avoids player, net and ball, and aims to reach a target using its state and avoid hitting player, net and ball with using multiple obstcale's status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, avoids flag, student and mat, and aims to reach a target using its state and avoid hitting flag, studnet and net with using multiple obstcale's status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor hospital with a drone, where the drone must reach the target while detecting patient as obstacles and avoiding collisions, assuming the agent’s state, patient states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_patient, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_patient = calculate_distance_min(s_agent, s_patient) \n    penalty_collision = -15 if (d_agent_patient < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor hospital with a drone, where the drone must reach the target while detecting doctor as obstacles and avoiding collisions, assuming the agent’s state, doctor states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_doctor, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_doctor = calculate_distance_min(s_agent, s_doctor) \n    penalty_collision = -15 if (d_agent_doctor < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor hospital with a drone, where the drone must reach the target while detecting wire as obstacles and avoiding collisions, assuming the agent’s state, wire states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_wire, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wire = calculate_distance_min(s_agent, s_wire) \n    penalty_collision = -15 if (d_agent_wire < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor hospital with a drone, where the drone must reach the target while detecting equipment as obstacles and avoiding collisions, assuming the agent’s state, equipment states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_equipment, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_equipment = calculate_distance_min(s_agent, s_equipment) \n    penalty_collision = -15 if (d_agent_equipment < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor hospital with a drone, where the drone must reach the target while detecting wheelchair as obstacles and avoiding collisions, assuming the agent’s state, wheelchair states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_wheelchair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wheelchair = calculate_distance_min(s_agent, s_wheelchair) \n    penalty_collision = -15 if (d_agent_wheelchair < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor hospital with a drone, where the drone must reach the target while detecting staff as obstacles and avoiding collisions, assuming the agent’s state, staff states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_staff = calculate_distance_min(s_agent, s_staffr) \n    penalty_collision = -15 if (d_agent_staff < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor hospital with a drone, where the drone must reach the target while detecting defibrillator as obstacles and avoiding collisions, assuming the agent’s state, defibrillator states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_defibrillator, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_defibrillator = calculate_distance_min(s_agent, s_defibrillator) \n    penalty_collision = -15 if (d_agent_defibrillator < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor hospital with a drone, where the drone must reach the target while detecting device as obstacles and avoiding collisions, assuming the agent’s state, device states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_device, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_device = calculate_distance_min(s_agent, s_device) \n    penalty_collision = -15 if (d_agent_device < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor hospital with a drone, where the drone must reach the target while detecting lamp as obstacles and avoiding collisions, assuming the agent’s state, lamp states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_lamp, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lamp = calculate_distance_min(s_agent, s_lamp) \n    penalty_collision = -15 if (d_agent_lamp < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor hospital with a drone, where the drone must reach the target while detecting staff and patient as obstacles and avoiding collisions, assuming the agent’s state, multiple obstacles states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor hospital with a drone, where the drone must reach the target while detecting defibrillator and equipment as obstacles and avoiding collisions, assuming the agent’s state, multiple obstacles states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor hospital with a drone, where the drone must reach the target while detecting wheelchair and wire as obstacles and avoiding collisions, assuming the agent’s state, multiple obstacles states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor hospital with a drone, where the drone must reach the target while detecting wheelchair and patient as obstacles and avoiding collisions, assuming the agent’s state, multiple obstacles states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor hospital with a drone, where the drone must reach the target while detecting staff and lamp as obstacles and avoiding collisions, assuming the agent’s state, multiple obstacles states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor hospital with a drone, where the drone must reach the target while detecting multiple obstacles as obstacles and avoiding collisions, assuming the agent’s state, multiple obstacles states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor hospital with a drone, where the drone must reach the target while detecting wheelchair, doctor and patient as obstacles and avoiding collisions, assuming the agent’s state, multiple obstacles states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor hospital with a drone, where the drone must reach the target while detecting defibrillator, equipment and patient as obstacles and avoiding collisions, assuming the agent’s state, multiple obstacles states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor hospital with a drone, where the drone must reach the target while detecting lamp, bed and screen as obstacles and avoiding collisions, assuming the agent’s state, multiple obstacles states, and the target state are observable. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor hospital with a drone, where the drone must reach the target while detecting counter, nurse and bed as obstacles and avoiding collisions, assuming the agent’s state, multiple obstacles states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor hospital with a drone, where the drone must reach the target while detecting multiple package as obstacles and avoiding collisions, assuming the agent’s state, multiple obstacles states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.8) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -3*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a room, aims to reach a designated target, and must steer clear of ceiling fan blades by relying on its own state, the blades’ status, and the target information. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_blade, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_blade = calculate_distance_min(s_agent, s_blade) \n    penalty_collision = -10 if (d_agent_blade < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a room, aims to reach a designated target, and must steer clear of chair by relying on its own state, the chair status, and the target information. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -10 if (d_agent_chair < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a room, aims to reach a designated target, and must steer clear of lamp by relying on its own state, the lamp status, and the target information. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_lamp, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lamp = calculate_distance_min(s_agent, s_lamp) \n    penalty_collision = -10 if (d_agent_lamp < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a room, aims to reach a designated target, and must steer clear of curtain by relying on its own state, the curtain status, and the target information. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_curtain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_curtain = calculate_distance_min(s_agent, s_curtain) \n    penalty_collision = -10 if (d_agent_curtain < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a room, aims to reach a designated target, and must steer clear of obstacles by relying on its own state, the obstacles status, and the target information. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a room, aims to reach a designated target, and must steer clear of drape by relying on its own state, the drape status, and the target information. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_drape, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_drape = calculate_distance_min(s_agent, s_drape) \n    penalty_collision = -10 if (d_agent_drape < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a room, aims to reach a designated target, and must steer clear of wardrobe by relying on its own state, the wardrope status, and the target information. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_wardrobe, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wardrobe = calculate_distance_min(s_agent, s_wardrobe) \n    penalty_collision = -10 if (d_agent_wardrobe < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a room, aims to reach a designated target, and must steer clear of walls by relying on its own state, the wall status, and the target information. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -10 if (d_agent_wall < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a room, aims to reach a designated target, and must steer clear of people by relying on its own state, the people status, and the target information. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_people, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_people = calculate_distance_min(s_agent, s_people) \n    penalty_collision = -10 if (d_agent_people < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a room, aims to reach a designated target, and must avoid collisions with multiple obstacles such as chair and table by relying on its own state, the obstacles’ status, and the target information. Create a reward function in Python.n Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < c_1) else 6 \n    reward_reaching_target = 11 if (d_agent_target < c_2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a room, aims to reach a designated target, and must avoid collisions with multiple obstacles such as fan blades and table by relying on its own state, the obstacles’ status, and the target information. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < c_1) else 6 \n    reward_reaching_target = 11 if (d_agent_target < c_2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a room, aims to reach a designated target, and must avoid collisions with multiple obstacles such as drape and lamp by relying on its own state, the obstacles’ status, and the target information. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < c_1) else 6 \n    reward_reaching_target = 11 if (d_agent_target < c_2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a room, aims to reach a designated target, and must avoid collisions with multiple obstacles such as lamps and projectors by relying on its own state, the obstacles’ status, and the target information. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < c_1) else 6 \n    reward_reaching_target = 11 if (d_agent_target < c_2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a room, aims to reach a designated target, and must avoid collisions with multiple obstacles such as boxes and drapes by relying on its own state, the obstacles’ status, and the target information. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < c_1) else 6 \n    reward_reaching_target = 11 if (d_agent_target < c_2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a room, aims to reach a designated target, and must avoid collisions with multiple obstacles such as boxes and chair by relying on its own state, the obstacles’ status, and the target information. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < c_1) else 6 \n    reward_reaching_target = 11 if (d_agent_target < c_2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a room, aims to reach a designated target, and must avoid collisions with multiple obstacles such as human and table by relying on its own state, the obstacles’ status, and the target information. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < c_1) else 6 \n    reward_reaching_target = 11 if (d_agent_target < c_2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a room, aims to reach a designated target, and must avoid collisions with multiple obstacles such as human, chair and tabke by relying on its own state, the obstacles’ status, and the target information. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < c_1) else 6 \n    reward_reaching_target = 11 if (d_agent_target < c_2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a room, aims to reach a designated target, and must avoid collisions with multiple obstacles such as human and projectors, bench by relying on its own state, the obstacles’ status, and the target information. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < c_1) else 6 \n    reward_reaching_target = 11 if (d_agent_target < c_2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a room, aims to reach a designated target, and must avoid collisions with multiple obstacles such as boxes, bench and boxes by relying on its own state, the obstacles’ status, and the target information. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < c_1) else 6 \n    reward_reaching_target = 11 if (d_agent_target < c_2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a room, aims to reach a designated target, and must avoid collisions with multiple obstacles such as shelf, plant and decor by relying on its own state, the obstacles’ status, and the target information. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < c_1) else 6 \n    reward_reaching_target = 11 if (d_agent_target < c_2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone must navigate toward a target while avoiding crashes into the walls, relying on its state, the walls’ positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -10 if (d_agent_wall < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone must navigate toward a target while avoiding crashes into the opposite walls, relying on the drone state, the walls’ positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -10 if (d_agent_wall < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone must navigate toward a target while avoiding crashes into the chair, relying on the drone state, the chair's positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -10 if (d_agent_chair < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone must navigate toward a target while avoiding crashes into the table, relying on the drone state, the table's positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_table, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_table_min = calculate_distance_min(s_agent, s_table) \n    penalty_collision = -10 if (d_agent_table_min < c_1) else 6 \n    reward_reaching_target = 11 if (d_agent_target < c_2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone must navigate toward a target while avoiding crashes into the bookshelves, relying on the drone state, the bookshelve's positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_bookshelve, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bookshelve = calculate_distance_min(s_agent, s_bookshelve) \n    penalty_collision = -10 if (d_agent_bookshelve < c_1) else 6 \n    reward_reaching_target = 11 if (d_agent_target < c_2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone must navigate toward a target while avoiding crashes into the smoke detector, relying on the drone state, the smoke detector's positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_smoke_detector, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_smoke_detectore = calculate_distance_min(s_agent, s_smoke_detector) \n    penalty_collision = -10 if (d_agent_smoke_detector < c_1) else 6 \n    reward_reaching_target = 11 if (d_agent_target < c_2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone must navigate toward a target while avoiding crashes into the book, relying on the drone state, the book's positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_book, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_book = calculate_distance_min(s_agent, s_book) \n    penalty_collision = -10 if (d_agent_book < c_1) else 6 \n    reward_reaching_target = 11 if (d_agent_target < c_2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone must navigate toward a target while avoiding crashes into the human, relying on the drone state, the human's positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -10 if (d_agent_human < c_1) else 6 \n    reward_reaching_target = 11 if (d_agent_target < c_2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone must navigate toward a target while avoiding crashes into the obstacles, relying on the drone state, the obstacle's positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < c_1) else 6 \n    reward_reaching_target = 11 if (d_agent_target < c_2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone is tasked with navigating toward a target target while steering clear of multiple obstacles like bookshelves and students. It has access to drone's state, the multiple obstacles’ conditions, and target data. Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < c_1) else 6 \n    reward_reaching_target = 11 if (d_agent_target < c_2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone is tasked with navigating toward a target target while steering clear of multiple obstacles like lamp and students. It has access to drone's state, the multiple obstacles’ conditions, and target data. Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone is tasked with navigating toward a target target while steering clear of multiple obstacles like tall bookshelves and decoration. It has access to drone's state, the multiple obstacles’ conditions, and target data. Design a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone is tasked with navigating toward a target target while steering clear of multiple obstacles like stacked book and lamp. It has access to drone's state, the multiple obstacles’ conditions, and target data. Design a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone is tasked with navigating toward a target target while steering clear of multiple obstacles like book and lights. It has access to drone's state, the multiple obstacles’ conditions, and target data. Design a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone is tasked with navigating toward a target target while steering clear of multiple obstacles like book, lamp and human. It has access to drone's state, the multiple obstacles’ conditions, and target data. Design a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone is tasked with navigating toward a target target while steering clear of multiple obstacles like chair, table and human. It has access to drone's state, the multiple obstacles’ conditions, and target data. Design a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone is tasked with navigating toward a target target while steering clear of multiple obstacles like desk, lamp and students. It has access to drone's state, the multiple obstacles’ conditions, and target data. Design a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone is tasked with navigating toward a target target while steering clear of multiple obstacles like book, computer and shelf. It has access to drone's state, the multiple obstacles’ conditions, and target data. Design a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone is tasked with navigating toward a target target while steering clear of multiple obstacles like stacked book, laptop and book cart. It has access to drone's state, the multiple obstacles’ conditions, and target data. Design a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a library, a drone is tasked with navigating toward a target target while steering clear of multiple obstacles like curtain, plant and staff. It has access to drone's state, the multiple obstacles’ conditions, and target data. Design a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in a forest, a drone must avoid crashing into a moving human by using the available information about its own state and the human’s state. Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -10 if (d_agent_human < 1) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.3) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in a forest, a drone must avoid crashing into a tree by using the available information about its own state and the tree's state. Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_tree, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tree = calculate_distance_min(s_agent, s_tree) \n    penalty_collision = -10 if (d_agent_tree < 1) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.3) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in a forest, a drone must avoid crashing into a leaf by using the available information about its own state and the leaf's state. Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_leaf, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_leaf = calculate_distance_min(s_agent, s_leaf) \n    penalty_collision = -10 if (d_agent_leaf < 1) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.3) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in a forest, a drone must avoid crashing into a bird by using the available information about its own state and the bird's state. Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_bird, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bird = calculate_distance_min(s_agent, s_bird) \n    penalty_collision = -10 if (d_agent_bird < 1) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.3) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in a forest, a drone must avoid crashing into a stump by using the available information about its own state and the stump's state. Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_stump, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_stump = calculate_distance_min(s_agent, s_stump) \n    penalty_collision = -10 if (d_agent_stump < 1) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.3) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in a forest, a drone must avoid crashing into a rock by using the available information about its own state and the rock's state. Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_rock, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rock = calculate_distance_min(s_agent, s_rock) \n    penalty_collision = -10 if (d_agent_rock < 1) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.3) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in a forest, a drone must avoid crashing into a branch by using the available information about its own state and the branch's state. Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_branch, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_branch = calculate_distance_min(s_agent, s_branch) \n    penalty_collision = -10 if (d_agent_branch < 1) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.3) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in a forest, a drone must avoid crashing into a tree by using the available information about its own state and the tree's state. Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_tree_branch, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tree_branch = calculate_distance_min(s_agent, s_tree_branch) \n    penalty_collision = -10 if (d_agent_tree_branch < 1) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.3) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in a forest, a drone must avoid crashing into a people by using the available information about its own state and the people's state. Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_people, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_people = calculate_distance_min(s_agent, s_people) \n    penalty_collision = -10 if (d_agent_people < 1) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.3) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in a forest, a drone must avoid crashing into a stone by using the available information about its own state and the stone's state. Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_stone, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_stone = calculate_distance_min(s_agent, s_stone) \n    penalty_collision = -10 if (d_agent_stone < 1) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.3) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in a forest, a drone must avoid crashing into a obstacle by using the available information about its own state and the tree's state. Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Assignment: A drone flies in a forest, tasked with avoiding collisions with branches and trees while progressing toward a target. Environment: Both the agent’s state and the obstacles’ states are known. Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Assignment: A drone flies in a forest, tasked with avoiding collisions with human and animal while progressing toward a target. Environment: Both the agent’s state and the obstacles’ states are known. Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Assignment: A drone flies in a forest, tasked with avoiding collisions with leave and shelf while progressing toward a target. Environment: Both the agent’s state and the obstacles’ states are known. Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Assignment: A drone flies in a forest, tasked with avoiding collisions with multiple trees while progressing toward a target. Environment: Both the agent’s state and the obstacles’ states are known. Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Assignment: A drone flies in a forest, tasked with avoiding collisions with human, branches and trees while progressing toward a target. Environment: Both the agent’s state and the obstacles’ states are known. Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Assignment: A drone flies in a forest, tasked with avoiding collisions with stump, rock and trees while progressing toward a target. Environment: Both the agent’s state and the obstacles’ states are known. Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Assignment: A drone flies in a forest, tasked with avoiding collisions with trre, animals and stone while progressing toward a target. Environment: Both the agent’s state and the obstacles’ states are known. Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Assignment: A drone flies in a forest, tasked with avoiding collisions with cliffs, sign and maple trees while progressing toward a target. Environment: Both the agent’s state and the obstacles’ states are known. Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collisions with maple trees, cliffs and signs. \n Conditions: Agent's state and obstacles' states are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, human's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -10 if (d_agent_human < 1) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.3) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid any collision with hanging lamps. \n Conditions: Agent's information and lamp's information are available \n Write a reward in Python.",
        "chosen": "def reward(s_agent, s_lamp, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lamp = calculate_distance_min(s_agent, s_lamp) \n    penalty_collision = -10 if (d_agent_lamp < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid any static obstacle such as food serving counters. \n Conditions: Agent's state, food counter's state are accessible. \n Write a reward in Python.",
        "chosen": "def reward(s_agent, s_counter, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_counter = calculate_distance_min(s_agent, s_counter) \n    penalty_collision = -10 if (d_agent_counter < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with a chair. \n Conditions: Agent's state, chair's states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -10 if (d_agent_chair < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with a table. \n Conditions: Agent's state, table's state are available. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_table, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_table = calculate_distance_min(s_agent, s_table) \n    penalty_collision = -10 if (d_agent_table < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with trash bins. \n Conditions: Agent's state and trash bin's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_trash_bin, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_trash_bin = calculate_distance_min(s_agent, s_trash_bin) \n    penalty_collision = -10 if (d_agent_trash_bin < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with a low-hanging decoration. \n Conditions: Agent's state, hanging decoration's state and desired speed are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_decoration, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_decoration = calculate_distance_min(s_agent, s_decoration) \n    penalty_collision = -10 if (d_agent_decoration < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with plants. \n Conditions: Agent's state and plant's state are available. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_plant, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_plant = calculate_distance_min(s_agent, s_plant) \n    penalty_collision = -10 if (d_agent_plant < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Do not collide with metal rails. \n Conditions: Agent's state, metal rail's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_rail, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rail = calculate_distance_min(s_agent, s_rail) \n    penalty_collision = -10 if (d_agent_rail < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Do not collide with cameras. \n Conditions: Agent's state and camera's state are available. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_cameras, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cameras = calculate_distance_min(s_agent, s_cameras) \n    penalty_collision = -10 if (d_agent_cameras < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a mensa. \n Objectives: Avoid collisions with people and table. \n Conditions: Agent's state and obstacles' states are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a mensa. \n Objectives: Avoid collisions with students and chairs. \n Conditions: Agent's state and obstacles' states are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a mensa. \n Objectives: Avoid collisions with plants and chairs. \n Conditions: Agent's state and obstacles' states are available. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a mensa. \n Objectives: Avoid collisions with plants and desk. \n Conditions: Agent's state and obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a mensa. \n Objectives: Avoid collisions with decoration and plant. \n Conditions: Agent's state and obstacles' states are available. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a mensa. \n Objectives: Avoid collisions with cameras, student and table. \n Conditions: Agent's state and obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a mensa. \n Objectives: Avoid collisions with chair, plant and table. \n Conditions: Agent's state and obstacles' states are available. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a mensa. \n Objectives: Avoid collisions with staff, student and table. \n Conditions: Agent's state and obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a mensa. \n Objectives: Avoid collisions with food counter, rails and camera. \n Conditions: Agent's state and obstacles' states are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone moves in a mensa. \n Objectives: Avoid collisions with food counter, trash bin and staff. \n Conditions: Agent's state and obstacles' states are available. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone must fly within a classroom environment, avoiding any contact with the walls and moving toward a designated target. The system provides access to the drone’s state, the wall information, and the target state.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -10 if (d_agent_wall < 0.6) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.3) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone must fly within a classroom environment, avoiding any contact with the people and moving toward a designated target. The system provides access to the drone’s state, the people information, and the target state.",
        "chosen": "def reward(s_agent, s_people, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_people = calculate_distance_min(s_agent, s_people) \n    penalty_collision = -10 if (d_agent_people < 0.6) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.3) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone must fly within a classroom environment, avoiding any contact with the projector and moving toward a designated target. The system provides access to the drone’s state, the projector information, and the target state.",
        "chosen": "def reward(s_agent, s_projector, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_projector = calculate_distance_min(s_agent, s_projector) \n    penalty_collision = -10 if (d_agent_projector < 0.6) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.3) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone must fly within a classroom environment, avoiding any contact with the lamp and moving toward a designated target. The system provides access to the drone’s state, the lamp information, and the target state.",
        "chosen": "def reward(s_agent, s_lamp, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lamp = calculate_distance_min(s_agent, s_lamp) \n    penalty_collision = -10 if (d_agent_lamp < 0.6) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.3) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone must fly within a classroom environment, avoiding any contact with the blackboard and moving toward a designated target. The system provides access to the drone’s state, the blackboard information, and the target state.",
        "chosen": "def reward(s_agent, s_blackboard, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_blackboard = calculate_distance_min(s_agent, s_blackboard) \n    penalty_collision = -10 if (d_agent_blackboard < 0.6) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.3) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone must fly within a classroom environment, avoiding any contact with the bookshelf and moving toward a designated target. The system provides access to the drone’s state, the bookshelf information, and the target state.",
        "chosen": "def reward(s_agent, s_bookshelve, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bookshelve = calculate_distance_min(s_agent, s_bookshelve) \n    penalty_collision = -10 if (d_agent_bookshelve < 0.6) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.3) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone must fly within a classroom environment, avoiding any contact with the backpack and moving toward a designated target. The system provides access to the drone’s state, the backpack information, and the target state.",
        "chosen": "def reward(s_agent, s_backpack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_backpack = calculate_distance_min(s_agent, s_backpack) \n    penalty_collision = -10 if (d_agent_backpack < 0.6) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.3) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone must fly within a classroom environment, avoiding any contact with the human and moving toward a designated target. The system provides access to the drone’s state, the human information, and the target state.",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -10 if (d_agent_human < 0.6) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.3) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone must fly within a classroom environment, avoiding any contact with the teacher and moving toward a designated target. The system provides access to the drone’s state, the teacher information, and the target state.",
        "chosen": "def reward(s_agent, s_teacher, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_teacher = calculate_distance_min(s_agent, s_teacher) \n    penalty_collision = -10 if (d_agent_teacher < 0.6) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.3) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone must fly within a classroom environment, avoiding any contact with the student and moving toward a designated target. The system provides access to the drone’s state, the student information, and the target state.",
        "chosen": "def reward(s_agent, s_student, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_student = calculate_distance_min(s_agent, s_student) \n    penalty_collision = -10 if (d_agent_student < 0.6) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.3) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone must fly within a classroom environment, avoiding any contact with the different obstacles and moving toward a designated target. The system provides access to the drone’s state, the multiple obstacles information, and the target state.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone must fly within a classroom environment, avoiding any contact with the chair and table and moving toward a designated target. The system provides access to the drone’s state, the multiple obstacles information, and the target state.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone must fly within a classroom environment, avoiding any contact with the student and teacher and moving toward a designated target. The system provides access to the drone’s state, the multiple obstacles information, and the target state.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone must fly within a classroom environment, avoiding any contact with the backpack and shelves and moving toward a designated target. The system provides access to the drone’s state, the multiple obstacles information, and the target state.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone must fly within a classroom environment, avoiding any contact with the blackboard and whiteboard and moving toward a designated target. The system provides access to the drone’s state, the multiple obstacles information, and the target state.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone must fly within a classroom environment, avoiding any contact with the student, blackboard and whiteboard and moving toward a designated target. The system provides access to the drone’s state, the multiple obstacles information, and the target state.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone must fly within a classroom environment, avoiding any contact with the teacher, pupil and desk and moving toward a designated target. The system provides access to the drone’s state, the multiple obstacles information, and the target state",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone must fly within a classroom environment, avoiding any contact with the table, chair and table and moving toward a designated target. The system provides access to the drone’s state, the multiple obstacles information, and the target state.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone must fly within a classroom environment, avoiding any contact with the human, backpack and table and moving toward a designated target. The system provides access to the drone’s state, the multiple obstacles information, and the target state.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone must fly within a classroom environment, avoiding any contact with the many boxes and moving toward a designated target. The system provides access to the drone’s state, the multiple obstacles information, and the target state.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state, people's state accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_people, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_people = calculate_distance_min(s_agent, s_people) \n    penalty_collision = -10 if (d_agent_people < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Avoid collision with desk. \n Conditions: Agent's state, desk's state accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_desk, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_desk = calculate_distance_min(s_agent, s_desk) \n    penalty_collision = -10 if (d_agent_desk < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect the sideboard and do not crash in it. \n Conditions: Agent's state and sideboard's state accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_sideboard, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sideboard = calculate_distance_min(s_agent, s_sideboard) \n    penalty_collision = -10 if (d_agent_sideboard < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect bench and avoid collision. \n Conditions: Agent's state, bench's state and wanted velocity are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bench, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bench = calculate_distance_min(s_agent, s_bench) \n    penalty_collision = -10 if (d_agent_bench < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: Avoid collision the whiteboard. \n Conditions: Agent's state, whiteboard's state and wanted velocity are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_whiteboard, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_whiteboard = calculate_distance_min(s_agent, s_whiteboard) \n    penalty_collision = -10 if (d_agent_sideboard < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Do not not collied with the projector. \n Conditions: Agent's state, projector's state accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_projector, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_projector = calculate_distance_min(s_agent, s_projector) \n    penalty_collision = -10 if (d_agent_projector < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: There is a hanging equipment do not collied with it. \n Conditions: Agent's state and equipment's state accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_equipment, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_equipment = calculate_distance_min(s_agent, s_equipment) \n    penalty_collision = -10 if (d_agent_equipment < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Do not collide with the speaker. \n Conditions: Agent's state, speaker's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_speaker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_speaker = calculate_distance_min(s_agent, s_speaker) \n    penalty_collision = -10 if (d_agent_speaker < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Avoid collision with the laptop. \n Conditions: Agent's state and laptop's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_laptop, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_laptop = calculate_distance_min(s_agent, s_laptop) \n    penalty_collision = -10 if (d_agent_laptop < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: There is a screen in the room. Do not collide with it \n Conditions: Agent's state and screen's state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_screen, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_screen = calculate_distance_min(s_agent, s_screen) \n    penalty_collision = -10 if (d_agent_screen < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: There is a screen and a projector in the room. Do not collide with them \n Conditions: Agent's state and obstacles state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: There is a table and a chair in the room. Do not collide with them \n Conditions: Agent's state and obstacles state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: There is a computer and a person in the room. Do not collide with them \n Conditions: Agent's state and obstacles state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: There is a plant and a shelf in the room. Do not collide with them \n Conditions: Agent's state and obstacles state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: There is a two boxes in the room. Do not collide with them \n Conditions: Agent's state and obstacles state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: There is a chair, a table and the person in the room. Do not collide with them \n Conditions: Agent's state and obstacles state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: There is a laptop, a plant and a shelf in the room. Do not collide with them \n Conditions: Agent's state and obstacles state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: Avoid collision with the shelf, box, and the projector. \n Conditions: Agent's state and obstacles state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: Avoid collision with the whiteboard, screen, and the lamp. \n Conditions: Agent's state and obstacles state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: Avoid collision with the three boxes. \n Conditions: Agent's state and obstacles state are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a warehouse, a drone must reach its target safely and avoid crashimg into obstacles. \n It can observe its own state, the target’s state, and the state of one staff member. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_staff = calculate_distance_min(s_agent, s_staff) \n    penalty_collision = -10 if (d_agent_staff < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a warehouse, a drone must reach its target safely and avoid crashimg into obstacles. \n It can observe its own state, the target’s state, and the state of one worker. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_worker = calculate_distance_min(s_agent, s_worker) \n    penalty_collision = -10 if (d_agent_worker < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a warehouse, a drone must reach its target safely and avoid crashimg into obstacles. \n It can observe its own state, the target’s state, and the state of one human. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -10 if (d_agent_human < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a warehouse, a drone must reach its target safely and avoid crashimg into obstacles. \n It can observe its own state, the target’s state, and the state of one human worker. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_worker = calculate_distance_min(s_agent, s_worker) \n    penalty_collision = -10 if (d_agent_worker < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a warehouse, a drone must reach its target safely and avoid crashimg into obstacles. \n It can observe its own state, the target’s state, and the state of one rack. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rack = calculate_distance_min(s_agent, s_rack) \n    penalty_collision = -10 if (d_agent_rack < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a warehouse, a drone must reach its target safely and avoid crashimg into obstacles. \n It can observe its own state, the target’s state, and the state of one forklift. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_forklift, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_forklift = calculate_distance_min(s_agent, s_forklift) \n    penalty_collision = -10 if (d_agent_forklift < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a warehouse, a drone must reach its target safely and avoid crashimg into obstacles. \n It can observe its own state, the target’s state, and the state of one cable. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_cable, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cable = calculate_distance_min(s_agent, s_cable) \n    penalty_collision = -10 if (d_agent_cable < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a warehouse, a drone must reach its target safely and avoid crashimg into obstacles. \n It can observe its own state, the target’s state, and the state of a pallet. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_pallet, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_pallet = calculate_distance_min(s_agent, s_pallet) \n    penalty_collision = -10 if (d_agent_pallet < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a warehouse, a drone must reach its target safely and avoid crashimg into obstacles. \n It can observe its own state, the target’s state, and the state of a package. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_package, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_package = calculate_distance_min(s_agent, s_package) \n    penalty_collision = -10 if (d_agent_package < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a warehouse, a drone must reach its target safely and avoid crashimg into obstacles. \n It can observe its own state, the target’s state, and the state of a pipe. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_pipe, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_pipe = calculate_distance_min(s_agent, s_pipe) \n    penalty_collision = -10 if (d_agent_pipe < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a warehouse, a drone must reach its target safely and avoid crashimg into obstacles. \n It can observe its own state, the target’s state, and the state of a beam. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_beam, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_pipe = calculate_distance_min(s_agent, s_beam) \n    penalty_collision = -10 if (d_agent_beam < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a warehouse, a drone must reach its target safely and avoid crashimg into obstacles. \n It can observe its own state, the target’s state, and the state of shelf and human. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a warehouse, a drone must reach its target safely and avoid crashimg into obstacles. \n It can observe its own state, the target’s state, and the state of packet and staff. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a warehouse, a drone must reach its target safely and avoid crashimg into obstacles. \n It can observe its own state, the target’s state, and the state of stacked boxes. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a warehouse, a drone must reach its target safely and avoid crashimg into obstacles. \n It can observe its own state, the target’s state, and the state of maschine and staff. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a warehouse, a drone must reach its target safely and avoid crashimg into obstacles. \n It can observe its own state, the target’s state, and the state of beam and lamp. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "MIn a warehouse, a drone must reach its target safely and avoid crashimg into obstacles. \n It can observe its own state, the target’s state, and the state of cable, rack and person. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a warehouse, a drone must reach its target safely and avoid crashimg into obstacles. \n It can observe its own state, the target’s state, and the state of desk, computer and chair. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a warehouse, a drone must reach its target safely and avoid crashimg into obstacles. \n It can observe its own state, the target’s state, and the state of beam, pipe and lamp. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a warehouse, a drone must reach its target safely and avoid crashimg into obstacles. \n It can observe its own state, the target’s state, and the state of forklift, rail and stair. \n Write a Python reward function",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Drone moves in a theater. \n Goal: Detect people and avoid collison with people. \n Given: Agent's state, people's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_people, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_people = calculate_distance_min(s_agent, s_people) \n    penalty_collision = -10 if (d_agent_people < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Drone moves in a theater. \n Goal: Detect spotlamp and avoid collison with the spotlamp. \n Given: Agent's state and spotlamp information are available. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_spotlamp, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_spotlamp = calculate_distance_min(s_agent, s_spotlamp) \n    penalty_collision = -10 if (d_agent_spotlamp < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Drone moves in a theater. \n Goal: Avoid collison with the balcony. \n Given: Agent's state and balcony's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_balcony, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_balcony = calculate_distance_min(s_agent, s_balcony) \n    penalty_collision = -10 if (d_agent_balcony < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Pilot a drone through a theater. \n Goal: Avoid collison with the visitor's seat. \n Given: Agent's states and seat's information are available. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_seat, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_seat = calculate_distance_min(s_agent, s_seat) \n    penalty_collision = -10 if (d_agent_seat < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Drone moves in a theater. \n Goal: Avoid collison with the projector. \n Given: Agent's state, projectors's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_projector, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_projector = calculate_distance_min(s_agent, s_projector) \n    penalty_collision = -10 if (d_agent_projector < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Pilot a drone through a theater. \n Goal: Avoid collision with a cable. \n Given: Agent's state, cable's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_cable, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cable = calculate_distance_min(s_agent, s_cable) \n    penalty_collision = -10 if (d_agent_cable < 0.5) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Drone explores the theater. \n Goal: Detect dynamic obstacle (human) and avoid collision. \n Given: Agent's state and human's state obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -10 if (d_agent_human < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Pilot a drone through a theater. \n Goal: Detect static obstacle (overhead platform) and avoid collision. \n Given: Agent's state, platform's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_platform, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_platform = calculate_distance_min(s_agent, s_platform) \n    penalty_collision = -10 if (d_agent_platform < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Drone explores the theater. \n Goal: Detect static obstacle (drape) and avoid collision. \n Given: Agent's state, drape's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_drape, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_drape = calculate_distance_min(s_agent, s_drape) \n    penalty_collision = -10 if (d_agent_drape < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Drone explores the theater. \n Goal: Detect dynamic obstacle (hangings microphone) and avoid collision. \n Given: Agent's state, microphone's state are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_microphone, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_microphone = calculate_distance_min(s_agent, s_microphone) \n    penalty_collision = -10 if (d_agent_microphone < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Operate the drone in a theater. \n Goal: Detect the spotlamp and projectors and do not collide with them. \n Given: Agent's states and overhead beam information are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Operate the drone in a theater. \n Goal: Detect the chair and people and do not collide with them. \n Given: Agent's states and overhead beam information are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Operate the drone in a theater. \n Goal: Detect the stage and drape and do not collide with them. \n Given: Agent's states and overhead beam information are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Operate the drone in a v. \n Goal: Detect the microphone and music box do not collide with them. \n Given: Agent's states and overhead beam information are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Operate the drone in a theater. \n Goal: Detect the staff and visitor do not collide with them. \n Given: Agent's states and overhead beam information are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Operate the drone in a theater. \n Goal: Detect the chandelier, catwalk and rigging ropes and do not collide with it. \n Given: Agent's states and obstacles information are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Operate the drone in a theater. \n Goal: Detect the table, chair and human and do not collide with it. \n Given: Agent's states and obstacles information are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Operate the drone in a theater. \n Goal: Detect the three boxes and do not collide with it. \n Given: Agent's states and obstacles information are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Operate the drone in a theater. \n Goal: Detect the ceiling beam, balcony railing and spotlight and do not collide with it. \n Given: Agent's states and obstacles information are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Operate the drone in a theater. \n Goal: Detect the platform, microphone and drapes and do not collide with it. \n Given: Agent's states and obstacles information are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Operate the drone in a theater. \n Goal: Detect the visitorn seat and box and do not collide with it. \n Given: Agent's states and obstacles information are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, attempts to reach a target, and must prevent collisions with human using drone state, the human status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -10 if (d_agent_human < 0.6) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, attempts to reach a target, and must prevent collisions with person using drone state, the person status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_person, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_person = calculate_distance_min(s_agent, s_person) \n    penalty_collision = -10 if (d_agent_person < 0.6) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, attempts to reach a target, and must prevent collisions with basketball hoop using drone state, the basketball hoop status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_basket_hoop, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_basket_hoop = calculate_distance_min(s_agent, s_basket_hoop) \n    penalty_collision = -10 if (d_agent_basket_hoop < 0.6) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, attempts to reach a target, and must prevent collisions with scorboard using drone state, the scorboard status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_scoreboard, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_scoreboard = calculate_distance_min(s_agent, s_scoreboard) \n    penalty_collision = -10 if (d_agent_scoreboard < 0.6) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, attempts to reach a target, and must prevent collisions with rope using drone state, the rope status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_rope, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rope = calculate_distance_min(s_agent, s_rope) \n    penalty_collision = -10 if (d_agent_rope < 0.6) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, attempts to reach a target, and must prevent collisions with net using drone state, the net status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_net, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_net = calculate_distance_min(s_agent, s_net) \n    penalty_collision = -10 if (d_agent_net < 0.6) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, attempts to reach a target, and must prevent collisions with flag using drone state, the flag status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_flag, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_flag = calculate_distance_min(s_agent, s_flag) \n    penalty_collision = -10 if (d_agent_flag < 0.6) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, attempts to reach a target, and must prevent collisions with player using drone state, the player status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_player, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_player = calculate_distance_min(s_agent, s_player) \n    penalty_collision = -10 if (d_agent_player < 0.6) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, attempts to reach a target, and must prevent collisions with hall using drone state, the hall status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_ball, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ball = calculate_distance_min(s_agent, s_ball) \n    penalty_collision = -10 if (d_agent_ball < 0.6) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, attempts to reach a target, and must prevent collisions with ball using drone state, the ball status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_ball, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ball = calculate_distance_min(s_agent, s_ball) \n    penalty_collision = -10 if (d_agent_ball < 0.6) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, attempts to reach a target, and must prevent collisions with player using drone state, the player status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_player, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_player = calculate_distance_min(s_agent, s_player) \n    penalty_collision = -10 if (d_agent_player < 0.6) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, attempts to reach a target, and must prevent collisions with player and coach using drone state, the multiple obstacles status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, attempts to reach a target, and must prevent collisions with rope and net and coach using drone state, the multiple obstacles status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, attempts to reach a target, and must prevent collisions with flag and coach using drone state, the multiple obstacles status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, attempts to reach a target, and must prevent collisions with basketball and rope using drone state, the multiple obstacles status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, attempts to reach a target, and must prevent collisions with basketball and net using drone state, the multiple obstacles status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, attempts to reach a target, and must prevent collisions with net and flag using drone state, the multiple obstacles status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, attempts to reach a target, and must prevent collisions with basketball, net and coach using drone state, the multiple obstacles status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, attempts to reach a target, and must prevent collisions with player, rope and ball using drone state, the multiple obstacles status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Sports hall, attempts to reach a target, and must prevent collisions with flag, student and net using drone state, the multiple obstacles status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a hospital. \n Duties: Detect patient and avoid collision. \n Inputs: Agent's state, patient's state obtainable. \n Write a reward function in Python..",
        "chosen": "def reward(s_agent, s_patient, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_patient = calculate_distance_min(s_agent, s_patient) \n    penalty_collision = -10 if (d_agent_patient < 0.6) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a hospital. \n Duties: Detect doctor and avoid collision. \n Inputs: Agent's state, doctor's state obtainable. \n Write a reward function in Python..",
        "chosen": "def reward(s_agent, s_doctor, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_doctor = calculate_distance_min(s_agent, s_doctor) \n    penalty_collision = -10 if (d_agent_doctor < 0.6) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a hospital. \n Duties: Detect wire and avoid collision. \n Inputs: Agent's state, wire's state obtainable. \n Write a reward function in Python..",
        "chosen": "def reward(s_agent, s_wire, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wire = calculate_distance_min(s_agent, s_wire) \n    penalty_collision = -10 if (d_agent_wire < 0.6) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a hospital. \n Duties: Detect equipment and avoid collision. \n Inputs: Agent's state, equipment's state obtainable. \n Write a reward function in Python..",
        "chosen": "def reward(s_agent, s_equipment, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_equipment = calculate_distance_min(s_agent, s_equipment) \n    penalty_collision = -10 if (d_agent_equipment < 0.6) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a hospital. \n Duties: Detect wheelchair and avoid collision. \n Inputs: Agent's state, wheelchair's state obtainable. \n Write a reward function in Python..",
        "chosen": "def reward(s_agent, s_wheelchair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wheelchair = calculate_distance_min(s_agent, s_wheelchair) \n    penalty_collision = -10 if (d_agent_wheelchair < 0.6) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a hospital. \n Duties: Detect staff and avoid collision. \n Inputs: Agent's state, staff's state obtainable. \n Write a reward function in Python..",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_staff = calculate_distance_min(s_agent, s_staffr) \n    penalty_collision = -10 if (d_agent_staff < 0.6) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a hospital. \n Duties: Detect defibrillator and avoid collision. \n Inputs: Agent's state, defibrillator's state obtainable. \n Write a reward function in Python..",
        "chosen": "def reward(s_agent, s_defibrillator, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_defibrillator = calculate_distance_min(s_agent, s_defibrillator) \n    penalty_collision = -10 if (d_agent_defibrillator < 0.6) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a hospital. \n Duties: Detect device and avoid collision. \n Inputs: Agent's state, device's state obtainable. \n Write a reward function in Python..",
        "chosen": "def reward(s_agent, s_device, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_device = calculate_distance_min(s_agent, s_device) \n    penalty_collision = -10 if (d_agent_device < 0.6) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a hospital. \n Duties: Detect lamp and avoid collision. \n Inputs: Agent's state, lamp's state obtainable. \n Write a reward function in Python..",
        "chosen": "def reward(s_agent, s_lamp, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lamp = calculate_distance_min(s_agent, s_lamp) \n    penalty_collision = -10 if (d_agent_lamp < 0.6) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a hospital. \n Duties: Detect staff and patient and avoid collision. \n Inputs: Agent's state, multiple obstacle's state obtainable. \n Write a reward function in Python..",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a hospital. \n Duties: Detect defibrillator and equipment and avoid collision. \n Inputs: Agent's state, multiple obstacle's state obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a hospital. \n Duties: Detect wire and wheelchair and avoid collision. \n Inputs: Agent's state, multiple obstacle's state obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a hospital. \n Duties: Detect wheelchair and patient and avoid collision. \n Inputs: Agent's state, multiple obstacle's state obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a hospital. \n Duties: Detect staff and lamp and avoid collision. \n Inputs: Agent's state, multiple obstacle's state obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a hospital. \n Duties: Detect multiple obstacles and avoid collision. \n Inputs: Agent's state, multiple obstacle's state obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a hospital. \n Duties: Detect doctor, wheelchair and patient and avoid collision. \n Inputs: Agent's state, multiple obstacle's state obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a hospital. \n Duties: Detect equipment, wheelchair and defibrillator and avoid collision. \n Inputs: Agent's state, multiple obstacle's state obtainable. \n Write a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a hospital. \n Duties: Detect lamp, bed and screen and avoid collision. \n Inputs: Agent's state, multiple obstacle's state obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a hospital. \n Duties: Detect counter, nurse and counter and avoid collision. \n Inputs: Agent's state, multiple obstacle's state obtainable. \n Write a reward function in Python",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a hospital. \n Duties: Detect multiple boxes and avoid collision. \n Inputs: Agent's state, multiple obstacle's state obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.8) else 6 \n    reward_reaching_target = 11 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Parking Garage. It must avoid colliding with concrete columns or wayfinding signs. The states of the agent, the ride guardrail, and the wayfinding signs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a busy Parking Garage. Detect ramp guardrail and aviod it. The states of the agent, the ramp guardrail are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ramp, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ramp = calculate_distance_min(s_agent, s_ramp) \n    penalty_collision = -17 if (d_agent_ramp < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Parking Garage common room. Objectives: It must avoid colliding with payment kiosks. Conditions: States of the drone and the payment kiosk are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_payment_kiosk, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_payment_kiosk = calculate_distance_min(s_agent, s_payment_kiosk) \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a drive aisle, the drone must maneuver around roll-up door. All entity states (agent, roll-up door) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_door, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_door = calculate_distance_min(s_agent, s_door) \n    penalty_collision = -17 if (d_agent_door < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a drive aisle. Requirement: Detect the three wheel stops and do not collide with it. Given: The states for the drone and many wheel stops are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone operates near a service corridor during an event. So not collide with car. The states of the agent, the car are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_car, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_car = calculate_distance_min(s_agent, s_car) \n    penalty_collision = -17 if (d_agent_car < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate Parking Garage. Objectives: Do not collide hit with truck. Conditions: States of the agent, truck are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_truck, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_truck = calculate_distance_min(s_agent, s_truck) \n    penalty_collision = -17 if (d_agent_truck < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a drive aisle, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (d_agent_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: parking level. Avoid: Hitting with Parking Garage's vehicle. Known: Agent, Parking Garage vehicle states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_vehicle, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_vehicle = calculate_distance_min(s_agent, s_vehicle) \n    penalty_collision = -17 if (d_agent_vehicle < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Parking Garage. Objectives: It must avoid colliding with pay stations human. Conditions: The states of the drone and human are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -19 if (d_agent_human < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Parking Garage. Objectives: Avoid collision with two traffic cone lines. Conditions: The states of the agent, traffic cone lines and attendants are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Parking Garage. Objectives: It must avoid colliding with any truck, overhead cable bundle and gate arm. Conditions: Agent's, truck and gate arm states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Parking Garage. Objectives: Do not collide with any EV charging station. Conditions: The states of the drone and the EV charging station are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_charging_station, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_charging_station = calculate_distance_min(s_agent, s_charging_station) \n    penalty = -19 if (d_agent_charging_station < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Parking Garage and must avoid contact with any gate. The states of the drone and the gate are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_gate, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_gate = calculate_distance_min(s_agent, s_gate) \n    penalty = -19 if (d_agent_gate < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Parking Garage. It must avoid colliding with the motorcycle, attendant and sign. The states of the agent, motorcycle, attendant and sign are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Parking Garage. It must avoid colliding with bollards and wayfinding signs. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Parking Garage. It must avoid colliding with Parking Garage ventilation ducts. Agent's state and Parking Garage ventilation ducts's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ventilation, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ventilation = calculate_distance_min(s_agent, s_ventilation) \n    penalty_collision = -19 if (d_agent_ventilation < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Parking Garage. Goal: It must avoid colliding with overhead conduit. Assumptions: The states of the drone and the overhead conduit area are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_conduit, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_conduit = calculate_distance_min(s_agent, s_conduit) \n    penalty_collision = -19 if (d_agent_conduit < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Parking Garage. Goal: It must avoid colliding with Parking Garage ventilation ducts, wayfinding signs and wheel stops. Assumptions: The agent's state, Parking Garage ventilation ducts's state, wheel stop's state and wayfinding signs' states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Parking Garage. Goal: Detect wayfinding signs and structural brackets and avoid drive aisle corners. Assumptions: The states of the drone and gates, pay stations and barricades state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Parking Garage. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is operating along Rainforest. \n Objectives: It must avoid colliding with tree trunks or hanging vines. \n Conditions: The states of the agent, the ride guardrail, and the trail markers are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is operating inside a busy Rainforest. \n Objectives: Detect fallen log barrier and avoid it. \n Conditions: The states of the agent, the fallen log barrier are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_fallen_log, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_fallen_log = calculate_distance_min(s_agent, s_fallen_log) \n    penalty_collision = -17 if (d_agent_fallen_log < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Rainforest clearing. Objectives: It must avoid colliding with termite mounds. Conditions: States of the drone and the payment kiosk are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_termite_mound, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_termite_mound = calculate_distance_min(s_agent, s_termite_mound) \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a forest trail, the drone must maneuver around thick vine curtain. All entity states (agent, thick vine curtain) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_vine_curtain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_vine_curtain = calculate_distance_min(s_agent, s_vine_curtain) \n    penalty_collision = -17 if (d_agent_vine_curtain < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a forest trail. Requirement: Detect the three boulders and do not collide with it. Given: The states for the drone and many boulders are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone operates near a riverbank during the monsoon. \n Objectives: Do not collide with wild boar. \n Conditions: The states of the agent, the wild boar are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_boar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_boar = calculate_distance_min(s_agent, s_boar) \n    penalty_collision = -17 if (d_agent_boar < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navibamboo gate Rainforest. \n Objectives: Do not collide with with elephant. \n Conditions: States of the agent, elephant are obtainable. \n Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_elephant, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_elephant = calculate_distance_min(s_agent, s_elephant) \n    penalty_collision = -17 if (d_agent_elephant < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a forest trail, the drone must steer clear of cliff face. All states (agent, cliff face) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_cliff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cliff = calculate_distance_min(s_agent, s_cliff) \n    penalty_collision = -17 if (d_agent_cliff < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: forest floor. Avoid: Hitting Rainforest's vehicle. Known: Agent, Rainforest vehicle states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_wild_animal, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wild_animal = calculate_distance_min(s_agent, s_wild_animal) \n    penalty_collision = -17 if (d_agent_wild_animal < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Rainforest. Objectives: It must avoid colliding with forest ranger. Conditions: The states of the drone and human are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ranger, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ranger = calculate_distance_min(s_agent, s_ranger) \n    penalty_collision = -19 if (d_agent_ranger < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Rainforest. Objectives: Avoid collision with two thorny bush lines. Conditions: The states of the agent, thorny bush lines and attendants are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Rainforest. Objectives: It must avoid colliding with any elephant, overhanging liana bundle and fallen branch. Conditions: Agent's, elephant and fallen branch states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Rainforest. \n Objectives: Do not collide with any large tree stump. \n Conditions: The states of the drone and the large tree stump are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_tree_stump, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tree_stump = calculate_distance_min(s_agent, s_tree_stump) \n    penalty = -19 if (d_agent_tree_stump < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is operating along Rainforest and must avoid contact with any bamboo gate. \n Conditions: The states of the drone and the bamboo gate are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_bamboo_gate, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bamboo_gate = calculate_distance_min(s_agent, s_bamboo_gate) \n    penalty = -19 if (d_agent_bamboo_gate < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Rainforest. \n Objectives: It must avoid colliding with the vine tangles, ranger and signpost. \n Conditions: The states of the agent, vine tangles, ranger and signpost are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Rainforest. It must avoid colliding with bollards and trail markers. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Rainforest. It must avoid colliding with Rainforest buttress roots. Agent's state and Rainforest buttress roots's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_buttress_roots, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_buttress_roots = calculate_distance_min(s_agent, s_buttress_roots) \n    penalty_collision = -19 if (d_agent_buttress_roots < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Rainforest. Goal: It must avoid colliding with overhanging lianas. Assumptions: The states of the drone and the overhanging lianas area are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_lianas, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lianas = calculate_distance_min(s_agent, s_lianas) \n    penalty_collision = -19 if (d_agent_lianas < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Rainforest. Goal: It must avoid colliding with Rainforest buttress roots, trail markers and boulders. Assumptions: The agent's state, Rainforest buttress roots's state, wheel stop's state and trail markers' states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Rainforest. Goal: Detect trail markers and tree buttresses and avoid forest trail corners. Assumptions: The states of the drone and bamboo gates, pay stations and thorny barriers state are available. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Rainforest. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is operating along Rainforest. \n Objectives: It must avoid colliding with tree trunks or hanging vines. \n Conditions: The states of the agent, the ride guardrail, and the trail markers are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is operating inside a busy Rainforest. \n Objectives: Detect fallen log barrier and avoid it. \n Conditions: The states of the agent, the fallen log barrier are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_fallen_log, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_fallen_log = calculate_distance_min(s_agent, s_fallen_log) \n    penalty_collision = -17 if (d_agent_fallen_log < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Rainforest clearing. Objectives: It must avoid colliding with termite mounds. Conditions: States of the drone and the payment kiosk are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_termite_mound, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_termite_mound = calculate_distance_min(s_agent, s_termite_mound) \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a forest trail, the drone must maneuver around thick vine curtain. All entity states (agent, thick vine curtain) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_vine_curtain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_vine_curtain = calculate_distance_min(s_agent, s_vine_curtain) \n    penalty_collision = -17 if (d_agent_vine_curtain < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a forest trail. Requirement: Detect the three boulders and do not collide with it. Given: The states for the drone and many boulders are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone operates near a riverbank during the monsoon. \n Objectives: Do not collide with wild boar. \n Conditions: The states of the agent, the wild boar are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_boar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_boar = calculate_distance_min(s_agent, s_boar) \n    penalty_collision = -17 if (d_agent_boar < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navibamboo gate Rainforest. \n Objectives: Do not collide with with elephant. \n Conditions: States of the agent, elephant are obtainable. \n Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_elephant, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_elephant = calculate_distance_min(s_agent, s_elephant) \n    penalty_collision = -17 if (d_agent_elephant < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a forest trail, the drone must steer clear of cliff face. All states (agent, cliff face) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_cliff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cliff = calculate_distance_min(s_agent, s_cliff) \n    penalty_collision = -17 if (d_agent_cliff < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: forest floor. Avoid: Hitting Rainforest's vehicle. Known: Agent, Rainforest vehicle states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_wild_animal, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wild_animal = calculate_distance_min(s_agent, s_wild_animal) \n    penalty_collision = -17 if (d_agent_wild_animal < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Rainforest. Objectives: It must avoid colliding with forest ranger. Conditions: The states of the drone and human are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ranger, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ranger = calculate_distance_min(s_agent, s_ranger) \n    penalty_collision = -19 if (d_agent_ranger < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Rainforest. Objectives: Avoid collision with two thorny bush lines. Conditions: The states of the agent, thorny bush lines and attendants are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Rainforest. Objectives: It must avoid colliding with any elephant, overhanging liana bundle and fallen branch. Conditions: Agent's, elephant and fallen branch states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Rainforest. \n Objectives: Do not collide with any large tree stump. \n Conditions: The states of the drone and the large tree stump are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_tree_stump, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tree_stump = calculate_distance_min(s_agent, s_tree_stump) \n    penalty = -19 if (d_agent_tree_stump < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is operating along Rainforest and must avoid contact with any bamboo gate. \n Conditions: The states of the drone and the bamboo gate are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_bamboo_gate, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bamboo_gate = calculate_distance_min(s_agent, s_bamboo_gate) \n    penalty = -19 if (d_agent_bamboo_gate < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Rainforest. \n Objectives: It must avoid colliding with the vine tangles, ranger and signpost. \n Conditions: The states of the agent, vine tangles, ranger and signpost are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Rainforest. It must avoid colliding with bollards and trail markers. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Rainforest. It must avoid colliding with Rainforest buttress roots. Agent's state and Rainforest buttress roots's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_buttress_roots, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_buttress_roots = calculate_distance_min(s_agent, s_buttress_roots) \n    penalty_collision = -19 if (d_agent_buttress_roots < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Rainforest. Goal: It must avoid colliding with overhanging lianas. Assumptions: The states of the drone and the overhanging lianas area are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_lianas, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lianas = calculate_distance_min(s_agent, s_lianas) \n    penalty_collision = -19 if (d_agent_lianas < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Rainforest. Goal: It must avoid colliding with Rainforest buttress roots, trail markers and boulders. Assumptions: The agent's state, Rainforest buttress roots's state, wheel stop's state and trail markers' states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Rainforest. Goal: Detect trail markers and tree buttresses and avoid forest trail corners. Assumptions: The states of the drone and bamboo gates, pay stations and thorny barriers state are available. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Rainforest. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: \n A drone is operating along Science Lab. Objectives: \n It must avoid colliding with concrete columns or wayfinding signs. Conditions: \n The states of the agent, the ride guardrail, and the wayfinding signs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: \n A drone is operating inside a busy Science Lab. Objectives: \n Detect centrifuge machine and aviod it. Conditions: \n The states of the agent, the centrifuge machine are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_centrifuge_machine, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_centrifuge_machine = calculate_distance_min(s_agent, s_centrifuge_machine) \n    penalty_collision = -17 if (d_agent_centrifuge_machine < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Science Lab open workspace. Objectives: It must avoid colliding with lab equipment. Conditions: States of the drone and the lab equipment are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_lab_equipment, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lab_equipment = calculate_distance_min(s_agent, s_lab_equipment) \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a lab corridor, the drone must maneuver around lab curtain. All entity states (agent, lab curtain) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_lab_curtain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lab_curtain = calculate_distance_min(s_agent, s_lab_curtain) \n    penalty_collision = -17 if (d_agent_lab_curtain < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a lab corridor. Requirement: Detect the three lab technicians and do not collide with it. Given: The states for the drone and many lab technicians are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: \n A drone operates near a chemical storage area during experiments. Objectives: \n So not collide with robot. Conditions: \n The states of the agent, the robot are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_lab_robot, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lab_robot = calculate_distance_min(s_agent, s_lab_robot) \n    penalty_collision = -17 if (d_agent_lab_robot < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate Science Lab. Objectives: Do not collide hit with large apparatus. Conditions: States of the agent, apparatus are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_apparatus, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_apparatus = calculate_distance_min(s_agent, s_apparatus) \n    penalty_collision = -17 if (d_agent_apparatus < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a lab corridor, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_lab_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lab_wall = calculate_distance_min(s_agent, s_lab_wall) \n    penalty_collision = -17 if (d_agent_lab_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: lab floor. Avoid: Hitting with robotic arm. Known: Agent, robotic arm states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_robotic_arm, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_robotic_arm = calculate_distance_min(s_agent, s_robotic_arm) \n    penalty_collision = -17 if (d_agent_robotic_arm < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Science Lab. Objectives: It must avoid colliding with technician. Conditions: The states of the drone and technician are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_technician, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_technician = calculate_distance_min(s_agent, s_technician) \n    penalty_collision = -19 if (d_agent_technician < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Science Lab. Objectives: Avoid collision with two traffic cone lines. Conditions: The states of the agent, wires, technician and warning signs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Science Lab. Objectives: It must avoid colliding with any truck, overhead cable bundle and gate arm. Conditions: Agent's, wall and dropped tool states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Science Lab. Objectives: Do not collide with any centrifuge. Conditions: The states of the drone and the centrifuge are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_centrifuge, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_centrifuge = calculate_distance_min(s_agent, s_centrifuge) \n    penalty = -19 if (d_agent_centrifuge < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Science Lab and must avoid contact with any secure_door. The states of the drone and the secure_door are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_secure_door, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_secure_door = calculate_distance_min(s_agent, s_secure_door) \n    penalty = -19 if (d_agent_secure_door < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Science Lab. It must avoid colliding with the motorcycle, attendant and sign. The states of the agent, cables, storage racks and safety barricades are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Science Lab. It must avoid colliding with bollards and wayfinding signs. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Science Lab. It must avoid colliding with Science Lab pipes. Agent's state and Science Lab pipes's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_pipe, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_pipe = calculate_distance_min(s_agent, s_pipe) \n    penalty_collision = -19 if (d_agent_pipe < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Science Lab. Goal: It must avoid colliding with cable. Assumptions: The states of the drone and the cable's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_cable, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cable = calculate_distance_min(s_agent, s_cable) \n    penalty_collision = -19 if (d_agent_cable < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Science Lab. Goal: It must avoid colliding with Science Lab's chair, cabinet and student. Assumptions: The agent's state, chair, cabinet and student' states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Science Lab. Goal: Detect waste containers and gas cylindes and avoid lab door and do not collide. Assumptions: The states of the drone, waste containers, gas cylindes and avoid lab door state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Science Lab. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: \n A drone is operating along Church. Objectives: \n It must avoid colliding with concrete columns or hymn boards. Conditions: \n The states of the agent, the altar rail, and the hymn boards are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: \n A drone is operating inside a busy Church. Objectives: \n Detect candelabra and aviod it. Conditions: \n The states of the agent, the candelabra are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_candelabra, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_centrifuge_machine = calculate_distance_min(s_agent, s_candelabra) \n    penalty_collision = -17 if (d_agent_centrifuge_machine < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Church open workspace. Objectives: It must avoid colliding with church equipment. Conditions: States of the drone and the church equipment are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_church_fixtures, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lab_equipment = calculate_distance_min(s_agent, s_church_fixtures) \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a church corridor, the drone must maneuver around church curtain. All entity states (agent, church curtain) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_choir_curtain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lab_curtain = calculate_distance_min(s_agent, s_choir_curtain) \n    penalty_collision = -17 if (d_agent_lab_curtain < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a church corridor. Requirement: Detect the three church parishioners and do not collide with it. Given: The states for the drone and many church parishioners are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: \n A drone operates near a sacristy area during experiments. Objectives: \n So not collide with processional stand. Conditions: \n The states of the agent, the processional stand are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_processional_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_processional_stand = calculate_distance_min(s_agent, s_processional_stand) \n    penalty_collision = -17 if (d_agent_processional_stand < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate Church. Objectives: Do not collide hit with baptismal font. Conditions: States of the agent, baptismal font are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_baptismal_font, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_baptismal_font = calculate_distance_min(s_agent, s_baptismal_font) \n    penalty_collision = -17 if (d_agent_baptismal_font < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a church corridor, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_church_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_church_wall = calculate_distance_min(s_agent, s_church_wall) \n    penalty_collision = -17 if (d_agent_church_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: church floor. Avoid: Hitting with processional stand. Known: Agent, processional stand states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_processional_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_robotic_arm = calculate_distance_min(s_agent, s_processional_stand) \n    penalty_collision = -17 if (d_agent_robotic_arm < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Church. Objectives: It must avoid colliding with parishioner. Conditions: The states of the drone and parishioner are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_parishioner, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_technician = calculate_distance_min(s_agent, s_parishioner) \n    penalty_collision = -19 if (d_agent_technician < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Church. Objectives: Avoid collision with two traffic cone lines. Conditions: The states of the agent, wires, parishioner and warning signs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Church. Objectives: It must avoid colliding with any truck, overhead hanging lamp chains and entrance barrier. Conditions: Agent's, wall and pastor are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Church. Objectives: Do not collide with any bible. Conditions: The states of the drone and the bible are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bible, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bible = calculate_distance_min(s_agent, s_bible) \n    penalty = -19 if (d_agent_bible < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Church and must avoid contact with any sacristy  door. The states of the drone and the sacristy door are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_sacristy_door, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sacristy_door = calculate_distance_min(s_agent, s_sacristy_door) \n    penalty = -19 if (d_agent_sacristy_door < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Church. It must avoid colliding with the confessional booth, usher and hymn board. The states of the agent, hanging lamp chains, storage racks and safety barricades are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Church. It must avoid colliding with altar rails and hymn boards. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Church. It must avoid colliding with Church organ pipess. Agent's state and Church organ pipess's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_organ_pipes, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_pipe = calculate_distance_min(s_agent, s_organ_pipes) \n    penalty_collision = -19 if (d_agent_pipe < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Church. Goal: It must avoid colliding with hanging lamp chain. Assumptions: The states of the drone and the hanging lamp chain's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_lamp_chain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lamp_chain = calculate_distance_min(s_agent, s_lamp_chain) \n    penalty_collision = -19 if (d_agent_lamp_chain < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Church. Goal: It must avoid colliding with Church's pew, pulpit and parishioner. Assumptions: The agent's state, pew, pulpit and parishioner' states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Church. Goal: Detect altars and benchs and avoid church door and do not collide. Assumptions: The states of the drone, altar, benches and avoid church door state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Church. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: \n A drone is operating along Art Museum. Objectives: \n It must avoid colliding with concrete columns or exhibit placards. Conditions: \n The states of the agent, the stanchion, and the exhibit placards are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: \n A drone is operating inside a busy Art Museum. Objectives: \n Detect sculpture pedestal and aviod it. Conditions: \n The states of the agent, the sculpture pedestal are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_sculpture_pedestal, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sculpture_pedestal = calculate_distance_min(s_agent, s_sculpture_pedestal) \n    penalty_collision = -17 if (d_agent_sculpture_pedestal < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Art Museum open workspace. Objectives: It must avoid colliding with gallery fixtures. Conditions: States of the drone and the gallery fixturest are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_gallery_fixtures, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_gallery_fixtures = calculate_distance_min(s_agent, s_gallery_fixtures) \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a art museum corridor, the drone must maneuver around art museum curtain. All entity states (agent, art museum curtain) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_gallery_curtain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_gallery_curtain = calculate_distance_min(s_agent, s_gallery_curtain) \n    penalty_collision = -17 if (d_agent_gallery_curtain < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a art museum corridor. Requirement: Detect the three art museum visitors and do not collide with it. Given: The states for the drone and many art museum visitors are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: \n A drone operates near a staff service area during experiments. Objectives: \n So not collide with display_stand. Conditions: \n The states of the agent, the display_stand are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_display_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_display_stand = calculate_distance_min(s_agent, s_display_stand) \n    penalty_collision = -17 if (d_agent_display_stand < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate Art Museum. Objectives: Do not collide hit with sculpture plinth. Conditions: States of the agent, sculpture plinth are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_sculpture_plinth, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sculpture_plinth = calculate_distance_min(s_agent, s_sculpture_plinth) \n    penalty_collision = -17 if (d_agent_sculpture_plinth < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a art museum corridor, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_gallery_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_gallery_wall = calculate_distance_min(s_agent, s_gallery_wall) \n    penalty_collision = -17 if (d_agent_gallery_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: art museum floor. Avoid: Hitting with display stand. Known: Agent, display stand states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_display_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_display_stand = calculate_distance_min(s_agent, s_display_stand) \n    penalty_collision = -17 if (d_agent_display_stand < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Art Museum. Objectives: It must avoid colliding with visitor. Conditions: The states of the drone and visitor are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_visitor, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_visitor = calculate_distance_min(s_agent, s_visitor) \n    penalty_collision = -19 if (d_agent_visitor < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Art Museum. Objectives: Avoid collision with two cone lines. Conditions: The states of the agent and two cone lines are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Art Museum. Objectives: It must avoid colliding with any picture, overhead wires and ticket barrier. Conditions: Agent's, picture's, wire's and ticket barrier's states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Art Museum. Objectives: Do not collide with any sculpture pedestal. Conditions: The states of the drone and the sculpture pedestal are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_sculpture_pedestal, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sculpture_pedestal = calculate_distance_min(s_agent, s_sculpture_pedestal) \n    penalty = -19 if (d_agent_sculpture_pedestal < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Art Museum and must avoid contact with any staff only door. The states of the drone and the staff only door are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_staff_door, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_door = calculate_distance_min(s_agent, s_staff_door) \n    penalty = -19 if (d_agent_door < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Art Museum. It must avoid colliding with the freestanding partition, docent and exhibit placard. The states of the agent, suspension wires, storage racks and safety barricades are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Art Museum. It must avoid colliding with stanchions and exhibit placards. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Art Museum. It must avoid colliding with Art Museum organ hanging installations. Agent's state and Art Museum organ hanging installations's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_installation, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_installation = calculate_distance_min(s_agent, s_installation) \n    penalty_collision = -19 if (d_agent_installation < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Art Museum. Goal: It must avoid colliding with suspension wire. Assumptions: The states of the drone and the suspension wire's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_suspension_wire, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_suspension_wire = calculate_distance_min(s_agent, s_suspension_wire) \n    penalty_collision = -19 if (d_agent_suspension_wire < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Art Museum. Goal: It must avoid colliding with Art Museum's pew, pulpit and visitor. Assumptions: The agent's state, pew, pulpit and visitor' states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Art Museum. Goal: Detect waste containers and gas cylindes and avoid art museum door and do not collide. Assumptions: The states of the drone, freestanding partition, docent and exhibit placard state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Art Museum. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Coastal wind-farm drone inspecting turbines. \n Objectives: Avoid collision with turbine towers and rotating blades. \n Conditions: Drone state and the multiple obstacle states are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Coastal wind-farm drone inspecting turbines. \n Objectives: Avoid collision with the obstacle. \n Conditions: Drone state and the single obstacle state. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_single_obstacle, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_single_obstacle = calculate_distance_min(s_agent, s_single_obstacle) \n    penalty_collision = -15 if (d_agent_single_obstacles < 0.5) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Coastal wind-farm drone inspecting turbines. \n Goal: Prevent collision with the turbine tower. \n Assumptions: Drone state and tower state are available. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_tower, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_tower = calculate_distance_min(s_agent, s_tower) \n    penalty_collision = -15 if (dist_tower < 0.5) else 0 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Coastal wind-farm drone inspecting turbines. \n Aim: Avoid hitting the turbine tower. \n Prerequisites: Positions and statuses of the drone and tower are known. \n Requirement: Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_tower, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_tower = calculate_distance_min(s_agent, s_tower) \n    penalty_collision = -15 if (dist_tower < 0.5) else 0 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Coastal wind-farm drone inspecting turbines. \n Aim: Reach the target. \n Prerequisites: Positions and statuses of the drone are known. \n requirement: Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 20 if (dist_target < 0.2) else -10 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Coastal wind-farm drone inspecting turbines. \n Aim: Avoid hitting the turbine tower and blades. \n Prerequisites: Positions and statuses of the drone, tower and blades are known. \n Requirement: Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -15 if (dist_multiple_obstacles < 0.5) else 0 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Coastal wind-farm drone inspecting turbines. \n Aim: Avoid hitting the turbine tower and blades. \n Prerequisites: Positions and statuses of the drone, tower and blades are known. \n Requirement: Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (dist_obstacles < 0.5) else 0 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Coastal wind-farm drone inspecting turbines. \n Aim: Avoid hitting the maintenance crane. \n Prerequisites: Positions and statuses of the agent and maintance crane are known. \n Requirement: Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_crane, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_crane = calculate_distance_min(s_agent, s_crane) \n    penalty_collision = -15 if (dist_crane < 0.5) else 0 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Coastal wind-farm drone inspecting turbines. \n Aim: Avoid hitting the maintenance crane. \n Prerequisites: Positions and statuses of the agent and crane are known. \n Requirement: Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_crane, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_crane = calculate_distance_min(s_agent, s_crane) \n    penalty_collision = -15 if (dist_crane < 0.5) else 0 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Coastal wind-farm drone inspecting turbines. \n Objectives: Avoid collision with weather balloon. \n Conditions: Drone and weather balloons states are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_balloon, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_balloon = calculate_distance_min(s_agent, s_balloon) \n    penalty_collision = -15 if (d_agent_balloon < 0.6) else 4 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Coastal wind-farm drone inspecting turbines. \n Objectives: Avoid collision with weather balloons amd wires. \n Conditions: Drone, weather balloons and wires states are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -18 if (dist_multiple_obstacles < 0.7) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.3) else -15 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Coastal wind-farm drone inspecting turbines. \n Objectives: Avoid collision with weather balloons amd wires. \n Conditions: Drone, weather balloons and wires states are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -18 if (dist_multiple_obstacles < 0.7) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.3) else -15 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Coastal wind-farm drone inspecting turbines. \n Objectives: Do not collide with employee. \n Conditions: Drone's state and employee's state are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_employee, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_employee = calculate_distance_min(s_agent, s_employee) \n    penalty = -18 if (d_employee < 0.7) else 10 \n    reward_reaching_target = 10 if (d_target < 0.3) else -15 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In wind-farm drone inspecting turbines must avoid colliding with an equipment, given that the states of both the drone and the equipment are known, and a reward function should be written in Python.",
        "chosen": "def reward(s_agent, s_equipment, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_equipment = calculate_distance_min(s_agent, s_equipment) \n    penalty = -18 if (d_equipment < 0.7) else 10 \n    reward_reaching_target = 10 if (d_target < 0.3) else -15 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In wind-farm drone inspecting turbines must avoid colliding with an equipment, given that the states of the agent, birds the equipment are known, and a reward function should be written in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a wind farm, a drone inspecting turbines must avoid colliding with equipment or birds. The states of the agent, the blades, and the equipments are known. Write a Python reward function for this scenario.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -15 if (dist_obstacles_min < 0.6) else 0 \n    reward_reaching_target = 10 if (dist_target < 0.3) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting turbines in a wind farm. It must avoid colliding with turbin tower. The states of the agent and tower are known. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_tower, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tower = calculate_distance_min(s_agent, s_tower) \n    penalty_collision = -15 if (d_agent_tower < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Coastal wind-farm drone inspecting turbines. \n Goal: Prevent collision with the lift. \n Assumptions: Drone state and lift state are available. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_lift, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_lift = calculate_distance_min(s_agent, s_lift) \n    penalty_collision = -15 if (dist_lift < 0.5) else 0 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Coastal wind-farm drone inspecting turbines. \n Goal: Prevent collision with any obstacles. \n Assumptions: Agent's state, target's state and all obstacles are available. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Coastal wind-farm drone inspecting turbines. \n Goal: Prevent collision with any obstacles. \n Assumptions: Agent's state, target's state and all obstacles are available. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Coastal wind-farm drone inspecting turbines. \n Goal: REach the target. \n Assumptions: Agent's state are available. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else 0 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is navigating a factory floor. It must avoid colliding with machinery or workers. The states of the agent, the workers, and the machinery are known. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is navigating a factory floor. It must avoid colliding with machinery. The states of the agent and the machinery are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_machinery, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_machinery = calculate_distance_min(s_agent, s_machinery) \n    penalty_collision = -20 if (d_agent_machinery < 0.6) else 10 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a factory floor. \n Goal: It must avoid colliding with material cart. \n Assumptions: The states of the agent and the material cart are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_cart, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_cart = calculate_distance_min(s_agent, s_cart) \n    penalty_collision = -20 if (dist_cart < 0.5) else 10 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: A drone is navigating a factory floor. \n Aim: It must avoid colliding with workbench. \n Prerequisites: Drone's state and the workbench's state are known. \n Requirement: Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_workbench, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_workbench = calculate_distance_min(s_agent, s_workbench) \n    penalty_collision = -20 if (dist_workbench < 0.5) else 10 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: A drone is navigating a factory floor. \n Aim: It must reach the target. \n Prerequisites: The states of the agent are known. \n requirement: Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 20 if (dist_target < 0.2) else -d_agent_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: A drone is navigating a factory floor. \n Aim: It must avoid colliding with maintenance equipment and pipes. \n Prerequisites: The states of the agent, equipment and pipes are known. \n Requirement: Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (dist_multiple_obstacles < 0.5) else 10 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: A drone is navigating a factory floor. \n Aim: It must avoid colliding with pipes and wires. \n Prerequisites: The states of the agent and multiple obstacles are known. \n Requirement: Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -20 if (dist_obstacles < 0.5) else 10 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: A drone is navigating a factory floor. \n Aim: It must avoid colliding with packaging stations. \n Prerequisites: The states of the agent and the packaging station are known. \n Requirement: Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_station, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_station = calculate_distance_min(s_agent, s_station) \n    penalty_collision = -20 if (dist_station < 0.5) else 10 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: A drone is navigating a factory floor. \n Aim: Avoid colliding with cable. \n Prerequisites: The states of the agent and the cable are known. \n Requirement: Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_cable, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_cable = calculate_distance_min(s_agent, s_cables) \n    penalty_collision = -20 if (dist_cable < 0.5) else 10 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a factory floor. \n Objectives: It must avoid colliding with container. \n Conditions: The states of the agent and the container are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_container, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_container = calculate_distance_min(s_agent, s_container) \n    penalty_collision = -20 if (d_agent_container < 0.6) else 4 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a factory floor. \n Objectives: Avoid collision with container amd cable. \n Conditions: The states of the agent, container and cable are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -18 if (dist_multiple_obstacles < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a factory floor. \n Objectives: It must avoid colliding any facing obstacles. \n Conditions: Agent's and obstacles's are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -18 if (dist_multiple_obstacles < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.2) else -dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a factory floor. \n Objectives: Do not collide with any personnel. \n Conditions: The states of the agent and the personnel are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_personnel, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_personnel = calculate_distance_min(s_agent, s_personnel) \n    penalty = -18 if (d_personnel < 0.6) else 10 \n    reward_reaching_target = 10 if (d_target < 0.2) else -d_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "An autonomous drone is operating on a factory floor. It must avoid colliding with pallets placed in the workspace. The states of the drone and the pallets are known. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_pallet, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_pallet = calculate_distance_min(s_agent, s_equipment) \n    penalty = -18 if (d_pallet < 0.6) else 10 \n    reward_reaching_target = 10 if (d_target < 0.2) else -d_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "An drone is operating on a factory floor. It must avoid colliding with boxes and walls placed in the workspace. The states of the drone and the boxes and walls are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "An drone is operating on a factory floor. It must avoid colliding with product and tools. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (dist_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting turbines in a wind farm. It must avoid colliding with tool. Agent's state, target's state and tool's state are accessible. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_tool, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tool = calculate_distance_min(s_agent, s_tool) \n    penalty_collision = -20 if (d_agent_tool < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a factory. \n Goal: It must avoid colliding with lift. \n Assumptions: The states of the agent and the lift are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_lift, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_lift = calculate_distance_min(s_agent, s_lift) \n    penalty_collision = -20 if (dist_lift < 0.5) else 10 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a factory. \n Goal: It must avoid colliding with robotics arm and workstation. \n Assumptions: The Agent's state, target's state and arm's state and workstation's state are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a factory. \n Goal: Detect humans and wires and avoid collisions. \n Assumptions: The states of the agent and multiple obstacles are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a factory. \n Goal: Dorne target is to reach the target. \n Assumptions: The states of the agent and the target are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -d_agent_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a train station. It must avoid colliding with train or human. The states of the agent, the train, and the human are known. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 25 if (d_agent_target < 0.1) else -2*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a train station. It must avoid colliding with train. The states of the agent and the train are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_train, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_train = calculate_distance_min(s_agent, s_train) \n    penalty_collision = -25 if (d_agent_train < 0.6) else 10 \n    reward_reaching_target = 25 if (d_agent_target < 0.1) else -2*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is moving inside a train station. \n Goal: It must avoid colliding with platform. \n Assumptions: The states of the agent and the platform are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_platform, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_platform = calculate_distance_min(s_agent, s_platform) \n    penalty_collision = -25 if (dist_platform < 0.5) else 10 \n    reward_reaching_target = 25 if (dist_target < 0.1) else -2*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: A drone is navigating a train station. \n Aim: Drone must avoid colliding with ticket machine. \n Prerequisites: Drone's state and the ticket machine's state are known. \n Requirement: Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_ticket_machine, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_ticket_machine = calculate_distance_min(s_agent, s_ticket_machine) \n    penalty_collision = -25 if (dist_ticket_machine < 0.5) else 10 \n    reward_reaching_target = 25 if (dist_target < 0.1) else -2*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: A drone is piloting inside a train station. \n Aim: It must reach the target. \n Prerequisites: The states of the agent are known. \n requirement: Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 25 if (dist_target < 0.1) else -2*d_agent_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: A drone is piloting inside a train station. \n Aim: It must avoid colliding with information board and benches. \n Prerequisites: The states of the agent, information board and benches are known. \n Requirement: Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_board, s_bench, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_board = calculate_distance(s_agent, s_board) \n    dist_bench = calculate_distance(s_agent, s_bench) \n    min_dist_obstacles = min(dist_board, dist_bench) \n    penalty_collision = -25 if (min_dist_obstacles < 0.5) else 10 \n     reward_reaching_target = 25 if (dist_target < 0.1) else -2 * dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: A drone is navigating a train station. \n Aim: It must avoid colliding with pillar and escalator. \n Prerequisites: The states of the agent, pillar's state and escalator's state are known. \n Requirement: Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_pillar, s_escalator, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_pillar = calculate_distance(s_agent, s_pillar) \n    dist_escalator = calculate_distance(s_agent, s_bench) \n    min_dist_obstacles = min(dist_pillar, dist_escalator) \n    penalty_collision = -25 if (min_dist_obstacles < 0.5) else 10 \n     reward_reaching_target = 25 if (dist_target < 0.1) else -2 * dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: A drone is navigating a train station. \n Aim: Avoid colliding with escalator. \n Prerequisites: The states of the agent and the escalator are known. \n Requirement: Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_escalator, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_escalator = calculate_distance_min(s_agent, s_escalator) \n    penalty_collision = -25 if (dist_escalator < 0.5) else 10 \n    reward_reaching_target = 25 if (dist_target < 0.1) else -2*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: A drone is flying through a train station. \n Aim: Avoid colliding with vending machine. \n Prerequisites: The states of the agent and the vending machine are known. \n Requirement: Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_vending_machine, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_vending_machine = calculate_distance_min(s_agent, s_vending_machine) \n    penalty_collision = -25 if (dist_vending_machine < 0.5) else 10 \n    reward_reaching_target = 25 if (dist_target < 0.1) else -2*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a train station. \n Objectives: It must avoid colliding with staircase. \n Conditions: The states of the agent and the staircase are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_staircase, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_staircase = calculate_distance_min(s_agent, s_staircase) \n    penalty_collision = -25 if (d_agent_staircase < 0.6) else 4 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -2*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through a train station. \n Objectives: Avoid collision with escalator and staircase. \n Conditions: The states of the agent, escalator and staircase are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (dist_multiple_obstacles < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -2*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a train station. \n Objectives: It must avoid colliding any trash bin, construction barrier and service vehicle. \n Conditions: Agent's and all obstacles's are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -25 if (dist_multiple_obstacles < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -2*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through a train station. \n Objectives: Do not collide with any personnel. \n Conditions: The states of the agent and the personnel are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_personnel, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_personnel = calculate_distance_min(s_agent, s_personnel) \n    penalty = -25 if (d_personnel < 0.6) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -2*d_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a train station and must avoid contact with any moving or stationary train. The states of the drone and the trains are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_train, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_train = calculate_distance_min(s_agent, s_train) \n    penalty = -25 if (d_train < 0.6) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -2*d_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "An drone is operating on a train station. It must avoid colliding with boxes and walls placed in the workspace. The states of the drone and the boxes and walls are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -2*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "An drone is operating on a train station. It must avoid colliding with luggage and dog. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (dist_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -2*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in a train station. It must avoid colliding with staff. Agent's state, target's state and staff's state are accessible. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_staff = calculate_distance_min(s_agent, s_staff) \n    penalty_collision = -25 if (d_agent_staff < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -2*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a train station. \n Goal: It must avoid colliding with lift. \n Assumptions: The states of the agent and the lift are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_lift, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_lift = calculate_distance_min(s_agent, s_lift) \n    penalty_collision = -25 if (dist_lift < 0.5) else 10 \n    reward_reaching_target = 25 if (dist_target < 0.1) else -2*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a train station. \n Goal: It must avoid colliding with robotics arm and workstation. \n Assumptions: The Agent's state, target's state and arm's state and workstation's state are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -2*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a train station. \n Goal: Detect humans and wires and avoid collisions. \n Assumptions: The states of the agent and multiple obstacles are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -2*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a train station. \n Goal: Dorne target is to reach the target. \n Assumptions: The states of the agent and the target are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -2*d_agent_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a airport terminal. It must avoid colliding with plane or human. The states of the agent, the palne, and the human are known. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    2**d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 25 if (d_agent_target < 0.1) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a busy airport concourse. It must avoid colliding with cleaning carts. The states of the agent, the carts, and the target are known. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_cleaning_carts, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_cleaning_carts) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.7) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a terminal gate area. \nObjectives: It must avoid colliding with advertising kiosks. \n Conditions: States of the agent, the kiosks, and the target are known. \nWrite a reward function in Python.",
        "chosen": "def reward(s_agent, s_ad_kiosk, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ad_kiosk = calculate_distance_min(s_agent, s_ad_kiosk) \n    penalty_collision = -10 if (d_agent_ad_kiosk < 0.7) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside an airport retail corridor, the drone must maneuver around temporary stanchions (queue barriers). All entity states (agent, barriers, target) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_queue_barrier, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_queue_barrier) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.7) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through baggage claim. \n Requirement: Avoid collisions with luggage carts. \n Given: The states for the agent, carts, and target are known. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_luggage_cart, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_luggage_cart = calculate_distance_min(s_agent, s_luggage_cart) \n    penalty_collision = -10 if (d_agent_luggage_cart < 0.7) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone operates on the departures level near security. It must not collide with retractable belt barriers or portable signs. The states of the agent, the obstacles, and the target are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_portable_sign, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_portable_sign) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.7) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in a terminal lounge without hitting autonomous floor scrubbers. \n Conditions: States of the agent, scrubbers, and target are available. \nDeliver: Python reward function.",
        "chosen": "def reward(s_agent, s_floor_scrubber, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_floor_scrubber = calculate_distance_min(s_agent, s_floor_scrubber) \n    penalty_collision = -10 if (d_agent_floor_scrubber < 0.7) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within an international terminal atrium, the drone must steer clear of decorative pillars. All states (agent, pillars, target) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_pillar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_pillar) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.7) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: \n Environment: Airport food court. \n Avoid: Delivery robots shuttling orders. \n Known: Agent, robots, and target states. \n Create the Python reward function.",
        "chosen": "def reward(s_agent, s_delivery_robot, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_delivery_robot = calculate_distance_min(s_agent, s_delivery_robot) \n    penalty_collision = -10 if (d_agent_delivery_robot < 0.7) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a airport terminal. \n Objectives: It must avoid colliding with luggage. \n Conditions: The states of the agent and the luggage are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_luggage, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_luggage = calculate_distance_min(s_agent, s_luggage) \n    penalty_collision = -25 if (d_agent_luggage < 0.6) else 4 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through a airport terminal. \n Objectives: Avoid collision with escalator and staircase. \n Conditions: The states of the agent, escalator and staircase are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (dist_multiple_obstacles < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a airport terminal. \n Objectives: It must avoid colliding any trash bin, construction barrier and service vehicle. \n Conditions: Agent's and all obstacles's are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -25 if (dist_multiple_obstacles < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -2**dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through a airport terminal. \n Objectives: Do not collide with any personnel. \n Conditions: The states of the agent and the personnel are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_personnel, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_personnel = calculate_distance_min(s_agent, s_personnel) \n    penalty = -25 if (d_personnel < 0.6) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -2**d_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a airport and must avoid contact with any Check-in counter. The states of the drone and the counter are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_counter, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_counter = calculate_distance_min(s_agent, s_counter) \n    penalty = -25 if (d_counter < 0.6) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -2**d_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "An drone is operating on a airport. It must avoid colliding with boxes and walls placed in the seating area. The states of the drone and the boxes and walls are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "An drone is operating on a airport. It must avoid colliding with seats and passenger. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (dist_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in a airport. It must avoid colliding with staff. Agent's state, target's state and staff's state are accessible. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_staff = calculate_distance_min(s_agent, s_staff) \n    penalty_collision = -25 if (d_agent_staff < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a airport. \n Goal: It must avoid colliding with stairs. \n Assumptions: The states of the agent and the stair are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_stair, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_stair = calculate_distance_min(s_agent, s_stair) \n    penalty_collision = -25 if (dist_stair < 0.5) else 10 \n    reward_reaching_target = 25 if (dist_target < 0.1) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a airport. \n Goal: It must avoid colliding with staff, passenger and check-ins. \n Assumptions: The Agent's state, target's state and staff's state, counter's state and passenger's state are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a airport. \n Goal: Detect humans and pillars and avoid boards. \n Assumptions: The states of the agent and multiple obstacles are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a airport. \n Goal: Dorne target is to reach the target. \n Assumptions: The states of the agent and the target are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -2**d_agent_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a shopping mall. It must avoid colliding with escalators or shoppers. The states of the agent, the escalator, and the shoppers are known. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    4*d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 25 if (d_agent_target < 0.1) else -4*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a busy shopping mall concourse. It must avoid colliding with cleaning carts. The states of the agent, the carts, and the target are known. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_cleaning_cart, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cleaning_cart = calculate_distance_min(s_agent, s_cleaning_cart) \n    penalty_collision = -10 if (d_agent_cleaning_cart < 0.7) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -4*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a shopping mall food court. \nObjectives: It must avoid colliding with advertising kiosks. \n Conditions: States of the agent, the kiosks, and the target are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ad_kiosk, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ad_kiosk = calculate_distance_min(s_agent, s_ad_kiosk) \n    penalty_collision = -10 if (d_agent_ad_kiosk < 0.7) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -4*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a shopping mall corridor, the drone must maneuver around temporary stanchions (queue barriers). All entity states (agent, barriers, target) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_queue_barrier, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_queue_barrier = calculate_distance_min(s_agent, s_queue_barrier) \n    penalty_collision = -10 if (d_agent_queue_barrier < 0.7) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -4*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a shopping mall atrium. \n Requirement: Avoid collisions with shopping carts. \n Given: The states for the agent, carts, and target are known. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_shopping_cart, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_shopping_cart = calculate_distance_min(s_agent, s_shopping_cart) \n    penalty_collision = -10 if (d_agent_shopping_cart < 0.7) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -4*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone operates near a mall entrance during an event. It must not collide with retractable belt barriers or portable signs. The states of the agent, the obstacles, and the target are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_portable_sign, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_portable_sign) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.7) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -4*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in a mall lounge without hitting autonomous floor scrubbers. \n Conditions: States of the agent, scrubbers, and target are available. \nDeliver: Python reward function.",
        "chosen": "def reward(s_agent, s_floor_scrubber, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_floor_scrubber = calculate_distance_min(s_agent, s_floor_scrubber) \n    penalty_collision = -10 if (d_agent_floor_scrubber < 0.7) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -4*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a shopping mall atrium, the drone must steer clear of decorative pillars. All states (agent, pillars, target) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_pillar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_pillar) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.7) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -4*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: \n Environment: Shopping mall food court. \n Avoid: Delivery robots shuttling orders. \n Known: Agent, robots, and target states. \n Create the Python reward function.",
        "chosen": "def reward(s_agent, s_delivery_robot, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_delivery_robot = calculate_distance_min(s_agent, s_delivery_robot) \n    penalty_collision = -10 if (d_agent_delivery_robot < 0.7) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -4*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a shopping mall. \n Objectives: It must avoid colliding with baby strollers. \n Conditions: The states of the agent and the strollers are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_stroller, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_stroller = calculate_distance_min(s_agent, s_stroller) \n    penalty_collision = -25 if (d_agent_stroller < 0.6) else 3 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -4*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through a shopping mall. \n Objectives: Avoid collision with escalator and staircase. \n Conditions: The states of the agent, escalator and staircase are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (dist_multiple_obstacles < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a shopping mall. \n Objectives: It must avoid colliding with any trash bin, construction hoarding and service cart. \n Conditions: Agent's and all obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -25 if (dist_multiple_obstacles < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -4*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through a shopping mall. \n Objectives: Do not collide with any mall staff. \n Conditions: The states of the agent and the staff are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_staff = calculate_distance_min(s_agent, s_staff) \n    penalty = -25 if (d_staff < 0.6) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -4*d_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a shopping mall and must avoid contact with any information desk. The states of the drone and the desk are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_info_desk, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_info_desk = calculate_distance_min(s_agent, s_info_desk) \n    penalty = -25 if (d_info_desk < 0.6) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -4*d_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in a shopping mall. It must avoid colliding with display boxes and glass partitions placed in the seating area. The states of the drone and the obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -4*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in a shopping mall. It must avoid colliding with benches and shoppers. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (dist_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in a shopping mall. It must avoid colliding with staff. Agent's state, target's state and staff's state are accessible. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_staff = calculate_distance_min(s_agent, s_staff) \n    penalty_collision = -25 if (d_agent_staff < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -4*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a shopping mall. \n Goal: It must avoid colliding with stairs. \n Assumptions: The states of the agent and the stair are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_stair, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_stair = calculate_distance_min(s_agent, s_stair) \n    penalty_collision = -25 if (dist_stair < 0.5) else 10 \n    reward_reaching_target = 25 if (dist_target < 0.1) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a shopping mall. \n Goal: It must avoid colliding with staff, shoppers and information desks. \n Assumptions: The Agent's state, target's state and staff's state, desk's state and shoppers' states are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -4*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a shopping mall. \n Goal: Detect shoppers and pillars and avoid sign boards. \n Assumptions: The states of the agent and multiple obstacles are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -4*d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a shopping mall. \n Goal: Drone target is to reach the target. \n Assumptions: The states of the agent and the target are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -4*d_agent_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a supermarket. It must avoid colliding with checkout counters or customers. The states of the agent, the freezer aisles, and the customers are known. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a busy supermarket concourse. It must avoid colliding with stock carts. The states of the agent, the carts, and the target are known. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_cleaning_carts, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_cleaning_carts) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a supermarket food court. \nObjectives: It must avoid colliding with advertising kiosks. \n Conditions: States of the agent, the kiosks, and the target are known. \nWrite a reward function in Python.",
        "chosen": "def reward(s_agent, s_product_display, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_product_display = calculate_distance_min(s_agent, s_product_display) \n    penalty_collision = -15 if (d_agent_product_display < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a supermarket corridor, the drone must maneuver around temporary stanchions (aisle endcaps). All entity states (agent, barriers, target) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_queue_barrier, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_queue_barrier) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a supermarket atrium. \n Requirement: Avoid collisions with shopping carts. \n Given: The states for the agent, carts, and target are known. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_trolley, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_trolley = calculate_distance_min(s_agent, s_trolley) \n    penalty_collision = -15 if (d_agent_trolley < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone operates near a mall entrance during an event. It must not collide with retractable belt barriers or portable signs. The states of the agent, the obstacles, and the target are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_sale_sign, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_sale_sign) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in a mall lounge without hitting autonomous floor scrubbers. \n Conditions: States of the agent, scrubbers, and target are available. \nDeliver: Python reward function.",
        "chosen": "def reward(s_agent, s_floor_polisher, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_floor_polisher = calculate_distance_min(s_agent, s_floor_polisher) \n    penalty_collision = -15 if (d_agent_floor_polisher < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a supermarket atrium, the drone must steer clear of decorative support columns. All states (agent, support columns) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_support_column, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_support_column) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: \n Environment: Shopping mall food court. \n Avoid: Delivery robots shuttling orders. \n Known: Agent, robots states. \n Create the Python reward function.",
        "chosen": "def reward(s_agent, s_restocking_robot, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_restocking_robot = calculate_distance_min(s_agent, s_restocking_robot) \n    penalty_collision = -15 if (d_agent_restocking_robots < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a supermarket. \n Objectives: It must avoid colliding with baby shopping baskets. \n Conditions: The states of the agent and the shopping baskets are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_shopping_basket, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_shopping_basket = calculate_distance_min(s_agent, s_shopping_basket) \n    penalty_collision = -19 if (d_agent_shopping_basket < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through a supermarket. \n Objectives: Avoid collision with freezer aisles and loading dock ramps. \n Conditions: The states of the agent, freezer aisles and loading dock ramps are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a supermarket. \n Objectives: It must avoid colliding with any recycling bin, storage shelves and pallet jack. \n Conditions: Agent's and all obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -dist_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through a supermarket. \n Objectives: Do not collide with any mall store clerks. \n Conditions: The states of the agent and the store clerks are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_store_clerk, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_store_clerk = calculate_distance_min(s_agent, s_store_clerk) \n    penalty = -19 if (d_store_clerk < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -d_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a supermarket and must avoid contact with any information desk. The states of the drone and the desk are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_customer_service_desk, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_customer_service_desk = calculate_distance_min(s_agent, s_customer_service_desk) \n    penalty = -19 if (d_customer_service_desk < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -d_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in a supermarket. It must avoid colliding with bulk item pallets and refrigerated glass doors placed in the seating area. The states of the drone and the obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in a supermarket. It must avoid colliding with checkout queue rails and customers. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in a supermarket. It must avoid colliding with store clerks. Agent's state, target's state and store clerks's state are accessible. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_store_clerk, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_store_clerk = calculate_distance_min(s_agent, s_store_clerk) \n    penalty_collision = -19 if (d_agent_store_clerk < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a supermarket. \n Goal: It must avoid colliding with stairs. \n Assumptions: The states of the agent and the stair are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_stair, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_stair = calculate_distance_min(s_agent, s_stair) \n    penalty_collision = -19 if (dist_stair < 0.5) else 15 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a supermarket. \n Goal: It must avoid colliding with store clerks, customers and information desks. \n Assumptions: The Agent's state, target's state and store clerks's state, desk's state and customers' states are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a supermarket. \n Goal: Detect customers and support columns and avoid endcap displays. \n Assumptions: The states of the agent and multiple obstacles are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a supermarket. \n Goal: Drone target is to reach the target. \n Assumptions: The states of the agent and the target are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a turbines frame, where it must avoid colliding with the turbines Towers and rotating blades and reach the target. The drone’s state, target's state and the information of the multiple obstacle are available. Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a turbines frame, where it must avoid colliding with the single obstacle and reach the target. The drone’s state, target's state and the information of the single obstacle are available. Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_single_obstacle, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_single_obstacle = calculate_distance_min(s_agent, s_single_obstacle) \n    penalty_collision = -15 if (d_agent_single_obstacles < 0.5) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a turbines frame, where it must avoid colliding with the tower which is a obstacle and reach the target. The drone’s state, target's state and the information of the tower are available. Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_tower, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_tower = calculate_distance_min(s_agent, s_tower) \n    penalty_collision = -15 if (dist_tower < 0.5) else 0 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Coastal wind-farm drone inspecting turbines. \n Aim: Avoid hitting the turbine tower. \n Prerequisites: Positions and statuses of the drone and tower are known. \n Requirement: Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_tower, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_tower = calculate_distance_min(s_agent, s_tower) \n    penalty_collision = -15 if (dist_tower < 0.5) else 0 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Coastal wind-farm drone inspecting turbines. \n Aim: Reach the target. \n Prerequisites: Positions and statuses of the drone are known. \n requirement: Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 20 if (dist_target < 0.2) else -10 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a turbines frame, where it must avoid colliding with the Towers and blades and reach the target. The drone’s state, target's state and the information of the multiple obstacle are available. Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -15 if (dist_multiple_obstacles < 0.5) else 0 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a turbines frame, where it must avoid colliding with the electrical panel and wall and reach the target. The drone’s state, target's state and the information of the multiple obstacle are available. Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (dist_obstacles < 0.5) else 0 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a turbines frame, where it must avoid colliding with the crane which is a obstacle and reach the target. The drone’s state, target's state and the information of the crane are available. Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_crane, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_crane = calculate_distance_min(s_agent, s_crane) \n    penalty_collision = -15 if (dist_crane < 0.5) else 0 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Coastal wind-farm drone inspecting turbines. \n Aim: Avoid hitting the maintenance crane. \n Prerequisites: Positions and statuses of the agent and crane are known. \n Requirement: Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_crane, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_crane = calculate_distance_min(s_agent, s_crane) \n    penalty_collision = -15 if (dist_crane < 0.5) else 0 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a turbines frame, where it must avoid colliding with the ballon which is a obstacle and reach the target. The drone’s state, target's state and the information of the ballon are available. Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_balloon, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_balloon = calculate_distance_min(s_agent, s_balloon) \n    penalty_collision = -15 if (d_agent_balloon < 0.6) else 4 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a turbines frame, where it must avoid colliding with the weather ballon and wires and reach the target. The drone’s state, target's state and the information of the multiple obstacle are available. Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -18 if (dist_multiple_obstacles < 0.7) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.3) else -15 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a turbines frame, where it must avoid colliding with the electrical panel, wires and blades and reach the target. The drone’s state, target's state and the information of the multiple obstacle are available. Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -18 if (dist_multiple_obstacles < 0.7) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.3) else -15 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a turbines frame, where it must avoid colliding with the employee which is a obstacle and reach the target. The drone’s state, target's state and the information of the employee are available. Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_employee, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_employee = calculate_distance_min(s_agent, s_employee) \n    penalty = -18 if (d_employee < 0.7) else 10 \n    reward_reaching_target = 10 if (d_target < 0.3) else -15 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a turbines frame, where it must avoid colliding with the equipment which is a obstacle and reach the target. The drone’s state, target's state and the information of the equipment are available. Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_equipment, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_equipment = calculate_distance_min(s_agent, s_equipment) \n    penalty = -18 if (d_equipment < 0.7) else 10 \n    reward_reaching_target = 10 if (d_target < 0.3) else -15 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a turbines frame, where it must avoid colliding with the equipment and bird and reach the target. The drone’s state, target's state and the information of the multiple obstacle are available. Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a turbines frame, where it must avoid colliding with the wall and ladder and reach the target. The drone’s state, target's state and the information of the multiple obstacle are available. Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -15 if (dist_obstacles_min < 0.6) else 0 \n    reward_reaching_target = 10 if (dist_target < 0.3) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting turbines in a wind farm. It must avoid colliding with turbin tower. The states of the agent and tower are known. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_tower, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tower = calculate_distance_min(s_agent, s_tower) \n    penalty_collision = -15 if (d_agent_tower < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a turbines frame, where it must avoid colliding with the lift which is a obstacle and reach the target. The drone’s state, target's state and the information of the lift are available. Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_lift, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_lift = calculate_distance_min(s_agent, s_lift) \n    penalty_collision = -15 if (dist_lift < 0.5) else 0 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a turbines frame, where it must avoid colliding with the ladder, equipment and wall and reach the target. The drone’s state, target's state and the information of the multiple obstacle are available. Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a turbines frame, where it must avoid colliding with the person and ladder and reach the target. The drone’s state, target's state and the information of the multiple obstacle are available. Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Coastal wind-farm drone inspecting turbines. \n Goal: REach the target. \n Assumptions: Agent's state are available. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else 0 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor factory floor environment using a drone.  \n targets: Successfully reach the designated target state, Detect machinery and workers as obstacles, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of multiple obstacles, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor factory floor environment using a drone.  \n targets: Successfully reach the designated target state, Detect machinery as obstacles, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of machinery, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_machinery, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_machinery = calculate_distance_min(s_agent, s_machinery) \n    penalty_collision = -20 if (d_agent_machinery < 0.6) else 10 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor factory floor environment using a drone.  \n targets: Successfully reach the designated target state, Detect cart as obstacles, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of cart, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_cart, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_cart = calculate_distance_min(s_agent, s_cart) \n    penalty_collision = -20 if (dist_cart < 0.5) else 10 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor factory floor environment using a drone.  \n targets: Successfully reach the designated target state, Detect workbench as obstacles, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of workbench, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_workbench, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_workbench = calculate_distance_min(s_agent, s_workbench) \n    penalty_collision = -20 if (dist_workbench < 0.5) else 10 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor factory floor environment using a drone.  \n targets: Successfully reach the designated target state .  \n Assumptions: The drone’s own state,  and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.exp(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor factory floor environment using a drone.  \n targets: Successfully reach the designated target state, Detect pipes and maintance equipment as obstacles, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of multiple obstacles, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (dist_multiple_obstacles < 0.5) else 10 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor factory floor environment using a drone.  \n targets: Successfully reach the designated target state, Detect pipe as obstacles, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of pipe, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_pipe, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_pipe = calculate_distance_min(s_agent, s_pipe) \n    penalty_collision = -20 if (dist_pipe < 0.5) else 10 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor factory floor environment using a drone.  \n targets: Successfully reach the designated target state, Detect station as obstacles, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of station, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_station, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_station = calculate_distance_min(s_agent, s_station) \n    penalty_collision = -20 if (dist_station < 0.5) else 10 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor factory floor environment using a drone.  \n targets: Successfully reach the designated target state, Detect cable as obstacles, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of cable, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_cable, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_cable = calculate_distance_min(s_agent, s_cables) \n    penalty_collision = -20 if (dist_cable < 0.5) else 10 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor factory floor environment using a drone.  \n targets: Successfully reach the designated target state, Detect container as obstacles, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of container, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_container, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_container = calculate_distance_min(s_agent, s_container) \n    penalty_collision = -20 if (d_agent_container < 0.6) else 4 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor factory floor environment using a drone.  \n targets: Successfully reach the designated target state, Detect container and cable as obstacles, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of multiple obstacles, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -18 if (dist_multiple_obstacles < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor factory floor environment using a drone.  \n targets: Successfully reach the designated target state, Detect human and wall as obstacles, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of multiple obstacles, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -18 if (dist_multiple_obstacles < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor factory floor environment using a drone.  \n targets: Successfully reach the designated target state, Detect personnel as obstacles, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of personnel, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_personnel, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_personnel = calculate_distance_min(s_agent, s_personnel) \n    penalty = -18 if (d_personnel < 0.6) else 10 \n    reward_reaching_target = 10 if (d_target < 0.2) else -math.exp(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor factory floor environment using a drone.  \n targets: Successfully reach the designated target state, Detect pallet as obstacles, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of pallet, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_pallet, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_pallet = calculate_distance_min(s_agent, s_equipment) \n    penalty = -18 if (d_pallet < 0.6) else 10 \n    reward_reaching_target = 10 if (d_target < 0.2) else -math.exp(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor factory floor environment using a drone.  \n targets: Successfully reach the designated target state, Detect boxes and walls as obstacles, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of multiple obstacles, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor factory floor environment using a drone.  \n targets: Successfully reach the designated target state, Detect product and tools as obstacles, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of multiple obstacles, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (dist_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor factory floor environment using a drone.  \n targets: Successfully reach the designated target state, Detect tool as obstacles, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of tool, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_tool, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tool = calculate_distance_min(s_agent, s_tool) \n    penalty_collision = -20 if (d_agent_tool < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor factory floor environment using a drone.  \n targets: Successfully reach the designated target state, Detect lift as obstacles, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of lift, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_lift, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_lift = calculate_distance_min(s_agent, s_lift) \n    penalty_collision = -20 if (dist_lift < 0.5) else 10 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor factory floor environment using a drone.  \n targets: Successfully reach the designated target state, Detect arm and workstation as obstacles, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of multiple obstacles, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor factory floor environment using a drone.  \n targets: Successfully reach the designated target state, Detect human and wire as obstacles, and Avoid collisions.  \n Assumptions: The drone’s own state, the states of multiple obstacles, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: Explore the indoor factory floor environment using a drone.  \n targets: Successfully reach the designated target state.  \n Assumptions: The drone’s own state, and the target state can all be accessed.  \n Write a python code for reward function.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Train station, avoids human and train, and aims to reach a target using its state and avoid hitting train with using multiple obstacles's status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 25 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Train station, avoids ceiling train, and aims to reach a target using its state and avoid hitting train with using train's status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_train, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_train = calculate_distance_min(s_agent, s_train) \n    penalty_collision = -25 if (d_agent_train < 0.6) else 10 \n    reward_reaching_target = 25 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Train station, avoids ceiling platform, and aims to reach a target using its state and avoid hitting platform with using platform's status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_platform, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_platform = calculate_distance_min(s_agent, s_platform) \n    penalty_collision = -25 if (dist_platform < 0.5) else 10 \n    reward_reaching_target = 25 if (dist_target < 0.1) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Train station, avoids ceiling ticket machine, and aims to reach a target using its state and avoid hitting ticket machine with using ticket machine's status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ticket_machine, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_ticket_machine = calculate_distance_min(s_agent, s_ticket_machine) \n    penalty_collision = -25 if (dist_ticket_machine < 0.5) else 10 \n    reward_reaching_target = 25 if (dist_target < 0.1) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Train station and aims to reach a target using its state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 25 if (dist_target < 0.1) else -math.exp(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Train station, avoids board and train, and aims to reach a target using its state and avoid hitting board and train with using board's state and bench's status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_board, s_bench, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_board = calculate_distance(s_agent, s_board) \n    dist_bench = calculate_distance(s_agent, s_bench) \n    min_dist_obstacles = min(dist_board, dist_bench) \n    penalty_collision = -25 if (min_dist_obstacles < 0.5) else 10 \n     reward_reaching_target = 25 if (dist_target < 0.1) else -2 * dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Train station, avoids pillar and escalator, and aims to reach a target using its state and avoid hitting pillar and escalator with using pillar's state and escalator's status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_pillar, s_escalator, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_pillar = calculate_distance(s_agent, s_pillar) \n    dist_escalator = calculate_distance(s_agent, s_bench) \n    min_dist_obstacles = min(dist_pillar, dist_escalator) \n    penalty_collision = -25 if (min_dist_obstacles < 0.5) else 10 \n     reward_reaching_target = 25 if (dist_target < 0.1) else -2 * dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Train station, avoids escalator, and aims to reach a target using its state and avoid hitting escalator with using escalator's status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_escalator, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_escalator = calculate_distance_min(s_agent, s_escalator) \n    penalty_collision = -25 if (dist_escalator < 0.5) else 10 \n    reward_reaching_target = 25 if (dist_target < 0.1) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Train station, avoids vending machine, and aims to reach a target using its state and avoid hitting vending machine with using vending machine's status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_vending_machine, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_vending_machine = calculate_distance_min(s_agent, s_vending_machine) \n    penalty_collision = -25 if (dist_vending_machine < 0.5) else 10 \n    reward_reaching_target = 25 if (dist_target < 0.1) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Train station, avoids staircase, and aims to reach a target using its state and avoid hitting staircase with using staircase's status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_staircase, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_staircase = calculate_distance_min(s_agent, s_staircase) \n    penalty_collision = -25 if (d_agent_staircase < 0.6) else 4 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Train station, avoids escalator and staircase, and aims to reach a target using its state and avoid hitting train with using multiple obstacles's status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (dist_multiple_obstacles < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Train station, avoidstrash bin, construction barrier and service vehicle, and aims to reach a target using its state and avoid hitting train with using multiple obstacles's status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -25 if (dist_multiple_obstacles < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -math.exp(dist_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Train station, avoids personnel, and aims to reach a target using its state and avoid hitting personnel with using personnel's status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_personnel, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_personnel = calculate_distance_min(s_agent, s_personnel) \n    penalty = -25 if (d_personnel < 0.6) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -math.exp(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a train station and must avoid contact with any moving or stationary train. The states of the drone and the trains are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_train, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_train = calculate_distance_min(s_agent, s_train) \n    penalty = -25 if (d_train < 0.6) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -math.exp(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "An drone is operating on a train station. It must avoid colliding with boxes and walls placed in the workspace. The states of the drone and the boxes and walls are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Train station, avoids luggage and dog, and aims to reach a target using its state and avoid hitting train with using multiple obstacles's status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (dist_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Train station, avoids staff, and aims to reach a target using its state and avoid hitting staff with using staff's status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_staff = calculate_distance_min(s_agent, s_staff) \n    penalty_collision = -25 if (d_agent_staff < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Train station, avoids ceiling lift, and aims to reach a target using its state and avoid hitting lift with using lift's status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_lift, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_lift = calculate_distance_min(s_agent, s_lift) \n    penalty_collision = -25 if (dist_lift < 0.5) else 10 \n    reward_reaching_target = 25 if (dist_target < 0.1) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Train station, avoids robotics and workstation, and aims to reach a target using its state and avoid hitting train with using multiple obstacles's status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Train station, avoids human and wires, and aims to reach a target using its state and avoid hitting train with using multiple obstacles's status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a Train station,  and aims to reach a target using its state. Write a reward function in Python..",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a aiport terminal, attempts to reach a target, and must prevent collisions with human and plane using drone state, the multiple obstacles's state, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    math.exp(d_agent_target) = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 25 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a aiport terminal, attempts to reach a target, and must prevent collisions with cleaning cart using drone state, the cleaning cart status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_cleaning_carts, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_cleaning_carts) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.7) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a aiport terminal, attempts to reach a target, and must prevent collisions with ad kiosk using drone state, the ad kiosk status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_ad_kiosk, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ad_kiosk = calculate_distance_min(s_agent, s_ad_kiosk) \n    penalty_collision = -10 if (d_agent_ad_kiosk < 0.7) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a aiport terminal, attempts to reach a target, and must prevent collisions with queue barrier using drone state, the queue barrier status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_queue_barrier, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_queue_barrier) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.7) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a aiport terminal, attempts to reach a target, and must prevent collisions with luggage cart using drone state, the luggage cart status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_luggage_cart, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_luggage_cart = calculate_distance_min(s_agent, s_luggage_cart) \n    penalty_collision = -10 if (d_agent_luggage_cart < 0.7) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a aiport terminal, attempts to reach a target, and must prevent collisions with portable sign using drone state, the portable sign status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_portable_sign, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_portable_sign) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.7) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a aiport terminal, attempts to reach a target, and must prevent collisions with floor scrubber using drone state, the floor scrubber status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_floor_scrubber, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_floor_scrubber = calculate_distance_min(s_agent, s_floor_scrubber) \n    penalty_collision = -10 if (d_agent_floor_scrubber < 0.7) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a aiport terminal, attempts to reach a target, and must prevent collisions with pilllar using drone state, the pillar status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_pillar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_pillar) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.7) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a aiport terminal, attempts to reach a target, and must prevent collisions with delivery robot using drone state, the delivery robot status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_delivery_robot, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_delivery_robot = calculate_distance_min(s_agent, s_delivery_robot) \n    penalty_collision = -10 if (d_agent_delivery_robot < 0.7) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a aiport terminal, attempts to reach a target, and must prevent collisions with luggage using drone state, the luggage status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_luggage, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_luggage = calculate_distance_min(s_agent, s_luggage) \n    penalty_collision = -25 if (d_agent_luggage < 0.6) else 4 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a aiport terminal, attempts to reach a target, and must prevent collisions with escalator and staircase using drone state, the multiple obstacles's state, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (dist_multiple_obstacles < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a aiport terminal, attempts to reach a target, and must prevent collisions with trash bin, construction barrier and service vehicle using drone state, the multiple obstacles's state, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -25 if (dist_multiple_obstacles < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -math.exp(dist_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a aiport terminal, attempts to reach a target, and must prevent collisions with personnel using drone state, the personnel status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_personnel, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_personnel = calculate_distance_min(s_agent, s_personnel) \n    penalty = -25 if (d_personnel < 0.6) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -math.exp(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a aiport terminal, attempts to reach a target, and must prevent collisions with counter using drone state, the counter status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_counter, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_counter = calculate_distance_min(s_agent, s_counter) \n    penalty = -25 if (d_counter < 0.6) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -math.exp(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a aiport terminal, attempts to reach a target, and must prevent collisions with boxes and walls placed in the seating area using drone state, the multiple obstacles's state, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a aiport terminal, attempts to reach a target, and must prevent collisions with seats and passenger using drone state, the multiple obstacles's state, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (dist_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a aiport terminal, attempts to reach a target, and must prevent collisions with staff using drone state, the staff status, and target information. Implement a reward function in Python..",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_staff = calculate_distance_min(s_agent, s_staff) \n    penalty_collision = -25 if (d_agent_staff < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a aiport terminal, attempts to reach a target, and must prevent collisions with stair using drone state, the stair status, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_stair, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_stair = calculate_distance_min(s_agent, s_stair) \n    penalty_collision = -25 if (dist_stair < 0.5) else 10 \n    reward_reaching_target = 25 if (dist_target < 0.1) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a aiport terminal, attempts to reach a target, and must prevent collisions with staff, passenger and check-ins using drone state, the multiple obstacles's state, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a aiport terminal, attempts to reach a target, and must prevent collisions with human and pillars using drone state, the multiple obstacles's state, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a airport. \n Goal: Dorne target is to reach the target. \n Assumptions: The states of the agent and the target are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a shopping mall. It must avoid colliding with escalators or shoppers. The states of the agent, the escalator, and the shoppers are known. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    math.exp(d_agent_target) = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 25 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a Shopping mall, aims to detect a obstacle (a cleaning cart), and must avoid collisions by using its own state and the cleaning cart’s state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_cleaning_cart, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cleaning_cart = calculate_distance_min(s_agent, s_cleaning_cart) \n    penalty_collision = -10 if (d_agent_cleaning_cart < 0.7) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a Shopping mall, aims to detect a obstacle (a ad kiosk), and must avoid collisions by using its own state and the ad kiosk’s state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_ad_kiosk, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ad_kiosk = calculate_distance_min(s_agent, s_ad_kiosk) \n    penalty_collision = -10 if (d_agent_ad_kiosk < 0.7) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a Shopping mall, aims to detect a obstacle (a queue barrier), and must avoid collisions by using its own state and the queue barrier’s state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_queue_barrier, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_queue_barrier = calculate_distance_min(s_agent, s_queue_barrier) \n    penalty_collision = -10 if (d_agent_queue_barrier < 0.7) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a Shopping mall, aims to detect a obstacle (a shopping cart), and must avoid collisions by using its own state and the shopping cart’s state. Create a reward function in Python..",
        "chosen": "def reward(s_agent, s_shopping_cart, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_shopping_cart = calculate_distance_min(s_agent, s_shopping_cart) \n    penalty_collision = -10 if (d_agent_shopping_cart < 0.7) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a Shopping mall, aims to detect a obstacle (a portable sign), and must avoid collisions by using its own state and the portable sign’s state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_portable_sign, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_portable_sign) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.7) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a Shopping mall, aims to detect a obstacle (a floor scrubber), and must avoid collisions by using its own state and the floor scrubber’s state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_floor_scrubber, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_floor_scrubber = calculate_distance_min(s_agent, s_floor_scrubber) \n    penalty_collision = -10 if (d_agent_floor_scrubber < 0.7) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a Shopping mall, aims to detect a obstacle (a pillar), and must avoid collisions by using its own state and the pillar’s state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_pillar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_pillar) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.7) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a Shopping mall, aims to detect a obstacle (a delivery robot), and must avoid collisions by using its own state and the delivery robot’s state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_delivery_robot, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_delivery_robot = calculate_distance_min(s_agent, s_delivery_robot) \n    penalty_collision = -10 if (d_agent_delivery_robot < 0.7) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a Shopping mall, aims to detect a obstacle (a stroller), and must avoid collisions by using its own state and the stroller’s state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_stroller, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_stroller = calculate_distance_min(s_agent, s_stroller) \n    penalty_collision = -25 if (d_agent_stroller < 0.6) else 3 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a Shopping mall, aims to detect a obstacle (a escalator, staircase), and must avoid collisions by using its own state and the multiple obstacles’s state: escalator, staircase. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (dist_multiple_obstacles < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a Shopping mall, aims to detect a obstacle (a trash bin, construction hoarding, service cart), and must avoid collisions by using its own state and the multiple obstacles’s state: trash bin, construction hoarding, service cart. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -25 if (dist_multiple_obstacles < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -math.exp(dist_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a Shopping mall, aims to detect a obstacle (a staff), and must avoid collisions by using its own state and the staff’s state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_staff = calculate_distance_min(s_agent, s_staff) \n    penalty = -25 if (d_staff < 0.6) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -math.exp(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a Shopping mall, aims to detect a obstacle (a info desk), and must avoid collisions by using its own state and the info desk’s state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_info_desk, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_info_desk = calculate_distance_min(s_agent, s_info_desk) \n    penalty = -25 if (d_info_desk < 0.6) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -math.exp(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a Shopping mall, aims to detect a obstacle (a box, glass partition), and must avoid collisions by using its own state and the multiple obstacles’s state: box, glass partition. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a Shopping mall, aims to detect a obstacle (a shoppers, bench), and must avoid collisions by using its own state and the multiple obstacles’s state: shoppers, bench. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (dist_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in a shopping mall. It must avoid colliding with staff. Agent's state, target's state and staff's state are accessible. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_staff = calculate_distance_min(s_agent, s_staff) \n    penalty_collision = -25 if (d_agent_staff < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a Shopping mall, aims to detect a obstacle (a stair), and must avoid collisions by using its own state and the stair’s state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_stair, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_stair = calculate_distance_min(s_agent, s_stair) \n    penalty_collision = -25 if (dist_stair < 0.5) else 10 \n    reward_reaching_target = 25 if (dist_target < 0.1) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a Shopping mall, aims to detect a obstacle (a shoppers, staff, information desks), and must avoid collisions by using its own state and the multiple obstacles’s state: shoppers, staff, information desks. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a Shopping mall, aims to detect a obstacle (a shoppers, pillars), and must avoid collisions by using its own state and the multiple obstacles’s state: shoppers, pillars. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a Shopping mall, reach target by using its own state and the target’s state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.exp(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a supermarket with the aim of reaching a target while avoiding collisions with checkout counters and customers. Available Information: The agent’s state and the multiple obstacles’ states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a supermarket with the aim of reaching a target while avoiding collisions with cleaning cart. Available Information: The agent’s state and the cleaning cart’ states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_cleaning_carts, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_cleaning_carts) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a supermarket with the aim of reaching a target while avoiding collisions with product display. Available Information: The agent’s state and the product display’ states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_product_display, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_product_display = calculate_distance_min(s_agent, s_product_display) \n    penalty_collision = -15 if (d_agent_product_display < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a supermarket with the aim of reaching a target while avoiding collisions with queue barrier. Available Information: The agent’s state and the queue barrier’ states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_queue_barrier, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_queue_barrier) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a supermarket with the aim of reaching a target while avoiding collisions with trolley. Available Information: The agent’s state and the trolley states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_trolley, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_trolley = calculate_distance_min(s_agent, s_trolley) \n    penalty_collision = -15 if (d_agent_trolley < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a supermarket with the aim of reaching a target while avoiding collisions with sale sign. Available Information: The agent’s state and the sale sign’ states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_sale_sign, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_sale_sign) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a supermarket with the aim of reaching a target while avoiding collisions with floor polisher. Available Information: The agent’s state and the floor polisher's states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_floor_polisher, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_floor_polisher = calculate_distance_min(s_agent, s_floor_polisher) \n    penalty_collision = -15 if (d_agent_floor_polisher < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a supermarket with the aim of reaching a target while avoiding collisions with support column. Available Information: The agent’s state and the support column’ states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_support_column, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_support_column) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a supermarket with the aim of reaching a target while avoiding collisions with restocking robot. Available Information: The agent’s state and the restocking robott’ states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_restocking_robot, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_restocking_robot = calculate_distance_min(s_agent, s_restocking_robot) \n    penalty_collision = -15 if (d_agent_restocking_robots < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a supermarket with the aim of reaching a target while avoiding collisions with shopping basket. Available Information: The agent’s state and the shopping basket’ states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_shopping_basket, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_shopping_basket = calculate_distance_min(s_agent, s_shopping_basket) \n    penalty_collision = -19 if (d_agent_shopping_basket < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a supermarket with the aim of reaching a target while avoiding collisions with freezer aisle and dock ramps. Available Information: The agent’s state and the multiple obstacles’ states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a supermarket with the aim of reaching a target while avoiding collisions with recycling bin, storage shelves and pallet jack. Available Information: The agent’s state and the multiple obstacles’ states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a supermarket with the aim of reaching a target while avoiding collisions with store clerk. Available Information: The agent’s state and the store clerk’ states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_store_clerk, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_store_clerk = calculate_distance_min(s_agent, s_store_clerk) \n    penalty = -19 if (d_store_clerk < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.exp(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a supermarket with the aim of reaching a target while avoiding collisions with customer service desk. Available Information: The agent’s state and the customer service desk’ states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_customer_service_desk, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_customer_service_desk = calculate_distance_min(s_agent, s_customer_service_desk) \n    penalty = -19 if (d_customer_service_desk < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.exp(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a supermarket with the aim of reaching a target while avoiding collisions with bulf item and refrigerated door. Available Information: The agent’s state and the multiple obstacles’ states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a supermarket with the aim of reaching a target while avoiding collisions with checkout queue rails and customers. Available Information: The agent’s state and the multiple obstacles’ states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in a supermarket. It must avoid colliding with store clerks. Agent's state, target's state and store clerks's state are accessible. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_store_clerk, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_store_clerk = calculate_distance_min(s_agent, s_store_clerk) \n    penalty_collision = -19 if (d_agent_store_clerk < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a supermarket with the aim of reaching a target while avoiding collisions with stair. Available Information: The agent’s state and the stair’ states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_stair, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_stair = calculate_distance_min(s_agent, s_stair) \n    penalty_collision = -19 if (dist_stair < 0.5) else 15 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a supermarket with the aim of reaching a target while avoiding collisions with store clerks, customers and information desks. Available Information: The agent’s state and the multiple obstacles’ states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a supermarket with the aim of reaching a target while avoiding collisions with support columns and endcap display. Available Information: The agent’s state and the multiple obstacles’ states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a supermarket with the aim of reaching a target. Available Information: The agent’s state and the target’ states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a turbian setting, a drone navigates while tracking the turbine tower and blades and avoiding collisions and reach target. The agent can access drone's state, target's state and the multiple obstacles’s state. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < c_1) else 0 \n    reward_reaching_target = 10 if (d_agent_target < c_2) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a turbian setting, a drone navigates while tracking the single obstacle and avoiding collisions and reach target. The agent can access drone's state, target's state and the single obstacle’s state. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_single_obstacle, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_single_obstacle = calculate_distance_min(s_agent, s_single_obstacle) \n    penalty_collision = -15 if (d_agent_single_obstacles < 0.5) else 0 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a turbian setting, a drone navigates while tracking the tower and avoiding collisions and reach target. The agent can access drone's state, target's state and the tower’s state. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_tower, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_tower = calculate_distance_min(s_agent, s_tower) \n    penalty_collision = -15 if (dist_tower < 0.5) else 0 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a turbian setting, a drone navigates while tracking the tower and avoiding collisions and reach target. The agent can access drone's state, target's state and the backbord’s state. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_tower, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_tower = calculate_distance_min(s_agent, s_tower) \n    penalty_collision = -15 if (dist_tower < 0.5) else 0 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a turbian setting, a drone navigates  reach target. The agent can access drone's state, target's state. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 20 if (dist_target < 0.2) else -10 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a turbian setting, a drone navigates while tracking the tower and blades and avoiding collisions and reach target. The agent can access drone's state, target's state and the multiple obstacles’s state. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -15 if (dist_multiple_obstacles < 0.5) else 0 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Coastal wind-farm drone inspecting turbines. \n Aim: Avoid hitting the turbine tower and blades. \n Prerequisites: Positions and statuses of the drone, tower and blades are known. \n Requirement: Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -15 if (dist_obstacles < 0.5) else 0 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a turbian setting, a drone navigates while tracking the crane and avoiding collisions and reach target. The agent can access drone's state, target's state and the crane’s state. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_crane, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_crane = calculate_distance_min(s_agent, s_crane) \n    penalty_collision = -15 if (dist_crane < 0.5) else 0 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a turbian setting, a drone navigates while tracking the ballon and avoiding collisions and reach target. The agent can access drone's state, target's state and the ballon’s state. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_balloon, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_balloon = calculate_distance_min(s_agent, s_balloon) \n    penalty_collision = -15 if (d_agent_balloon < 0.6) else 4 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a turbian setting, a drone navigates while tracking the multiple obstacles and wires, employess, visitors and reach target. The agent can access drone's state, target's state and the multiple obstacles’s state. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -18 if (dist_multiple_obstacles < 0.7) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.3) else -15 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a turbian setting, a drone navigates while tracking the multiple obstacles and weather ballons and wires and reach target. The agent can access drone's state, target's state and the multiple obstacles’s state. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -18 if (dist_multiple_obstacles < 0.7) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.3) else -15 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a turbian setting, a drone navigates while tracking the employee and avoiding collisions and reach target. The agent can access drone's state, target's state and the employee’s state. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_employee, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_employee = calculate_distance_min(s_agent, s_employee) \n    penalty = -18 if (d_employee < 0.7) else 10 \n    reward_reaching_target = 10 if (d_target < 0.3) else -15 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a turbian setting, a drone navigates while tracking the equipment and avoiding collisions and reach target. The agent can access drone's state, target's state and the equipment’s state. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_equipment, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_equipment = calculate_distance_min(s_agent, s_equipment) \n    penalty = -18 if (d_equipment < 0.7) else 10 \n    reward_reaching_target = 10 if (d_target < 0.3) else -15 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a turbian setting, a drone navigates while tracking the different equipment and avoiding collisions and reach target. The agent can access drone's state, target's state and the multiple obstacles’s state. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a turbian setting, a drone navigates while tracking the equipment and birds and avoiding collisions and reach target. The agent can access drone's state, target's state and the multiple obstacles’s state. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -15 if (dist_obstacles_min < 0.6) else 0 \n    reward_reaching_target = 10 if (dist_target < 0.3) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a turbian setting, a drone navigates while tracking the blade and avoiding collisions and reach target. The agent can access drone's state, target's state and the blade’s state. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_blade, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_blade = calculate_distance_min(s_agent, s_blade) \n    penalty_collision = -15 if (d_agent_blade < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a turbian setting, a drone navigates while tracking the lift and avoiding collisions and reach target. The agent can access drone's state, target's state and the lift’s state. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_lift, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_lift = calculate_distance_min(s_agent, s_lift) \n    penalty_collision = -15 if (dist_lift < 0.5) else 0 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a turbian setting, a drone navigates while tracking the multiple obstacles and avoiding collisions and reach target. The agent can access drone's state, target's state and the multiple obstacles’s state. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.6) else 0 \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else 0 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a turbian setting, a drone navigates reach target. The agent can access drone's state, target's state. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 10 if (d_agent_target < 0.3) else 0 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a factory, aims to reach a designated target, and must steer clear of machinery and worker by relying on its own state, the multipel obstcales’ status, and the target information. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a factory, aims to reach a designated target, and must steer clear of machinery by relying on its own state, the machinery’ status, and the target information. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_machinery, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_machinery = calculate_distance_min(s_agent, s_machinery) \n    penalty_collision = -20 if (d_agent_machinery < 0.6) else 10 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a factory, aims to reach a designated target, and must steer clear of cart by relying on its own state, the cart’ status, and the target information. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_cart, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_cart = calculate_distance_min(s_agent, s_cart) \n    penalty_collision = -20 if (dist_cart < 0.5) else 10 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.log(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a factory, aims to reach a designated target, and must steer clear of machinery by relying on its own state, the machinery’ status, and the target information. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_workbench, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_workbench = calculate_distance_min(s_agent, s_workbench) \n    penalty_collision = -20 if (dist_workbench < 0.5) else 10 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.log(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a factory, aims to reach a designated target by relying on its own state, and the target information. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.log(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a factory, aims to reach a designated target, and must steer clear of maintenance equipment and pipe by relying on its own state, the multipel obstcales’ status, and the target information. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (dist_multiple_obstacles < 0.5) else 10 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.log(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a factory, aims to reach a designated target, and must steer clear of obstacles by relying on its own state, the obstacles’ status, and the target information. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles = calculate_distance_min(s_agent, s_obstacles) \n    penalty_collision = -20 if (dist_obstacles < 0.5) else 10 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.log(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a factory, aims to reach a designated target, and must steer clear of station by relying on its own state, the station’ status, and the target information. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_station, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_station = calculate_distance_min(s_agent, s_station) \n    penalty_collision = -20 if (dist_station < 0.5) else 10 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.log(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a factory, aims to reach a designated target, and must steer clear of cable by relying on its own state, the cable status, and the target information. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_cable, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_cable = calculate_distance_min(s_agent, s_cables) \n    penalty_collision = -20 if (dist_cable < 0.5) else 10 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.log(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a factory, aims to reach a designated target, and must steer clear of container by relying on its own state, the container status, and the target information. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_container, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_container = calculate_distance_min(s_agent, s_container) \n    penalty_collision = -20 if (d_agent_container < 0.6) else 4 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a factory floor. \n Objectives: Avoid collision with container amd cable. \n Conditions: The states of the agent, container and cable are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -18 if (dist_multiple_obstacles < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.2) else -math.log(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a factory floor. \n Objectives: It must avoid colliding any facing obstacles. \n Conditions: Agent's and obstacles's are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -18 if (dist_multiple_obstacles < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.2) else -math.log(dist_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": " drone navigates inside a factory, aims to reach a designated target, and must steer clear of personnel by relying on its own state, the personel’ status, and the target information. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_personnel, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_personnel = calculate_distance_min(s_agent, s_personnel) \n    penalty = -18 if (d_personnel < 0.6) else 10 \n    reward_reaching_target = 10 if (d_target < 0.2) else -math.log(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a factory, aims to reach a designated target, and must steer clear of pallet by relying on its own state, the machinery’ status, and the target information. Create a reward function in Python..",
        "chosen": "def reward(s_agent, s_pallet, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_pallet = calculate_distance_min(s_agent, s_equipment) \n    penalty = -18 if (d_pallet < 0.6) else 10 \n    reward_reaching_target = 10 if (d_target < 0.2) else -math.log(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a factory, aims to reach a designated target, and must steer clear of boxes and walls by relying on its own state, the multipel obstcales’ status, and the target information. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a factory, aims to reach a designated target, and must steer clear of product and tools by relying on its own state, the multipel obstcales’ status, and the target information. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (dist_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.2) else -math.log(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a factory, aims to reach a designated target, and must steer clear of tool by relying on its own state, the tool status, and the target information. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_tool, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tool = calculate_distance_min(s_agent, s_tool) \n    penalty_collision = -20 if (d_agent_tool < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a factory, aims to reach a designated target, and must steer clear of lift by relying on its own state, the lift status, and the target information. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_lift, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_lift = calculate_distance_min(s_agent, s_lift) \n    penalty_collision = -20 if (dist_lift < 0.5) else 10 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.log(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a factory, aims to reach a designated target, and must steer clear of robotics arm and workstation by relying on its own state, the multipel obstcales’ status, and the target information. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a factory, aims to reach a designated target, and must steer clear of human and wire by relying on its own state, the multipel obstcales’ status, and the target information. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a factory, aims to reach a designated target, the target information and agent's state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 10 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a train station. It must avoid colliding with train or human. The states of the agent, the train, and the human are known. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 25 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a Train station collection, a drone must navigate toward a target while avoiding crashes into the train, relying on its state, the train’s positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_train, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_train = calculate_distance_min(s_agent, s_train) \n    penalty_collision = -25 if (d_agent_train < 0.6) else 10 \n    reward_reaching_target = 25 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a Train station collection, a drone must navigate toward a target while avoiding crashes into the platform, relying on its state, the platform’s positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_platform, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_platform = calculate_distance_min(s_agent, s_platform) \n    penalty_collision = -25 if (dist_platform < 0.5) else 10 \n    reward_reaching_target = 25 if (dist_target < 0.1) else -math.log(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a Train station collection, a drone must navigate toward a target while avoiding crashes into the ticket machine, relying on its state, the ticket machine’s positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_ticket_machine, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_ticket_machine = calculate_distance_min(s_agent, s_ticket_machine) \n    penalty_collision = -25 if (dist_ticket_machine < 0.5) else 10 \n    reward_reaching_target = 25 if (dist_target < 0.1) else -math.log(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a Train station collection, a drone must navigate toward a target relying on drone state and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 25 if (dist_target < 0.1) else -math.log(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a Train station collection, a drone must navigate toward a target while avoiding crashes into the bench and board, relying on its state, the bench’s and board's positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_board, s_bench, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_board = calculate_distance(s_agent, s_board) \n    dist_bench = calculate_distance(s_agent, s_bench) \n    min_dist_obstacles = min(dist_board, dist_bench) \n    penalty_collision = -25 if (min_dist_obstacles < 0.5) else 10 \n     reward_reaching_target = 25 if (dist_target < 0.1) else -2 * dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a Train station collection, a drone must navigate toward a target while avoiding crashes into the pillar and escalator, relying on its state, the pillar’s and escalator's positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_pillar, s_escalator, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_pillar = calculate_distance(s_agent, s_pillar) \n    dist_escalator = calculate_distance(s_agent, s_bench) \n    min_dist_obstacles = min(dist_pillar, dist_escalator) \n    penalty_collision = -25 if (min_dist_obstacles < 0.5) else 10 \n     reward_reaching_target = 25 if (dist_target < 0.1) else -2 * dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a Train station collection, a drone must navigate toward a target while avoiding crashes into the escalator, relying on its state, the escalator’s positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_escalator, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_escalator = calculate_distance_min(s_agent, s_escalator) \n    penalty_collision = -25 if (dist_escalator < 0.5) else 10 \n    reward_reaching_target = 25 if (dist_target < 0.1) else -math.log(dist_targetl) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a Train station collection, a drone must navigate toward a target while avoiding crashes into the escalator, relying on its state, the escalator’s positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_vending_machine, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_vending_machine = calculate_distance_min(s_agent, s_vending_machine) \n    penalty_collision = -25 if (dist_vending_machine < 0.5) else 10 \n    reward_reaching_target = 25 if (dist_target < 0.1) else -math.log(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a Train station collection, a drone must navigate toward a target while avoiding crashes into the staircase, relying on its state, the staircase’s positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_staircase, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_staircase = calculate_distance_min(s_agent, s_staircase) \n    penalty_collision = -25 if (d_agent_staircase < 0.6) else 4 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through a train station. \n Objectives: Avoid collision with escalator and staircase. \n Conditions: The states of the agent, escalator and staircase are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (dist_multiple_obstacles < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -math.log(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a Train station collection, a drone must navigate toward a target while avoiding crashes into the trsh bin and construction barrier and service verhicle, relying on its state, the multiple obstcales’s positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -25 if (dist_multiple_obstacles < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -math.log(dist_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a Train station collection, a drone must navigate toward a target while avoiding crashes into the personnel, relying on its state, the personel’s positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_personnel, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_personnel = calculate_distance_min(s_agent, s_personnel) \n    penalty = -25 if (d_personnel < 0.6) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -math.log(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a Train station collection, a drone must navigate toward a target while avoiding crashes into the train, relying on its state, the train’s positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_train, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_train = calculate_distance_min(s_agent, s_train) \n    penalty = -25 if (d_train < 0.6) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -math.log(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a Train station collection, a drone must navigate toward a target while avoiding crashes into the boxes and walls, relying on its state, the multiple obstcales’s positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a Train station collection, a drone must navigate toward a target while avoiding crashes into luggage and dog, relying on its state, the multiple obstcales’s positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (dist_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -math.log(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a Train station collection, a drone must navigate toward a target while avoiding crashes into the staff, relying on its state, the staff’s positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_staff = calculate_distance_min(s_agent, s_staff) \n    penalty_collision = -25 if (d_agent_staff < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a Train station collection, a drone must navigate toward a target while avoiding crashes into the lift, relying on its state, the lift’s positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_lift, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_lift = calculate_distance_min(s_agent, s_lift) \n    penalty_collision = -25 if (dist_lift < 0.5) else 10 \n    reward_reaching_target = 25 if (dist_target < 0.1) else -math.log(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a Train station collection, a drone must navigate toward a target while avoiding crashes into the robotics and workkstaion, relying on its state, the multiple obstcales’s positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a Train station collection, a drone must navigate toward a target while avoiding crashes into the human and wires, relying on its state, the multiple obstcales’s positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a Train station collection, a drone must navigate toward a target relying on agnet's state and target data. Design a reward function using Python..",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in Airport terminal, a drone must avoid crashing into a plane and human and reaching target by using the available information about its own state, target's state and the plane and human’s state. Design a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    math.log(d_agent_target) = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 25 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in Airport terminal, a drone must avoid crashing into cleaning carts and reaching target by using the available information about its own state, target's state and the cleaning carts’s state. Design a reward function in Python",
        "chosen": "def reward(s_agent, s_cleaning_carts, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_cleaning_carts) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.7) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in Airport terminal, a drone must avoid crashing into advertising kiosks and reaching target by using the available information about its own state, target's state and the advertising kiosks’s state. Design a reward function in Python",
        "chosen": "def reward(s_agent, s_ad_kiosk, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ad_kiosk = calculate_distance_min(s_agent, s_ad_kiosk) \n    penalty_collision = -10 if (d_agent_ad_kiosk < 0.7) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in Airport terminal, a drone must avoid crashing into temporary stanchions (queue barriers) and reaching target by using the available information about its own state, target's state and the temporary stanchions (queue barriers)’s state. Design a reward function in Python",
        "chosen": "def reward(s_agent, s_queue_barrier, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_queue_barrier) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.7) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in Airport terminal, a drone must avoid crashing into luggage carts and reaching target by using the available information about its own state, target's state and the luggage carts’s state. Design a reward function in Python",
        "chosen": "def reward(s_agent, s_luggage_cart, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_luggage_cart = calculate_distance_min(s_agent, s_luggage_cart) \n    penalty_collision = -10 if (d_agent_luggage_cart < 0.7) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in Airport terminal, a drone must avoid crashing into retractable belt barriers and portable signs and reaching target by using the available information about its own state, target's state and the retractable belt barriers and portable signs’s state. Design a reward function in Python",
        "chosen": "def reward(s_agent, s_portable_sign, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_portable_sign) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.7) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in Airport terminal, a drone must avoid crashing into autonomous floor scrubbers and reaching target by using the available information about its own state, target's state and the autonomous floor scrubbers’s state. Design a reward function in Python",
        "chosen": "def reward(s_agent, s_floor_scrubber, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_floor_scrubber = calculate_distance_min(s_agent, s_floor_scrubber) \n    penalty_collision = -10 if (d_agent_floor_scrubber < 0.7) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in Airport terminal, a drone must avoid crashing into decorative pillars and reaching target by using the available information about its own state, target's state and the decorative pillars’s state. Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_pillar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_pillar) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.7) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in Airport terminal, a drone must avoid crashing into delivery robots and reaching target by using the available information about its own state, target's state and the delivery robots’s state. Design a reward function in Python",
        "chosen": "def reward(s_agent, s_delivery_robot, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_delivery_robot = calculate_distance_min(s_agent, s_delivery_robot) \n    penalty_collision = -10 if (d_agent_delivery_robot < 0.7) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in Airport terminal, a drone must avoid crashing into luggage and reaching target by using the available information about its own state, target's state and the luggage’s state. Design a reward function in Python",
        "chosen": "def reward(s_agent, s_luggage, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_luggage = calculate_distance_min(s_agent, s_luggage) \n    penalty_collision = -25 if (d_agent_luggage < 0.6) else 4 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in Airport terminal, a drone must avoid crashing into an escalator and staircase and reaching target by using the available information about its own state, target's state and the multiple bstcales: escalator and staircase’s state. Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (dist_multiple_obstacles < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -math.log(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in Airport terminal, a drone must avoid crashing into any trash bin, construction barrier and service vehicle and reaching target by using the available information about its own state, target's state and the multiple obstacles: trash bin, construction barrier and service vehicle’s state. Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -25 if (dist_multiple_obstacles < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -math.log(dist_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in Airport terminal, a drone must avoid crashing into personnel and reaching target by using the available information about its own state, target's state and the personnel’s state. Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_personnel, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_personnel = calculate_distance_min(s_agent, s_personnel) \n    penalty = -25 if (d_personnel < 0.6) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -math.log(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in Airport terminal, a drone must avoid crashing into a check-in counter and reaching target by using the available information about its own state, target's state and the check-in counter’s state. Design a reward function in Python",
        "chosen": "def reward(s_agent, s_counter, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_counter = calculate_distance_min(s_agent, s_counter) \n    penalty = -25 if (d_counter < 0.6) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -math.log(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in Airport terminal, a drone must avoid crashing into boxes and walls and reaching target by using the available information about its own state, target's state and multiple obstcales state: the boxes and walls’s state. Design a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in Airport terminal, a drone must avoid crashing into seats and passengers and reaching target by using the available information about its own state, target's state and multiple obstacle's state: the seats and passengers’s state. Design a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (dist_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -math.log(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in Airport terminal, a drone must avoid crashing into staff and reaching target by using the available information about its own state, target's state and the staff’s state. Design a reward function in Python",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_staff = calculate_distance_min(s_agent, s_staff) \n    penalty_collision = -25 if (d_agent_staff < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in Airport terminal, a drone must avoid crashing into stairs and reaching target by using the available information about its own state, target's state and the stairs’s state. Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_stair, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_stair = calculate_distance_min(s_agent, s_stair) \n    penalty_collision = -25 if (dist_stair < 0.5) else 10 \n    reward_reaching_target = 25 if (dist_target < 0.1) else -math.log(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in Airport terminal, a drone must avoid crashing into staff, passengers, and check-ins and reaching target by using the available information about its own state, target's state and multiple obstacle's state: the staff, passengers, and check-ins’s state. Design a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in Airport terminal, a drone must avoid crashing into humans, pillars, and boards and reaching target by using the available information about its own state, target's state and multiple obstacle's state: the humans, pillars, and boards’s state. Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in Airport terminal. Reaching target by using the available information about its own state, target's state. Design a reward function in Python",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a Shopping mall environment while ensuring it does not crash into the escalators or shoppers and reaches target. State about the drone’s state, target’s state and multiple obstcale's state: the states of the escalators or shoppers can be accessed.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    math.log(d_agent_target) = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 25 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a Shopping mall environment while ensuring it does not crash into the cleaning carts and reaches target. State about the drone’s state, target’s state and the states of the cleaning carts can be accessed.",
        "chosen": "def reward(s_agent, s_cleaning_cart, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cleaning_cart = calculate_distance_min(s_agent, s_cleaning_cart) \n    penalty_collision = -10 if (d_agent_cleaning_cart < 0.7) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a Shopping mall environment while ensuring it does not crash into the advertising kiosks and reaches target. State about the drone’s state, target’s state and the states of the advertising kiosks can be accessed.",
        "chosen": "def reward(s_agent, s_ad_kiosk, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ad_kiosk = calculate_distance_min(s_agent, s_ad_kiosk) \n    penalty_collision = -10 if (d_agent_ad_kiosk < 0.7) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a Shopping mall environment while ensuring it does not crash into the qeueing barriers and reaches target. State about the drone’s state, target’s state and the states of the queue barrier can be accessed.",
        "chosen": "def reward(s_agent, s_queue_barrier, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_queue_barrier = calculate_distance_min(s_agent, s_queue_barrier) \n    penalty_collision = -10 if (d_agent_queue_barrier < 0.7) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a Shopping mall environment while ensuring it does not crash into the shopping carts and reaches target. State about the drone’s state, target’s state and the states of the shopping carts can be accessed.",
        "chosen": "def reward(s_agent, s_shopping_cart, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_shopping_cart = calculate_distance_min(s_agent, s_shopping_cart) \n    penalty_collision = -10 if (d_agent_shopping_cart < 0.7) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a Shopping mall environment while ensuring it does not crash into portable sign and reaches target. State about the drone’s state, target’s state and the states of the portable sign can be accessed.",
        "chosen": "def reward(s_agent, s_portable_sign, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_portable_sign) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.7) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a Shopping mall environment while ensuring it does not crash into the floor scrubber and reaches target. State about the drone’s state, target’s state and the states of the floor scrubber can be accessed.",
        "chosen": "def reward(s_agent, s_floor_scrubber, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_floor_scrubber = calculate_distance_min(s_agent, s_floor_scrubber) \n    penalty_collision = -10 if (d_agent_floor_scrubber < 0.7) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a Shopping mall environment while ensuring it does not crash into the pillar and reaches target. State about the drone’s state, target’s state and the states of the pillar can be accessed.",
        "chosen": "def reward(s_agent, s_pillar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_pillar) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.7) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a Shopping mall environment while ensuring it does not crash into the Delivery robots shuttling orders and reaches target. State about the drone’s state, target’s state and the states of the Delivery robots shuttling orders can be accessed.",
        "chosen": "def reward(s_agent, s_delivery_robot, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_delivery_robot = calculate_distance_min(s_agent, s_delivery_robot) \n    penalty_collision = -10 if (d_agent_delivery_robot < 0.7) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a Shopping mall environment while ensuring it does not crash into the baby strollers and reaches target. State about the drone’s state, target’s state and the states of the baby strollers can be accessed.",
        "chosen": "def reward(s_agent, s_stroller, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_stroller = calculate_distance_min(s_agent, s_stroller) \n    penalty_collision = -25 if (d_agent_stroller < 0.6) else 3 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a Shopping mall environment while ensuring it does not crash into the escalator and staircase and reaches target. State about the drone’s state, target’s state and the multiple obstacles's states of the escalator and staircase can be accessed.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (dist_multiple_obstacles < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -math.log(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a Shopping mall environment while ensuring it does not crash into the any trash bin, construction hoarding, and service cart and reaches target. State about the drone’s state, target’s state and the multiple obstacle's states of the any trash bin, construction hoarding, and service cart can be accessed.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -25 if (dist_multiple_obstacles < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -math.log(dist_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a Shopping mall environment while ensuring it does not crash into the any mall staff and reaches target. State about the drone’s state, target’s state and the states of the any staff can be accessed.",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_staff = calculate_distance_min(s_agent, s_staff) \n    penalty = -25 if (d_staff < 0.6) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -math.log(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a Shopping mall environment while ensuring it does not crash into the any information desk and reaches target. State about the drone’s state, target’s state and the states of the any info desk can be accessed.",
        "chosen": "def reward(s_agent, s_info_desk, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_info_desk = calculate_distance_min(s_agent, s_info_desk) \n    penalty = -25 if (d_info_desk < 0.6) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -math.log(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a Shopping mall environment while ensuring it does not crash into the display boxes and glass partitions placed in the seating area and reaches target. State about the drone’s state, target’s state and the states of multiple obstacles can be accessed.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a Shopping mall environment while ensuring it does not crash into the benches and shoppers and reaches target. State about the drone’s state, target’s state and the states of the multiple obstacles can be accessed.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (dist_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -math.log(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a Shopping mall environment while ensuring it does not crash into the staff and reaches target. State about the drone’s state, target’s state and the states of the staff can be accessed.",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_staff = calculate_distance_min(s_agent, s_staff) \n    penalty_collision = -25 if (d_agent_staff < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a Shopping mall environment while ensuring it does not crash into the stairs and reaches target. State about the drone’s state, target’s state and the states of the stairs can be accessed.",
        "chosen": "def reward(s_agent, s_stair, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_stair = calculate_distance_min(s_agent, s_stair) \n    penalty_collision = -25 if (dist_stair < 0.5) else 10 \n    reward_reaching_target = 25 if (dist_target < 0.1) else -math.log(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a Shopping mall environment while ensuring it does not crash into the staff, shoppers, and information desks and reaches target. State about the drone’s state, target’s state and the states of the multiple obstcales can be accessed.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a Shopping mall environment while ensuring it does not crash into the shoppers, pillars, and sign boards and reaches target. State about the drone’s state, target’s state and the states of the multiple obstcales can be accessed.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a Shopping mall environment while ensuring it does not crash into the walls and reaches target. State about the drone’s state, target’s state and the states of the walls can be accessed.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is flying within a supermarket, avoiding any contact with plane and human and moving toward a designated target. The system provides access to the drone’s state, the the multiple obstacles information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    math.log(d_agent_target) = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 25 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is flying within a supermarket, avoiding any contact with cleaning carts and moving toward a designated target. The system provides access to the drone’s state, the cleaning carts information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_cleaning_carts, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_cleaning_carts) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.7) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is flying within a supermarket, avoiding any contact with advertising kiosks and moving toward a designated target. The system provides access to the drone’s state, the advertising kiosks information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_ad_kiosk, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ad_kiosk = calculate_distance_min(s_agent, s_ad_kiosk) \n    penalty_collision = -10 if (d_agent_ad_kiosk < 0.7) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is flying within a supermarket, avoiding any contact with queue barrier and moving toward a designated target. The system provides access to the drone’s state, the queue barrier information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_queue_barrier, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_queue_barrier) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.7) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is flying within a supermarket, avoiding any contact with luggage carts and moving toward a designated target. The system provides access to the drone’s state, the luggage carts information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_luggage_cart, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_luggage_cart = calculate_distance_min(s_agent, s_luggage_cart) \n    penalty_collision = -10 if (d_agent_luggage_cart < 0.7) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is flying within a supermarket, avoiding any contact with portable sign and moving toward a designated target. The system provides access to the drone’s state, the portable sign information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_portable_sign, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_portable_sign) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.7) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is flying within a supermarket, avoiding any contact with floor scrubber and moving toward a designated target. The system provides access to the drone’s state, the floor scrubber information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_floor_scrubber, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_floor_scrubber = calculate_distance_min(s_agent, s_floor_scrubber) \n    penalty_collision = -10 if (d_agent_floor_scrubber < 0.7) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is flying within a supermarket, avoiding any contact with pillar and moving toward a designated target. The system provides access to the drone’s state, the pillar information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_pillar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_pillar) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.7) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is flying within a supermarket, avoiding any contact with Delivery robots shuttling orders and moving toward a designated target. The system provides access to the drone’s state, the Delivery robots shuttling orders information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_delivery_robot, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_delivery_robot = calculate_distance_min(s_agent, s_delivery_robot) \n    penalty_collision = -10 if (d_agent_delivery_robot < 0.7) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is flying within a supermarket, avoiding any contact with luggage and moving toward a designated target. The system provides access to the drone’s state, the luggage information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_luggage, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_luggage = calculate_distance_min(s_agent, s_luggage) \n    penalty_collision = -25 if (d_agent_luggage < 0.6) else 4 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is flying within a supermarket, avoiding any contact with escalator and staircase and moving toward a designated target. The system provides access to the drone’s state, the multiple obstacles information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (dist_multiple_obstacles < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -math.log(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is flying within a supermarket, avoiding any contact with colliding any trash bin, construction barrier, and service vehicle and moving toward a designated target. The system provides access to the drone’s state, the multiple obstacles information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -25 if (dist_multiple_obstacles < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -math.log(dist_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is flying within a supermarket, avoiding any contact with personnel and moving toward a designated target. The system provides access to the drone’s state, the personnel information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_personnel, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_personnel = calculate_distance_min(s_agent, s_personnel) \n    penalty = -25 if (d_personnel < 0.6) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -math.log(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is flying within a supermarket, avoiding any contact with Check-in counter and moving toward a designated target. The system provides access to the drone’s state, the Check-in counter information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_counter, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_counter = calculate_distance_min(s_agent, s_counter) \n    penalty = -25 if (d_counter < 0.6) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -math.log(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is flying within a supermarket, avoiding any contact with boxes and walls placed in the seating area and moving toward a designated target. The system provides access to the drone’s state, the multiple obstacles information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is flying within a supermarket, avoiding any contact with seats and passenger and moving toward a designated target. The system provides access to the drone’s state, the multiple obstacles information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (dist_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -math.log(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is flying within a supermarket, avoiding any contact with staff and moving toward a designated target. The system provides access to the drone’s state, the staff information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_staff = calculate_distance_min(s_agent, s_staff) \n    penalty_collision = -25 if (d_agent_staff < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is flying within a supermarket, avoiding any contact with stairs and moving toward a designated target. The system provides access to the drone’s state, the stairs information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_stair, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_stair = calculate_distance_min(s_agent, s_stair) \n    penalty_collision = -25 if (dist_stair < 0.5) else 10 \n    reward_reaching_target = 25 if (dist_target < 0.1) else -math.log(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is flying within a supermarket, avoiding any contact with staff, passenger, and check-ins and moving toward a designated target. The system provides access to the drone’s state, the multiple obstacles information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is flying within a supermarket, avoiding any contact with humans, pillars, and boards and moving toward a designated target. The system provides access to the drone’s state, the multiple obstacles information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is flying within a supermarket and  moving toward a designated target. The system provides access to the drone’s state, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a swimming pool area. It must avoid colliding with ticket counters or swimmers. The states of the agent, the lane ropes, and the swimmers are known. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a busy swimming pool area concourse. It must avoid colliding with lifeguard float storage racks. The states of the agent, the carts are known. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_cleaning_carts, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_cleaning_carts) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a swimming pool area food court. \nObjectives: It must avoid colliding with advertising kiosks. \n Conditions: States of the agent, the kiosks are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_safety_board, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_safety_board = calculate_distance_min(s_agent, s_safety_board) \n    penalty_collision = -15 if (d_agent_safety_board < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a swimming pool area corridor, the drone must maneuver around temporary stanchions (poolside barrier). All entity states (agent, barrier) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_queue_barrier, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_queue_barrier) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a swimming pool area atrium. \n Requirement: Avoid collisions with storage rack. \n Given: The states for the agent, storage rack are known. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_storage_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_storage_rack = calculate_distance_min(s_agent, s_storage_rack) \n    penalty_collision = -15 if (d_agent_storage_rack < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone operates near a mall entrance during an event. It must not collide with retractable sale signs. The states of the agent, the sale sign, and the target are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_sale_signs, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_sale_signs) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in a mall lounge without hitting autonomous floor polisher. \n Conditions: States of the agent, polisher, and target are available. \n Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_floor_polisher, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_floor_polisher = calculate_distance_min(s_agent, s_floor_polisher) \n    penalty_collision = -15 if (d_agent_floor_polishers < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a swimming pool area atrium, the drone must steer clear of support columnes. All states (agent, support colummes) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_support_columns, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_support_columns) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: \n Environment: Shopping mall food court. \n Avoid: Restocking robots shuttling orders. \n Known: Agent, robots states. \n Create the Python reward function.",
        "chosen": "def reward(s_agent, s_restocking_robot, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_restocking_robot = calculate_distance_min(s_agent, s_restocking_robot) \n    penalty_collision = -15 if (d_agent_restocking_robot < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a swimming pool area. \n Objectives: It must avoid colliding with baby kickboards. \n Conditions: The states of the agent and the kickboard are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_kickboard, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_kickboard = calculate_distance_min(s_agent, s_kickboard) \n    penalty_collision = -19 if (d_agent_kickboard < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through a swimming pool area. \n Objectives: Avoid collision with lane ropes and sloped entries. \n Conditions: The states of the agent, lane ropes and sloped entries are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a swimming pool area. \n Objectives: It must avoid colliding with any towel bins, equipment racks and supply carts. \n Conditions: Agent's and all obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -dist_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through a swimming pool area. \n Objectives: Do not collide with any mall lifeguards. \n Conditions: The states of the agent and the lifeguard are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_store_lifeguard, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_lifeguards = calculate_distance_min(s_agent, s_store_lifeguard) \n    penalty = -19 if (d_lifeguard < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -d_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a swimming pool area and must avoid contact with any information desk. The states of the drone and the desk are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_information_desk, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_information_desk = calculate_distance_min(s_agent, s_information_desk) \n    penalty = -19 if (d_information_desk < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -d_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in a swimming pool area. It must avoid colliding with stacked float mats and glass pool barriers placed in the seating area. The states of the drone and the obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in a swimming pool area. It must avoid colliding with queue ropes and swimmers. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in a swimming pool area. It must avoid colliding with lifeguards. Agent's state, target's state and lifeguards's state are accessible. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_lifeguards, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lifeguards = calculate_distance_min(s_agent, s_lifeguards) \n    penalty_collision = -19 if (d_agent_lifeguards < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a swimming pool area. \n Goal: It must avoid colliding with stiars. \n Assumptions: The states of the agent and the stair are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_stair, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_stair = calculate_distance_min(s_agent, s_stair) \n    penalty_collision = -19 if (dist_stair < 0.5) else 15 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a swimming pool area. \n Goal: It must avoid colliding with lifeguards, swimmers and information desks. \n Assumptions: The Agent's state, target's state and lifeguards's state, desk's state and swimmers' states are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a swimming pool area. \n Goal: Detect swimmers and diving boards and avoid lane markers. \n Assumptions: The states of the agent and multiple obstacles are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a swimming pool area. \n Goal: Drone target is to reach the target. \n Assumptions: The states of the agent and the target are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a construction site. It must avoid colliding with site gates or workers. The states of the agent, the trenches, and the workers are known. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a busy construction site concourse. It must avoid colliding with wheelbarrows. The states of the agent, the carts, and the target are known. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_wheelbarrows, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_wheelbarrows) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a site canteen. \nObjectives: It must avoid colliding with warning boards. \n Conditions: States of the agent, the warning boards, and the target are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_warning_boards, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_warning_board = calculate_distance_min(s_agent, s_warning_board) \n    penalty_collision = -15 if (d_agent_warning_board < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a material corridor, the drone must maneuver around scaffold frames. All entity states (agent, barriers) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_scaffold_frames, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_scaffold_frames) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a construction yard. \n Requirement: Avoid collisions with tool carts. \n Given: The states for the agent, carts are known. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_tool_cart, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tool_cart = calculate_distance_min(s_agent, s_tool_cart) \n    penalty_collision = -15 if (d_agent_tool_cart < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone operates near a site entrance during an event. It must not collide with caution signs. The states of the agent, the obstacles are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_caution_sign, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_caution_sign = calculate_distance_min(s_agent, s_caution_sign) \n    penalty_collision = -15 if (d_agent_caution_sign < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in a mall lounge without hitting autonomous floor scrubbers. \n Conditions: States of the agent, scrubbers are available. \n Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_scrubber, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_scrubber = calculate_distance_min(s_agent, s_scrubber) \n    penalty_collision = -15 if (d_agent_scrubber < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a construction yard, the drone must steer clear of decorative shoring posts. All states (agent, shoring posts) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_shoring_post, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_shoring_post) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: \n Environment: construction site break area. \n Avoid: site robots shuttling orders. \n Known: Agent, robots states. \n Create the Python reward function.",
        "chosen": "def reward(s_agent, s_site_robots, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_site_robots = calculate_distance_min(s_agent, s_site_robots) \n    penalty_collision = -15 if (d_agent_site_robots < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a construction site. \n Objectives: It must avoid colliding with buckets. \n Conditions: The states of the agent and the shopping baskets are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bucket, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bucket = calculate_distance_min(s_agent, s_bucket) \n    penalty_collision = -19 if (d_agent_bucket < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through a construction site. \n Objectives: Avoid collision with trenches and access ramps. \n Conditions: The states of the agent, trenches and access ramps are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a construction site. \n Objectives: It must avoid colliding with any debris bins, material racks and forklifts. \n Conditions: Agent's and all obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -dist_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through a construction site. \n Objectives: Do not collide with any mall crew member. \n Conditions: The states of the agent and the crew member are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_crew_member, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_crew_member = calculate_distance_min(s_agent, s_crew_member) \n    penalty = -19 if (d_crew_member < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -d_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a construction site and must avoid contact with any site office. The states of the drone and the desk are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_desk, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_desk = calculate_distance_min(s_agent, s_desk) \n    penalty = -19 if (d_desk < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -d_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in a construction site. It must avoid colliding with brick pallets and glass panels placed in the rest area. The states of the drone and the obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in a construction site. It must avoid colliding with temporary fencing and workers. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in a construction site. It must avoid colliding with crew members. Agent's state, target's state and crew members's state are accessible. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_crew_member, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_crew_member = calculate_distance_min(s_agent, s_crew_member) \n    penalty_collision = -19 if (d_agent_crew_member < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a construction site. \n Goal: It must avoid colliding with ladders. \n Assumptions: The states of the agent and the stair are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_stair, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_stair = calculate_distance_min(s_agent, s_stair) \n    penalty_collision = -19 if (dist_stair < 0.5) else 15 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a construction site. \n Goal: It must avoid colliding with crew members, workers and site offices. \n Assumptions: The Agent's state, target's state and crew members's state, desk's state and workers' states are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a construction site. \n Goal: Detect workers and shoring posts and avoid rebar bundles. \n Assumptions: The states of the agent and multiple obstacles are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a construction site. \n Goal: Drone target is to reach the target. \n Assumptions: The states of the agent and the target are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a dormitory. It must avoid colliding with reception desk or residents. The states of the agent, the vending area lanes, and the residents are known. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a busy dormitory concourse. It must avoid colliding with laundry cart. The states of the agent, the cart are known. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_laundry_cart, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_laundry_cart) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a dormitory common room. \n Objectives: It must avoid colliding with notice boards. \n Conditions: States of the agent, the notice boards, and the target are known. \nWrite a reward function in Python.",
        "chosen": "def reward(s_agent, s_notice_boards, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_notice_boards = calculate_distance_min(s_agent, s_notice_boards) \n    penalty_collision = -15 if (d_agent_notice_boards < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a dorm hallway, the drone must maneuver around hallway corners. All entity states (agent, barriers, target) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_hallway_corners, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_hallway_corners) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a dormitory atrium. \n Requirement: Avoid collisions with moving carts. \n Given: The states for the agent, carts, and target are known. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_moving_carts, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_moving_carts = calculate_distance_min(s_agent, s_moving_carts) \n    penalty_collision = -15 if (d_agent_moving_carts < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone operates near a dorm entrance during an event. So not collide with door sign. The states of the agent, the door sign are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_door_sign, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_door_sign = calculate_distance_min(s_agent, s_door_sign) \n    penalty_collision = -15 if (d_agent_door_sign < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in a dormitory. \n Objectives: DO not collide hit with bed, \n Conditions: States of the agent, bed are available. \n Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_bed, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bed = calculate_distance_min(s_agent, s_bed) \n    penalty_collision = -15 if (d_agent_floor_polishers < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a dormitory atrium, the drone must steer clear of decorative pillars. All states (agent, pillars) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_pillar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_pillar) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: \n Environment: dormitory lounge. \n Avoid: Hitting with student. \n Known: Agent, student states. \n Create the Python reward function.",
        "chosen": "def reward(s_agent, s_student, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_student = calculate_distance_min(s_agent, s_student) \n    penalty_collision = -15 if (d_agent_student < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a dormitory. \n Objectives: It must avoid colliding with laundry baskets. \n Conditions: The states of the agent and the shopping baskets are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_laundry_baskets, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_shopping_basket = calculate_distance_min(s_agent, s_laundry_baskets) \n    penalty_collision = -19 if (d_agent_shopping_basket < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through a dormitory. \n Objectives: Avoid collision with desk and drape. \n Conditions: The states of the agent, desk and drape are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a dormitory. \n Objectives: It must avoid colliding with any trash bin, chair and door. \n Conditions: Agent's and all obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -dist_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through a dormitory. \n Objectives: Do not collide with any dorm staff. \n Conditions: The states of the agent and the dorm staff are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_dorm_staff, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_dorm_staff = calculate_distance_min(s_agent, s_dorm_staff) \n    penalty = -19 if (d_dorm_staff < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -d_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a dormitory and must avoid contact with any desk. The states of the drone and the desk are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_desk, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_desk = calculate_distance_min(s_agent, s_desk) \n    penalty = -19 if (d_desk < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -d_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in a dormitory. It must avoid colliding with storage boxes and glass doors placed in the study area. The states of the drone and the obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in a dormitory. It must avoid colliding with queue stanchions and residents. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in a dormitory. It must avoid colliding with dorm staff. Agent's state, target's state and dorm staff's state are accessible. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_dorm_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_dorm_staff = calculate_distance_min(s_agent, s_dorm_staff) \n    penalty_collision = -19 if (d_agent_dorm_staff < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a dormitory. \n Goal: It must avoid colliding with stairwellwells. \n Assumptions: The states of the agent and the stairwell are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_stairwell, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_stairwell = calculate_distance_min(s_agent, s_stairwell) \n    penalty_collision = -19 if (dist_stairwell < 0.5) else 15 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a dormitory. \n Goal: It must avoid colliding with dorm staff, residents and RA desks. \n Assumptions: The Agent's state, target's state and dorm staff's state, desk's state and residents' states are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a dormitory. \n Goal: Detect residents and pillars and avoid doorway corners. \n Assumptions: The states of the agent and multiple obstacles are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a dormitory. \n Goal: Drone target is to reach the target. \n Assumptions: The states of the agent and the target are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a dormitory. It must avoid colliding with reception desk or residents. The states of the agent, the vending area lanes, and the residents are known. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a busy dormitory concourse. It must avoid colliding with laundry cart. The states of the agent, the cart are known. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_laundry_cart, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_laundry_cart) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a dormitory common room. \n Objectives: It must avoid colliding with notice boards. \n Conditions: States of the agent and the notice board are known. \nWrite a reward function in Python.",
        "chosen": "def reward(s_agent, s_notice_board, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_notice_board = calculate_distance_min(s_agent, s_notice_board) \n    penalty_collision = -15 if (d_agent_notice_board < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a dorm hallway, the drone must maneuver around hallway corners. All entity states (agent, barrier) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_barrier, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_barrier = calculate_distance_min(s_agent, s_barrier) \n    penalty_collision = -15 if (d_agent_barrier < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a dormitory atrium. \n Requirement: Avoid collisions with moving carts. \n Given: The states for the agent, cart are known. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_cart, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cart = calculate_distance_min(s_agent, s_cart) \n    penalty_collision = -15 if (d_agent_cart < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone operates near a dorm entrance during an event. So not collide with door sign. The states of the agent, the door sign are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_door_sign, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_door_sign = calculate_distance_min(s_agent, s_door_sign) \n    penalty_collision = -15 if (d_agent_door_sign < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in a dormitory. \n Objectives: DO not collide hit with bed, \n Conditions: States of the agent, bed are available. \n Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_bed, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bed = calculate_distance_min(s_agent, s_bed) \n    penalty_collision = -15 if (d_agent_bed < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a dormitory atrium, the drone must steer clear of decorative pillars. All states (agent, pillar) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_pillar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_pillar) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: \n Environment: dormitory lounge. \n Avoid: Hitting with student. \n Known: Agent, student states. \n Create the Python reward function.",
        "chosen": "def reward(s_agent, s_student, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_student = calculate_distance_min(s_agent, s_student) \n    penalty_collision = -15 if (d_agent_student < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a dormitory. \n Objectives: It must avoid colliding with laundry baskets. \n Conditions: The states of the agent and the laundry baskets are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_laundry_basket, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_laundry_basket = calculate_distance_min(s_agent, s_laundry_basket) \n    penalty_collision = -19 if (d_agent_laundry_basket < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through a dormitory. \n Objectives: Avoid collision with desk and drape. \n Conditions: The states of the agent, desk and drape are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a dormitory. \n Objectives: It must avoid colliding with any trash bin, chair and door. \n Conditions: Agent's and all obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -dist_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through a dormitory. \n Objectives: Do not collide with any dorm staff. \n Conditions: The states of the agent and the dorm staff are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_dorm_staff, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_dorm_staff = calculate_distance_min(s_agent, s_dorm_staff) \n    penalty = -19 if (d_dorm_staff < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -d_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a dormitory and must avoid contact with any desk. The states of the drone and the desk are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_desk, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_desk = calculate_distance_min(s_agent, s_desk) \n    penalty = -19 if (d_desk < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -d_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in a dormitory. It must avoid colliding with storage boxes and glass doors placed in the study area. The states of the drone and multiple boxes are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in a dormitory. It must avoid colliding with queue stanchions and residents. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in a dormitory. It must avoid colliding with dorm staff. Agent's state, target's state and dorm staff's state are accessible. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_dorm_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_dorm_staff = calculate_distance_min(s_agent, s_dorm_staff) \n    penalty_collision = -19 if (d_agent_dorm_staff < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a dormitory. \n Goal: It must avoid colliding with stairwellwells. \n Assumptions: The states of the agent and the stairwell are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_stairwell, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_stairwell = calculate_distance_min(s_agent, s_stairwell) \n    penalty_collision = -19 if (dist_stairwell < 0.5) else 15 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a dormitory. \n Goal: It must avoid colliding with dorm staff, residents and RA desks. \n Assumptions: The Agent's state, target's state and dorm staff's state, desk's state and residents' states are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a dormitory. \n Goal: Detect residents and pillars and avoid doorway corners. \n Assumptions: The states of the agent and doors, cupboards and walls state are availalbe. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a dormitory. \n Goal: Drone target is to reach the target. \n Assumptions: The states of the agent and the target are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a farmland. It must avoid colliding with irrigation pump or grazing animals. The states of the agent, the vending area lanes, and the grazing animals are known. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a busy farmland concourse. It must avoid colliding with tractor. The states of the agent, the tractor are known. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_tractor, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tractor = calculate_distance_min(s_agent, s_tractor) \n    penalty_collision = -15 if (d_agent_tractor < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a farmland common room. \n Objectives: It must avoid colliding with hay bales. \n Conditions: States of the agent and the hay bale are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_hay_bale, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_hay_bale = calculate_distance_min(s_agent, s_hay_bale) \n    penalty_collision = -15 if (d_agent_hay_bale < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a farm hallway, the drone must maneuver around irrigation canals. All entity states (agent, canal) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_canal, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_canal = calculate_distance_min(s_agent, s_canal) \n    penalty_collision = -15 if (d_agent_canal < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a farmland atrium. \n Requirement: Avoid collisions with harvest wagons. \n Given: The states for the agent and many wagons are known. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone operates near a farm entrance during an event. So not collide with fence post. The states of the agent, the fence post are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_fence_post, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_fence_post = calculate_distance_min(s_agent, s_fence_post) \n    penalty_collision = -15 if (d_agent_fence_post < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in a farmland. \n Objectives: Do not collide hit with grain silo, \n Conditions: States of the agent, grain silo are available. \n Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_grain_silo, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_grain_silo = calculate_distance_min(s_agent, s_grain_silo) \n    penalty_collision = -15 if (d_agent_grain_silo < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a farmland atrium, the drone must steer clear of windmills. All states (agent, windmill) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_windmill, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_windmill = calculate_distance_min(s_agent, s_windmill) \n    penalty_collision = -15 if (d_agent_windmill < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: \n Environment: farmland lounge. \n Avoid: Hitting with farm worker. \n Known: Agent, farm worker states. \n Create the Python reward function.",
        "chosen": "def reward(s_agent, s_farm_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_farm_worker = calculate_distance_min(s_agent, s_farm_worker) \n    penalty_collision = -15 if (d_agent_farm_worker < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a farmland. \n Objectives: It must avoid colliding with feed troughs. \n Conditions: The states of the agent and the feed troughs are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_feed_trough, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_feed_trough = calculate_distance_min(s_agent, s_feed_trough) \n    penalty_collision = -19 if (d_agent_feed_trough < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through a farmland. \n Objectives: Avoid collision with barn door and tarp. \n Conditions: The states of the agent, barn door and tarp are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a farmland. \n Objectives: It must avoid colliding with any fertilizer tank, hay rack and gate. \n Conditions: Agent's and all obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -dist_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through a farmland. \n Objectives: Do not collide with any farm staff. \n Conditions: The states of the agent and the farm staff are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_farm_staff, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_farm_staff = calculate_distance_min(s_agent, s_farm_staff) \n    penalty = -19 if (d_farm_staff < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -d_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a farmland and must avoid contact with any plow. The states of the drone and the plow are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_plow, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_plow = calculate_distance_min(s_agent, s_plow) \n    penalty = -19 if (d_plow < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -d_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in a farmland. It must avoid colliding with tool sheds and greenhouse doors placed in the study area. The states of the drone and multiple boxes are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in a farmland. It must avoid colliding with queue stanchions and grazing animals. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in a farmland. It must avoid colliding with farm staff. Agent's state, target's state and farm staff's state are accessible. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_farm_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_farm_staff = calculate_distance_min(s_agent, s_farm_staff) \n    penalty_collision = -19 if (d_agent_farm_staff < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a farmland. \n Goal: It must avoid colliding with grain elevator. \n Assumptions: The states of the agent and the elevator are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_elevator, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_elevator = calculate_distance_min(s_agent, s_elevator) \n    penalty_collision = -19 if (dist_elevator < 0.5) else 15 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a farmland. \n Goal: It must avoid colliding with farm staff, grazing animals and RA plows. \n Assumptions: The Agent's state, target's state and farm staff's state, plow's state and grazing animals' states are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a farmland. \n Goal: Detect grazing animals and pillars and avoid fence corners. \n Assumptions: The states of the agent and gates, coops and hedges state are availalbe. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a farmland. \n Goal: Drone target is to reach the target. \n Assumptions: The states of the agent and the target are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a zoo. It must avoid colliding with enclosure enclosure fence post or animals. The states of the agent, the fence, and the animals are known. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_obs_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_target < 0.2) else -d_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a busy zoo concourse. It must avoid colliding with zoo tram. The states of the agent, the zoo tram are known. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_zoo_tram, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_zoo_tram = calculate_distance_min(s_agent, s_zoo_tram) \n    penalty_collision = -15 if (d_zoo_tram < 0.5) else 15 \n    reward_reaching_target = 20 if (d_target < 0.2) else -d_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a zoo common room. \n Objectives: It must avoid colliding with crates. \n Conditions: States of the agent and the crate are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_crate, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_crate) \n    penalty_collision = -15 if (d_obs_min < 0.5) else 4 \n    reward_reaching_target = 15 if (d_target < 0.2) else -d_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a zoo corridor, the drone must maneuver around moats. All entity states (agent, moat) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_moat, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_agent_moat = calculate_distance_min(s_agent, s_moat) \n    penalty_collision = -15 if (d_agent_moat < 0.5) else 15 \n    reward_reaching_target = 20 if (d_target < 0.2) else -d_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a zoo atrium. \n Requirement: Avoid collisions with supply carts. \n Given: The states for the agent and many wagons are known. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_obs_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_target < 0.2) else -d_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone operates near a zoo entrance during an event. So not collide with enclosure fence post. The states of the agent, the enclosure fence post are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_fence_post, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_agent_fence_post = calculate_distance_min(s_agent, s_fence_post) \n    penalty_collision = -15 if (d_agent_fence_post < 0.5) else 15 \n    reward_reaching_target = 20 if (d_target < 0.2) else -d_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in a zoo. \n Objectives: Do not collide hit with aviary netting, \n Conditions: States of the agent, aviary netting are available. \n Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_aviary_netting, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_aviary_netting = calculate_distance_min(s_agent, s_aviary_netting) \n    penalty_collision = -15 if (d_aviary_netting < 0.5) else 4 \n    reward_reaching_target = 15 if (d_target < 0.2) else -d_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a zoo atrium, the drone must steer clear of viewing towers. All states (agent, windmill) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_windmill, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_windmill) \n    penalty_collision = -15 if (d_obs_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_target < 0.2) else -d_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: \n Environment: zoo lounge. \n Avoid: Hitting with zoo worker. \n Known: Agent, zoo worker states. \n Create the Python reward function.",
        "chosen": "def reward(s_agent, s_farm_worker, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_agent_windmill = calculate_distance_min(s_agent, s_farm_worker) \n    penalty_collision = -15 if (d_agent_windmill < 0.5) else 4 \n    reward_reaching_target = 15 if (d_target < 0.2) else -d_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a zoo. \n Objectives: It must avoid colliding with feeding stations. \n Conditions: The states of the agent and the feeding stations are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_feeding station, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_feeding station) \n    penalty_collision = -19 if (d_obs_min < 0.5) else 4 \n    reward_reaching_target = 15 if (d_target < 0.2) else -d_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through a zoo. \n Objectives: Avoid collision with service gate and tarp. \n Conditions: The states of the agent, service gate and tarp are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_obs_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -d_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a zoo. \n Objectives: It must avoid colliding with any chemical storage tank, feed rack and gate. \n Conditions: Agent's and all obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -19 if (d_obs_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -d_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through a zoo. \n Objectives: Do not collide with any zoo staff. \n Conditions: The states of the agent and the zoo staff are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_zoo_staff, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_zoo_staff = calculate_distance_min(s_agent, s_zoo_staff) \n    penalty = -19 if (d_zoo_staff < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -d_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a zoo and must avoid contact with any maintenance cart. The states of the drone and the maintenance cart are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_cart, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_agent_cart = calculate_distance_min(s_agent, s_cart) \n    penalty = -19 if (d_agent_cart < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -d_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in a zoo. It must avoid colliding with storage sheds and glass enclosures placed in the study area. The states of the drone and multiple boxes are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_obs_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -d_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in a zoo. It must avoid colliding with queue stanchions and animals. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_obs_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -d_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in a zoo. It must avoid colliding with zoo staff. Agent's state, target's state and zoo staff's state are accessible. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_agent_staff = calculate_distance_min(s_agent, s_staff) \n    penalty_collision = -19 if (d_agent_staff < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -d_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a zoo. \n Goal: It must avoid colliding with service elevator. \n Assumptions: The states of the agent and the elevator are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_elevator, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    dist_elevator = calculate_distance_min(s_agent, s_elevator) \n    penalty_collision = -19 if (dist_elevator < 0.5) else 15 \n    reward_reaching_target = 19 if (d_target < 0.2) else -d_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a zoo. \n Goal: It must avoid colliding with zoo staff, animals and RA maintenance carts. \n Assumptions: The Agent's state, target's state and zoo staff's state, maintenance cart's state and animals' states are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_obs_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -d_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a zoo. \n Goal: Detect animals and pillars and avoid fence corners. \n Assumptions: The states of the agent and gates, aviaries and barriers state are availalbe. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_obs_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -d_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a zoo. \n Goal: Drone target is to reach the target. \n Assumptions: The states of the agent and the target are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 15 if (d_target < 0.2) else -d_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a swmming  room. The drone must detect and avoid ticket counters and swimmers while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of multiple opstacles: ticket counters and swimmers in the room.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a swmming  room. The drone must detect and avoid cleaning cart racks while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of cleaning cart racks in the room.",
        "chosen": "def reward(s_agent, s_cleaning_carts, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_cleaning_carts) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a swmming  room. The drone must detect and avoid saftly board while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of advertising kiosks in the room.",
        "chosen": "def reward(s_agent, s_safety_board, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_safety_board = calculate_distance_min(s_agent, s_safety_board) \n    penalty_collision = -15 if (d_agent_safety_board < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a swmming  room. The drone must detect and avoid queue barrier while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of queue barrier in the room.",
        "chosen": "def reward(s_agent, s_queue_barrier, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_queue_barrier) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a swmming  room. The drone must detect and avoid storage rack while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of storage rack in the room.",
        "chosen": "def reward(s_agent, s_storage_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_storage_rack = calculate_distance_min(s_agent, s_storage_rack) \n    penalty_collision = -15 if (d_agent_storage_rack < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a swmming  room. The drone must detect and avoid sale sign while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of sale sign in the room.",
        "chosen": "def reward(s_agent, s_sale_signs, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_sale_signs) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a swmming  room. The drone must detect and avoid floor polisher while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of floor polisher in the room.",
        "chosen": "def reward(s_agent, s_floor_polisher, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_floor_polisher = calculate_distance_min(s_agent, s_floor_polisher) \n    penalty_collision = -15 if (d_agent_floor_polishers < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a swmming  room. The drone must detect and avoid support columns while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of support columns in the room.",
        "chosen": "def reward(s_agent, s_support_columns, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_support_columns) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a swmming  room. The drone must detect and avoid Restocking robots shuttling orders while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of Restocking robots shuttling orders in the room.",
        "chosen": "def reward(s_agent, s_restocking_robot, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_restocking_robot = calculate_distance_min(s_agent, s_restocking_robot) \n    penalty_collision = -15 if (d_agent_restocking_robot < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a swmming  room. The drone must detect and avoid baby kickboards while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of baby kickboards in the room.",
        "chosen": "def reward(s_agent, s_kickboard, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_kickboard = calculate_distance_min(s_agent, s_kickboard) \n    penalty_collision = -19 if (d_agent_kickboard < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a swmming  room. The drone must detect and avoid lane ropes and sloped entries while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of multiple obstacles: lane ropes and sloped entries in the room.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a swmming  room. The drone must detect and avoid towel bins, equipment racks, and supply carts while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of multiple obstacles: towel bins, equipment racks, and supply carts in the room.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a swmming  room. The drone must detect and avoid lifeguard while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of lifeguards in the room.",
        "chosen": "def reward(s_agent, s_lifeguard, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_lifeguards = calculate_distance_min(s_agent, s_lifeguard) \n    penalty = -19 if (d_lifeguard < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.exp(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a swmming  room. The drone must detect and avoid information desk while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of information desk in the room.",
        "chosen": "def reward(s_agent, s_information_desk, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_information_desk = calculate_distance_min(s_agent, s_information_desk) \n    penalty = -19 if (d_information_desk < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a swmming  room. The drone must detect and avoid stacked float mats and glass pool barriers placed in the seating area while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of multiple obstacles: stacked float mats and glass pool barriers placed in the seating area in the room.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a swmming  room. The drone must detect and avoid queue ropes and swimmers while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of multiple obstacles: queue ropes and swimmers in the room.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a swmming  room. The drone must detect and avoid lifeguard jacket while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of lifeguards jacket in the room.",
        "chosen": "def reward(s_agent, s_lifeguard_jaacket, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lifeguard_jaacket = calculate_distance_min(s_agent, s_lifeguard_jaacket) \n    penalty_collision = -19 if (d_agent_lifeguard_jaacket < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a swmming  room. The drone must detect and avoid stiars while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of stiars in the room.",
        "chosen": "def reward(s_agent, s_stair, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_stair = calculate_distance_min(s_agent, s_stair) \n    penalty_collision = -19 if (dist_stair < 0.5) else 15 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a swmming  room. The drone must detect and avoid lifeguards, swimmers, and information desks while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of multipe obstacles: lifeguards, swimmers, and information desks in the room.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a swmming  room. The drone must detect and avoid swimmers, diving boards, and lane markers while heading toward a target target. You are provided with the agent’s state, the target’s state, and the positions of multiple obstacles: swimmers, diving boards, and lane markers in the room.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a swmming  room. The drone must heading toward a target target. You are provided with the agent’s state, the target’s state in the room.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a construction site. \n The drone must detect site gates or workers, avoid collisions with them, and use the available agent state and multipel obstacles state: site gates or workers states",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a construction site. \n The drone must detect wheelbarrows, avoid collisions with them, and use the available agent and wheelbarrows states",
        "chosen": "def reward(s_agent, s_wheelbarrows, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_wheelbarrows) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a construction site. \n The drone must detect warning boards, avoid collisions with them, and use the available agent and warning boards states",
        "chosen": "def reward(s_agent, s_warning_boards, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_warning_board = calculate_distance_min(s_agent, s_warning_board) \n    penalty_collision = -15 if (d_agent_warning_board < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a construction site. \n The drone must use the available agent and target states aand scaffold frames state",
        "chosen": "def reward(s_agent, s_scaffold_frames, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_scaffold_frames) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a construction site. \n The drone must use the available agent and target states asn tool cart state",
        "chosen": "def reward(s_agent, s_tool_cart, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tool_cart = calculate_distance_min(s_agent, s_tool_cart) \n    penalty_collision = -15 if (d_agent_tool_cart < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a construction site. \n The drone must use the available agent and target states and caution sign state",
        "chosen": "def reward(s_agent, s_caution_sign, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_caution_sign = calculate_distance_min(s_agent, s_caution_sign) \n    penalty_collision = -15 if (d_agent_caution_sign < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a construction site. \n The drone must use the available agent and target states and scrubber state",
        "chosen": "def reward(s_agent, s_scrubber, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_scrubber = calculate_distance_min(s_agent, s_scrubber) \n    penalty_collision = -15 if (d_agent_scrubber < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a construction site. \n The drone must use the available agent and target states and shoring post state",
        "chosen": "def reward(s_agent, s_shoring_post, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_shoring_post) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a construction site. \n The drone must use the available agent and target states and site robot site ",
        "chosen": "def reward(s_agent, s_site_robots, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_site_robots = calculate_distance_min(s_agent, s_site_robots) \n    penalty_collision = -15 if (d_agent_site_robots < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a construction site. \n The drone must detect buckets, avoid collisions with them, and use the available agent and buckets states",
        "chosen": "def reward(s_agent, s_bucket, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bucket = calculate_distance_min(s_agent, s_bucket) \n    penalty_collision = -19 if (d_agent_bucket < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a construction site. \n The drone must detect trenches and access ramps, avoid collisions with them, and use the available agent state and multiple obstacles state: trenches and access ramps states",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a construction site. \n The drone must detect debris bins, material racks, and forklifts, avoid collisions with them, and use the available agent state and multiple obstacles state: debris bins, material racks, and forklifts states",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a construction site. \n The drone must use the available agent and target states and crew memeer state",
        "chosen": "def reward(s_agent, s_crew_member, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_crew_member = calculate_distance_min(s_agent, s_crew_member) \n    penalty = -19 if (d_crew_member < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.exp(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a construction site. \n The drone must detect desk, avoid collisions with them, and use the available agent and desk states",
        "chosen": "def reward(s_agent, s_desk, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_desk = calculate_distance_min(s_agent, s_desk) \n    penalty = -19 if (d_desk < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.exp(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a construction site. \n The drone must detect brick pallets and glass panels placed in the rest area, avoid collisions with them, and use the available agent state and multiple obstacles state: brick pallets and glass panels placed in the rest area states",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a construction site. \n The drone must detect temporary fencing and workers, avoid collisions with them, and use the available agent state and multiple obstacles state: temporary fencing and workers states",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a construction site. \n The drone must detect crew members, avoid collisions with them, and use the available agent and crew members states",
        "chosen": "def reward(s_agent, s_crew_member, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_crew_member = calculate_distance_min(s_agent, s_crew_member) \n    penalty_collision = -19 if (d_agent_crew_member < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a construction site. \n The drone must detect stair, avoid collisions with them, and use the available agent and stair states",
        "chosen": "def reward(s_agent, s_stair, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_stair = calculate_distance_min(s_agent, s_stair) \n    penalty_collision = -19 if (dist_stair < 0.5) else 15 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a construction site. \n The drone must detect crew members, workers, and site offices, avoid collisions with them, and use the available agent state and multiple obstacles state: crew members, workers, and site offices states",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a construction site. \n The drone must detect workers and ladders and use the available agent and target states and multiple obstacles state",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for a drone navigating inside a construction site. \n The drone must use the available agent and target states",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a dormitory, avoids desk and resident, and aims to reach a target using its own state and the multiple obstacles stae: desk and resident state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a dormitory, avoids laundry carts, and aims to reach a target using its own state and the laundry carts state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_laundry_cart, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_laundry_cart) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a dormitory, avoids notice board, and aims to reach a target using its own state and the notice board state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_notice_boards, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_notice_boards = calculate_distance_min(s_agent, s_notice_boards) \n    penalty_collision = -15 if (d_agent_notice_boards < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a dormitory, avoids hallway corner, and aims to reach a target using its own state and the hallway corner state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_hallway_corners, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_hallway_corners) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a dormitory, avoids moving carts, and aims to reach a target using its own state and the moving carts state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_moving_carts, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_moving_carts = calculate_distance_min(s_agent, s_moving_carts) \n    penalty_collision = -15 if (d_agent_moving_carts < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a dormitory, avoids door sing, and aims to reach a target using its own state and the door sing state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_door_sign, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_door_sign = calculate_distance_min(s_agent, s_door_sign) \n    penalty_collision = -15 if (d_agent_door_sign < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a dormitory, avoids bed, and aims to reach a target using its own state and the bed state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bed, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bed = calculate_distance_min(s_agent, s_bed) \n    penalty_collision = -15 if (d_agent_floor_polishers < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a dormitory, avoids pillar, and aims to reach a target using its own state and the pillar state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_pillar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_pillar) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a dormitory, avoids student, and aims to reach a target using its own state and the student state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_student, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_student = calculate_distance_min(s_agent, s_student) \n    penalty_collision = -15 if (d_agent_student < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a dormitory, avoids shopping basket, and aims to reach a target using its own state and the shopping basket state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_laundry_baskets, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_shopping_basket = calculate_distance_min(s_agent, s_laundry_baskets) \n    penalty_collision = -19 if (d_agent_shopping_basket < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a dormitory, avoids desk and drape, and aims to reach a target using its own state and multiple obstacles state: the desk and drape state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a dormitory, avoids trash bin, chair and door, and aims to reach a target using its own state and the trash bin, chair and door state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a dormitory, avoids dorm staff, and aims to reach a target using its own state and the dorm staff state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_dorm_staff, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_dorm_staff = calculate_distance_min(s_agent, s_dorm_staff) \n    penalty = -19 if (d_dorm_staff < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a dormitory, avoids desk, and aims to reach a target using its own state and the desk state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_desk, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_desk = calculate_distance_min(s_agent, s_desk) \n    penalty = -19 if (d_desk < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a dormitory, avoids boxes and glass door, and aims to reach a target using its own state and multiple obstacles state: the boxes and glass door state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a dormitory, avoids pqueue stanchion and resident, and aims to reach a target using its own state and multiple obstacles state: the queue stanchion and resident state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a dormitory, avoids staff, and aims to reach a target using its own state and the staff state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_staff = calculate_distance_min(s_agent, s_staff) \n    penalty_collision = -19 if (d_agent_staff < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a dormitory, avoids stairwell, and aims to reach a target using its own state and multiple obstacles state: the stairwell state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_stairwell, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_stairwell = calculate_distance_min(s_agent, s_stairwell) \n    penalty_collision = -19 if (dist_stairwell < 0.5) else 15 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a dormitory, avoids RA desk , staff and resident, and aims to reach a target using its own state and multiple obstacles state: the RA desk , staff and resident state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a dormitory, avoids pillar and resident, and aims to reach a target using its own state and multiple obstacles state: the pillar and resident state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a dormitory and aims to reach a target using its own state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a farmland, where it must avoid colliding with irrigation pump or grazing animals and reach the target. The drone’s state, target's state and the information of the multiple obstacles are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a farmland, where it must avoid colliding with tractor and reach the target. The drone’s state, target's state and the information of the tractor are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_tractor, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tractor = calculate_distance_min(s_agent, s_tractor) \n    penalty_collision = -15 if (d_agent_tractor < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a farmland, where it must avoid colliding with hay bales and reach the target. The drone’s state, target's state and the information of the hay bales are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_hay_bale, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_hay_bale = calculate_distance_min(s_agent, s_hay_bale) \n    penalty_collision = -15 if (d_agent_hay_bale < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a farmland, where it must avoid colliding with canal and reach the target. The drone’s state, target's state and the information of the canal are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_canal, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_canal = calculate_distance_min(s_agent, s_canal) \n    penalty_collision = -15 if (d_agent_canal < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a farmland, where it must avoid colliding with harvest wagons and reach the target. The drone’s state, target's state and the information of the multiple obstacles are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a farmland, where it must avoid colliding with fence post and reach the target. The drone’s state, target's state and the information of the fence post are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_fence_post, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_fence_post = calculate_distance_min(s_agent, s_fence_post) \n    penalty_collision = -15 if (d_agent_fence_post < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a farmland, where it must avoid colliding with grain silo and reach the target. The drone’s state, target's state and the information of the grain silo are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_grain_silo, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_grain_silo = calculate_distance_min(s_agent, s_grain_silo) \n    penalty_collision = -15 if (d_agent_grain_silo < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a farmland, where it must avoid colliding with windmills and reach the target. The drone’s state, target's state and the information of the windmills are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_windmill, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_windmill = calculate_distance_min(s_agent, s_windmill) \n    penalty_collision = -15 if (d_agent_windmill < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a farmland, where it must avoid colliding  with farm worker and reach the target. The drone’s state, target's state and the information of the farm worker are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_farm_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_farm_worker = calculate_distance_min(s_agent, s_farm_worker) \n    penalty_collision = -15 if (d_agent_farm_worker < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a farmland, where it must avoid colliding with feed troughs and reach the target. The drone’s state, target's state and the information of the feed troughs are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_feed_trough, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_feed_trough = calculate_distance_min(s_agent, s_feed_trough) \n    penalty_collision = -19 if (d_agent_feed_trough < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a farmland, where it must avoid colliding with obstacles and reach the target. The drone’s state, target's state and the information of the multiple obstacles are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a farmland, where it must avoid colliding with fertilizer tank, hay rack and gate and reach the target. The drone’s state, target's state and the information of the multiple obstacles are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a farmland, where it must avoid colliding with farm staff and reach the target. The drone’s state, target's state and the information of the farm staff are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_farm_staff, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_farm_staff = calculate_distance_min(s_agent, s_farm_staff) \n    penalty = -19 if (d_farm_staff < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.exp(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a farmland, where it must avoid colliding with plow and reach the target. The drone’s state, target's state and the information of the plow are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_plow, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_plow = calculate_distance_min(s_agent, s_plow) \n    penalty = -19 if (d_plow < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.exp(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a farmland, where it must avoid colliding with tool sheds and greenhouse doors and reach the target. The drone’s state, target's state and the information of the multiple obstacles are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a farmland, where it must avoid colliding with queue stanchions and grazing animals and reach the target. The drone’s state, target's state and the information of the multiple obstacles are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a farmland, where it must avoid colliding with farm staff and reach the target. The drone’s state, target's state and the information of the farm staff are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_farm_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_farm_staff = calculate_distance_min(s_agent, s_farm_staff) \n    penalty_collision = -19 if (d_agent_farm_staff < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a farmland, where it must avoid colliding with grain elevator and reach the target. The drone’s state, target's state and the information of the grain elevator are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_elevator, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_elevator = calculate_distance_min(s_agent, s_elevator) \n    penalty_collision = -19 if (dist_elevator < 0.5) else 15 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a farmland, where it must avoid colliding with farm staff, grazing animals and plows and reach the target. The drone’s state, target's state and the information of the multiple obstacles are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a farmland, where it must avoid colliding with cows, pigs and chicken and reach the target. The drone’s state, target's state and the information of the multiple obstacles are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a farmland, where it must reach the target. The drone’s state, target's state  are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a zoo, aims to detect obstacles (fence post or animals) and reach target, and must avoid collisions by using its own state and the multiple obstacles fence post or animals states. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_obs_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_target < 0.2) else -math.exp(d_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a zoo, aims to detect a obstacle (a tram) and reach target, and must avoid collisions by using its own state and the tram’s state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_zoo_tram, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_zoo_tram = calculate_distance_min(s_agent, s_zoo_tram) \n    penalty_collision = -15 if (d_zoo_tram < 0.5) else 15 \n    reward_reaching_target = 20 if (d_target < 0.2) else -math.exp(d_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a zoo, aims to detect obstacles (crates) and reach target, and must avoid collisions by using its own state and the crates states. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_crate, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_crate) \n    penalty_collision = -15 if (d_obs_min < 0.5) else 4 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.exp(d_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a zoo, aims to detect a obstacle (a moat) and reach target, and must avoid collisions by using its own state and the moat's state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_moat, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_agent_moat = calculate_distance_min(s_agent, s_moat) \n    penalty_collision = -15 if (d_agent_moat < 0.5) else 15 \n    reward_reaching_target = 20 if (d_target < 0.2) else -math.exp(d_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a zoo, aims to detect obstacles (supply carts, moat) and reach target, and must avoid collisions by using its own state and the multiple obstacles states. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_obs_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_target < 0.2) else -math.exp(d_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a zoo, aims to detect a obstacle (an fence post) and reach target, and must avoid collisions by using its own state and the fence post's state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_fence_post, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_agent_fence_post = calculate_distance_min(s_agent, s_fence_post) \n    penalty_collision = -15 if (d_agent_fence_post < 0.5) else 15 \n    reward_reaching_target = 20 if (d_target < 0.2) else -math.exp(d_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a zoo, aims to detect a obstacle (an aviary netting) and reach target, and must avoid collisions by using its own state and the averiary netting state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_aviary_netting, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_aviary_netting = calculate_distance_min(s_agent, s_aviary_netting) \n    penalty_collision = -15 if (d_aviary_netting < 0.5) else 4 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.exp(d_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a zoo, aims to detect obstacles (windmill) and reach target, and must avoid collisions by using its own state and the windmill's states. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_windmill, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_windmill) \n    penalty_collision = -15 if (d_obs_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_target < 0.2) else -math.exp(d_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a zoo, aims to detect a obstacle (zoo worker) and reach target, and must avoid collisions by using its own state and zoo worker’s state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_zoo_worker, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_agent_windmill = calculate_distance_min(s_agent, s_zoo_worker) \n    penalty_collision = -15 if (d_agent_windmill < 0.5) else 4 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.exp(d_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a zoo, aims to detect obstacles (feeding stations) and reach target, and must avoid collisions by using its own state and the feeding stations states. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_feeding_station, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_feeding_station) \n    penalty_collision = -19 if (d_obs_min < 0.5) else 4 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.exp(d_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a zoo, aims to detect a obstacle (service gate and tarp) and reach target, and must avoid collisions by using its own state and the multipler bstacle's: service gate and tarp state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_obs_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.exp(d_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a zoo, aims to detect obstacles (chemical storage tank, feed rack and gate) and reach target, and must avoid collisions by using its own state and the multiple obstacles: chemical storage tank, feed rack and gate states. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -19 if (d_obs_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.exp(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a zoo, aims to detect a obstacle (a zoo staff) and reach target, and must avoid collisions by using its own state and the zoo staff’s state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_zoo_staff, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_zoo_staff = calculate_distance_min(s_agent, s_zoo_staff) \n    penalty = -19 if (d_zoo_staff < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.exp(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a zoo, aims to detect a obstacle (an cart) and reach target, and must avoid collisions by using its own state and the cart's state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_cart, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_agent_cart = calculate_distance_min(s_agent, s_cart) \n    penalty = -19 if (d_agent_cart < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.exp(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a zoo, aims to detect obstacles (storage sheds and glass enclosures) and reach target, and must avoid collisions by using its own state and the multiple obstcales: storage sheds and glass enclosures area states. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_obs_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.exp(d_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a zoo, aims to detect obstacles (queue stanchions and animals) and reach target, and must avoid collisions by using its own state and the multiple obstacles: queue stanchions and animals states. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_obs_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.exp(d_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a zoo, aims to detect a obstacle (a zoo staff) and reach target, and must avoid collisions by using its own state and the zoo staff’s state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_agent_staff = calculate_distance_min(s_agent, s_staff) \n    penalty_collision = -19 if (d_agent_staff < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.exp(d_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a zoo, aims to detect a obstacle (an elevator) and reach target, and must avoid collisions by using its own state and the elevator’s state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_elevator, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    dist_elevator = calculate_distance_min(s_agent, s_elevator) \n    penalty_collision = -19 if (dist_elevator < 0.5) else 15 \n    reward_reaching_target = 19 if (d_target < 0.2) else -math.exp(d_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a zoo, aims to detect obstacles (zoo staff, animals and maintenance carts) and reach target, and must avoid collisions by using its own state and the multiple obstacles zoo staff, animals and maintenance carts states. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_obs_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.exp(d_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a zoo, aims to detect obstacles (animal and pillar) and reach target, and must avoid collisions by using its own state and the multiple obstacles states. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_obs_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.exp(d_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a zoo, aims to reach target, and must avoid collisions by using its own state and target's state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.exp(d_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a swmming room. \n Requirements: the drone should identify and avoid obstacles (escalators and shoppers) while progressing toward the target. \n The environment provides access to the agent state, target state, and multiple multiple obstacles states",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    math.log(d_agent_target) = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 25 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a swmming room. \n Requirements: the drone should identify and avoid obstacles (cleaning carts) while progressing toward the target. \n The environment provides access to the agent state, target state, and cleaning carts states",
        "chosen": "def reward(s_agent, s_cleaning_cart, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cleaning_cart = calculate_distance_min(s_agent, s_cleaning_cart) \n    penalty_collision = -10 if (d_agent_cleaning_cart < 0.7) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a swmming room. \n Requirements: the drone should identify and avoid obstacles (advertising kiosks) while progressing toward the target. \n The environment provides access to the agent state, target state, and advertising kiosks states",
        "chosen": "def reward(s_agent, s_ad_kiosk, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ad_kiosk = calculate_distance_min(s_agent, s_ad_kiosk) \n    penalty_collision = -10 if (d_agent_ad_kiosk < 0.7) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a swmming room. \n Requirements: the drone should identify and avoid obstacles (queue barrier) while progressing toward the target. \n The environment provides access to the agent state, target state, and queue barrier states",
        "chosen": "def reward(s_agent, s_queue_barrier, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_queue_barrier = calculate_distance_min(s_agent, s_queue_barrier) \n    penalty_collision = -10 if (d_agent_queue_barrier < 0.7) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a swmming room. \n Requirements: the drone should identify and avoid obstacles (shopping carts) while progressing toward the target. \n The environment provides access to the agent state, target state, and shopping carts states",
        "chosen": "def reward(s_agent, s_shopping_cart, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_shopping_cart = calculate_distance_min(s_agent, s_shopping_cart) \n    penalty_collision = -10 if (d_agent_shopping_cart < 0.7) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a swmming room. \n Requirements: the drone should identify and avoid obstacles (portable sign) while progressing toward the target. \n The environment provides access to the agent state, target state, and portable sing states",
        "chosen": "def reward(s_agent, s_portable_sign, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_portable_sign) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.7) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a swmming room. \n Requirements: the drone should identify and avoid obstacles (floor scrubber) while progressing toward the target. \n The environment provides access to the agent state, target state, and floor scrubber states",
        "chosen": "def reward(s_agent, s_floor_scrubber, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_floor_scrubber = calculate_distance_min(s_agent, s_floor_scrubber) \n    penalty_collision = -10 if (d_agent_floor_scrubber < 0.7) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a swmming room. \n Requirements: the drone should identify and avoid obstacles (pillar) while progressing toward the target. \n The environment provides access to the agent state, target state, and pillar states",
        "chosen": "def reward(s_agent, s_pillar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_pillar) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.7) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a swmming room. \n Requirements: the drone should identify and avoid obstacles (Delivery robots shuttling orders) while progressing toward the target. \n The environment provides access to the agent state, target state, and multiple obstacles states",
        "chosen": "def reward(s_agent, s_delivery_robot, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_delivery_robot = calculate_distance_min(s_agent, s_delivery_robot) \n    penalty_collision = -10 if (d_agent_delivery_robot < 0.7) else 3 \n    reward_reaching_target = 15 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a swmming room. \n Requirements: the drone should identify and avoid obstacles (baby strollers) while progressing toward the target. \n The environment provides access to the agent state, target state, and baby strollers states",
        "chosen": "def reward(s_agent, s_stroller, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_stroller = calculate_distance_min(s_agent, s_stroller) \n    penalty_collision = -25 if (d_agent_stroller < 0.6) else 3 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a swmming room. \n Requirements: the drone should identify and avoid obstacles (escalator and staircase) while progressing toward the target. \n The environment provides access to the agent state, target state, and multiple obstacles states",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (dist_multiple_obstacles < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -math.log(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a swmming room. \n Requirements: the drone should identify and avoid obstacles (trash bin, construction hoarding, and service cart) while progressing toward the target. \n The environment provides access to the agent state, target state, and multiple obstacles states",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -25 if (dist_multiple_obstacles < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -math.log(dist_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a swmming room. \n Requirements: the drone should identify and avoid obstacles (mall staff) while progressing toward the target. \n The environment provides access to the agent state, target state, and mall staff states",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_staff = calculate_distance_min(s_agent, s_staff) \n    penalty = -25 if (d_staff < 0.6) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -math.log(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a swmming room. \n Requirements: the drone should identify and avoid obstacles (information desk) while progressing toward the target. \n The environment provides access to the agent state, target state, and information desk states",
        "chosen": "def reward(s_agent, s_info_desk, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_info_desk = calculate_distance_min(s_agent, s_info_desk) \n    penalty = -25 if (d_info_desk < 0.6) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -math.log(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a swmming room. \n Requirements: the drone should identify and avoid obstacles (display boxes and glass partitions placed in the seating area) while progressing toward the target. \n The environment provides access to the agent state, target state, and multiple obstacles states",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a swmming room. \n Requirements: the drone should identify and avoid obstacles (benches and shoppers) while progressing toward the target. \n The environment provides access to the agent state, target state, and multiple obstacles states",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (dist_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -math.log(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a swmming room. \n Requirements: the drone should identify and avoid obstacles (staff) while progressing toward the target. \n The environment provides access to the agent state, target state, and staff states",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_staff = calculate_distance_min(s_agent, s_staff) \n    penalty_collision = -25 if (d_agent_staff < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a swmming room. \n Requirements: the drone should identify and avoid obstacles (stairs) while progressing toward the target. \n The environment provides access to the agent state, target state, and stairs states",
        "chosen": "def reward(s_agent, s_stair, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_stair = calculate_distance_min(s_agent, s_stair) \n    penalty_collision = -25 if (dist_stair < 0.5) else 10 \n    reward_reaching_target = 25 if (dist_target < 0.1) else -math.log(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a swmming room. \n Requirements: the drone should identify and avoid obstacles (staff, shoppers, and information desks) while progressing toward the target. \n The environment provides access to the agent state, target state, and multiple obstacles states",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a swmming room. \n Requirements: the drone should identify and avoid obstacles (shoppers, pillars, and sign boards) while progressing toward the target. \n The environment provides access to the agent state, target state, and multiple obstacles states",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_obstacles_min < 0.6) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a swmming room. \n Requirements: the drone should identify and progressing toward the target. \n The environment provides access to the agent state, target state.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a construction site environment. It should identify site gates or workers, as obstacles, prevent collisions, reach target use the available agent, and multipe obstacles state: site gates or worker state. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a construction site environment. It should identify wheelbarrows, as obstacles, prevent collisions, reach target and rely on the Agent, target and stage wheelbarrows. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_wheelbarrows, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_wheelbarrows) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a construction site environment. It should identify warning boards, as obstacles, prevent collisions, reach target and rely the available agent state, and warning boards state. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_warning_boards, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_warning_board = calculate_distance_min(s_agent, s_warning_board) \n    penalty_collision = -15 if (d_agent_warning_board < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a construction site environment. It should identify scaffold frames as obstacles, prevent collisions, reach target and rely on the Agent state, target state and scaffold frames state. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_scaffold_frames, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_scaffold_frames) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a construction site environment. It should identify tool cart as obstacles, prevent collisions, reach target and rely on the Agentstate, target state and tool cart state. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_tool_cart, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tool_cart = calculate_distance_min(s_agent, s_tool_cart) \n    penalty_collision = -15 if (d_agent_tool_cart < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a construction site environment. It should identify caution sogn as obstacles, prevent collisions, reach target and rely on the Agent state,  target state and caution sign state. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_caution_sign, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_caution_sign = calculate_distance_min(s_agent, s_caution_sign) \n    penalty_collision = -15 if (d_agent_caution_sign < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a construction site environment. It should identify scrubber as obstacles, prevent collisions, reach target and rely on the Agent state, target state and scrubber state. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_scrubber, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_scrubber = calculate_distance_min(s_agent, s_scrubber) \n    penalty_collision = -15 if (d_agent_scrubber < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a construction site environment. It should identify shoring post as obstacles prevent collisions, reach target and rely on the Agent state, target state and shoring post state. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_shoring_post, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_shoring_post) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a construction site environment. It should identify site robots as obstacles prevent collisions, reach target and rely on the Agent state,  target state and site robots state. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_site_robots, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_site_robots = calculate_distance_min(s_agent, s_site_robots) \n    penalty_collision = -15 if (d_agent_site_robots < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a construction site environment. It should identify buckets as obstacles, prevent collisions, reach target and rely on the Agent, target and bucket state. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_bucket, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bucket = calculate_distance_min(s_agent, s_bucket) \n    penalty_collision = -19 if (d_agent_bucket < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a construction site environment. It should identify trenches, access ramps as obstacles, prevent collisions, reach target and rely on the Agent, target and multiple obstacles state: trenches, access ramps. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a construction site environment. It should identify debris bins, material racks, forklifts as obstacles, prevent collisions, reach target and rely on the Agent, target and multiple obstacles state: debris bins, material racks, forklifts . Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a construction site environment. It should identify crew member as obstacles prevent collisions, reach target and rely on the Agent state, target state and crew member state. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_crew_member, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_crew_member = calculate_distance_min(s_agent, s_crew_member) \n    penalty = -19 if (d_crew_member < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.exp(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a construction site environment. It should identify desk as obstacles, prevent collisions, reach target and rely on the Agent state, target state and desk state. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_desk, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_desk = calculate_distance_min(s_agent, s_desk) \n    penalty = -19 if (d_desk < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.exp(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a construction site environment. It should identify brick pallets, glass panels placed in the rest area as obstacles, prevent collisions, reach target and rely on the Agent, target and multiple obstacles state: brick pallets, glass panels. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a construction site environment. It should identify temporary fencing, workers as obstacles, prevent collisions, reach target and rely on the Agent, target and mulitple obstacles state: temporary fencing, workers. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a construction site environment. It should identify crew members as obstacles, prevent collisions, reach target and rely on the Agent state, target state and state of crew members. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_crew_member, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_crew_member = calculate_distance_min(s_agent, s_crew_member) \n    penalty_collision = -19 if (d_agent_crew_member < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a construction site environment. It should identify stair as obstacles, prevent collisions, reach target and rely on the Agent state, target state and stair state. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_stair, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_stair = calculate_distance_min(s_agent, s_stair) \n    penalty_collision = -19 if (dist_stair < 0.5) else 15 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a construction site environment. It should identify crew members, workers, site offices as obstacles, prevent collisions, reach target and rely on the Agent state, target state and multiple obstacles state: crew members, workers, site offices. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a construction site environment. It should identify workers, ladders as obstacles, prevent collisions, reach target and rely on the Agent state, target state and multiple obstacles state: workers, ladders. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a construction site environment. It should prevent collisions, reach target and rely on the Agent and target. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a dormitory. It must avoid colliding with reception desk or residents. The states of the agent, the vending area lanes, and the residents are known. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a busy dormitory concourse. It must avoid colliding with laundry cart. The states of the agent, the cart are known. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_laundry_cart, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_laundry_cart) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a dormitory common room. \n Objectives: It must avoid colliding with notice boards. \n Conditions: States of the agent, the notice boards, and the target are known. \nWrite a reward function in Python.",
        "chosen": "def reward(s_agent, s_notice_boards, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_notice_boards = calculate_distance_min(s_agent, s_notice_boards) \n    penalty_collision = -15 if (d_agent_notice_boards < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a dorm hallway, the drone must maneuver around hallway corners. All entity states (agent, barriers, target) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_hallway_corners, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_hallway_corners) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a dormitory atrium. \n Requirement: Avoid collisions with moving carts. \n Given: The states for the agent, carts, and target are known. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_moving_carts, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_moving_carts = calculate_distance_min(s_agent, s_moving_carts) \n    penalty_collision = -15 if (d_agent_moving_carts < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone operates near a dorm entrance during an event. So not collide with door sign. The states of the agent, the door sign are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_door_sign, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_door_sign = calculate_distance_min(s_agent, s_door_sign) \n    penalty_collision = -15 if (d_agent_door_sign < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in a dormitory. \n Objectives: DO not collide hit with bed, \n Conditions: States of the agent, bed are available. \n Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_bed, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bed = calculate_distance_min(s_agent, s_bed) \n    penalty_collision = -15 if (d_agent_floor_polishers < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a dormitory atrium, the drone must steer clear of decorative pillars. All states (agent, pillars) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_pillar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_pillar) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: \n Environment: dormitory lounge. \n Avoid: Hitting with student. \n Known: Agent, student states. \n Create the Python reward function.",
        "chosen": "def reward(s_agent, s_student, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_student = calculate_distance_min(s_agent, s_student) \n    penalty_collision = -15 if (d_agent_student < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a dormitory. \n Objectives: It must avoid colliding with laundry baskets. \n Conditions: The states of the agent and the shopping baskets are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_laundry_baskets, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_shopping_basket = calculate_distance_min(s_agent, s_laundry_baskets) \n    penalty_collision = -19 if (d_agent_shopping_basket < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through a dormitory. \n Objectives: Avoid collision with desk and drape. \n Conditions: The states of the agent, desk and drape are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.log(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a dormitory. \n Objectives: It must avoid colliding with any trash bin, chair and door. \n Conditions: Agent's and all obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.log(dist_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through a dormitory. \n Objectives: Do not collide with any dorm staff. \n Conditions: The states of the agent and the dorm staff are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_dorm_staff, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_dorm_staff = calculate_distance_min(s_agent, s_dorm_staff) \n    penalty = -19 if (d_dorm_staff < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.log(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a dormitory and must avoid contact with any desk. The states of the drone and the desk are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_desk, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_desk = calculate_distance_min(s_agent, s_desk) \n    penalty = -19 if (d_desk < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.log(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in a dormitory. It must avoid colliding with storage boxes and glass doors placed in the study area. The states of the drone and the obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in a dormitory. It must avoid colliding with queue stanchions and residents. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.log(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in a dormitory. It must avoid colliding with dorm staff. Agent's state, target's state and dorm staff's state are accessible. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_dorm_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_dorm_staff = calculate_distance_min(s_agent, s_dorm_staff) \n    penalty_collision = -19 if (d_agent_dorm_staff < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a dormitory. \n Goal: It must avoid colliding with stairwellwells. \n Assumptions: The states of the agent and the stairwell are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_stairwell, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_stairwell = calculate_distance_min(s_agent, s_stairwell) \n    penalty_collision = -19 if (dist_stairwell < 0.5) else 15 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a dormitory. \n Goal: It must avoid colliding with dorm staff, residents and RA desks. \n Assumptions: The Agent's state, target's state and dorm staff's state, desk's state and residents' states are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a dormitory. \n Goal: Detect residents and pillars and avoid doorway corners. \n Assumptions: The states of the agent and multiple obstacles are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a dormitory. \n Goal: Drone target is to reach the target. \n Assumptions: The states of the agent and the target are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a dormitory. It must avoid colliding with reception desk or residents. The states of the agent, the vending area lanes, and the residents are known. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a busy dormitory concourse. It must avoid colliding with laundry cart. The states of the agent, the cart are known. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_laundry_cart, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_laundry_cart) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a dormitory common room. \n Objectives: It must avoid colliding with notice boards. \n Conditions: States of the agent and the notice board are known. \nWrite a reward function in Python.",
        "chosen": "def reward(s_agent, s_notice_board, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_notice_board = calculate_distance_min(s_agent, s_notice_board) \n    penalty_collision = -15 if (d_agent_notice_board < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a dorm hallway, the drone must maneuver around hallway corners. All entity states (agent, barrier) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_barrier, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_barrier = calculate_distance_min(s_agent, s_barrier) \n    penalty_collision = -15 if (d_agent_barrier < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a dormitory atrium. \n Requirement: Avoid collisions with moving carts. \n Given: The states for the agent, cart are known. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_cart, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cart = calculate_distance_min(s_agent, s_cart) \n    penalty_collision = -15 if (d_agent_cart < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone operates near a dorm entrance during an event. So not collide with door sign. The states of the agent, the door sign are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_door_sign, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_door_sign = calculate_distance_min(s_agent, s_door_sign) \n    penalty_collision = -15 if (d_agent_door_sign < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in a dormitory. \n Objectives: DO not collide hit with bed, \n Conditions: States of the agent, bed are available. \n Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_bed, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bed = calculate_distance_min(s_agent, s_bed) \n    penalty_collision = -15 if (d_agent_bed < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a dormitory atrium, the drone must steer clear of decorative pillars. All states (agent, pillar) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_pillar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_pillar) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: \n Environment: dormitory lounge. \n Avoid: Hitting with student. \n Known: Agent, student states. \n Create the Python reward function.",
        "chosen": "def reward(s_agent, s_student, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_student = calculate_distance_min(s_agent, s_student) \n    penalty_collision = -15 if (d_agent_student < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a dormitory. \n Objectives: It must avoid colliding with laundry baskets. \n Conditions: The states of the agent and the laundry baskets are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_laundry_basket, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_laundry_basket = calculate_distance_min(s_agent, s_laundry_basket) \n    penalty_collision = -19 if (d_agent_laundry_basket < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through a dormitory. \n Objectives: Avoid collision with desk and drape. \n Conditions: The states of the agent, desk and drape are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.log(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a dormitory. \n Objectives: It must avoid colliding with any trash bin, chair and door. \n Conditions: Agent's and all obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.log(dist_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through a dormitory. \n Objectives: Do not collide with any dorm staff. \n Conditions: The states of the agent and the dorm staff are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_dorm_staff, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_dorm_staff = calculate_distance_min(s_agent, s_dorm_staff) \n    penalty = -19 if (d_dorm_staff < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.log(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a dormitory and must avoid contact with any desk. The states of the drone and the desk are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_desk, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_desk = calculate_distance_min(s_agent, s_desk) \n    penalty = -19 if (d_desk < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.log(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in a dormitory. It must avoid colliding with storage boxes and glass doors placed in the study area. The states of the drone and multiple boxes are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in a dormitory. It must avoid colliding with queue stanchions and residents. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.log(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in a dormitory. It must avoid colliding with dorm staff. Agent's state, target's state and dorm staff's state are accessible. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_dorm_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_dorm_staff = calculate_distance_min(s_agent, s_dorm_staff) \n    penalty_collision = -19 if (d_agent_dorm_staff < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a dormitory. \n Goal: It must avoid colliding with stairwellwells. \n Assumptions: The states of the agent and the stairwell are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_stairwell, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_stairwell = calculate_distance_min(s_agent, s_stairwell) \n    penalty_collision = -19 if (dist_stairwell < 0.5) else 15 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a dormitory. \n Goal: It must avoid colliding with dorm staff, residents and RA desks. \n Assumptions: The Agent's state, target's state and dorm staff's state, desk's state and residents' states are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a dormitory. \n Goal: Detect residents and pillars and avoid doorway corners. \n Assumptions: The states of the agent and doors, cupboards and walls state are availalbe. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a dormitory. \n Goal: Drone target is to reach the target. \n Assumptions: The states of the agent and the target are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a farmalnd, aims to reach a target, and must avoid collisions with irrigation pump or grazing animals using its own state, the multiple obstcales statuses: irrigation pump or grazing animals statuses, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a farmalnd, aims to reach a target, and must avoid collisions with tractor using its own state, the tractor status, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_tractor, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tractor = calculate_distance_min(s_agent, s_tractor) \n    penalty_collision = -15 if (d_agent_tractor < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a farmalnd, aims to reach a target, and must avoid collisions with hay bales using its own state, the hay bales statuses, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_hay_bale, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_hay_bale = calculate_distance_min(s_agent, s_hay_bale) \n    penalty_collision = -15 if (d_agent_hay_bale < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a farmalnd, aims to reach a target, and must avoid collisions with canal using its own state, the canal statuses, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_canal, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_canal = calculate_distance_min(s_agent, s_canal) \n    penalty_collision = -15 if (d_agent_canal < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a farmalnd, aims to reach a target, and must avoid collisions with harvest wagons and human using its own state, the multiple obstcales statuses: harvest wagons and human statuses, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a farmalnd, aims to reach a target, and must avoid collisions with fence post using its own state, the fence post statuses, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_fence_post, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_fence_post = calculate_distance_min(s_agent, s_fence_post) \n    penalty_collision = -15 if (d_agent_fence_post < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a farmalnd, aims to reach a target, and must avoid collisions with grain silo using its own state, the grain silo statuses, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_grain_silo, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_grain_silo = calculate_distance_min(s_agent, s_grain_silo) \n    penalty_collision = -15 if (d_agent_grain_silo < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a farmalnd, aims to reach a target, and must avoid collisions with windmills using its own state, the windmill status, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_windmill, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_windmill = calculate_distance_min(s_agent, s_windmill) \n    penalty_collision = -15 if (d_agent_windmill < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a farmalnd, aims to reach a target, and must avoid collisions with farm worker using its own state, the farm worker status, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_farm_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_farm_worker = calculate_distance_min(s_agent, s_farm_worker) \n    penalty_collision = -15 if (d_agent_farm_worker < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a farmalnd, aims to reach a target, and must avoid collisions with feed troughs using its own state, the feed troughs statuses, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_feed_trough, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_feed_trough = calculate_distance_min(s_agent, s_feed_trough) \n    penalty_collision = -19 if (d_agent_feed_trough < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a farmalnd, aims to reach a target, and must avoid collisions with tractor, hayball using its own state, the multiple obstcales statuses, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.log(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a farmalnd, aims to reach a target, and must avoid collisions with fertilizer tank, hay rack and gate using its own state, the multiple obstcales statuses: fertilizer tank, hay rack and gate statuses, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.log(dist_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a farmalnd, aims to reach a target, and must avoid collisions with farm staff using its own state, the farm staff status, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_farm_staff, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_farm_staff = calculate_distance_min(s_agent, s_farm_staff) \n    penalty = -19 if (d_farm_staff < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.log(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a farmalnd, aims to reach a target, and must avoid collisions with plow using its own state, the plow status, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_plow, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_plow = calculate_distance_min(s_agent, s_plow) \n    penalty = -19 if (d_plow < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.log(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a farmalnd, aims to reach a target, and must avoid collisions with tool sheds and greenhouse doors placed in the study area using its own state, the multiple obstcales statuses: tool sheds and greenhouse doors placed in the study area statuses, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a farmalnd, aims to reach a target, and must avoid collisions with queue stanchions and grazing animals using its own state, the multiple obstcales statuses: queue stanchions and grazing animals statuses, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.log(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a farmalnd, aims to reach a target, and must avoid collisions with farm staff using its own state, the farm staff status, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_farm_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_farm_staff = calculate_distance_min(s_agent, s_farm_staff) \n    penalty_collision = -19 if (d_agent_farm_staff < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a farmalnd, aims to reach a target, and must avoid collisions with grain elevator using its own state, the grain elevator status, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_elevator, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_elevator = calculate_distance_min(s_agent, s_elevator) \n    penalty_collision = -19 if (dist_elevator < 0.5) else 15 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a farmalnd, aims to reach a target, and must avoid collisions with farm staff, grazing animals and plows using its own state, the farm staff, the multiple obstcales statuses: grazing animals and plows statuses, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a farmalnd, aims to reach a target, and must avoid collisions with grazing animals using its own state, the multiple obstcales statuses: grazing animals statuses, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a farmalnd, aims to reach a target, using its own state and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.log(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a zoo with the aim of reaching a target while avoiding collisions with enclosure fence post or animals. Available Information: The agent’s state, target's state and the multiple obstcales: fence post or animals’ states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_obs_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_target < 0.2) else -math.log(d_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a zoo with the aim of reaching a target while avoiding collisions with tram. Available Information: The agent’s state, target's state and the tram’s state can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_zoo_tram, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_zoo_tram = calculate_distance_min(s_agent, s_zoo_tram) \n    penalty_collision = -15 if (d_zoo_tram < 0.5) else 15 \n    reward_reaching_target = 20 if (d_target < 0.2) else -math.log(d_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a zoo with the aim of reaching a target while avoiding collisions with crates. Available Information: The agent’s state, target's state and the crates’ states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_crate, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_crate) \n    penalty_collision = -15 if (d_obs_min < 0.5) else 4 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.log(d_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a zoo with the aim of reaching a target while avoiding collisions with moat. Available Information: The agent’s state, target's state and the moat's state can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_moat, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_agent_moat = calculate_distance_min(s_agent, s_moat) \n    penalty_collision = -15 if (d_agent_moat < 0.5) else 15 \n    reward_reaching_target = 20 if (d_target < 0.2) else -math.log(d_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a zoo with the aim of reaching a target while avoiding collisions with supply carts and human. Available Information: The agent’s state, target's state and the multiple obstcales: supply carts’ and human'S states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_obs_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_target < 0.2) else -math.log(d_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a zoo with the aim of reaching a target while avoiding collisions with fence post. Available Information: The agent’s state, target's state and the fence post's state can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_fence_post, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_agent_fence_post = calculate_distance_min(s_agent, s_fence_post) \n    penalty_collision = -15 if (d_agent_fence_post < 0.5) else 15 \n    reward_reaching_target = 20 if (d_target < 0.2) else -math.log(d_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a zoo with the aim of reaching a target while avoiding collisions with aviary netting. Available Information: The agent’s state, target's state and the aviary netting state can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_aviary_netting, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_aviary_netting = calculate_distance_min(s_agent, s_aviary_netting) \n    penalty_collision = -15 if (d_aviary_netting < 0.5) else 4 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.log(d_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a zoo with the aim of reaching a target while avoiding collisions with  windmill. Available Information: The agent’s state, target's state and the windmill’ states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_windmill, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_windmill) \n    penalty_collision = -15 if (d_obs_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_target < 0.2) else -math.log(d_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a zoo with the aim of reaching a target while avoiding collisions with zoo worker. Available Information: The agent’s state, target's state and the zoo worker’s state can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_farm_worker, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_agent_windmill = calculate_distance_min(s_agent, s_farm_worker) \n    penalty_collision = -15 if (d_agent_windmill < 0.5) else 4 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.log(d_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a zoo with the aim of reaching a target while avoiding collisions with feeding stations. Available Information: The agent’s state, target's state and the feeding stations’ states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_feeding station, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_feeding station) \n    penalty_collision = -19 if (d_obs_min < 0.5) else 4 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.log(d_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a zoo with the aim of reaching a target while avoiding collisions with tigers and elephants. Available Information: The agent’s state, target's state and the multiple obstcaled state can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_obs_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.log(d_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a zoo with the aim of reaching a target while avoiding collisions with chemical storage tank, feed rack and gate. Available Information: The agent’s state, target's state and the multiple obstcales: chemical storage tank, feed rack and gate’ states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -19 if (d_obs_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.log(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a zoo with the aim of reaching a target while avoiding collisions with zoo staff. Available Information: The agent’s state, target's state and the zoo staff’s state can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_zoo_staff, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_zoo_staff = calculate_distance_min(s_agent, s_zoo_staff) \n    penalty = -19 if (d_zoo_staff < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.log(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a zoo with the aim of reaching a target while avoiding collisions with cart. Available Information: The agent’s state, target's state and the cart's state can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_cart, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_agent_cart = calculate_distance_min(s_agent, s_cart) \n    penalty = -19 if (d_agent_cart < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.log(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a zoo with the aim of reaching a target while avoiding collisions with storage sheds and glass enclosures placed in the study area. Available Information: The agent’s state, target's state and the multiple obstcales: storage sheds and glass enclosures’ states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_obs_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.log(d_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a zoo with the aim of reaching a target while avoiding collisions with queue stanchions and animals. Available Information: The agent’s state, target's state and the multiple obstcales: queue stanchions and animals’ states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_obs_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.log(d_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a zoo with the aim of reaching a target while avoiding collisions with zoo staff. Available Information: The agent’s state, target's state and the zoo staff’s state can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_agent_staff = calculate_distance_min(s_agent, s_staff) \n    penalty_collision = -19 if (d_agent_staff < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.log(d_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a zoo with the aim of reaching a target while avoiding collisions with elevator. Available Information: The agent’s state, target's state and the elevator’s state can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_elevator, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    dist_elevator = calculate_distance_min(s_agent, s_elevator) \n    penalty_collision = -19 if (dist_elevator < 0.5) else 15 \n    reward_reaching_target = 19 if (d_target < 0.2) else -math.log(d_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a zoo with the aim of reaching a target while avoiding collisions with zoo staff, animals and RA maintenance carts. Available Information: The agent’s state, target's state and the multiple obstcales: zoo staff, animals and RA maintenance carts’ states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_obs_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.log(d_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a zoo with the aim of reaching a target while avoiding collisions with tools and animals. Available Information: The agent’s state, target's state and the multiple obstcales: tool's animal's state can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_obs_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.log(d_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a zoo with the aim of reaching a target. Available Information: The agent’s state, target's state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.log(d_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a swmming pool, a drone must navigate to a target. \n It has access to its own state, the target’s state, and the states of multiple obstacles: ticket counters and swimmers present. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a swmming pool, a drone must navigate to a target. \n It has access to its own state, the target’s state, and the states of cleaning cart racks present. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_cleaning_carts, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_cleaning_carts) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a swmming pool, a drone must navigate to a target. \n It has access to its own state, the target’s state, and the states of saftly board present. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_safety_board, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_safety_board = calculate_distance_min(s_agent, s_safety_board) \n    penalty_collision = -15 if (d_agent_safety_board < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a swmming pool, a drone must navigate to a target. \n It has access to its own state, the target’s state, and the states of queue barrier present. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_queue_barrier, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_queue_barrier) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a swmming pool, a drone must navigate to a target. \n It has access to its own state, the target’s state, and the states of storage rack present. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_storage_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_storage_rack = calculate_distance_min(s_agent, s_storage_rack) \n    penalty_collision = -15 if (d_agent_storage_rack < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a swmming pool, a drone must navigate to a target. \n It has access to its own state, the target’s state, and the states of sale sign present. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_sale_signs, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_sale_signs) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a swmming pool, a drone must navigate to a target. \n It has access to its own state, the target’s state, and the states of floor polisher present. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_floor_polisher, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_floor_polisher = calculate_distance_min(s_agent, s_floor_polisher) \n    penalty_collision = -15 if (d_agent_floor_polishers < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a swmming pool, a drone must navigate to a target. \n It has access to its own state, the target’s state, and the states of support columns present. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_support_columns, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_support_columns) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a swmming pool, a drone must navigate to a target. \n It has access to its own state, the target’s state, and the states of Restocking robots shuttling orders present. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_restocking_robot, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_restocking_robot = calculate_distance_min(s_agent, s_restocking_robot) \n    penalty_collision = -15 if (d_agent_restocking_robot < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a swmming pool, a drone must navigate to a target. \n It has access to its own state, the target’s state, and the states of baby kickboards present. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_kickboard, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_kickboard = calculate_distance_min(s_agent, s_kickboard) \n    penalty_collision = -19 if (d_agent_kickboard < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a swmming pool, a drone must navigate to a target. \n It has access to its own state, the target’s state, and the states of multiple obstcales: lane ropes and sloped entries present. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a swmming pool, a drone must navigate to a target. \n It has access to its own state, the target’s state, and the states of multiple obstacles: towel bins, equipment racks, and supply carts present. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a swmming pool, a drone must navigate to a target. \n It has access to its own state, the target’s state, and the states of lifeguard present. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_lifeguard, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_lifeguards = calculate_distance_min(s_agent, s_lifeguard) \n    penalty = -19 if (d_lifeguard < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.exp(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a swmming pool, a drone must navigate to a target. \n It has access to its own state, the target’s state, and the states of information desk present. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_information_desk, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_information_desk = calculate_distance_min(s_agent, s_information_desk) \n    penalty = -19 if (d_information_desk < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a swmming pool, a drone must navigate to a target. \n It has access to its own state, the target’s state, and the states of multiple obstacles: stacked float mats and glass pool barriers placed in the seating area present. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a swmming pool, a drone must navigate to a target. \n It has access to its own state, the target’s state, and the states of multiple obstacles: queue ropes and swimmers present. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a swmming pool, a drone must navigate to a target. \n It has access to its own state, the target’s state, and the states of lifeguard jacket present. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_lifeguard_jaacket, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lifeguard_jaacket = calculate_distance_min(s_agent, s_lifeguard_jaacket) \n    penalty_collision = -19 if (d_agent_lifeguard_jaacket < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a swmming pool, a drone must navigate to a target. \n It has access to its own state, the target’s state, and the states of stairs present. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_stair, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_stair = calculate_distance_min(s_agent, s_stair) \n    penalty_collision = -19 if (dist_stair < 0.5) else 15 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a swmming pool, a drone must navigate to a target. \n It has access to its own state, the target’s state, and the states of multiple obstacles: lifeguards, swimmers, and information desks present. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a swmming pool, a drone must navigate to a target. \n It has access to its own state, the target’s state, and the states of multiple obstacles: swimmers, diving boards, and lane markers present. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a environment of a swmming pool, a drone must navigate to a target. \n It has access to its own state, the target’s state. \n Write a reward function in Python that rewards reaching the target.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a construction environment, a drone is required to navigate safely and reach target. It can access its own state and the states of site gates or workers as multiple obstacles and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a construction environment, a drone is required to navigate safely and reach target. It can access its own state and the states of wheelbarrows as obstacle and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_wheelbarrows, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_wheelbarrows) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a construction environment, a drone is required to navigate safely and reach target. It can access its own state and the states of warning boards as obstacle and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_warning_boards, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_warning_board = calculate_distance_min(s_agent, s_warning_board) \n    penalty_collision = -15 if (d_agent_warning_board < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a construction environment, a drone is required to navigate safely and reach target. It can access its own state and the states of scaffold frames as obstacle and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_scaffold_frames, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_scaffold_frames) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a construction environment, a drone is required to navigate safely and reach target. It can access its own state and the states of tool cart as obstacle and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_tool_cart, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tool_cart = calculate_distance_min(s_agent, s_tool_cart) \n    penalty_collision = -15 if (d_agent_tool_cart < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a construction environment, a drone is required to navigate safely and reach target. It can access its own state and the states of caution sign as obstacle and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_caution_sign, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_caution_sign = calculate_distance_min(s_agent, s_caution_sign) \n    penalty_collision = -15 if (d_agent_caution_sign < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a construction environment, a drone is required to navigate safely and reach target. It can access its own state and the states of scrubber as obstacle and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_scrubber, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_scrubber = calculate_distance_min(s_agent, s_scrubber) \n    penalty_collision = -15 if (d_agent_scrubber < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a construction environment, a drone is required to navigate safely and reach target. It can access its own state and the states of shoring post as obstacle and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_shoring_post, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_shoring_post) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a construction environment, a drone is required to navigate safely and reach target. It can access its own state and the states of site robots as obstacle and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_site_robots, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_site_robots = calculate_distance_min(s_agent, s_site_robots) \n    penalty_collision = -15 if (d_agent_site_robots < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a construction environment, a drone is required to navigate safely and reach target. It can access its own state and the states of buckets as obstacle and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_bucket, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bucket = calculate_distance_min(s_agent, s_bucket) \n    penalty_collision = -19 if (d_agent_bucket < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a construction environment, a drone is required to navigate safely and reach target. It can access its own state and the states of trenches and access ramps as multiple obstacle and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a construction environment, a drone is required to navigate safely and reach target. It can access its own state and the states of debris bins, material racks, and forklifts as multiple obstacle and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a construction environment, a drone is required to navigate safely and reach target. It can access its own state and the states of crew member as obstacle and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_crew_member, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_crew_member = calculate_distance_min(s_agent, s_crew_member) \n    penalty = -19 if (d_crew_member < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.exp(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a construction environment, a drone is required to navigate safely and reach target. It can access its own state and the states of desk as obstacle and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_desk, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_desk = calculate_distance_min(s_agent, s_desk) \n    penalty = -19 if (d_desk < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.exp(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a construction environment, a drone is required to navigate safely and reach target. It can access its own state and the states of brick pallets and glass panels placed in the rest area as multiple obstacle and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a construction environment, a drone is required to navigate safely and reach target. It can access its own state and the states of temporary fencing and workers as multiple obstacle and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a construction environment, a drone is required to navigate safely and reach target. It can access its own state and the states of crew members as obstacle and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_crew_member, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_crew_member = calculate_distance_min(s_agent, s_crew_member) \n    penalty_collision = -19 if (d_agent_crew_member < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a construction environment, a drone is required to navigate safely and reach target. It can access its own state and the states of stair as obstacle and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_stair, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_stair = calculate_distance_min(s_agent, s_stair) \n    penalty_collision = -19 if (dist_stair < 0.5) else 15 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a construction environment, a drone is required to navigate safely and reach target. It can access its own state and the states of crew members, workers, and site offices as multiple obstacle and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a construction environment, a drone is required to navigate safely and reach target. It can access its own state and the states of workers and ladders as multiple obstacle and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a construction environment, a drone is required to navigate safely and reach target. It can access its own state and the target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a dormitory. \n Duties: Detect reception desk or residents and avoid collision. \n Inputs: Agent's state, multiple obstacles status: reception desk or residents's state obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -17 if (d_agent_obstacles_min < 0.4) else 10 \n    reward_reaching_target = 17 if (d_agent_target < 0.1) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a dormitory. \n Duties: Detect laundry cart and avoid collision. \n Inputs: Agent's state, laundry cart's state obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_laundry_cart, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_laundry_cart) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.4) else 10 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a dormitory. \n Duties: Detect notice boards and avoid collision. \n Inputs: Agent's state, notice boards's state obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_notice_boards, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_notice_boards = calculate_distance_min(s_agent, s_notice_boards) \n    penalty_collision = -10 if (d_agent_notice_boards < 0.4) else 4 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a dormitory. \n Duties: Reach the target and avoid collision. \n Inputs: Agent's state, hallway corner's state obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_hallway_corners, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_hallway_corners) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.4) else 10 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a dormitory. \n Duties: Reach the target and avoid collision. \n Inputs: Agent's state, moving cart's state obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_moving_carts, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_moving_carts = calculate_distance_min(s_agent, s_moving_carts) \n    penalty_collision = -10 if (d_agent_moving_carts < 0.4) else 4 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a dormitory. \n Duties: Reach the target and avoid collision. \n Inputs: Agent's state, door sign's state obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_door_sign, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_door_sign = calculate_distance_min(s_agent, s_door_sign) \n    penalty_collision = -10 if (d_agent_door_sign < 0.4) else 10 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a dormitory. \n Duties: Reach the target and avoid collision. \n Inputs: Agent's state, bed's state obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bed, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bed = calculate_distance_min(s_agent, s_bed) \n    penalty_collision = -10 if (d_agent_floor_polishers < 0.4) else 4 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a dormitory. \n Duties: Reach the target and avoid collision. \n Inputs: Agent's, pillar's state state obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_pillar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_pillar) \n    penalty_collision = -10 if (d_agent_obstacles_min < 0.4) else 10 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a dormitory. \n Duties: Detect Hitting with student and avoid collision. \n Inputs: Agent's state,  with student's state obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_student, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_student = calculate_distance_min(s_agent, s_student) \n    penalty_collision = -10 if (d_agent_student < 0.4) else 4 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a dormitory. \n Duties: Detect laundry baskets and avoid collision. \n Inputs: Agent's state, laundry baskets's state obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_laundry_baskets, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_shopping_basket = calculate_distance_min(s_agent, s_laundry_baskets) \n    penalty_collision = -17 if (d_agent_shopping_basket < 0.4) else 4 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a dormitory. \n Duties: Reach the target and avoid collision with resident and staff. \n Inputs: Agent's state, target and multiple obstacle's state obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -17 if (dist_multiple_obstacles < 0.4) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a dormitory. \n Duties: Detect any trash bin, chair and door and avoid collision. \n Inputs: Agent's state, multiple obstacle: trash bin, chair and door's state obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -17 if (dist_multiple_obstacles < 0.4) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -dist_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a dormitory. \n Duties: Reach the target and avoid collision with dorm staff. \n Inputs: Agent's state, dorm staff's state obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_dorm_staff, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_dorm_staff = calculate_distance_min(s_agent, s_dorm_staff) \n    penalty = -17 if (d_dorm_staff < 0.4) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -d_agent_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a dormitory. \n Duties: Reach the target and avoid collision with desk. \n Inputs: Agent's state, desk's state obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_desk, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_desk = calculate_distance_min(s_agent, s_desk) \n    penalty = -17 if (d_desk < 0.4) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -d_agent_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a dormitory. \n Duties: Detect storage boxes and glass doors placed in the study area and avoid collision. \n Inputs: Agent's state, and multiple obstacle's state: storage boxes and glass doors state obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -17 if (d_agent_obstacles_min < 0.4) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a dormitory. \n Duties: Detect queue stanchions and residents and avoid collision. \n Inputs: Agent's state, and multiple obstacle's state: queue stanchions and residents's state obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -17 if (dist_obstacles_min < 0.4) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a dormitory. \n Duties: Detect dorm staff and avoid collision. \n Inputs: Agent's state, dorm staff's state obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_dorm_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_dorm_staff = calculate_distance_min(s_agent, s_dorm_staff) \n    penalty_collision = -17 if (d_agent_dorm_staff < 0.4) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a dormitory. \n Duties: Detect stairwellwells and avoid collision. \n Inputs: Agent's state, stairwellwells's state obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_stairwell, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_stairwell = calculate_distance_min(s_agent, s_stairwell) \n    penalty_collision = -17 if (dist_stairwell < 0.4) else 10 \n    reward_reaching_target = 17 if (dist_target < 0.1) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a dormitory. \n Duties: Detect dorm staff, residents and RA desks and avoid collision. \n Inputs: Agent's state, dorm staff, and multiple obstacle's state: residents and RA desks's state obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -17 if (d_agent_obstacles_min < 0.4) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a dormitory. \n Duties: Reach the target and avoid collision with door and desk. \n Inputs: Agent's state and multiple obstacle's state obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -17 if (d_agent_obstacles_min < 0.4) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone moves in a dormitory. \n Duties: Reach the target and avoid collision. \n Inputs: Agent's state obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a farmhall with the aim of reaching a target while avoiding collisions with irrigation pump or grazing animals. Available Information: The agent’s state, taregt's state and the multiple obstcales states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -17 if (d_agent_obstacles_min < 0.4) else 10 \n    reward_reaching_target = 17 if (d_agent_target < 0.1) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a farmhall with the aim of reaching a target while avoiding collisions with tractor. Available Information: The agent’s state, taregt's state and the tractor state can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_tractor, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tractor = calculate_distance_min(s_agent, s_tractor) \n    penalty_collision = -10 if (d_agent_tractor < 0.4) else 10 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a farmhall with the aim of reaching a target while avoiding collisions with hay bales. Available Information: The agent’s state, taregt's state and the hay bales states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_hay_bale, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_hay_bale = calculate_distance_min(s_agent, s_hay_bale) \n    penalty_collision = -10 if (d_agent_hay_bale < 0.4) else 4 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a farmhall with the aim of reaching a target while avoiding collisions with canal. Available Information: The agent’s state, taregt's state and the canal states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_canal, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_canal = calculate_distance_min(s_agent, s_canal) \n    penalty_collision = -10 if (d_agent_canal < 0.4) else 10 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a farmhall with the aim of reaching a target while avoiding collisions with multiple harvest wagons. Available Information: The agent’s state, taregt's state and the multiple obstcales states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -17 if (d_agent_obstacles_min < 0.4) else 10 \n    reward_reaching_target = 17 if (d_agent_target < 0.1) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a farmhall with the aim of reaching a target while avoiding collisions with fence post. Available Information: The agent’s state, taregt's state and the fence post states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_fence_post, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_fence_post = calculate_distance_min(s_agent, s_fence_post) \n    penalty_collision = -10 if (d_agent_fence_post < 0.4) else 10 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a farmhall with the aim of reaching a target while avoiding collisions with grian silo. Available Information: The agent’s state, taregt's state and the grain silo states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_grain_silo, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_grain_silo = calculate_distance_min(s_agent, s_grain_silo) \n    penalty_collision = -10 if (d_agent_grain_silo < 0.4) else 4 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a farmhall with the aim of reaching a target while avoiding collisions with windmills. Available Information: The agent’s state, taregt's state and the windmills states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_windmill, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_windmill = calculate_distance_min(s_agent, s_windmill) \n    penalty_collision = -10 if (d_agent_windmill < 0.4) else 10 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a farmhall with the aim of reaching a target while avoiding collisions with Hitting with farm worker. Available Information: The agent’s state, taregt's state and the Hitting with farm worker state can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_farm_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_farm_worker = calculate_distance_min(s_agent, s_farm_worker) \n    penalty_collision = -10 if (d_agent_farm_worker < 0.4) else 4 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a farmhall with the aim of reaching a target while avoiding collisions with feed troughs. Available Information: The agent’s state, taregt's state and the feed troughs states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_feed_trough, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_feed_trough = calculate_distance_min(s_agent, s_feed_trough) \n    penalty_collision = -17 if (d_agent_feed_trough < 0.4) else 4 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a farmhall with the aim of reaching a target while avoiding collisions with tractor and hay ball and tool. Available Information: The agent’s state, taregt's state and the multiple obstcales states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -17 if (dist_multiple_obstacles < 0.4) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a farmhall with the aim of reaching a target while avoiding collisions with fertilizer tank, hay rack and gate. Available Information: The agent’s state, taregt's state and the multiple obstcales states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -17 if (dist_multiple_obstacles < 0.4) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -dist_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a farmhall with the aim of reaching a target while avoiding collisions with farm staff. Available Information: The agent’s state, taregt's state and the farm staff state can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_farm_staff, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_farm_staff = calculate_distance_min(s_agent, s_farm_staff) \n    penalty = -17 if (d_farm_staff < 0.4) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -d_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a farmhall with the aim of reaching a target while avoiding collisions with plow. Available Information: The agent’s state, taregt's state and the plow states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_plow, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_plow = calculate_distance_min(s_agent, s_plow) \n    penalty = -17 if (d_plow < 0.4) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -d_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a farmhall with the aim of reaching a target while avoiding collisions with tool sheds and greenhouse doors placed in the study area. Available Information: The agent’s state, taregt's state and multiple obstcales states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -17 if (d_agent_obstacles_min < 0.4) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a farmhall with the aim of reaching a target while avoiding collisions with queue stanchions and grazing animals. Available Information: The agent’s state, taregt's state and the multiple obstcales states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -17 if (dist_obstacles_min < 0.4) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a farmhall with the aim of reaching a target while avoiding collisions with farm staff. Available Information: The agent’s state, taregt's state and the farm staff state can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_farm_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_farm_staff = calculate_distance_min(s_agent, s_farm_staff) \n    penalty_collision = -17 if (d_agent_farm_staff < 0.4) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a farmhall with the aim of reaching a target while avoiding collisions with grain elevator. Available Information: The agent’s state, taregt's state and the grain elevator state can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_elevator, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_elevator = calculate_distance_min(s_agent, s_elevator) \n    penalty_collision = -17 if (dist_elevator < 0.4) else 10 \n    reward_reaching_target = 17 if (dist_target < 0.1) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a farmhall with the aim of reaching a target while avoiding collisions with farm staff, grazing animals and plows. Available Information: The agent’s state, taregt's state and the multiple obstcales states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -17 if (d_agent_obstacles_min < 0.4) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a farmhall with the aim of reaching a target while avoiding collisions with grazing animals. Available Information: The agent’s state, taregt's state and the multiple obstcales states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -17 if (d_agent_obstacles_min < 0.4) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a farmhall with the aim of reaching a target. Available Information: The agent’s state, taregt's state can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a zoo environment. It should identify fence post or animals as obstacles, prevent collisions, reach target and rely on the agent, target and multiple obstcales states. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_DO not change the chosen and reject change prompt and adjust obstacle according to eaach dataentry , return JSON file new prompt:) \n    penalty_collision = -17 if (d_obs_min < 0.4) else 10 \n    reward_reaching_target = 17 if (d_target < 0.1) else -d_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a zoo environment. It should identify tram as obstacles, prevent collisions, reach target and rely on the agent, target and the tram’s state. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_zoo_tram, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_zoo_tram = calculate_distance_min(s_agent, s_zoo_tram) \n    penalty_collision = -10 if (d_zoo_tram < 0.4) else 10 \n    reward_reaching_target = 20 if (d_target < 0.1) else -d_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a zoo environment. It should identify crates as obstacles, prevent collisions, reach target and rely on the agent, target and the crates states. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_crate, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_crate) \n    penalty_collision = -10 if (d_obs_min < 0.4) else 4 \n    reward_reaching_target = 10 if (d_target < 0.1) else -d_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a zoo environment. It should identify moat as obstacles, prevent collisions, reach target and rely on the agent, target and the moat states. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_moat, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_agent_moat = calculate_distance_min(s_agent, s_moat) \n    penalty_collision = -10 if (d_agent_moat < 0.4) else 10 \n    reward_reaching_target = 20 if (d_target < 0.1) else -d_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a zoo environment. It should identify supply carts and birds as obstacles, prevent collisions, reach target and rely on the agent, target and the multiple obstcales states. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -17 if (d_obs_min < 0.4) else 10 \n    reward_reaching_target = 17 if (d_target < 0.1) else -d_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a zoo environment. It should identify fence post as obstacles, prevent collisions, reach target and rely on the agent, target and the fence post states. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_fence_post, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_agent_fence_post = calculate_distance_min(s_agent, s_fence_post) \n    penalty_collision = -10 if (d_agent_fence_post < 0.4) else 10 \n    reward_reaching_target = 20 if (d_target < 0.1) else -d_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a zoo environment. It should identify aviary netting as obstacles, prevent collisions, reach target and rely on the agent, target and the aviary netting states. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_aviary_netting, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_aviary_netting = calculate_distance_min(s_agent, s_aviary_netting) \n    penalty_collision = -10 if (d_aviary_netting < 0.4) else 4 \n    reward_reaching_target = 10 if (d_target < 0.1) else -d_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a zoo environment. It should identify windmill as obstacles, prevent collisions, reach target and rely on the agent, target and the windmill’s state. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_windmill, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_windmill) \n    penalty_collision = -10 if (d_obs_min < 0.4) else 10 \n    reward_reaching_target = 20 if (d_target < 0.1) else -d_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a zoo environment. It should identify zoo worker as obstacles, prevent collisions, reach target and rely on the agent, target and the zoo worker’s state. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_farm_worker, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_agent_windmill = calculate_distance_min(s_agent, s_farm_worker) \n    penalty_collision = -10 if (d_agent_windmill < 0.4) else 4 \n    reward_reaching_target = 10 if (d_target < 0.1) else -d_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a zoo environment. It should identify feeding stations as obstacles, prevent collisions, reach target and rely on the agent, target and the feeding stations states. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_feeding station, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_feeding station) \n    penalty_collision = -17 if (d_obs_min < 0.4) else 4 \n    reward_reaching_target = 10 if (d_target < 0.1) else -d_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a zoo environment. It should identify animals and staff as obstacles, prevent collisions, reach target and rely on the agent, target and the multiple obstcales states. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -17 if (d_obs_min < 0.4) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -d_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a zoo environment. It should identify chemical storage tank, feed rack and gate as obstacles, prevent collisions, reach target and rely on the agent, target and the multiple obstcales states. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -17 if (d_obs_min < 0.4) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -d_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a zoo environment. It should identify zoo staff as obstacles, prevent collisions, reach target and rely on the agent, target and the zoo staff’s state. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_zoo_staff, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_zoo_staff = calculate_distance_min(s_agent, s_zoo_staff) \n    penalty = -17 if (d_zoo_staff < 0.4) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -d_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a zoo environment. It should identify cart as obstacles, prevent collisions, reach target and rely on the agent, target and the cart states. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_cart, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_agent_cart = calculate_distance_min(s_agent, s_cart) \n    penalty = -17 if (d_agent_cart < 0.4) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -d_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a zoo environment. It should identify storage sheds and glass enclosures placed in the study area as obstacles, prevent collisions, reach target and rely on the agent, target and multiple obstcales states. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -17 if (d_obs_min < 0.4) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -d_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a zoo environment. It should identify queue stanchions and animals as obstacles, prevent collisions, reach target and rely on the agent, target and the multiple obstcales states. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -17 if (d_obs_min < 0.4) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -d_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a zoo environment. It should identify zoo staff as obstacles, prevent collisions, reach target and rely on the agent, target and the zoo staff’s state. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_agent_staff = calculate_distance_min(s_agent, s_staff) \n    penalty_collision = -17 if (d_agent_staff < 0.4) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -d_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a zoo environment. It should identify elevator as obstacles, prevent collisions, reach target and rely on the agent, target and the elevator’s state. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_elevator, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    dist_elevator = calculate_distance_min(s_agent, s_elevator) \n    penalty_collision = -17 if (dist_elevator < 0.4) else 10 \n    reward_reaching_target = 17 if (d_target < 0.1) else -d_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a zoo environment. It should identify zoo staff, animals and RA maintenance carts as obstacles, prevent collisions, reach target and rely on the agent, target and the multiple obstcalesstates. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -17 if (d_obs_min < 0.4) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -d_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a zoo environment. It should identify animals and pillars as obstacles, prevent collisions, reach target and rely on the agent, target and the multiple obstcales states. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -17 if (d_obs_min < 0.4) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -d_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: a drone moves through a zoo environment. It should reach target and rely on the agent, target states. Develop a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 10 if (d_target < 0.1) else -math.log(d_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating in a swmming pool area. The function should encourage target-reaching, penalize collisions with ticket counters and swimmers , and use the available agent, target, and multiple obstacles states.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating in a swmming pool area. The function should encourage target-reaching, penalize collisions with cleaning cart racks, and use the available agent, target, and cleaning cart racks states.",
        "chosen": "def reward(s_agent, s_cleaning_carts, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_cleaning_carts) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating in a swmming pool area. The function should encourage target-reaching, penalize collisions with safety board , and use the available agent, target, and safety board states.",
        "chosen": "def reward(s_agent, s_safety_board, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_safety_board = calculate_distance_min(s_agent, s_safety_board) \n    penalty_collision = -15 if (d_agent_safety_board < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating in a swmming pool area. The function should encourage target-reaching, penalize collisions with queue barrier , and use the available agent, target, and queue barrier states.",
        "chosen": "def reward(s_agent, s_queue_barrier, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_queue_barrier) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating in a swmming pool area. The function should encourage target-reaching, penalize collisions with storage rack , and use the available agent, target, and storage rack states.",
        "chosen": "def reward(s_agent, s_storage_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_storage_rack = calculate_distance_min(s_agent, s_storage_rack) \n    penalty_collision = -15 if (d_agent_storage_rack < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating in a swmming pool area. The function should encourage target-reaching, penalize collisions with sale sign , and use the available agent, target, and sale sign states.",
        "chosen": "def reward(s_agent, s_sale_signs, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_sale_signs) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating in a swmming pool area. The function should encourage target-reaching, penalize collisions with floor polisher , and use the available agent, target, and floor polisher states.",
        "chosen": "def reward(s_agent, s_floor_polisher, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_floor_polisher = calculate_distance_min(s_agent, s_floor_polisher) \n    penalty_collision = -15 if (d_agent_floor_polishers < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating in a swmming pool area. The function should encourage target-reaching, penalize collisions with support columns , and use the available agent, target, and support columns states.",
        "chosen": "def reward(s_agent, s_support_columns, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_support_columns) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating in a swmming pool area. The function should encourage target-reaching, penalize collisions with Restocking robots shuttling orders , and use the available agent, target, and Restocking robots shuttling orders states.",
        "chosen": "def reward(s_agent, s_restocking_robot, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_restocking_robot = calculate_distance_min(s_agent, s_restocking_robot) \n    penalty_collision = -15 if (d_agent_restocking_robot < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating in a swmming pool area. The function should encourage target-reaching, penalize collisions with baby kickboards , and use the available agent, target, and baby kickboards states.",
        "chosen": "def reward(s_agent, s_kickboard, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_kickboard = calculate_distance_min(s_agent, s_kickboard) \n    penalty_collision = -19 if (d_agent_kickboard < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating in a swmming pool area. The function should encourage target-reaching, penalize collisions with lane ropes and sloped entries , and use the available agent, target, and multiple obstacles states.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating in a swmming pool area. The function should encourage target-reaching, penalize collisions with towel bins, equipment racks, and supply carts , and use the available agent, target, and multiple obstacles states.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating in a swmming pool area. The function should encourage target-reaching, penalize collisions with lifeguard , and use the available agent, target, and lifeguard states.",
        "chosen": "def reward(s_agent, s_lifeguard, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_lifeguards = calculate_distance_min(s_agent, s_lifeguard) \n    penalty = -19 if (d_lifeguard < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.exp(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating in a swmming pool area. The function should encourage target-reaching, penalize collisions with information desk , and use the available agent, target, and information desk states.",
        "chosen": "def reward(s_agent, s_information_desk, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_information_desk = calculate_distance_min(s_agent, s_information_desk) \n    penalty = -19 if (d_information_desk < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating in a swmming pool area. The function should encourage target-reaching, penalize collisions with stacked float mats and glass pool barriers placed in the seating area , and use the available agent, target, and multiple obstacles states.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating in a swmming pool area. The function should encourage target-reaching, penalize collisions with queue ropes and swimmers , and use the available agent, target, and multiple obstacles states.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating in a swmming pool area. The function should encourage target-reaching, penalize collisions with lifeguard jacket , and use the available agent, target, and lifeguard jacket states.",
        "chosen": "def reward(s_agent, s_lifeguard_jaacket, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lifeguard_jaacket = calculate_distance_min(s_agent, s_lifeguard_jaacket) \n    penalty_collision = -19 if (d_agent_lifeguard_jaacket < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating in a swmming pool area. The function should encourage target-reaching, penalize collisions with stairs , and use the available agent, target, and stairs states.",
        "chosen": "def reward(s_agent, s_stair, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_stair = calculate_distance_min(s_agent, s_stair) \n    penalty_collision = -19 if (dist_stair < 0.5) else 15 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating in a swmming pool area. The function should encourage target-reaching, penalize collisions with lifeguards, swimmers, and information desks , and use the available agent, target, and multiple obstacles states.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating in a swmming pool area. The function should encourage target-reaching, penalize collisions with swimmers, diving boards, and lane markers , and use the available agent, target, and multiple obstacles states.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Write a Python reward function for an indoor drone navigating in a swmming pool area. The function should encourage target-reaching and use the available agent and target states.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a construction frame, where it must avoid colliding with the site gates or workers and reach the target. The drone’s state, target's state and the information of the multiple obstacles: site gates or workers are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a construction frame, where it must avoid colliding with the wheelbarrows and reach the target. The drone’s state, target's state and the information of the wheelbarrows are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_wheelbarrows, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_wheelbarrows) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a construction frame, where it must avoid colliding with the warning boards and reach the target. The drone’s state, target's state and the information of the warning boards are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_warning_boards, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_warning_board = calculate_distance_min(s_agent, s_warning_board) \n    penalty_collision = -15 if (d_agent_warning_board < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a construction frame, where it must avoid colliding with the scaffold frames and reach the target. The drone’s state, target's state and the information of the scaffold frames are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_scaffold_frames, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_scaffold_frames) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a construction frame, where it must avoid colliding with the tool cart and reach the target. The drone’s state, target's state and the information of the tool cart are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_tool_cart, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tool_cart = calculate_distance_min(s_agent, s_tool_cart) \n    penalty_collision = -15 if (d_agent_tool_cart < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a construction frame, where it must avoid colliding with the caution sign and reach the target. The drone’s state, target's state and the information of the caution sign are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_caution_sign, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_caution_sign = calculate_distance_min(s_agent, s_caution_sign) \n    penalty_collision = -15 if (d_agent_caution_sign < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a construction frame, where it must avoid colliding with the scrubber and reach the target. The drone’s state, target's state and the information of the scrubber are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_scrubber, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_scrubber = calculate_distance_min(s_agent, s_scrubber) \n    penalty_collision = -15 if (d_agent_scrubber < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a construction frame, where it must avoid colliding with the shoring post and reach the target. The drone’s state, target's state and the information of the shoring post are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_shoring_post, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_shoring_post) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a construction frame, where it must avoid colliding with the site robots and reach the target. The drone’s state, target's state and the information of the site robots are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_site_robots, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_site_robots = calculate_distance_min(s_agent, s_site_robots) \n    penalty_collision = -15 if (d_agent_site_robots < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a construction frame, where it must avoid colliding with the buckets and reach the target. The drone’s state, target's state and the information of the buckets are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_bucket, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bucket = calculate_distance_min(s_agent, s_bucket) \n    penalty_collision = -19 if (d_agent_bucket < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a construction frame, where it must avoid colliding with the trenches and access ramps and reach the target. The drone’s state, target's state and the information of the multiple obstacle are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a construction frame, where it must avoid colliding with the debris bins, material racks, and forklifts and reach the target. The drone’s state, target's state and the information of the multiple obstacle are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a construction frame, where it must avoid colliding with the crew member and reach the target. The drone’s state, target's state and the information of the crew member are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_crew_member, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_crew_member = calculate_distance_min(s_agent, s_crew_member) \n    penalty = -19 if (d_crew_member < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.exp(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a construction frame, where it must avoid colliding with the desk and reach the target. The drone’s state, target's state and the information of the desk are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_desk, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_desk = calculate_distance_min(s_agent, s_desk) \n    penalty = -19 if (d_desk < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.exp(d_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a construction frame, where it must avoid colliding with the brick pallets and glass panels placed in the rest area and reach the target. The drone’s state, target's state and the information of the multiple obstacle are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a construction frame, where it must avoid colliding with the temporary fencing and workers and reach the target. The drone’s state, target's state and the information of the multiple obstacle are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a construction frame, where it must avoid colliding with the crew members and reach the target. The drone’s state, target's state and the information of the crew members are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_crew_member, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_crew_member = calculate_distance_min(s_agent, s_crew_member) \n    penalty_collision = -19 if (d_agent_crew_member < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a construction frame, where it must avoid colliding with the stair and reach the target. The drone’s state, target's state and the information of the stair are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_stair, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_stair = calculate_distance_min(s_agent, s_stair) \n    penalty_collision = -19 if (dist_stair < 0.5) else 15 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a construction frame, where it must avoid colliding with the crew members, workers, and site offices and reach the target. The drone’s state, target's state and the information of the multiple obstacle are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a construction frame, where it must avoid colliding with the workers and ladders and reach the target. The drone’s state, target's state and the information of the multiple obstacle are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a construction frame, where it must reach the target. The drone’s state and target's state are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor dorm with a drone, where the drone must reach the target while detecting desk as multiple obstacles and avoiding collisions, assuming the agent’s state, desk's states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor dorm with a drone, where the drone must reach the target while detecting laundry carts as obstacles and avoiding collisions, assuming the agent’s state, the laundry carts states and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_laundry_cart, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_laundry_cart) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor dorm with a drone, where the drone must reach the target while detecting notice board as obstacles and avoiding collisions, assuming the agent’s state, the notice board states and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_notice_boards, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_notice_boards = calculate_distance_min(s_agent, s_notice_boards) \n    penalty_collision = -15 if (d_agent_notice_boards < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor dorm with a drone, where the drone must reach the target while detecting hallway corner as obstacles and avoiding collisions, assuming the agent’s state, the hallway corner states and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_hallway_corners, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_hallway_corners) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor dorm with a drone, where the drone must reach the target while detecting moving carts as obstacles and avoiding collisions, assuming the agent’s state, the moving carts states and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_moving_carts, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_moving_carts = calculate_distance_min(s_agent, s_moving_carts) \n    penalty_collision = -15 if (d_agent_moving_carts < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor dorm with a drone, where the drone must reach the target while detecting door sign as obstacles and avoiding collisions, assuming the agent’s state, the door sign states and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_door_sign, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_door_sign = calculate_distance_min(s_agent, s_door_sign) \n    penalty_collision = -15 if (d_agent_door_sign < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor dorm with a drone, where the drone must reach the target while detecting bed as obstacles and avoiding collisions, assuming the agent’s state, the bed states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bed, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bed = calculate_distance_min(s_agent, s_bed) \n    penalty_collision = -15 if (d_agent_floor_polishers < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor dorm with a drone, where the drone must reach the target while detecting pillar as obstacles and avoiding collisions, assuming the agent’s state, the pillar states and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_pillar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_pillar) \n    penalty_collision = -15 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor dorm with a drone, where the drone must reach the target while detecting student as obstacles and avoiding collisions, assuming the agent’s state, the student states and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_student, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_student = calculate_distance_min(s_agent, s_student) \n    penalty_collision = -15 if (d_agent_student < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor dorm with a drone, where the drone must reach the target while detecting laundry basket as obstacles and avoiding collisions, assuming the agent’s state, the laundry basket states and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_laundry_baskets, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_shopping_basket = calculate_distance_min(s_agent, s_laundry_baskets) \n    penalty_collision = -19 if (d_agent_shopping_basket < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor dorm with a drone, where the drone must reach the target while detecting desk as obstacles and avoiding collisions, assuming the agent’s state, desk's states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor dorm with a drone, where the drone must reach the target while detecting trash bin, chair as multiple obstacles and avoiding collisions, assuming the agent’s state, the trash bin, chair states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -19 if (dist_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor dorm with a drone, where the drone must reach the target while detecting dorm staff as obstacles and avoiding collisions, assuming the agent’s state, the dorm staff states and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_dorm_staff, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_dorm_staff = calculate_distance_min(s_agent, s_dorm_staff) \n    penalty = -19 if (d_dorm_staff < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor dorm with a drone, where the drone must reach the target while detecting desk as obstacles and avoiding collisions, assuming the agent’s state, the desk states and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_desk, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_desk = calculate_distance_min(s_agent, s_desk) \n    penalty = -19 if (d_desk < 0.5) else 15 \n    reward_reaching_target = 15 if (d_target < 0.2) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor dorm with a drone, where the drone must reach the target while detecting boxes as multiple obstacles and avoiding collisions, assuming the agent’s state, the boxes states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor dorm with a drone, where the drone must reach the target while detecting pqueue stanchion as multiple obstacles and avoiding collisions, assuming the agent’s state, pqueue stanchion's states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor dorm with a drone, where the drone must reach the target while detecting staff as obstacles and avoiding collisions, assuming the agent’s state, the staff states and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_staff = calculate_distance_min(s_agent, s_staff) \n    penalty_collision = -19 if (d_agent_staff < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor dorm with a drone, where the drone must reach the target while detecting stairwell as obstacles and avoiding collisions, assuming the agent’s state, the stairwell states and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_stairwell, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_stairwell = calculate_distance_min(s_agent, s_stairwell) \n    penalty_collision = -19 if (dist_stairwell < 0.5) else 15 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor dorm with a drone, where the drone must reach the target while detecting RA desk , staff as multiple obstacles and avoiding collisions, assuming the agent’s state, the RA desk , staff states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor dorm with a drone, where the drone must reach the target while detecting pillar and students as multiple obstacles and avoiding collisions, assuming the agent’s state, pillar's states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to explore the indoor dorm with a drone, where the drone must reach the target, assuming the agent’s state, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a farmland. It must avoid colliding with irrigation pump or grazing animals. The states of the agent, the vending area lanes, and the grazing animals are known. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -17 if (d_agent_obstacles_min < 0.4) else 10 \n    reward_reaching_target = 17 if (d_agent_target < 0.1) else -d_agent_target*2.5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a busy farmland concourse. It must avoid colliding with tractor. The states of the agent, the tractor are known. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_tractor, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tractor = calculate_distance_min(s_agent, s_tractor) \n    penalty_collision = -10 if (d_agent_tractor < 0.4) else 10 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -d_agent_target*2.5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a farmland common room. \n Objectives: It must avoid colliding with hay bales. \n Conditions: States of the agent and the hay bale are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_hay_bale, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_hay_bale = calculate_distance_min(s_agent, s_hay_bale) \n    penalty_collision = -10 if (d_agent_hay_bale < 0.4) else 4 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -d_agent_target*2.5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a farm hallway, the drone must maneuver around irrigation canals. All entity states (agent, canal) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_canal, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_canal = calculate_distance_min(s_agent, s_canal) \n    penalty_collision = -10 if (d_agent_canal < 0.4) else 10 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -d_agent_target*2.5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a farmland atrium. \n Requirement: Avoid collisions with harvest wagons. \n Given: The states for the agent and many wagons are known. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -17 if (d_agent_obstacles_min < 0.4) else 10 \n    reward_reaching_target = 17 if (d_agent_target < 0.1) else -d_agent_target*2.5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone operates near a farm entrance during an event. So not collide with fence post. The states of the agent, the fence post are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_fence_post, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_fence_post = calculate_distance_min(s_agent, s_fence_post) \n    penalty_collision = -10 if (d_agent_fence_post < 0.4) else 10 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -d_agent_target*2.5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in a farmland. \n Objectives: Do not collide hit with grain silo, \n Conditions: States of the agent, grain silo are available. \n Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_grain_silo, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_grain_silo = calculate_distance_min(s_agent, s_grain_silo) \n    penalty_collision = -10 if (d_agent_grain_silo < 0.4) else 4 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -d_agent_target*2.5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a farmland atrium, the drone must steer clear of windmills. All states (agent, windmill) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_windmill, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_windmill = calculate_distance_min(s_agent, s_windmill) \n    penalty_collision = -10 if (d_agent_windmill < 0.4) else 10 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -d_agent_target*2.5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: \n Environment: farmland lounge. \n Avoid: Hitting with farm worker. \n Known: Agent, farm worker states. \n Create the Python reward function.",
        "chosen": "def reward(s_agent, s_farm_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_farm_worker = calculate_distance_min(s_agent, s_farm_worker) \n    penalty_collision = -10 if (d_agent_farm_worker < 0.4) else 4 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -d_agent_target*2.5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a farmland. \n Objectives: It must avoid colliding with feed troughs. \n Conditions: The states of the agent and the feed troughs are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_feed_trough, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_feed_trough = calculate_distance_min(s_agent, s_feed_trough) \n    penalty_collision = -17 if (d_agent_feed_trough < 0.4) else 4 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -d_agent_target*2.5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through a farmland. \n Objectives: Avoid collision with barn door and tarp. \n Conditions: The states of the agent, barn door and tarp are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -17 if (dist_multiple_obstacles < 0.4) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -dist_target*2.5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating a farmland. \n Objectives: It must avoid colliding with any fertilizer tank, hay rack and gate. \n Conditions: Agent's and all obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -17 if (dist_multiple_obstacles < 0.4) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -dist_target*2.5 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through a farmland. \n Objectives: Do not collide with any farm staff. \n Conditions: The states of the agent and the farm staff are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_farm_staff, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_farm_staff = calculate_distance_min(s_agent, s_farm_staff) \n    penalty = -17 if (d_farm_staff < 0.4) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -d_target*2.5 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a farmland and must avoid contact with any plow. The states of the drone and the plow are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_plow, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_plow = calculate_distance_min(s_agent, s_plow) \n    penalty = -17 if (d_plow < 0.4) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -d_target*2.5 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in a farmland. It must avoid colliding with tool sheds and greenhouse doors placed in the study area. The states of the drone and multiple boxes are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -17 if (d_agent_obstacles_min < 0.4) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -d_agent_target*2.5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in a farmland. It must avoid colliding with queue stanchions and grazing animals. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -17 if (dist_obstacles_min < 0.4) else 10 \n    reward_reaching_target = 10 if (dist_target < 0.1) else -dist_target*2.5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in a farmland. It must avoid colliding with farm staff. Agent's state, target's state and farm staff's state are accessible. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_farm_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_farm_staff = calculate_distance_min(s_agent, s_farm_staff) \n    penalty_collision = -17 if (d_agent_farm_staff < 0.4) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -d_agent_target*2.5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a farmland. \n Goal: It must avoid colliding with grain elevator. \n Assumptions: The states of the agent and the elevator are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_elevator, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_elevator = calculate_distance_min(s_agent, s_elevator) \n    penalty_collision = -17 if (dist_elevator < 0.4) else 10 \n    reward_reaching_target = 17 if (dist_target < 0.1) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a farmland. \n Goal: It must avoid colliding with farm staff, grazing animals and RA plows. \n Assumptions: The Agent's state, target's state and farm staff's state, plow's state and grazing animals' states are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -17 if (d_agent_obstacles_min < 0.4) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -d_agent_target*2.5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a farmland. \n Goal: Detect grazing animals and pillars and avoid fence corners. \n Assumptions: The states of the agent and gates, coops and hedges state are availalbe. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -17 if (d_agent_obstacles_min < 0.4) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -d_agent_target*2.5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating a farmland. \n Goal: Drone target is to reach the target. \n Assumptions: The states of the agent and the target are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 10 if (d_agent_target < 0.1) else -math.log(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a zoo environment, a drone is required to navigate safely and reach target. It can access its own state and the states of fence post or animals as obstacles and target’s state. Write function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -17 if (d_obs_min < 0.4) else 10 \n    reward_reaching_target = 17 if (d_target < 0.1) else -d_target*2.5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a zoo environment, a drone is required to navigate safely and reach target. It can access its own state and the states of tram as obstacle and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_zoo_tram, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_zoo_tram = calculate_distance_min(s_agent, s_zoo_tram) \n    penalty_collision = -10 if (d_zoo_tram < 0.4) else 10 \n    reward_reaching_target = 20 if (d_target < 0.1) else -d_target*2.5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a zoo environment, a drone is required to navigate safely and reach target. It can access its own state and the states of crates as obstacles and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_crate, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_crate) \n    penalty_collision = -10 if (d_obs_min < 0.4) else 4 \n    reward_reaching_target = 10 if (d_target < 0.1) else -d_target*2.5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a zoo environment, a drone is required to navigate safely and reach target. It can access its own state and the states of moat as obstacles and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_moat, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_agent_moat = calculate_distance_min(s_agent, s_moat) \n    penalty_collision = -10 if (d_agent_moat < 0.4) else 10 \n    reward_reaching_target = 20 if (d_target < 0.1) else -d_target*2.5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a zoo environment, a drone is required to navigate safely and reach target. It can access its own state and the states of supply carts and bench as obstacles and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -17 if (d_obs_min < 0.4) else 10 \n    reward_reaching_target = 17 if (d_target < 0.1) else -d_target*2.5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a zoo environment, a drone is required to navigate safely and reach target. It can access its own state and the states of fence post as obstacles and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_fence_post, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_agent_fence_post = calculate_distance_min(s_agent, s_fence_post) \n    penalty_collision = -10 if (d_agent_fence_post < 0.4) else 10 \n    reward_reaching_target = 20 if (d_target < 0.1) else -d_target*2.5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a zoo environment, a drone is required to navigate safely and reach target. It can access its own state and the states of aviary netting as obstacles and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_aviary_netting, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_aviary_netting = calculate_distance_min(s_agent, s_aviary_netting) \n    penalty_collision = -10 if (d_aviary_netting < 0.4) else 4 \n    reward_reaching_target = 10 if (d_target < 0.1) else -d_target*2.5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a zoo environment, a drone is required to navigate safely and reach target. It can access its own state and the states of windmill as obstacle and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_windmill, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_windmill) \n    penalty_collision = -10 if (d_obs_min < 0.4) else 10 \n    reward_reaching_target = 20 if (d_target < 0.1) else -d_target*2.5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a zoo environment, a drone is required to navigate safely and reach target. It can access its own state and the states of zoo worker as obstacle and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_zoo_worker, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_agent_windmill = calculate_distance_min(s_agent, s_zoo_worker) \n    penalty_collision = -10 if (d_agent_windmill < 0.4) else 4 \n    reward_reaching_target = 10 if (d_target < 0.1) else -d_target*2.5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a zoo environment, a drone is required to navigate safely and reach target. It can access its own state and the states of feeding stations as obstacles and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_feeding_station, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_feeding_station) \n    penalty_collision = -17 if (d_obs_min < 0.4) else 4 \n    reward_reaching_target = 10 if (d_target < 0.1) else -d_target*2.5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a zoo environment, a drone is required to navigate safely and reach target. It can access its own state and the states of animals, fences as obstacles and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -17 if (d_obs_min < 0.4) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -d_target*2.5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a zoo environment, a drone is required to navigate safely and reach target. It can access its own state and the states of chemical storage tank, feed rack and gate as obstacles and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -17 if (d_obs_min < 0.4) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -d_target*2.5 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a zoo environment, a drone is required to navigate safely and reach target. It can access its own state and the states of zoo staff as obstacle and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_zoo_staff, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_zoo_staff = calculate_distance_min(s_agent, s_zoo_staff) \n    penalty = -17 if (d_zoo_staff < 0.4) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -d_target*2.5 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a zoo environment, a drone is required to navigate safely and reach target. It can access its own state and the states of cart as obstacles and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_cart, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_agent_cart = calculate_distance_min(s_agent, s_cart) \n    penalty = -17 if (d_agent_cart < 0.4) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -d_target*2.5 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a zoo environment, a drone is required to navigate safely and reach target. It can access its own state and the states of storage sheds and glass enclosures placed in the study area as obstacles and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -17 if (d_obs_min < 0.4) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -d_target*2.5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a zoo environment, a drone is required to navigate safely and reach target. It can access its own state and the states of queue stanchions and animals as obstacles and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -17 if (d_obs_min < 0.4) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -d_target*2.5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a zoo environment, a drone is required to navigate safely and reach target. It can access its own state and the states of zoo staff as obstacle and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_agent_staff = calculate_distance_min(s_agent, s_staff) \n    penalty_collision = -17 if (d_agent_staff < 0.4) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -d_target*2.5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a zoo environment, a drone is required to navigate safely and reach target. It can access its own state and the states of elevator as obstacle and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_elevator, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    dist_elevator = calculate_distance_min(s_agent, s_elevator) \n    penalty_collision = -17 if (dist_elevator < 0.4) else 10 \n    reward_reaching_target = 17 if (d_target < 0.1) else -d_target*2.5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a zoo environment, a drone is required to navigate safely and reach target. It can access its own state and the states of zoo staff, animals and RA maintenance carts as obstacles and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -17 if (d_obs_min < 0.4) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -d_target*2.5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a zoo environment, a drone is required to navigate safely and reach target. It can access its own state and the states of animals and pillars as obstacles and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -17 if (d_obs_min < 0.4) else 10 \n    reward_reaching_target = 10 if (d_target < 0.1) else -d_target*2.5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a zoo environment, a drone is required to navigate safely and reach target. It can access its own state and target’s state. Write a reward function in Python supporting safe movement.",
        "chosen": "def reward(s_agent, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 10 if (d_target < 0.1) else -math.log(d_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a market street, where it must avoid colliding with lamp post or pedestrians and reach the target. The drone’s state, target's state and the information of multiple obstacles: lamp post or pedestrians are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_obs_min < 0.5) else 15 \n    reward_reaching_target = -19 if (d_agent_target < 0.2) else d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a market street, where it must avoid colliding with tram and reach the target. The drone’s state, target's state and the information of tram are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_tram, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tram = calculate_distance_min(s_agent, s_tram) \n    penalty_collision = -15 if (d_agent_tram < 0.5) else 15 \n    reward_reaching_target = -20 if (d_agent_target < 0.2) else d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a market street, where it must avoid colliding with vendor carts and reach the target. The drone’s state, target's state and the information of vendor carts are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_crate, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_carte = calculate_distance_min(s_agent, s_crate) \n    penalty_collision = -15 if (d_agent_carte < 0.5) else 4 \n    reward_reaching_target = -15 if (d_agent_target < 0.2) else d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a market street, where it must avoid colliding with obstacles and reach the target. The drone’s state, target's state and the information of construction barrier are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_construction_barrier, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_construction_barrier = calculate_distance_min(s_agent, s_moat) \n    penalty_collision = -15 if (d_agent_construction_barrier < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a market street, where it must avoid colliding with multiple delivery carts and reach the target. The drone’s state, target's state and the information of multiple delivery carts are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_multiple_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a market street, where it must avoid colliding with lamp post and reach the target. The drone’s state, target's state and the information of lamp post are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_lamp_post, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lamp_post = calculate_distance_min(s_agent, s_fence_post) \n    penalty_collision = -15 if (d_agent_lamp_post < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a market street, where it must avoid colliding with banner and reach the target. The drone’s state, target's state and the information of banner are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_banner, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_banner = calculate_distance_min(s_agent, s_banner) \n    penalty_collision = -15 if (d_agent_banner < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a market street, where it must avoid colliding with traffic light poles and reach the target. The drone’s state, target's state and the information of traffic light poles are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_traffic_light, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_traffic_light = calculate_distance_min(s_agent, s_traffic_light) \n    penalty_collision = -15 if (d_agent_traffic_light < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a market street, where it must avoid colliding with Market Street worker and reach the target. The drone’s state, target's state and the information of Market Street worker are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_street_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_street_worker = calculate_distance_min(s_agent, s_street_worker) \n    penalty_collision = -15 if (d_agent_street_worker < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a market street, where it must avoid colliding with kiosk and reach the target. The drone’s state, target's state and the information of kiosk are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_kiosk, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_kiosk = calculate_distance_min(s_agent, s_kiosk) \n    penalty_collision = -19 if (d_agent_kiosk < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a market street, where it must avoid colliding with customers and vegatable and reach the target. The drone’s state, target's state and the information of multiple obstacles: customers and vegatable are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a market street, where it must avoid colliding with propane tank, sidewalk sign and gate and reach the target. The drone’s state, target's state and the information of multiple obstacles: propane tank, sidewalk sign and gate are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -19 if (dagent_multiple_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a market street, where it must avoid colliding with workers and reach the target. The drone’s state, target's state and the information of workers are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_market_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_market_staff = calculate_distance_min(s_agent, s_market_staff) \n    penalty = -19 if (d_agent_market_staff < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a market street, where it must avoid colliding with obstacles and reach the target. The drone’s state, target's state and the information of obstacles are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_service_vehicle, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_service_vehicle = calculate_distance_min(s_agent, s_service_vehicle) \n    penalty = -19 if (d_agent_service_vehicle < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a market street, where it must avoid colliding with storefronts and glass doors placed in the study area and reach the target. The drone’s state, target's state and the information of multiple obstacles: storefronts and glass doors placed in the study area are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a market street, where it must avoid colliding with queue stanchions and pedestrians and reach the target. The drone’s state, target's state and the information of multiple obstacles: queue stanchions and pedestrians are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a market street, where it must avoid colliding with workers and reach the target. The drone’s state, target's state and the information of workers are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_staff = calculate_distance_min(s_agent, s_staff) \n    penalty_collision = -19 if (d_agent_staff < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a market street, where it must avoid colliding with elevator entrance and reach the target. The drone’s state, target's state and the information of elevator entrance are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_elevator, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_elevator = calculate_distance_min(s_agent, s_elevator) \n    penalty_collision = -19 if (d_agent_elevator < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a market street, where it must avoid colliding with workers, pedestrians and RA maintenance carts and reach the target. The drone’s state, target's state and the information of multiple obstacles: workers, pedestrians and RA maintenance carts are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a market street, where it must avoid colliding with pedestrians and pillars and reach the target. The drone’s state, target's state and the information of multiple obstacles: pedestrians and pillars are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is tasked with navigating indoors within a market street, where it must avoid colliding with obstacles and reach the target. The drone’s state, target's state and the information of obstacles are available.  Create a Python code for reward function.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Film Studio. It must avoid colliding with light stand or extras. The states of the agent, the set partitions, and the extras are known. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a busy Film Studio. It must avoid colliding with Film Studio camera dolly. The states of the agent, the Film Studio camera dolly are known. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_camera, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_camera = calculate_distance_min(s_agent, s_camera) \n    penalty_collision = -15 if (d_agent_camera < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Film Studio common room. \n Objectives: It must avoid colliding with prop carts. \n Conditions: States of the agent and the prop cart are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_prop_cart, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_prop_cart = calculate_distance_min(s_agent, s_prop_cart) \n    penalty_collision = -15 if (d_agent_prop_cart < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a backlot alley, the drone must maneuver around set barricades. All entity states (agent, set barricade) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_set_barrier, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_set_barrier = calculate_distance_min(s_agent, s_set_barrier) \n    penalty_collision = -15 if (d_agent_set_barrier < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a soundstage. \n Requirement: Avoid collisions with equipment carts. \n Given: The states for the agent and many carts are known. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacle_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone operates near a soundstage entrance during an event. So not collide with light stand. The states of the agent, the light stand are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_light_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_light_stand = calculate_distance_min(s_agent, s_light_stand) \n    penalty_collision = -15 if (d_agent_light_stand < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Film Studio. \n Objectives: Do not collide hit with overhead cable rigging, \n Conditions: States of the agent, overhead cable rigging are available. \n Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_cable, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cable = calculate_distance_min(s_agent, s_cable) \n    penalty_collision = -15 if (d_agent_cable < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a soundstage, the drone must steer clear of lighting towers. All states (agent, lighting tower) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_lighting_tower, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lighting_tower = calculate_distance_min(s_agent, s_lighting_tower) \n    penalty_collision = -15 if (d_agent_lighting_tower < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: \n Environment: stage-side area. \n Avoid: Hitting with Film Studio worker. \n Known: Agent, Film Studio worker states. \n Create the Python reward function.",
        "chosen": "def reward(s_agent, s_studio_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_studio_worker = calculate_distance_min(s_agent, s_studio_worker) \n    penalty_collision = -15 if (d_agent_studio_worker < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Film Studio. \n Objectives: It must avoid colliding with set dressing. \n Conditions: The states of the agent and the set dressing are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_set, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_set = calculate_distance_min(s_agent, s_set) \n    penalty_collision = -19 if (d_agent_set < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Film Studio. \n Objectives: Avoid collision with lighting scaffold and blackout drape. \n Conditions: The states of the agent, lighting scaffold and blackout drape are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacle < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Film Studio. \n Objectives: It must avoid colliding with any gas cylinder, C-stand and stage door. \n Conditions: Agent's and all obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -19 if (d_obs_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Film Studio. \n Objectives: Do not collide with any Film Studio staff. \n Conditions: The states of the agent and the Film Studio staff are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_studio_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_studio_staff = calculate_distance_min(s_agent, s_studio_staff) \n    penalty = -19 if (d_agent_studio_staff < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Film Studio and must avoid contact with any grip truck. The states of the drone and the grip truck are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_grip_truck, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_grip_truck = calculate_distance_min(s_agent, s_grip_truck) \n    penalty = -19 if (d_agent_grip_truck < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Film Studio. It must avoid colliding with set walls and glass panels placed in the study area. The states of the drone and multiple boxes are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Film Studio. It must avoid colliding with queue stanchions and extras. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Film Studio. It must avoid colliding with Film Studio staff. Agent's state, target's state and Film Studio staff's state are accessible. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_staff = calculate_distance_min(s_agent, s_staff) \n    penalty_collision = -19 if (d_agent_staff < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Film Studio. \n Goal: It must avoid colliding with loading dock entrance. \n Assumptions: The states of the agent and the lift are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_lift, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lift = calculate_distance_min(s_agent, s_lift) \n    penalty_collision = -19 if (d_agent_lift < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Film Studio. \n Goal: It must avoid colliding with Film Studio staff, extras and RA maintenance carts. \n Assumptions: The Agent's state, target's state and Film Studio staff's state, grip truck's state and extras' states are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Film Studio. \n Goal: Detect extras and pillars and avoid set corners. \n Assumptions: The states of the agent and gates, set dressing and barricades state are availalbe. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Film Studio. \n Goal: Drone target is to reach the target. \n Assumptions: The states of the agent and the target are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Mountain Cliff. It must avoid colliding with rock outcrop or mountain goats. The states of the agent, the fence, and the mountain goats are known. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a busy Mountain Cliff. It must avoid colliding with Mountain Cliff tram. The states of the agent, the Mountain Cliff tram are known. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_tram, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tram = calculate_distance_min(s_agent, s_tram) \n    penalty_collision = -15 if (d_agent_tram < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through Mountain Cliff. \n Objectives: It must avoid colliding with talus boulders. \n Conditions: States of the agent and the talus boulder are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_talus_boulder, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_talus_boulder = calculate_distance_min(s_agent, s_talus_boulder) \n    penalty_collision = -15 if (d_agent_talus_boulder < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a narrow ledge, the drone must maneuver around rockfall barriers. All entity states (agent, rockfall barrier) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_rockfall_barrier, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rockfall_barrier = calculate_distance_min(s_agent, s_rockfall_barrier) \n    penalty_collision = -15 if (d_agent_rockfall_barrier < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a cliff terrace. \n Requirement: Avoid collisions with scree slopes. \n Given: The states for the agent and many carts are known. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone operates near a trail junction during an event. So not collide with rock outcrop. The states of the agent, the rock outcrop are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_rock_outcrop, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rock_outcrop = calculate_distance_min(s_agent, s_rock_outcrop) \n    penalty_collision = -15 if (d_agent_rock_outcrop < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Mountain Cliff. \n Objectives: Do not collide hit with overhanging ledges, \n Conditions: States of the agent, overhanging ledge are available. \n Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_ledge, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ledge = calculate_distance_min(s_agent, s_ledge) \n    penalty_collision = -15 if (d_agent_ledge < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a cliff terrace, the drone must steer clear of rock spires. All states (agent, rock spire) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_rock_spire, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rock_spire = calculate_distance_min(s_agent, s_rock_spire) \n    penalty_collision = -15 if (d_agent_rock_spire < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: \n Environment: ridge area. \n Avoid: Hitting with Mountain Cliff worker. \n Known: Agent, Mountain Cliff worker states. \n Create the Python reward function.",
        "chosen": "def reward(s_agent, s_cliff_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cliff_worker = calculate_distance_min(s_agent, s_cliff_worker) \n    penalty_collision = -15 if (d_agent_cliff_worker < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Mountain Cliff. \n Objectives: It must avoid colliding with low-hanging branches. \n Conditions: The states of the agent and the branche are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_branche, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_branche = calculate_distance_min(s_agent, s_branche) \n    penalty_collision = -19 if (d_agent_branche < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Mountain Cliff. \n Objectives: Avoid collision with ice overhang and snow cornice. \n Conditions: The states of the agent, ice overhang and snow cornice are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Mountain Cliff. \n Objectives: It must avoid colliding with any boulder, tree root and rock arch. \n Conditions: Agent's and all obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -19 if (d_agent_multiple_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Mountain Cliff. \n Objectives: Do not collide with any Mountain Cliff staff. \n Conditions: The states of the agent and the Mountain Cliff staff are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_cliff_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cliff_staff = calculate_distance_min(s_agent, s_cliff_staff) \n    penalty = -19 if (d_agent_cliff_staff < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Mountain Cliff and must avoid contact with any fallen boulder. The states of the drone and the fallen boulder are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_boulder, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_boulder = calculate_distance_min(s_agent, s_boulder) \n    penalty = -19 if (d_agent_boulder < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Mountain Cliff. It must avoid colliding with rock walls and ice sheets placed in the study area. The states of the drone and multiple boxes are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Mountain Cliff. It must avoid colliding with queue stanchions and mountain goats. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Mountain Cliff. It must avoid colliding with animal. Agent's state, target's state and Mountain animal's state are accessible. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_animal, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_animal = calculate_distance_min(s_agent, s_animal) \n    penalty_collision = -19 if (d_agent_animal < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Mountain Cliff. \n Goal: It must avoid colliding with cave entrance. \n Assumptions: The states of the agent and the entrance are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_entrance, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_entrance = calculate_distance_min(s_agent, s_entrance) \n    penalty_collision = -19 if (d_agent_entrance < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Mountain Cliff. \n Goal: It must avoid colliding with Mountain Cliff staff, mountain goats and RA maintenance carts. \n Assumptions: The Agent's state, target's state and Mountain Cliff staff's state, fallen boulder's state and mountain goats' states are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Mountain Cliff. \n Goal: Detect mountain goats and pillars and avoid ridge corners. \n Assumptions: The states of the agent and gates, low-hanging branches and barricades state are availalbe. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Mountain Cliff. \n Goal: Drone target is to reach the target. \n Assumptions: The states of the agent and the target are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 15 if (d_target < 0.2) else -d_agent_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Gymnasium. It must avoid colliding with padded wall mat or athletes. The states of the agent, the fence, and the athletes are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a busy Gymnasium. Detect treadmill and aviod it. The states of the agent, the treadmill are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_treadmill, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_treadmill = calculate_distance_min(s_agent, s_treadmill) \n    penalty_collision = -15 if (d_agent_treadmil < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Gymnasium common room. \n Objectives: It must avoid colliding with plyometric boxes. \n Conditions: States of the agent and the plyometric box are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_box, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_box = calculate_distance_min(s_agent, s_box) \n    penalty_collision = -15 if (d_agent_box < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a sideline walkway, the drone must maneuver around punching bag. All entity states (agent, punching bag) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_punching_bag, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_punching_bag = calculate_distance_min(s_agent, s_punching_bag) \n    penalty_collision = -15 if (d_agent_punching_bag < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a gym floor. \n Requirement: Avoid collisions with stacked gym mats. \n Given: The states for the agent and many gym mats are known. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone operates near a court intersection during an event. So not collide with padded wall mat. The states of the agent, the padded wall mat are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_wall_mat, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall_mat = calculate_distance_min(s_agent, s_wall_mat) \n    penalty_collision = -15 if (d_agent_wall_mat < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Gymnasium. \n Objectives: Do not collide hit with hanging rings, \n Conditions: States of the agent, hanging rings are available. \n Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_ring, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ring = calculate_distance_min(s_agent, s_ring) \n    penalty_collision = -15 if (d_agent_ring < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a gym floor, the drone must steer clear of volleyball poles. All states (agent, volleyball pole) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_volleyball_pole, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_volleyball_pole = calculate_distance_min(s_agent, s_volleyball_pole) \n    penalty_collision = -15 if (d_agent_volleyball_pole < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: \n Environment: sideline area. \n Avoid: Hitting with Gymnasium worker. \n Known: Agent, Gymnasium worker states. \n Create the Python reward function.",
        "chosen": "def reward(s_agent, s_gymnasium_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_gymnasium_worker = calculate_distance_min(s_agent, s_gymnasium_worker) \n    penalty_collision = -15 if (d_agent_gymnasium_worker < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Gymnasium. \n Objectives: It must avoid colliding with equipment racks. \n Conditions: The states of the agent and the equipment rack are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_equipment_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_equipment_rack = calculate_distance_min(s_agent, s_equipment_rack) \n    penalty_collision = -19 if (d_agent_equipment_rack < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Gymnasium. \n Objectives: Avoid collision with divider curtain. \n Conditions: The states of the agent, divider curtain and staff are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Gymnasium. \n Objectives: It must avoid colliding with any medicine ball rack, floor marker and gate. \n Conditions: Agent's and all obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Gymnasium. \n Objectives: Do not collide with any pull-up bar. \n Conditions: The states of the agent and the pull-up bar are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bar = calculate_distance_min(s_agent, s_bar) \n    penalty = -19 if (d_agent_bar < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Gymnasium and must avoid contact with any tumbling block. The states of the drone and the tumbling block are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_tumbling_block, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tumbling_block = calculate_distance_min(s_agent, s_tumbling_block) \n    penalty = -19 if (d_agent_tumbling_block < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Gymnasium. It must avoid colliding with storefronts and glass doors placed in the study area. The states of the drone and multiple boxes are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Gymnasium. It must avoid colliding with queue stanchions and athletes. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Gymnasium. It must avoid colliding with Gymnasium crew. Agent's state, target's state and Gymnasium crew's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_crew, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_crew = calculate_distance_min(s_agent, s_crew) \n    penalty_collision = -19 if (d_agent_crew < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Gymnasium. \n Goal: It must avoid colliding with rope. \n Assumptions: The states of the agent and the rope area are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_rope, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rope = calculate_distance_min(s_agent, s_rope) \n    penalty_collision = -19 if (d_agent_rope < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Gymnasium. \n Goal: It must avoid colliding with Gymnasium crew, athletes and RA maintenance carts. \n Assumptions: The Agent's state, target's state and Gymnasium crew's state, tumbling block's state and athletes' states are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Gymnasium. \n Goal: Detect athletes and pillars and avoid court corners. \n Assumptions: The states of the agent and gates, equipment racks and barricades state are availalbe. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Gymnasium. \n Goal: Drone target is to reach the target. \n Assumptions: The states of the agent and the target are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 15 if (d_target < 0.2) else -d_agent_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Amusement Park. It must avoid colliding with roller coaster supports or park visitors. The states of the agent, the ride ride fence, and the park visitors are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a busy Amusement Park. Detect spinning teacup platform and aviod it. The states of the agent, the spinning teacup platform are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_treadmill, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_treadmill = calculate_distance_min(s_agent, s_treadmill) \n    penalty_collision = -15 if (d_agent_treadmill < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Amusement Park common room. \n Objectives: It must avoid colliding with carnival game counters. \n Conditions: States of the agent and the carnival game counter are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_game_counter, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_game_counter = calculate_distance_min(s_agent, s_game_counter) \n    penalty_collision = -15 if (d_agent_game_counter < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a midway, the drone must maneuver around inflatable mascot. All entity states (agent, inflatable mascot) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_mascot, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_mascot = calculate_distance_min(s_agent, s_mascot) \n    penalty_collision = -15 if (d_agent_mascot < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a central plaza. \n Requirement: Avoid collisions with multiple stroller parking racks. \n Given: The states for the agent and many stroller parking racks are known. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone operates near a ride concourse during an event. So not collide with roller coaster supports. The states of the agent, the roller coaster supports are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_support, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_support = calculate_distance_min(s_agent, s_support) \n    penalty_collision = -15 if (d_agent_support < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Amusement Park. \n Objectives: Do not collide hit with zipline cable, \n Conditions: States of the agent, zipline cable are available. \n Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_zipline_cable, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_zipline_cable = calculate_distance_min(s_agent, s_zipline_cable) \n    penalty_collision = -15 if (d_agent_zipline_cable < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a central plaza, the drone must steer clear of banner pylons. All states (agent, banner pylon) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_banner_pylon, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_banner_pylon = calculate_distance_min(s_agent, s_banner_pylon) \n    penalty_collision = -15 if (d_agent_banner_pylon < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: \n Environment: midway zone. \n Avoid: Hitting with Amusement Park worker. \n Known: Agent, Amusement Park worker states. \n Create the Python reward function.",
        "chosen": "def reward(s_agent, s_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_worker = calculate_distance_min(s_agent, s_worker) \n    penalty_collision = -15 if (d_agent_worker < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Amusement Park. \n Objectives: It must avoid colliding with souvenir stands. \n Conditions: The states of the agent and the souvenir stand are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_souvenir_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_souvenir_stand = calculate_distance_min(s_agent, s_souvenir_stand) \n    penalty_collision = -19 if (d_agent_souvenir_stand < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Amusement Park. \n Objectives: Avoid collision with parade float. \n Conditions: The states of the agent, parade float and staff are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Amusement Park. \n Objectives: It must avoid colliding with any popcorn machine, balloon cluster and turnstile. \n Conditions: Agent's, food counter and cars states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Amusement Park. \n Objectives: Do not collide with any photo booth frame. \n Conditions: The states of the agent and the photo booth frame are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_photo_booth, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_photo_booth = calculate_distance_min(s_agent, s_photo_booth) \n    penalty = -19 if (d_agent_photo_booth < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Amusement Park and must avoid contact with any park bench. The states of the agent and the park bench are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_park_bench, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_park_bench = calculate_distance_min(s_agent, s_park_bench) \n    penalty = -19 if (d_agent_park_bench < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Amusement Park. It must avoid colliding with arcade facades and glass displays placed in the study area. The states of the agent, game_counteres and crew are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Amusement Park. It must avoid colliding with queue stanchions and park visitors. The states of the agent and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Amusement Park. It must avoid colliding with Amusement Park animatronic figures. Agent's state, target's state and Amusement Park animatronic figures's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_figure, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_figure = calculate_distance_min(s_agent, s_figure) \n    penalty_collision = -19 if (d_agent_figure < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Amusement Park. \n Goal: It must avoid colliding with suspension bridge rope. \n Assumptions: The states of the agent and the suspension bridge rope area are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_rope, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rope = calculate_distance_min(s_agent, s_rope) \n    penalty_collision = -19 if (d_agent_rope < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Amusement Park. \n Goal: It must avoid colliding with Amusement Park animatronic figures, park visitors and RA maintenance carts. \n Assumptions: The Agent's state, target's state and Amusement Park animatronic figures's state, park bench's state and park visitors' states are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Amusement Park. \n Goal: Detect park visitors and topiary sculptures and avoid court corners. \n Assumptions: The states of the agent and gates, souvenir stands and barricades state are availalbe. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Amusement Park. \n Goal: Drone target is to reach the target. \n Assumptions: The states of the agent and the target are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = -15 if (d_target < 0.2) else d_agent_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent is operating along Greenhouse. It must avoid colliding with trellis frames or hanging baskets. The states of the agent, the ride plant trelli_frame fence, and the hanging baskets are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent is operating inside a busy Greenhouse. Detect rotating irrigation boom and aviod it. The states of the agent, the rotating irrigation boom are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_irrigation, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_irrigation = calculate_distance_min(s_agent, s_irrigation) \n    penalty_collision = -15 if (d_agent_irrigation < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A agent is navigating in a Greenhouse. \n Objectives: It must avoid colliding with potting benches. \n Conditions: States of the agent and the potting bench are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bench, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bench = calculate_distance_min(s_agent, s_bench) \n    penalty_collision = -15 if (d_agent_bench < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a aisle, the agent must maneuver around shade cloth drape. All entity states (agent, shade cloth drape) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_cloth_drape, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cloth_drape = calculate_distance_min(s_agent, s_cloth_drape) \n    penalty_collision = -15 if (d_agent_cloth_drape < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A agent is flying through a central aisle. \n Requirement: Detect the three plant racks and do not collide with it. \n Given: The states for the agent and many plant racks are known. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent operates near a service corridor during an event. So not collide with trellis frame. The states of the agent, the trellis frames are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_trelli_frame, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_trelli_frame = calculate_distance_min(s_agent, s_trelli_frame) \n    penalty_collision = -15 if (d_agent_trelli_frame < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in a Greenhouse. \n Objectives: Do not collide hit with overhead mist line, \n Conditions: States of the agent, overhead mist line are available. \n Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_mist_line, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_mist_line = calculate_distance_min(s_agent, s_mist_line) \n    penalty_collision = -15 if (d_agent_mist_line < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a central aisle, the agent must steer clear of sensor masts. All states (agent, sensor mast) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_sensor_mast, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sensor_mast = calculate_distance_min(s_agent, s_sensor_mast) \n    penalty_collision = -15 if (d_agent_sensor_mast < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: \n Environment: propagation zone. \n Avoid: Hitting with Greenhouse maintenance robot. \n Known: Agent, Greenhouse maintenance robot states. \n Create the Python reward function.",
        "chosen": "def reward(s_agent, s_robot, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_robot = calculate_distance_min(s_agent, s_robot) \n    penalty_collision = -15 if (d_agent_robot < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A agent is navigating Greenhouse. \n Objectives: It must avoid colliding with supply shelves. \n Conditions: The states of the agent and the supply shelf are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_supply_shelf, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_supply_shelf = calculate_distance_min(s_agent, s_supply_shelf) \n    penalty_collision = -19 if (d_agent_supply_shelf < 0.5) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A agent is flying through Greenhouse. \n Objectives: Avoid collision with two rolling benchs. \n Conditions: The states of the agent, rolling bench and staff are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A agent is navigating Greenhouse. \n Objectives: It must avoid colliding with any fertilizer mixer, hanging vines bundle and sliding partition. \n Conditions: Agent's, food counter and cars states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A agent is flying through Greenhouse. \n Objectives: Do not collide with any hose reel frame. \n Conditions: The states of the agent and the hose reel frame are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_frame, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_frame = calculate_distance_min(s_agent, s_frame) \n    penalty = -19 if (d_agent_frame < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent is operating along Greenhouse and must avoid contact with any water barrel. The states of the agent and the water barrel are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_water_barrel, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_water_barrel = calculate_distance_min(s_agent, s_water_barrel) \n    penalty = -19 if (d_agent_water_barrel < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent is operating in Greenhouse. It must avoid colliding with polycarbonate panels and glass panes placed in the study area. The states of the agent, benches and crew are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent is operating in Greenhouse. It must avoid colliding with row markers and hanging baskets. The states of the agent and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent is inspecting in Greenhouse. It must avoid colliding with Greenhouse humidifier towers. Agent's state, target's state and Greenhouse humidifier towers's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_tower, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tower = calculate_distance_min(s_agent, s_tower) \n    penalty_collision = -19 if (d_agent_tower < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A agent is navigating Greenhouse. \n Goal: It must avoid colliding with drip irrigation hose. \n Assumptions: The states of the agent and the drip irrigation hose area are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_hose_area, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_hose_area = calculate_distance_min(s_agent, s_hose_area) \n    penalty_collision = -19 if (d_agent_hose_area < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A agent is navigating Greenhouse. \n Goal: It must avoid colliding with Greenhouse humidifier towers, hanging baskets and RA maintenance carts. \n Assumptions: The Agent's state, target's state and Greenhouse humidifier towers's state, water barrel's state and hanging baskets' states are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A agent is navigating Greenhouse. \n Goal: Detect hanging baskets and tall planters and avoid court corners. \n Assumptions: The states of the agent and gates, supply shelves and barricades state are availalbe. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A agent is navigating Greenhouse. \n Goal: Drone target is to reach the target. \n Assumptions: The states of the agent and the target are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Market Street. It must avoid colliding with lamp post or pedestrians. The states of the agent, the fence, and the pedestrians are known. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_obs_min < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a busy Market Street. It must avoid colliding with Market Street tram. The states of the agent, the Market Street tram are known. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_tram, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tram = calculate_distance_min(s_agent, s_tram) \n    penalty_collision = -16 if (d_agent_tram < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Market Street common room. \n Objectives: It must avoid colliding with vendor carts. \n Conditions: States of the agent and the vendor cart are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_crate, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_carte = calculate_distance_min(s_agent, s_crate) \n    penalty_collision = -16 if (d_agent_carte < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a side street, the drone must maneuver around construction barriers. All entity states (agent, construction barrier) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_construction_barrier, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_construction_barrier = calculate_distance_min(s_agent, s_moat) \n    penalty_collision = -16 if (d_agent_construction_barrier < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a street plaza. \n Requirement: Avoid collisions with delivery carts. \n Given: The states for the agent and many carts are known. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone operates near a street intersection during an event. So not collide with lamp post. The states of the agent, the lamp post are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_lamp_post, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lamp_post = calculate_distance_min(s_agent, s_fence_post) \n    penalty_collision = -16 if (d_agent_lamp_post < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in a Market Street. \n Objectives: Do not collide hit with overhead banners, \n Conditions: States of the agent, overhead banners are available. \n Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_banner, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_banner = calculate_distance_min(s_agent, s_banner) \n    penalty_collision = -16 if (d_agent_banner < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a street plaza, the drone must steer clear of traffic light poles. All states (agent, traffic light pole) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_traffic_light, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_traffic_light = calculate_distance_min(s_agent, s_traffic_light) \n    penalty_collision = -16 if (d_agent_traffic_light < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: \n Environment: street-side area. \n Avoid: Hitting with Market Street worker. \n Known: Agent, Market Street worker states. \n Create the Python reward function.",
        "chosen": "def reward(s_agent, s_street_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_street_worker = calculate_distance_min(s_agent, s_street_worker) \n    penalty_collision = -16 if (d_agent_street_worker < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Market Street. \n Objectives: It must avoid colliding with kiosks. \n Conditions: The states of the agent and the kiosk are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_kiosk, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_kiosk = calculate_distance_min(s_agent, s_kiosk) \n    penalty_collision = -19 if (d_agent_kiosk < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Market Street. \n Objectives: Avoid collision with construction scaffold and tarp. \n Conditions: The states of the agent, construction scaffold and tarp are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Market Street. \n Objectives: It must avoid colliding with any propane tank, sidewalk sign and gate. \n Conditions: Agent's and all obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -19 if (dagent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Market Street. \n Objectives: Do not collide with any Market Street staff. \n Conditions: The states of the agent and the Market Street staff are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_market_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_market_staff = calculate_distance_min(s_agent, s_market_staff) \n    penalty = -19 if (d_agent_market_staff < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Market Street and must avoid contact with any service vehicle. The states of the drone and the service vehicle are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_service_vehicle, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_service_vehicle = calculate_distance_min(s_agent, s_service_vehicle) \n    penalty = -19 if (d_agent_service_vehicle < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Market Street. It must avoid colliding with storefronts and glass doors placed in the study area. The states of the drone and multiple boxes are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Market Street. It must avoid colliding with queue stanchions and pedestrians. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Market Street. It must avoid colliding with Market Street staff. Agent's state, target's state and Market Street staff's state are accessible. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_staff = calculate_distance_min(s_agent, s_staff) \n    penalty_collision = -19 if (d_agent_staff < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Market Street. \n Goal: It must avoid colliding with freight elevator entrance. \n Assumptions: The states of the agent and the elevator are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_elevator, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_elevator = calculate_distance_min(s_agent, s_elevator) \n    penalty_collision = -19 if (d_agent_elevator < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Market Street. \n Goal: It must avoid colliding with Market Street staff, pedestrians and RA maintenance carts. \n Assumptions: The Agent's state, target's state and Market Street staff's state, service vehicle's state and pedestrians' states are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Market Street. \n Goal: Detect pedestrians and pillars and avoid building corners. \n Assumptions: The states of the agent and gates, kiosks and barricades state are availalbe. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Market Street. \n Goal: Drone target is to reach the target. \n Assumptions: The states of the agent and the target are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a film studio, attempts to reach a target, and must prevent collisions with lamp post or pedestrians as obstacles, using drone state, the multiple obstacle: lamp post or pedestrians as obstacles's statuses, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_obs_min < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a film studio, attempts to reach a target, and must prevent collisions with tram as obstacles, using drone state, the tram as obstacles's statuses, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_tram, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tram = calculate_distance_min(s_agent, s_tram) \n    penalty_collision = -16 if (d_agent_tram < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a film studio, attempts to reach a target, and must prevent collisions with vendor carts as obstacles, using drone state, the vendor carts as obstacles's statuses, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_crate, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_carte = calculate_distance_min(s_agent, s_crate) \n    penalty_collision = -16 if (d_agent_carte < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a film studio, attempts to reach a target, and must prevent collisions with construction barrier as obstacles, using drone state, the construction barrier as obstacles's statuses, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_construction_barrier, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_construction_barrier = calculate_distance_min(s_agent, s_moat) \n    penalty_collision = -16 if (d_agent_construction_barrier < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a film studio, attempts to reach a target, and must prevent collisions with many delivery carts as obstacles, using drone state, the multiple delivery carts as obstacles's statuses, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a film studio, attempts to reach a target, and must prevent collisions with lamp post as obstacles, using drone state, the lamp post as obstacles's statuses, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_lamp_post, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lamp_post = calculate_distance_min(s_agent, s_fence_post) \n    penalty_collision = -16 if (d_agent_lamp_post < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a film studio, attempts to reach a target, and must prevent collisions with banner as obstacles, using drone state, the banner as obstacles's statuses, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_banner, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_banner = calculate_distance_min(s_agent, s_banner) \n    penalty_collision = -16 if (d_agent_banner < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a film studio, attempts to reach a target, and must prevent collisions with traffic light poles as obstacles, using drone state, the traffic light poles as obstacles's statuses, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_traffic_light, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_traffic_light = calculate_distance_min(s_agent, s_traffic_light) \n    penalty_collision = -16 if (d_agent_traffic_light < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a film studio, attempts to reach a target, and must prevent collisions with Market Street worker as obstacles, using drone state, the Market Street worker as obstacles's statuses, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_street_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_street_worker = calculate_distance_min(s_agent, s_street_worker) \n    penalty_collision = -16 if (d_agent_street_worker < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a film studio, attempts to reach a target, and must prevent collisions with kiosks as obstacles, using drone state, the kiosks as obstacles's statuses, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_kiosk, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_kiosk = calculate_distance_min(s_agent, s_kiosk) \n    penalty_collision = -19 if (d_agent_kiosk < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a film studio, attempts to reach a target, and must prevent collisions with service vehicle, custommer as obstacles, using drone state, the multiple obstacle##: service vehicle, custommer as obstacles's statuses, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a film studio, attempts to reach a target, and must prevent collisions with propane tank, sidewalk sign and gate as obstacles, using drone state, the multiple obstacle: propane tank, sidewalk sign and gate as obstacles's statuses, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -19 if (dagent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a film studio, attempts to reach a target, and must prevent collisions with market staff as obstacles, using drone state, the market staff as obstacles's statuses, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_market_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_market_staff = calculate_distance_min(s_agent, s_market_staff) \n    penalty = -19 if (d_agent_market_staff < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a film studio, attempts to reach a target, and must prevent collisions with service vehicle as obstacles, using drone state, the service vehicle as obstacles's statuses, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_service_vehicle, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_service_vehicle = calculate_distance_min(s_agent, s_service_vehicle) \n    penalty = -19 if (d_agent_service_vehicle < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a film studio, attempts to reach a target, and must prevent collisions with storefronts and glass doors placed as obstacles, using drone state, the multiple obstacle: storefronts and glass doors placed in the study area as obstacles's statuses, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a film studio, attempts to reach a target, and must prevent collisions with queue stanchions and pedestrians as obstacles, using drone state, the multiple obstacle: queue stanchions and pedestrians as obstacles's statuses, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a film studio, attempts to reach a target, and must prevent collisions with market staff as obstacles, using drone state, the market staff as obstacles's statuses, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_staff = calculate_distance_min(s_agent, s_staff) \n    penalty_collision = -19 if (d_agent_staff < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a film studio, attempts to reach a target, and must prevent collisions with elevator entrance as obstacles, using drone state, the elevator entrance as obstacles's statuses, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_elevator, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_elevator = calculate_distance_min(s_agent, s_elevator) \n    penalty_collision = -19 if (d_agent_elevator < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a film studio, attempts to reach a target, and must prevent collisions with market staff, pedestrians and maintenance carts as obstacles, using drone state, the multiple obstacle: market staff, pedestrians and maintenance carts as obstacles's statuses, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a film studio, attempts to reach a target, and must prevent collisions with pedestrians and pillars as obstacles, using drone state, the multiple obstacle: pedestrians and pillars as obstacles's statuses, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a film studio, attempts to reach a target, and must prevent collisions with service vehicle as obstacles, using drone state, the service vehicle as obstacles's statuses, and target information. Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a mountain cliff, aims to reach a target, and must avoid collisions with rock outcrop or mou rock outcrop or mou using its own state, the rock outcrop or mou's state, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a mountain cliff, aims to reach a target, and must avoid collisions with tram using its own state, the tram's state, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_tram, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tram = calculate_distance_min(s_agent, s_tram) \n    penalty_collision = -16 if (d_agent_tram < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a mountain cliff, aims to reach a target, and must avoid collisions with talus boulders using its own state, the talus boulders' states, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_talus_boulder, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_talus_boulder = calculate_distance_min(s_agent, s_talus_boulder) \n    penalty_collision = -16 if (d_agent_talus_boulder < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a mountain cliff, aims to reach a target, and must avoid collisions with rockfall barrier using its own state, the rockfall barrier's state, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_rockfall_barrier, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rockfall_barrier = calculate_distance_min(s_agent, s_rockfall_barrier) \n    penalty_collision = -16 if (d_agent_rockfall_barrier < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a mountain cliff, aims to reach a target, and must avoid collisions with scree slopes and rocks using its own state, multiple obstcales: the scree slopes and rocks' states, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a mountain cliff, aims to reach a target, and must avoid collisions with rock outcrop using its own state, the rock outcrop's state, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_rock_outcrop, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rock_outcrop = calculate_distance_min(s_agent, s_rock_outcrop) \n    penalty_collision = -16 if (d_agent_rock_outcrop < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a mountain cliff, aims to reach a target, and must avoid collisions with ledge using its own state, the ledge's state, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_ledge, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ledge = calculate_distance_min(s_agent, s_ledge) \n    penalty_collision = -16 if (d_agent_ledge < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a mountain cliff, aims to reach a target, and must avoid collisions with rock spires using its own state, the rock spires' states, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_rock_spire, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rock_spire = calculate_distance_min(s_agent, s_rock_spire) \n    penalty_collision = -16 if (d_agent_rock_spire < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a mountain cliff, aims to reach a target, and must avoid collisions with cliff worker using its own state, the cliff worker's state, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_cliff_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cliff_worker = calculate_distance_min(s_agent, s_cliff_worker) \n    penalty_collision = -16 if (d_agent_cliff_worker < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a mountain cliff, aims to reach a target, and must avoid collisions with branch using its own state, the branch's state, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_branche, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_branche = calculate_distance_min(s_agent, s_branche) \n    penalty_collision = -19 if (d_agent_branche < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a mountain cliff, aims to reach a target, and must avoid collisions with ice branch and human using its own state, the multiple obstacles's state, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a mountain cliff, aims to reach a target, and must avoid collisions with a trees, climbers and rock walls using its own state, the multiple obstacles's state, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a mountain cliff, aims to reach a target, and must avoid collisions with cliff staff a using its own state, the cliff staff's state, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_cliff_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cliff_staff = calculate_distance_min(s_agent, s_cliff_staff) \n    penalty = -19 if (d_agent_cliff_staff < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a mountain cliff, aims to reach a target, and must avoid collisions with boulder a using its own state, the boulder's state, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_boulder, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_boulder = calculate_distance_min(s_agent, s_boulder) \n    penalty = -19 if (d_agent_boulder < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a mountain cliff, aims to reach a target, and must avoid collisions with rock walls a using its own state, the rock wall's state, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a mountain cliff, aims to reach a target, and must avoid collisions with staff, mountain goats and RA maintenance carts using its own state, the multiple obstacles's state, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a mountain cliff, aims to reach a target, and must avoid collisions with animal a using its own state, the animal's state, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_animal, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_animal = calculate_distance_min(s_agent, s_animal) \n    penalty_collision = -19 if (d_agent_animal < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a mountain cliff, aims to reach a target, and must avoid collisions with entrance using its own state, the cave entrace's state, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_entrance, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_entrance = calculate_distance_min(s_agent, s_entrance) \n    penalty_collision = -19 if (d_agent_entrance < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a mountain cliff, aims to reach a target, and must avoid collisions with Mou Mou using its own state, the Mou's state, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a mountain cliff, aims to reach a target, and must avoid collisions with  mountain goats and pillars  using its own state, the multiple obstacles's state, and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent flies inside a mountain cliff, aims to reach a target using its own state and the target state. Code a reward function in python.",
        "chosen": "def reward(s_agent, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 16 if (d_target < 0.3) else -d_agent_target*2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores an indoor gymnasium, avoids collisions with padded wall mat or athletes and reach target, and uses its state, multiple obstacles' status, and target information to not touch obstacles and reach the target. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores an indoor gymnasium, avoids collisions with treadmill and reach target, and uses its state, treadmill' status, and target information to not touch treadmill and reach the target. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_treadmill, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_treadmill = calculate_distance_min(s_agent, s_treadmill) \n    penalty_collision = -16 if (d_agent_treadmil < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores an indoor gymnasium, avoids collisions with plyometric box and reach target, and uses its state, plyometric box' status, and target information to not touch plyometric box and reach the target. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_box, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_box = calculate_distance_min(s_agent, s_box) \n    penalty_collision = -16 if (d_agent_box < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores an indoor gymnasium, avoids collisions with punching bag and reach target, and uses its state, punching bag' status, and target information to not touch punching bag and reach the target. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_punching_bag, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_punching_bag = calculate_distance_min(s_agent, s_punching_bag) \n    penalty_collision = -16 if (d_agent_punching_bag < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores an indoor gymnasium, avoids collisions with stacked gym mats and reach target, and uses its state, multiple obstacles' status, and target information to not touch obstacles and reach the target. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores an indoor gymnasium, avoids collisions with padded wall mat and reach target, and uses its state, padded wall mat' status, and target information to not touch padded wall mat and reach the target. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_wall_mat, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall_mat = calculate_distance_min(s_agent, s_wall_mat) \n    penalty_collision = -16 if (d_agent_wall_mat < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores an indoor gymnasium, avoids collisions with hanging rings and reach target, and uses its state, hanging rings' status, and target information to not touch hanging rings and reach the target. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_ring, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ring = calculate_distance_min(s_agent, s_ring) \n    penalty_collision = -16 if (d_agent_ring < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores an indoor gymnasium, avoids collisions with volleyball pole and reach target, and uses its state, volleyball pole' status, and target information to not touch volleyball pole and reach the target. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_volleyball_pole, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_volleyball_pole = calculate_distance_min(s_agent, s_volleyball_pole) \n    penalty_collision = -16 if (d_agent_volleyball_pole < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores an indoor gymnasium, avoids collisions with gymnasium worker and reach target, and uses its state, gymnasium worker' status, and target information to not touch gymnasium worker and reach the target. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_gymnasium_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_gymnasium_worker = calculate_distance_min(s_agent, s_gymnasium_worker) \n    penalty_collision = -16 if (d_agent_gymnasium_worker < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores an indoor gymnasium, avoids collisions with equipment rack and reach target, and uses its state, equipment rack' status, and target information to not touch equipment rack and reach the target. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_equipment_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_equipment_rack = calculate_distance_min(s_agent, s_equipment_rack) \n    penalty_collision = -19 if (d_agent_equipment_rack < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores an indoor gymnasium, avoids collisions with divider curtain and people and coach and reach target, and uses its state, multiple obstacles' status, and target information to not touch  obstacles and reach the target. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores an indoor gymnasium, avoids collisions with medicine ball rack, floor marker and gate and reach target, and uses its state, multiple obstacles' status, and target information to not touch obstacles and reach the target. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores an indoor gymnasium, avoids collisions with pull-up bar and reach target, and uses its state, pull-up bar' status, and target information to not touch pull-up bar and reach the target. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_bar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bar = calculate_distance_min(s_agent, s_bar) \n    penalty = -19 if (d_agent_bar < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores an indoor gymnasium, avoids collisions with tumbling block and reach target, and uses its state, tumbling block' status, and target information to not touch tumbling block and reach the target. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_tumbling_block, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tumbling_block = calculate_distance_min(s_agent, s_tumbling_block) \n    penalty = -19 if (d_agent_tumbling_block < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores an indoor gymnasium, avoids collisions with ball, monkey bar and mirror and reach target, and uses its state, multiple obstacles' status, and target information to not touch obstacles and reach the target. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores an indoor gymnasium, avoids collisions with pillar, plant and lights and reach target, and uses its state, multiple obstacles' status, and target information to not touch obstacles and reach the target. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores an indoor gymnasium, avoids collisions with gymnasium crew and reach target, and uses its state, gymnasium crew' status, and target information to not touch gymnasium crew and reach the target. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_crew, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_crew = calculate_distance_min(s_agent, s_crew) \n    penalty_collision = -19 if (d_agent_crew < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores an indoor gymnasium, avoids collisions with rope and reach target, and uses its state, rope' status, and target information to not touch rope and reach the target. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_rope, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rope = calculate_distance_min(s_agent, s_rope) \n    penalty_collision = -19 if (d_agent_rope < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores an indoor gymnasium, avoids collisions with rope, bar and walls and reach target, and uses its state, multiple obstacles' status, and target information to not touch multiple obstacles and reach the target. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores an indoor gymnasium, avoids collisions with treadmill, decor and camera and reach target, and uses its state, multiple obstacles' status, and target information to not touch obstacles and reach the target. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores an indoor gymnasium and aims to reach the target using its own state and target information. Develop a reward function with Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 16 if (d_target < 0.3) else -d_agent_target*2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a Amusement Park, aims to reach a designated target, and must steer clear of roller coaster supports or park visitors by relying on its own state, the roller coaster supports or park visitors’ statuses, and the target information. Create a reward function in Python. &",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a Amusement Park, aims to reach a designated target, and must steer clear of park bench by relying on its own state, the park bench’ status, and the target information. Create a reward function in Python. &",
        "chosen": "def reward(s_agent, s_bench, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bench = calculate_distance_min(s_agent, s_bench) \n    penalty_collision = -16 if (d_agent_bench < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a Amusement Park, aims to reach a designated target, and must steer clear of carnival game counters by relying on its own state, the carnival game counters’ statuses, and the target information. Create a reward function in Python. &",
        "chosen": "def reward(s_agent, s_game_counter, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_game_counter = calculate_distance_min(s_agent, s_game_counter) \n    penalty_collision = -16 if (d_agent_game_counter < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a Amusement Park, aims to reach a designated target, and must steer clear of mascot by relying on its own state, the mascot’ status, and the target information. Create a reward function in Python. &",
        "chosen": "def reward(s_agent, s_mascot, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_mascot = calculate_distance_min(s_agent, s_mascot) \n    penalty_collision = -16 if (d_agent_mascot < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a Amusement Park, aims to reach a designated target, and must steer clear of multiple stroller parking racks by relying on its own state, the multiple stroller parking racks’ statuses, and the target information. Create a reward function in Python. &",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a Amusement Park, aims to reach a designated target, and must steer clear of supporter by relying on its own state, the supporter’ status, and the target information. Create a reward function in Python. &",
        "chosen": "def reward(s_agent, s_supporter, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_support = calculate_distance_min(s_agent, s_supporter) \n    penalty_collision = -16 if (d_agent_support < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a Amusement Park, aims to reach a designated target, and must steer clear of zipline cable by relying on its own state, the zipline cable’ status, and the target information. Create a reward function in Python. &",
        "chosen": "def reward(s_agent, s_zipline_cable, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_zipline_cable = calculate_distance_min(s_agent, s_zipline_cable) \n    penalty_collision = -16 if (d_agent_zipline_cable < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a Amusement Park, aims to reach a designated target, and must steer clear of banner pylons by relying on its own state, the banner pylons’ statuses, and the target information. Create a reward function in Python. &",
        "chosen": "def reward(s_agent, s_banner_pylon, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_banner_pylon = calculate_distance_min(s_agent, s_banner_pylon) \n    penalty_collision = -16 if (d_agent_banner_pylon < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a Amusement Park, aims to reach a designated target, and must steer clear of  worker by relying on its own state, the worker’ status, and the target information. Create a reward function in Python. &",
        "chosen": "def reward(s_agent, s_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_worker = calculate_distance_min(s_agent, s_worker) \n    penalty_collision = -16 if (d_agent_worker < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a Amusement Park, aims to reach a designated target, and must steer clear of souvenir stands by relying on its own state, the souvenir stands’ statuses, and the target information. Create a reward function in Python. &",
        "chosen": "def reward(s_agent, s_souvenir_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_souvenir_stand = calculate_distance_min(s_agent, s_souvenir_stand) \n    penalty_collision = -19 if (d_agent_souvenir_stand < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a Amusement Park, aims to reach a designated target, and must steer clear of park bench and shops by relying on its own state, the multiple obstacles’ status, and the target information. Create a reward function in Python. &",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a Amusement Park, aims to reach a designated target, and must steer clear of popcorn machine, balloon cluster and turnstile by relying on its own state, the multiple obstacles’ statuses, and the target information. Create a reward function in Python. &",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a Amusement Park, aims to reach a designated target, and must steer clear of photo booth frame by relying on its own state, the photo booth frame’ status, and the target information. Create a reward function in Python. &",
        "chosen": "def reward(s_agent, s_photo_booth, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_photo_booth = calculate_distance_min(s_agent, s_photo_booth) \n    penalty = -19 if (d_agent_photo_booth < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a Amusement Park, aims to reach a designated target, and must steer clear of park bench by relying on its own state, the park bench’ status, and the target information. Create a reward function in Python. &",
        "chosen": "def reward(s_agent, s_park_bench, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_park_bench = calculate_distance_min(s_agent, s_park_bench) \n    penalty = -19 if (d_agent_park_bench < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a Amusement Park, aims to reach a designated target, and must steer clear of arcade facades and glass displays placed in the study area by relying on its own state, the arcade facades and glass displays placed in the study area’ statuses, and the target information. Create a reward function in Python. &",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a Amusement Park, aims to reach a designated target, and must steer clear of queue stanchions and park visitors by relying on its own state, the queue stanchions and park visitors’ statuses, and the target information. Create a reward function in Python. &",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a Amusement Park, aims to reach a designated target, and must steer clear of Amusement Park animatronic figures by relying on its own state, the Amusement Park animatronic figures’ statuses, and the target information. Create a reward function in Python. &",
        "chosen": "def reward(s_agent, s_figure, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_figure = calculate_distance_min(s_agent, s_figure) \n    penalty_collision = -19 if (d_agent_figure < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a Amusement Park, aims to reach a designated target, and must steer clear of suspension bridge rope by relying on its own state, the suspension bridge rope’ status, and the target information. Create a reward function in Python. &",
        "chosen": "def reward(s_agent, s_rope, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rope = calculate_distance_min(s_agent, s_rope) \n    penalty_collision = -19 if (d_agent_rope < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a Amusement Park, aims to reach a designated target, and must steer clear of multiple obstacles animatronic figures, park visitors and RA maintenance carts by relying on its own state, the  animatronic figures, park visitors and RA maintenance carts’ statuses, and the target information. Create a reward function in Python. &",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a Amusement Park, aims to reach a designated target, and must steer clear of multiple obstacles park visitors and topiary sculptures by relying on its own state, the park visitors and topiary sculptures’ statuses, and the target information. Create a reward function in Python. &",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone navigates inside a Amusement Park, aims to reach a designated target, and must steer clear of park bench by relying on its own state, the park bench’ status, and the target information. Create a reward function in Python. &",
        "chosen": "def reward(s_agent, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 16 if (d_target < 0.3) else -d_agent_target*2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in a Greenhouse, a drone must avoid crashing into trellis frames or hanging baskets and reach the target by using the available information about its own state, the target's state, and the trellis frames or hanging baskets state(s). Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in a Greenhouse, a drone must avoid crashing into a rotating irrigation boom and reach the target by using the available information about its own state, the target's state, and the rotating irrigation boom state(s). Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_irrigation, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_irrigation = calculate_distance_min(s_agent, s_irrigation) \n    penalty_collision = -16 if (d_agent_irrigation < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in a Greenhouse, a drone must avoid crashing into potting benches and reach the target by using the available information about its own state, the target's state, and the potting benches state(s). Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_bench, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bench = calculate_distance_min(s_agent, s_bench) \n    penalty_collision = -16 if (d_agent_bench < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in a Greenhouse, a drone must avoid crashing into a shade cloth drape and reach the target by using the available information about its own state, the target's state, and the shade cloth drape state(s). Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_cloth_drape, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cloth_drape = calculate_distance_min(s_agent, s_cloth_drape) \n    penalty_collision = -16 if (d_agent_cloth_drape < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in a Greenhouse, a drone must avoid crashing into plant racks and plants reach the target by using the available information about its own state, the target's state, and the multiple plant racks state(s). Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in a Greenhouse, a drone must avoid crashing into trellis frames and reach the target by using the available information about its own state, the target's state, and the trellis frames state(s). Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_trelli_frame, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_trelli_frame = calculate_distance_min(s_agent, s_trelli_frame) \n    penalty_collision = -16 if (d_agent_trelli_frame < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in a Greenhouse, a drone must avoid crashing into an overhead mist line and reach the target by using the available information about its own state, the target's state, and the overhead mist line state(s). Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_mist_line, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_mist_line = calculate_distance_min(s_agent, s_mist_line) \n    penalty_collision = -16 if (d_agent_mist_line < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in a Greenhouse, a drone must avoid crashing into sensor masts and reach the target by using the available information about its own state, the target's state, and the sensor masts state(s). Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_sensor_mast, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sensor_mast = calculate_distance_min(s_agent, s_sensor_mast) \n    penalty_collision = -16 if (d_agent_sensor_mast < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in a Greenhouse, a drone must avoid crashing into a maintenance robot and reach the target by using the available information about its own state, the target's state, and the maintenance robot state(s). Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_robot, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_robot = calculate_distance_min(s_agent, s_robot) \n    penalty_collision = -16 if (d_agent_robot < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in a Greenhouse, a drone must avoid crashing into supply shelves and reach the target by using the available information about its own state, the target's state, and the supply shelves state(s). Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_supply_shelf, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_supply_shelf = calculate_distance_min(s_agent, s_supply_shelf) \n    penalty_collision = -19 if (d_agent_supply_shelf < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in a Greenhouse, a drone must avoid crashing into rolling benches and shelves and reach the target by using the available information about its own state, the target's state, and the multiple obstacles. Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in a Greenhouse, a drone must avoid crashing into a fertilizer mixer, hanging vine bundles, and a sliding partition and reach the target by using the available information about its own state, the target's state, and the multiple obstacles state(s). Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in a Greenhouse, a drone must avoid crashing into a hose reel frame and reach the target by using the available information about its own state, the target's state, and the hose reel frame state(s). Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_frame, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_frame = calculate_distance_min(s_agent, s_frame) \n    penalty = -19 if (d_agent_frame < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in a Greenhouse, a drone must avoid crashing into a water barrel and reach the target by using the available information about its own state, the target's state, and the water barrel state(s). Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_water_barrel, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_water_barrel = calculate_distance_min(s_agent, s_water_barrel) \n    penalty = -19 if (d_agent_water_barrel < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in a Greenhouse, a drone must avoid crashing into polycarbonate panels and glass panes and reach the target by using the available information about its own state, the target's state, and the multiple polycarbonate panels and glass panes state(s). Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in a Greenhouse, a drone must avoid crashing into row markers and hanging baskets and reach the target by using the available information about its own state, the target's state, and the multiple obstacles row markers and hanging baskets state(s). Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in a Greenhouse, a drone must avoid crashing into humidifier towers and reach the target by using the available information about its own state, the target's state, and the humidifier towers state(s). Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_tower, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tower = calculate_distance_min(s_agent, s_tower) \n    penalty_collision = -19 if (d_agent_tower < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in a Greenhouse, a drone must avoid crashing into a drip irrigation hose and reach the target by using the available information about its own state, the target's state, and the drip irrigation hose state(s). Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_hose_area, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_hose_area = calculate_distance_min(s_agent, s_hose_area) \n    penalty_collision = -19 if (d_agent_hose_area < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in a Greenhouse, a drone must avoid crashing into humidifier towers, hanging baskets, and maintenance carts and reach the target by using the available information about its own state, the target's state, and the multiple obstacles humidifier towers, hanging baskets, and maintenance carts state(s). Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in a Greenhouse, a drone must avoid crashing into hanging baskets and tall planters and reach the target by using the available information about its own state, the target's state, and the multiple obstacles: hanging baskets and tall planters state(s). Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "While operating in a Greenhouse, a drone must reach the target by using the available information about its own state and the target's state. Design a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a market farm, avoids lamp post or pedestrians, and aims to reach a target using its state and avoid hitting lamp post or pedestrians with using the lamp post or pedestrians states. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_obs_min < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a market farm, avoids tram, and aims to reach a target using its state and avoid hitting tram with using tram's status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_tram, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tram = calculate_distance_min(s_agent, s_tram) \n    penalty_collision = -16 if (d_agent_tram < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a market farm, avoids vendor carts, and aims to reach a target using its state and avoid hitting vendor carts with using the vendor carts states. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_crate, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_carte = calculate_distance_min(s_agent, s_crate) \n    penalty_collision = -16 if (d_agent_carte < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a market farm, avoids construction barrier, and aims to reach a target using its state and avoid hitting construction barrier with using construction barrier's status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_construction_barrier, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_construction_barrier = calculate_distance_min(s_agent, s_moat) \n    penalty_collision = -16 if (d_agent_construction_barrier < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a market farm, avoids many delivery carts, and aims to reach a target using its state and avoid hitting many delivery carts with using the many delivery carts states. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a market farm, avoids lamp post, and aims to reach a target using its state and avoid hitting lamp post with using lamp post's status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_lamp_post, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lamp_post = calculate_distance_min(s_agent, s_fence_post) \n    penalty_collision = -16 if (d_agent_lamp_post < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a market farm, avoids banner, and aims to reach a target using its state and avoid hitting banner with using banner's status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_banner, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_banner = calculate_distance_min(s_agent, s_banner) \n    penalty_collision = -16 if (d_agent_banner < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a market farm, avoids traffic light poles, and aims to reach a target using its state and avoid hitting traffic light poles with using the traffic light poles states. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_traffic_light, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_traffic_light = calculate_distance_min(s_agent, s_traffic_light) \n    penalty_collision = -16 if (d_agent_traffic_light < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a market farm, avoids market staff, and aims to reach a target using its state and avoid hitting market staff with using market staff's status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_street_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_street_worker = calculate_distance_min(s_agent, s_street_worker) \n    penalty_collision = -16 if (d_agent_street_worker < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a market farm, avoids kiosks, and aims to reach a target using its state and avoid hitting kiosks with using the kiosks states. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_kiosk, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_kiosk = calculate_distance_min(s_agent, s_kiosk) \n    penalty_collision = -19 if (d_agent_kiosk < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a market farm, avoids service vehicle, customer, and aims to reach a target using its state and avoid hitting service vehicle, customer with using the service vehicle, customer states. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a market farm, avoids propane tank, sidewalk sign and gate, and aims to reach a target using its state and avoid hitting propane tank, sidewalk sign and gate with using the propane tank, sidewalk sign and gate states. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -19 if (dagent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a market farm, avoids market staff, and aims to reach a target using its state and avoid hitting market staff with using market staff's status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_market_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_market_staff = calculate_distance_min(s_agent, s_market_staff) \n    penalty = -19 if (d_agent_market_staff < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a market farm, avoids service vehicle, and aims to reach a target using its state and avoid hitting service vehicle with using service vehicle's status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_service_vehicle, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_service_vehicle = calculate_distance_min(s_agent, s_service_vehicle) \n    penalty = -19 if (d_agent_service_vehicle < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a market farm, avoids storefronts and glass doors placed in the study area, and aims to reach a target using its state and avoid hitting storefronts and glass doors placed in the study area with using the storefronts and glass doors placed in the study area states. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a market farm, avoids queue stanchions and pedestrians, and aims to reach a target using its state and avoid hitting queue stanchions and pedestrians with using the queue stanchions and pedestrians states. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a market farm, avoids market staff, and aims to reach a target using its state and avoid hitting market staff with using market staff's status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_staff = calculate_distance_min(s_agent, s_staff) \n    penalty_collision = -19 if (d_agent_staff < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a market farm, avoids elevator entrance, and aims to reach a target using its state and avoid hitting elevator entrance with using elevator entrance's status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_elevator, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_elevator = calculate_distance_min(s_agent, s_elevator) \n    penalty_collision = -19 if (d_agent_elevator < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a market farm, avoids market staff, pedestrians and maintenance carts, and aims to reach a target using its state and avoid hitting market staff, pedestrians and maintenance carts with using the market staff, pedestrians and maintenance carts states. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a market farm, avoids pedestrians and pillars, and aims to reach a target using its state and avoid hitting pedestrians and pillars with using the pedestrians and pillars states. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone explores a market farm, avoids service vehicle, and aims to reach a target using its state and avoid hitting service vehicle with using service vehicle's status. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone flys in a film studio. \n Duties: Detect lamp post or pedestrians and avoid collision. \n Inputs: Agent's state, multiple obstacles: lamp post or pedestrians's states obtainable. \n Write a reward function in Python.C",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_obs_min < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone flys in a film studio. \n Duties: Detect Market Street tram and avoid collision. \n Inputs: Agent's state, Market Street tram's state obtainable. \n Write a reward function in Python.C",
        "chosen": "def reward(s_agent, s_tram, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tram = calculate_distance_min(s_agent, s_tram) \n    penalty_collision = -16 if (d_agent_tram < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone flys in a film studio. \n Duties: Detect vendor carts and avoid collision. \n Inputs: Agent's state, vendor carts's states obtainable. \n Write a reward function in Python.C",
        "chosen": "def reward(s_agent, s_crate, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_carte = calculate_distance_min(s_agent, s_crate) \n    penalty_collision = -16 if (d_agent_carte < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone flys in a film studio. \n Duties: Detect construction barrier and avoid collision. \n Inputs: Agent's state, construction barrier's state obtainable. \n Write a reward function in Python.C",
        "chosen": "def reward(s_agent, s_construction_barrier, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_construction_barrier = calculate_distance_min(s_agent, s_moat) \n    penalty_collision = -16 if (d_agent_construction_barrier < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone flys in a film studio. \n Duties: Detect multiple delivery carts and avoid collision. \n Inputs: Agent's state, multiple delivery carts's states obtainable. \n Write a reward function in Python.C",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone flys in a film studio. \n Duties: Detect light post and avoid collision. \n Inputs: Agent's state, light post's state obtainable. \n Write a reward function in Python.C",
        "chosen": "def reward(s_agent, s_lamp_post, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lamp_post = calculate_distance_min(s_agent, s_fence_post) \n    penalty_collision = -16 if (d_agent_lamp_post < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone flys in a film studio. \n Duties: Detect banner and avoid collision. \n Inputs: Agent's state, banner's state obtainable. \n Write a reward function in Python.C",
        "chosen": "def reward(s_agent, s_banner, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_banner = calculate_distance_min(s_agent, s_banner) \n    penalty_collision = -16 if (d_agent_banner < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone flys in a film studio. \n Duties: Detect traffic light poles and avoid collision. \n Inputs: Agent's state, traffic light poles's states obtainable. \n Write a reward function in Python.C",
        "chosen": "def reward(s_agent, s_traffic_light, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_traffic_light = calculate_distance_min(s_agent, s_traffic_light) \n    penalty_collision = -16 if (d_agent_traffic_light < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone flys in a film studio. \n Duties: Detect Hitting with Market Street worker and avoid collision. \n Inputs: Agent's state, Hitting with Market Street worker's state obtainable. \n Write a reward function in Python.C",
        "chosen": "def reward(s_agent, s_street_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_street_worker = calculate_distance_min(s_agent, s_street_worker) \n    penalty_collision = -16 if (d_agent_street_worker < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone flys in a film studio. \n Duties: Detect kiosks and avoid collision. \n Inputs: Agent's state, kiosks's states obtainable. \n Write a reward function in Python.C",
        "chosen": "def reward(s_agent, s_kiosk, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_kiosk = calculate_distance_min(s_agent, s_kiosk) \n    penalty_collision = -19 if (d_agent_kiosk < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone flys in a film studio. \n Duties: Detect light stand and customers and avoid collision. \n Inputs: Agent's state, multiple obstacles: light stand's and customers state obtainable. \n Write a reward function in Python.C",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone flys in a film studio. \n Duties: Detect propane tank, sidewalk sign and gate and avoid collision. \n Inputs: Agent's state, multiple obstacles: propane tank, sidewalk sign and gate's states obtainable. \n Write a reward function in Python.C",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -19 if (dagent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone flys in a film studio. \n Duties: Detect Market Street staff and avoid collision. \n Inputs: Agent's state, Market Street staff's state obtainable. \n Write a reward function in Python.C",
        "chosen": "def reward(s_agent, s_market_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_market_staff = calculate_distance_min(s_agent, s_market_staff) \n    penalty = -19 if (d_agent_market_staff < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone flys in a film studio. \n Duties: Detect service vehicle and avoid collision. \n Inputs: Agent's state, service vehicle's state obtainable. \n Write a reward function in Python.C",
        "chosen": "def reward(s_agent, s_service_vehicle, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_service_vehicle = calculate_distance_min(s_agent, s_service_vehicle) \n    penalty = -19 if (d_agent_service_vehicle < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone flys in a film studio. \n Duties: Detect storefronts and glass doors placed in the study area and avoid collision. \n Inputs: Agent's state, multiple obstacles: storefronts and glass doors placed in the study area's states obtainable. \n Write a reward function in Python.C",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone flys in a film studio. \n Duties: Detect queue stanchions and pedestrians and avoid collision. \n Inputs: Agent's state, multiple obstacles: queue stanchions and pedestrians's states obtainable. \n Write a reward function in Python.C",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone flys in a film studio. \n Duties: Detect Market Street staff and avoid collision. \n Inputs: Agent's state, Market Street staff's state obtainable. \n Write a reward function in Python.C",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_staff = calculate_distance_min(s_agent, s_staff) \n    penalty_collision = -19 if (d_agent_staff < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone flys in a film studio. \n Duties: Detect elevator entrance and avoid collision. \n Inputs: Agent's state, elevator entrance's state obtainable. \n Write a reward function in Python.C",
        "chosen": "def reward(s_agent, s_elevator, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_elevator = calculate_distance_min(s_agent, s_elevator) \n    penalty_collision = -19 if (d_agent_elevator < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone flys in a film studio. \n Duties: Detect Market Street staff, pedestrians and maintenance carts and avoid collision. \n Inputs: Agent's state, multiple obstacles: Market Street staff, pedestrians and maintenance carts's states obtainable. \n Write a reward function in Python.C",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone flys in a film studio. \n Duties: Detect pedestrians and pillars and avoid collision. \n Inputs: Agent's state, multiple obstacles: pedestrians and pillars's states obtainable. \n Write a reward function in Python.C",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Operation: Drone flys in a film studio. \n Duties: Detect light stand and avoid collision. \n Inputs: Agent's state, light stand's state obtainable. \n Write a reward function in Python.C",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a mountain cliff, aims to detect multiple obstacles (rock outcrop or mountain goats) and reach target, and must avoid collisions by using its own state and the rock outcrop or mountain goats states. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a mountain cliff, aims to detect a obstacle (a tram) and reach target, and must avoid collisions by using its own state and the tram’s state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_tram, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tram = calculate_distance_min(s_agent, s_tram) \n    penalty_collision = -16 if (d_agent_tram < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a mountain cliff, aims to detect obstacles (talus boulders) and reach target, and must avoid collisions by using its own state and the talus boulders states. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_talus_boulder, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_talus_boulder = calculate_distance_min(s_agent, s_talus_boulder) \n    penalty_collision = -16 if (d_agent_talus_boulder < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a mountain cliff, aims to detect a obstacle (an rockfall barrier) and reach target, and must avoid collisions by using its own state and the rockfall barrier’s state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_rockfall_barrier, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rockfall_barrier = calculate_distance_min(s_agent, s_rockfall_barrier) \n    penalty_collision = -16 if (d_agent_rockfall_barrier < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a mountain cliff, aims to detect obstacles (scree slopes) and reach target, and must avoid collisions by using its own state and the scree slopes states. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a mountain cliff, aims to detect a obstacle (a rock outcrop) and reach target, and must avoid collisions by using its own state and the rock outcrop’s state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_rock_outcrop, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rock_outcrop = calculate_distance_min(s_agent, s_rock_outcrop) \n    penalty_collision = -16 if (d_agent_rock_outcrop < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a mountain cliff, aims to detect a obstacle (ledge) and reach target, and must avoid collisions by using its own state and the ledgee’s state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_ledge, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ledge = calculate_distance_min(s_agent, s_ledge) \n    penalty_collision = -16 if (d_agent_ledge < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a mountain cliff, aims to detect obstacles (rock spires) and reach target, and must avoid collisions by using its own state and the rock spires states. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_rock_spire, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rock_spire = calculate_distance_min(s_agent, s_rock_spire) \n    penalty_collision = -16 if (d_agent_rock_spire < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a mountain cliff, aims to detect a obstacle (cliff worker) and reach target, and must avoid collisions by using its own state and cliff worker’s state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_cliff_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cliff_worker = calculate_distance_min(s_agent, s_cliff_worker) \n    penalty_collision = -16 if (d_agent_cliff_worker < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a mountain cliff, aims to detect obstacles (low-hanging branchs) and reach target, and must avoid collisions by using its own state and the low-hanging branchs states. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_branche, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_branche = calculate_distance_min(s_agent, s_branche) \n    penalty_collision = -19 if (d_agent_branche < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a mountain cliff, aims to detect a obstacle (an , walker) and reach target, and must avoid collisions by using its own state and the multiple obstacles’s state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a mountain cliff, aims to detect obstacles (boulder, tree root and rock arch) and reach target, and must avoid collisions by using its own state and the boulder, tree root and rock arch states. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a mountain cliff, aims to detect a obstacle (a cliff staff) and reach target, and must avoid collisions by using its own state and the cliff staff’s state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_cliff_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cliff_staff = calculate_distance_min(s_agent, s_cliff_staff) \n    penalty = -19 if (d_agent_cliff_staff < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a mountain cliff, aims to detect a obstacle (boulder) and reach target, and must avoid collisions by using its own state and the boulder’s state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_boulder, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_boulder = calculate_distance_min(s_agent, s_boulder) \n    penalty = -19 if (d_agent_boulder < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a mountain cliff, aims to detect multiple obstacles (rock walls and ice sheets placed in the study area) and reach target, and must avoid collisions by using its own state and the rock walls and ice sheets states. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a mountain cliff, aims to detect multiple obstacles (queue stanchions and mountain goats) and reach target, and must avoid collisions by using its own state and the queue stanchions and mountain goats states. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a mountain cliff, aims to detect a obstacle (an animal) and reach target, and must avoid collisions by using its own state and the animal’s state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_animal, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_animal = calculate_distance_min(s_agent, s_animal) \n    penalty_collision = -19 if (d_agent_animal < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a mountain cliff, aims to detect a obstacle (an entrance) and reach target, and must avoid collisions by using its own state and the entrance’s state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_entrance, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_entrance = calculate_distance_min(s_agent, s_entrance) \n    penalty_collision = -19 if (d_agent_entrance < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a mountain cliff, aims to detect obstacles (cliff staff, mountain goats and RA maintenance carts) and reach target, and must avoid collisions by using its own state and the cliff staff, mountain goats and RA maintenance carts states. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a mountain cliff, aims to detect obstacles (mountain goats and pillars) and reach target, and must avoid collisions by using its own state and the mountain goats and pillars states. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone moves inside a mountain cliff, aims to reach target, and must avoid collisions by using its own state and the target'S state. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 16 if (d_target < 0.3) else -d_agent_target*3 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a gymnasium setting, a drone navigates while tracking the obstacles and avoiding padded wall mat or athletes and reach target. The agent can access drone's state, target's state and the multiple obstacles' state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a gymnasium setting, a drone navigates while tracking the treadmill and avoiding collisions and reach target. The agent can access drone's state, target's state and the treadmill's state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_treadmill, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_treadmill = calculate_distance_min(s_agent, s_treadmill) \n    penalty_collision = -16 if (d_agent_treadmil < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a gymnasium setting, a drone navigates while tracking the plyometric box and avoiding collisions and reach target. The agent can access drone's state, target's state and the plyometric box's state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_box, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_box = calculate_distance_min(s_agent, s_box) \n    penalty_collision = -16 if (d_agent_box < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a gymnasium setting, a drone navigates while tracking the punching bag and avoiding collisions and reach target. The agent can access drone's state, target's state and the punching bag's state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_punching_bag, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_punching_bag = calculate_distance_min(s_agent, s_punching_bag) \n    penalty_collision = -16 if (d_agent_punching_bag < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a gymnasium setting, a drone navigates while tracking the obstacles and avoiding many stacked gym mats and reach target. The agent can access drone's state, target's state and the multiple obstacles' state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a gymnasium setting, a drone navigates while tracking the padded wall mat and avoiding collisions and reach target. The agent can access drone's state, target's state and the padded wall mat's state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_wall_mat, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall_mat = calculate_distance_min(s_agent, s_wall_mat) \n    penalty_collision = -16 if (d_agent_wall_mat < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a gymnasium setting, a drone navigates while tracking the hanging rings and avoiding collisions and reach target. The agent can access drone's state, target's state and the hanging rings' state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ring, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ring = calculate_distance_min(s_agent, s_ring) \n    penalty_collision = -16 if (d_agent_ring < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a gymnasium setting, a drone navigates while tracking the volleyball pole and avoiding collisions and reach target. The agent can access drone's state, target's state and the volleyball pole's state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_volleyball_pole, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_volleyball_pole = calculate_distance_min(s_agent, s_volleyball_pole) \n    penalty_collision = -16 if (d_agent_volleyball_pole < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a gymnasium setting, a drone navigates while tracking the gymnasium worker and avoiding collisions and reach target. The agent can access drone's state, target's state and the gymnasium worker's state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_gymnasium_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_gymnasium_worker = calculate_distance_min(s_agent, s_gymnasium_worker) \n    penalty_collision = -16 if (d_agent_gymnasium_worker < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a gymnasium setting, a drone navigates while tracking the equipment rack and avoiding collisions and reach target. The agent can access drone's state, target's state and the equipment rack's state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_equipment_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_equipment_rack = calculate_distance_min(s_agent, s_equipment_rack) \n    penalty_collision = -19 if (d_agent_equipment_rack < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a gymnasium setting, a drone navigates while tracking the obstacles and avoiding pillar and mirror and reach target. The agent can access drone's state, target's state and the multiple obstacles' state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a gymnasium setting, a drone navigates while tracking the obstacles and avoiding coach and desk  and reach target. The agent can access drone's state, target's state and the many obstacles' state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a gymnasium setting, a drone navigates while tracking the pull-up bar and avoiding collisions and reach target. The agent can access drone's state, target's state and the pull-up bar's state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bar = calculate_distance_min(s_agent, s_bar) \n    penalty = -19 if (d_agent_bar < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a gymnasium setting, a drone navigates while tracking the tumbling block and avoiding collisions and reach target. The agent can access drone's state, target's state and the tumbling block's state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_tumbling_block, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tumbling_block = calculate_distance_min(s_agent, s_tumbling_block) \n    penalty = -19 if (d_agent_tumbling_block < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a gymnasium setting, a drone navigates while tracking the obstacles and avoiding rings and bars and reach target. The agent can access drone's state, target's state and the rings and bar' state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a gymnasium setting, a drone navigates while tracking the obstacles and avoiding plants ans lights and fan and reach target. The agent can access drone's state, target's state and the multiple obstacles' state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a gymnasium setting, a drone navigates while tracking the gymnasium crew and avoiding collisions and reach target. The agent can access drone's state, target's state and the gymnasium crew's state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_crew, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_crew = calculate_distance_min(s_agent, s_crew) \n    penalty_collision = -19 if (d_agent_crew < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a gymnasium setting, a drone navigates while tracking the rope and avoiding collisions and reach target. The agent can access drone's state, target's state and the rope's state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_rope, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rope = calculate_distance_min(s_agent, s_rope) \n    penalty_collision = -19 if (d_agent_rope < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a gymnasium setting, a drone navigates while tracking the human and balls and avoiding collisions and reach target. The agent can access drone's state, target's state and the multiple obstacles' state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a gymnasium setting, a drone navigates while tracking the human, balls and bars and avoiding collisions and reach target. The agent can access drone's state, target's state and the multiple obstacles' state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "In a gymnasium setting, a drone navigates while avoiding collisions and reaching the target. The agent can access the drone's state and the target's state. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 16 if (d_target < 0.3) else -d_agent_target*3 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Amusement Park. It must avoid colliding with roller coaster visitors or park visitors. The states of the agent, the ride ride fence, and the park visitors are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a Amusement Park, a drone must navigate toward a target while avoiding crashes into the eating table, relying on its state, the table’s positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_table, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_table = calculate_distance_min(s_agent, s_table) \n    penalty_collision = -16 if (d_agent_table < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a Amusement Park, a drone must navigate toward a target while avoiding crashes into the game counter, relying on its state, the game counter’s positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_game_counter, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_game_counter = calculate_distance_min(s_agent, s_game_counter) \n    penalty_collision = -16 if (d_agent_game_counter < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a Amusement Park, a drone must navigate toward a target while avoiding crashes into the mascot, relying on its state, the mascot’s positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_mascot, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_mascot = calculate_distance_min(s_agent, s_mascot) \n    penalty_collision = -16 if (d_agent_mascot < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a Amusement Park, a drone must navigate toward a target while avoiding crashes into the multiple obstacles mascot, visitor, relying on its state, the mascot’s and visitor's positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a Amusement Park, a drone must navigate toward a target while avoiding crashes into the visitor, relying on its state, the visitor’s positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_visitor, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_visitor = calculate_distance_min(s_agent, s_visitor) \n    penalty_collision = -16 if (d_agent_visitor < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a Amusement Park, a drone must navigate toward a target while avoiding crashes into the zipline cable, relying on its state, the zipline cable’s positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_zipline_cable, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_zipline_cable = calculate_distance_min(s_agent, s_zipline_cable) \n    penalty_collision = -16 if (d_agent_zipline_cable < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a Amusement Park, a drone must navigate toward a target while avoiding crashes into the banner pylon, relying on its state, the banner pylon’s positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_banner_pylon, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_banner_pylon = calculate_distance_min(s_agent, s_banner_pylon) \n    penalty_collision = -16 if (d_agent_banner_pylon < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a Amusement Park, a drone must navigate toward a target while avoiding crashes into the worker, relying on its state, the worker’s positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_worker = calculate_distance_min(s_agent, s_worker) \n    penalty_collision = -16 if (d_agent_worker < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a Amusement Park, a drone must navigate toward a target while avoiding crashes into the souvenir, relying on its state, the souvenir’s positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_souvenir_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_souvenir_stand = calculate_distance_min(s_agent, s_souvenir_stand) \n    penalty_collision = -19 if (d_agent_souvenir_stand < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a Amusement Park, a drone must navigate toward a target while avoiding crashes into the mascot, table, chair, relying on its state, the multiple obstacles’s positions, and target data. Design a reward function using Python..",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a Amusement Park, a drone must navigate toward a target while avoiding crashes into the popcorn machine and ballons relying on its state, the multiple obstacles’s positions, and target data. Design a reward function using Python..",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a Amusement Park, a drone must navigate toward a target while avoiding crashes into the photo both, relying on its state, the photot booth’s positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_photo_booth, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_photo_booth = calculate_distance_min(s_agent, s_photo_booth) \n    penalty = -19 if (d_agent_photo_booth < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a Amusement Park, a drone must navigate toward a target while avoiding crashes into the park bench, relying on its state, the park bench’s positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_park_bench, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_park_bench = calculate_distance_min(s_agent, s_park_bench) \n    penalty = -19 if (d_agent_park_bench < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a Amusement Park, a drone must navigate toward a target while avoiding crashes into the mascot, relying on its state, the mascot’s positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a Amusement Park, a drone must navigate toward a target while avoiding crashes into the ballongs and shops, relying on its state, the multiple obstacles’s positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a Amusement Park, a drone must navigate toward a target while avoiding crashes into the figure, relying on its state, the figure's positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_figure, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_figure = calculate_distance_min(s_agent, s_figure) \n    penalty_collision = -19 if (d_agent_figure < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a Amusement Park, a drone must navigate toward a target while avoiding crashes into the rope, relying on its state, the rope's positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_rope, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rope = calculate_distance_min(s_agent, s_rope) \n    penalty_collision = -19 if (d_agent_rope < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Inside a Amusement Park, a drone must navigate toward a target while avoiding crashes into the rope, figure, roller coster, relying on its state, the multiple obstacles's positions, and target data. Design a reward function using Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Amusement Park. \n Goal: Detect park visitors and topiary sculptures and avoid court corners. \n Assumptions: The states of the agent and gates, souvenir stands and barricades state are availalbe. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Amusement Park. \n Goal: Drone target is to reach the target. \n Assumptions: The states of the agent and the target are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 16 if (d_target < 0.3) else -d_agent_target*3 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a greenhouse environment while ensuring it does not crash into trellis frames or hanging baskets and reaches target. State about the drones state, targets state and the states of multiple obstacle trellis frames or hanging baskets can be accessed.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a greenhouse environment while ensuring it does not crash into rotating irrigation boom and reaches target. State about the drones state, targets state and the states of rotating irrigation boom can be accessed.",
        "chosen": "def reward(s_agent, s_irrigation, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_irrigation = calculate_distance_min(s_agent, s_irrigation) \n    penalty_collision = -16 if (d_agent_irrigation < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a greenhouse environment while ensuring it does not crash into potting benches and reaches target. State about the drones state, targets state and the states of potting benches can be accessed.",
        "chosen": "def reward(s_agent, s_bench, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bench = calculate_distance_min(s_agent, s_bench) \n    penalty_collision = -16 if (d_agent_bench < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a greenhouse environment while ensuring it does not crash into shade cloth drape and reaches target. State about the drones state, targets state and the states of shade cloth drape can be accessed.",
        "chosen": "def reward(s_agent, s_cloth_drape, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cloth_drape = calculate_distance_min(s_agent, s_cloth_drape) \n    penalty_collision = -16 if (d_agent_cloth_drape < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a greenhouse environment while ensuring it does not crash into plants and planter and reaches target. State about the drones state, targets state and the states of multiple obstacles can be accessed.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a greenhouse environment while ensuring it does not crash into trellis frame and reaches target. State about the drones state, targets state and the states of trellis frame can be accessed.",
        "chosen": "def reward(s_agent, s_trelli_frame, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_trelli_frame = calculate_distance_min(s_agent, s_trelli_frame) \n    penalty_collision = -16 if (d_agent_trelli_frame < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a greenhouse environment while ensuring it does not crash into overhead mist line and reaches target. State about the drones state, targets state and the states of overhead mist line can be accessed.",
        "chosen": "def reward(s_agent, s_mist_line, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_mist_line = calculate_distance_min(s_agent, s_mist_line) \n    penalty_collision = -16 if (d_agent_mist_line < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a greenhouse environment while ensuring it does not crash into sensor masts and reaches target. State about the drones state, targets state and the states of sensor masts can be accessed.",
        "chosen": "def reward(s_agent, s_sensor_mast, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sensor_mast = calculate_distance_min(s_agent, s_sensor_mast) \n    penalty_collision = -16 if (d_agent_sensor_mast < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a greenhouse environment while ensuring it does not crash into maintenance robot and reaches target. State about the drones state, targets state and the states of maintenance robot can be accessed.",
        "chosen": "def reward(s_agent, s_robot, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_robot = calculate_distance_min(s_agent, s_robot) \n    penalty_collision = -16 if (d_agent_robot < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a greenhouse environment while ensuring it does not crash into supply shelves and reaches target. State about the drones state, targets state and the states of supply shelves can be accessed.",
        "chosen": "def reward(s_agent, s_supply_shelf, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_supply_shelf = calculate_distance_min(s_agent, s_supply_shelf) \n    penalty_collision = -19 if (d_agent_supply_shelf < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a greenhouse environment while ensuring it does not crash into two rolling benchs and reaches target. State about the drones state, targets state and the states of multipel rolling benchs can be accessed.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a greenhouse environment while ensuring it does not crash into fertilizer mixer, hanging vines bundle and sliding partition and reaches target. State about the drones state, targets state and the states of multiple obstacles fertilizer mixer, hanging vines bundle and sliding partition can be accessed.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a greenhouse environment while ensuring it does not crash into hose reel frame and reaches target. State about the drones state, targets state and the states of hose reel frame can be accessed.",
        "chosen": "def reward(s_agent, s_frame, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_frame = calculate_distance_min(s_agent, s_frame) \n    penalty = -19 if (d_agent_frame < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a greenhouse environment while ensuring it does not crash into water barrel and reaches target. State about the drones state, targets state and the states of water barrel can be accessed.",
        "chosen": "def reward(s_agent, s_water_barrel, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_water_barrel = calculate_distance_min(s_agent, s_water_barrel) \n    penalty = -19 if (d_agent_water_barrel < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a greenhouse environment while ensuring it does not crash into polycarbonate panels and glass panes placed in the study area and reaches target. State about the drones state, targets state and the states of multiple obstacles polycarbonate panels and glass panes placed in the study area can be accessed.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a greenhouse environment while ensuring it does not crash into row markers and hanging baskets and reaches target. State about the drones state, targets state and the states of row markers and hanging baskets can be accessed.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a greenhouse environment while ensuring it does not crash into Greenhouse humidifier towers and reaches target. State about the drones state, targets state and the states of Greenhouse humidifier towers can be accessed.",
        "chosen": "def reward(s_agent, s_tower, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tower = calculate_distance_min(s_agent, s_tower) \n    penalty_collision = -19 if (d_agent_tower < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a greenhouse environment while ensuring it does not crash into drip irrigation hose and reaches target. State about the drones state, targets state and the states of drip irrigation hose can be accessed.",
        "chosen": "def reward(s_agent, s_hose_area, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_hose_area = calculate_distance_min(s_agent, s_hose_area) \n    penalty_collision = -19 if (d_agent_hose_area < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a greenhouse environment while ensuring it does not crash into Greenhouse humidifier towers, hanging baskets and RA maintenance carts and reaches target. State about the drones state, targets state and the states of Greenhouse humidifier towers, hanging baskets and RA maintenance carts can be accessed.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a greenhouse environment while ensuring it does not crash into hanging baskets and tall planters and reaches target. State about the drones state, targets state and the states of hanging baskets and tall planters can be accessed.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The problem involves guiding a drone through a greenhouse environment while ensuring it reaches target. State about the drones state and the targets state can be accessed.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = -16 if (d_agent_target < 0.3) else d_agent_target*3 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Market Street. It must avoid colliding with lamp post or pedestrians. The states of the agent, the fence, and the pedestrians are known. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_obs_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_obs_min < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a busy Market Street. It must avoid colliding with Market Street tram. The states of the agent, the Market Street tram are known. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_tram, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tram = calculate_distance_min(s_agent, s_tram) \n    penalty_collision = -16 if (d_agent_tram < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Market Street common room. \n Objectives: It must avoid colliding with vendor carts. \n Conditions: States of the agent and the vendor cart are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_crate, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_carte = calculate_distance_min(s_agent, s_crate) \n    penalty_collision = -16 if (d_agent_carte < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a side street, the drone must maneuver around construction barriers. All entity states (agent, construction barrier) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_construction_barrier, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_construction_barrier = calculate_distance_min(s_agent, s_moat) \n    penalty_collision = -16 if (d_agent_construction_barrier < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a street plaza. \n Requirement: Avoid collisions with delivery carts. \n Given: The states for the agent and many carts are known. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone operates near a street intersection during an event. So not collide with lamp post. The states of the agent, the lamp post are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_lamp_post, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lamp_post = calculate_distance_min(s_agent, s_fence_post) \n    penalty_collision = -16 if (d_agent_lamp_post < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in a Market Street. \n Objectives: Do not collide hit with overhead banners, \n Conditions: States of the agent, overhead banners are available. \n Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_banner, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_banner = calculate_distance_min(s_agent, s_banner) \n    penalty_collision = -16 if (d_agent_banner < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a street plaza, the drone must steer clear of traffic light poles. All states (agent, traffic light pole) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_traffic_light, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_traffic_light = calculate_distance_min(s_agent, s_traffic_light) \n    penalty_collision = -16 if (d_agent_traffic_light < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: \n Environment: street-side area. \n Avoid: Hitting with Market Street worker. \n Known: Agent, Market Street worker states. \n Create the Python reward function.",
        "chosen": "def reward(s_agent, s_street_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_street_worker = calculate_distance_min(s_agent, s_street_worker) \n    penalty_collision = -16 if (d_agent_street_worker < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Market Street. \n Objectives: It must avoid colliding with kiosks. \n Conditions: The states of the agent and the kiosk are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_kiosk, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_kiosk = calculate_distance_min(s_agent, s_kiosk) \n    penalty_collision = -19 if (d_agent_kiosk < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Market Street. \n Objectives: Avoid collision with construction scaffold and tarp. \n Conditions: The states of the agent, construction scaffold and tarp are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Market Street. \n Objectives: It must avoid colliding with any propane tank, sidewalk sign and gate. \n Conditions: Agent's and all obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -19 if (dagent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Market Street. \n Objectives: Do not collide with any Market Street staff. \n Conditions: The states of the agent and the Market Street staff are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_market_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_market_staff = calculate_distance_min(s_agent, s_market_staff) \n    penalty = -19 if (d_agent_market_staff < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Market Street and must avoid contact with any service vehicle. The states of the drone and the service vehicle are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_service_vehicle, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_service_vehicle = calculate_distance_min(s_agent, s_service_vehicle) \n    penalty = -19 if (d_agent_service_vehicle < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Market Street. It must avoid colliding with storefronts and glass doors placed in the study area. The states of the drone and multiple boxes are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Market Street. It must avoid colliding with queue stanchions and pedestrians. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Market Street. It must avoid colliding with Market Street staff. Agent's state, target's state and Market Street staff's state are accessible. Write a reward function in Python for this scenario.",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_staff = calculate_distance_min(s_agent, s_staff) \n    penalty_collision = -19 if (d_agent_staff < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Market Street. \n Goal: It must avoid colliding with freight elevator entrance. \n Assumptions: The states of the agent and the elevator are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_elevator, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_elevator = calculate_distance_min(s_agent, s_elevator) \n    penalty_collision = -19 if (d_agent_elevator < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Market Street. \n Goal: It must avoid colliding with Market Street staff, pedestrians and RA maintenance carts. \n Assumptions: The Agent's state, target's state and Market Street staff's state, service vehicle's state and pedestrians' states are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Market Street. \n Goal: Detect pedestrians and pillars and avoid building corners. \n Assumptions: The states of the agent and gates, kiosks and barricades state are availalbe. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Market Street. \n Goal: Drone target is to reach the target. \n Assumptions: The states of the agent and the target are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move inside an indoor film studio with a drone. The drone must reach the target while detecting light stas as obstacles and avoiding collisions, assuming the agent’s state,  multiple obstacles: light stas' states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -d_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move inside an indoor film studio with a drone. The drone must reach the target while detecting camera as obstacles and avoiding collisions, assuming the agent’s state, camera' states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_camera, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_camera = calculate_distance_min(s_agent, s_camera) \n    penalty_collision = -16 if (d_agent_camera < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move inside an indoor film studio with a drone. The drone must reach the target while detecting prop carts as obstacles and avoiding collisions, assuming the agent’s state, prop carts' states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_prop_cart, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_prop_cart = calculate_distance_min(s_agent, s_prop_cart) \n    penalty_collision = -16 if (d_agent_prop_cart < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move inside an indoor film studio with a drone. The drone must reach the target while detecting set barricades as obstacles and avoiding collisions, assuming the agent’s state, set barricades' states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_set_barrier, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_set_barrier = calculate_distance_min(s_agent, s_set_barrier) \n    penalty_collision = -16 if (d_agent_set_barrier < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move inside an indoor film studio with a drone. The drone must reach the target while detecting actors and actress as obstacles and avoiding collisions, assuming the agent’s state,  multiple obstacles' states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacle_min < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -d_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move inside an indoor film studio with a drone. The drone must reach the target while detecting light stands as obstacles and avoiding collisions, assuming the agent’s state, light stands' states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_light_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_light_stand = calculate_distance_min(s_agent, s_light_stand) \n    penalty_collision = -16 if (d_agent_light_stand < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move inside an indoor film studio with a drone. The drone must reach the target while detecting overhead cable rigging as obstacles and avoiding collisions, assuming the agent’s state, overhead cable rigging' states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_cable, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cable = calculate_distance_min(s_agent, s_cable) \n    penalty_collision = -16 if (d_agent_cable < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move inside an indoor film studio with a drone. The drone must reach the target while detecting lighting towers as obstacles and avoiding collisions, assuming the agent’s state, lighting towers' states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_lighting_tower, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lighting_tower = calculate_distance_min(s_agent, s_lighting_tower) \n    penalty_collision = -16 if (d_agent_lighting_tower < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -d_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move inside an indoor film studio with a drone. The drone must reach the target while detecting workers as obstacles and avoiding collisions, assuming the agent’s state, workers' states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_studio_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_studio_worker = calculate_distance_min(s_agent, s_studio_worker) \n    penalty_collision = -16 if (d_agent_studio_worker < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move inside an indoor film studio with a drone. The drone must reach the target while detecting set  as obstacles and avoiding collisions, assuming the agent’s state, set' states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_set, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_set = calculate_distance_min(s_agent, s_set) \n    penalty_collision = -19 if (d_agent_set < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move inside an indoor film studio with a drone. The drone must reach the target while detecting stage and mic as obstacles and avoiding collisions, assuming the agent’s state,  multiple obstacles' states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacle < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move inside an indoor film studio with a drone. The drone must reach the target while detecting props and tables as as obstacles and avoiding collisions, assuming the agent’s state,  multiple obstacles' states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -19 if (d_obs_min < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move inside an indoor film studio with a drone. The drone must reach the target while detecting studio staff as as obstacles and avoiding collisions, assuming the agent’s state, as' states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_studio_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_studio_staff = calculate_distance_min(s_agent, s_studio_staff) \n    penalty = -19 if (d_agent_studio_staff < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move inside an indoor film studio with a drone. The drone must reach the target while detecting grip truck as obstacles and avoiding collisions, assuming the agent’s state, as' states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_grip_truck, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_grip_truck = calculate_distance_min(s_agent, s_grip_truck) \n    penalty = -19 if (d_agent_grip_truck < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move inside an indoor film studio with a drone. The drone must reach the target while detecting set walls and actor as obstacles and avoiding collisions, assuming the agent’s state,  multiple obstacles:  walls and actor' states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move inside an indoor film studio with a drone. The drone must reach the target while detecting food and regessieur as obstacles and avoiding collisions, assuming the agent’s state,  multiple obstacles: actor and ressigeur' states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move inside an indoor film studio with a drone. The drone must reach the target while detecting staff as obstacles and avoiding collisions, assuming the agent’s state, Film Studio staff' states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_staff = calculate_distance_min(s_agent, s_staff) \n    penalty_collision = -19 if (d_agent_staff < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move inside an indoor film studio with a drone. The drone must reach the target while detecting lift as obstacles and avoiding collisions, assuming the agent’s state, lift' states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_lift, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lift = calculate_distance_min(s_agent, s_lift) \n    penalty_collision = -19 if (d_agent_lift < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -d_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move inside an indoor film studio with a drone. The drone must reach the target while detecting Film Studio staff, extras a as obstacles and avoiding collisions, assuming the agent’s state, multiple obstacles' states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move inside an indoor film studio with a drone. The drone must reach the target while detecting set BG and human as obstacles and avoiding collisions, assuming the agent’s state, set BG and human as multiple obstacles' states, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move inside an indoor film studio with a drone. The drone must reach the target, assuming the agent’s state, and the target state are observable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_targetl*3 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a mountain cliff with the aim of reaching a target while avoiding rock outcrop or mountain goats with obstacles. Available Information: The agent’s state, target's state and the multiple obstacles' states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a mountain cliff with the aim of reaching a target while avoiding collisions with tram. Available Information: The agent’s state, target's state and the tram's state can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_tram, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tram = calculate_distance_min(s_agent, s_tram) \n    penalty_collision = -16 if (d_agent_tram < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a mountain cliff with the aim of reaching a target while avoiding collisions with talus boulder. Available Information: The agent’s state, target's state and the talus boulder's state can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_talus_boulder, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_talus_boulder = calculate_distance_min(s_agent, s_talus_boulder) \n    penalty_collision = -16 if (d_agent_talus_boulder < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a mountain cliff with the aim of reaching a target while avoiding collisions with rockfall barrier. Available Information: The agent’s state, target's state and the rockfall barrier's state can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_rockfall_barrier, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rockfall_barrier = calculate_distance_min(s_agent, s_rockfall_barrier) \n    penalty_collision = -16 if (d_agent_rockfall_barrier < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a mountain cliff with the aim of reaching a target while avoiding multiple carts with obstacles. Available Information: The agent’s state, target's state and the multiple obstacles' states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a mountain cliff with the aim of reaching a target while avoiding collisions with rock outcrop. Available Information: The agent’s state, target's state and the rock outcrop's state can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_rock_outcrop, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rock_outcrop = calculate_distance_min(s_agent, s_rock_outcrop) \n    penalty_collision = -16 if (d_agent_rock_outcrop < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a mountain cliff with the aim of reaching a target while avoiding collisions with overhanging ledge. Available Information: The agent’s state, target's state and the overhanging ledge's state can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_ledge, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ledge = calculate_distance_min(s_agent, s_ledge) \n    penalty_collision = -16 if (d_agent_ledge < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a mountain cliff with the aim of reaching a target while avoiding collisions with rock spire. Available Information: The agent’s state, target's state and the rock spire's state can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_rock_spire, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rock_spire = calculate_distance_min(s_agent, s_rock_spire) \n    penalty_collision = -16 if (d_agent_rock_spire < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a mountain cliff with the aim of reaching a target while avoiding collisions with Mountain Cliff worker. Available Information: The agent’s state, target's state and the Mountain Cliff worker's state can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_cliff_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cliff_worker = calculate_distance_min(s_agent, s_cliff_worker) \n    penalty_collision = -16 if (d_agent_cliff_worker < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a mountain cliff with the aim of reaching a target while avoiding collisions with low-hanging branches. Available Information: The agent’s state, target's state and the low-hanging branches' states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_branche, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_branche = calculate_distance_min(s_agent, s_branche) \n    penalty_collision = -19 if (d_agent_branche < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a mountain cliff with the aim of reaching a target while avoiding collisions with ice overhang and snow cornice. Available Information: The agent’s state, target's state and the multiple obstacles; ice overhang and snow cornice' states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a mountain cliff with the aim of reaching a target while avoiding collisions with boulder, tree root and rock arch. Available Information: The agent’s state, target's state and the multiple obstacles: boulder, tree root and rock arch' states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a mountain cliff with the aim of reaching a target while avoiding collisions with Mountain Cliff staff. Available Information: The agent’s state, target's state and the Mountain Cliff staff's state can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_cliff_staff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cliff_staff = calculate_distance_min(s_agent, s_cliff_staff) \n    penalty = -19 if (d_agent_cliff_staff < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a mountain cliff with the aim of reaching a target while avoiding collisions with fallen boulder. Available Information: The agent’s state, target's state and the fallen boulder's state can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_boulder, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_boulder = calculate_distance_min(s_agent, s_boulder) \n    penalty = -19 if (d_agent_boulder < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a mountain cliff with the aim of reaching a target while avoiding collisions with rock walls and ice sheets. Available Information: The agent’s state, target's state and the multiple obstacles' states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a mountain cliff with the aim of reaching a target while avoiding collisions with obstacles. Available Information: The agent’s state, target's state and the obstacles' states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a mountain cliff with the aim of reaching a target while avoiding collisions with animal. Available Information: The agent’s state, target's state and the animal's state can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_animal, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_animal = calculate_distance_min(s_agent, s_animal) \n    penalty_collision = -19 if (d_agent_animal < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a mountain cliff with the aim of reaching a target while avoiding collisions with cave entrance. Available Information: The agent’s state, target's state and the cave entrance's state can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_entrance, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_entrance = calculate_distance_min(s_agent, s_entrance) \n    penalty_collision = -19 if (d_agent_entrance < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a mountain cliff with the aim of reaching a target while avoiding collisions with queue and mountain goats. Available Information: The agent’s state, target's state and the multiple obstacles' states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a mountain cliff with the aim of reaching a target while avoiding collisions with visitors, vendors. Available Information: The agent’s state, target's state and the multiple obstacles: visitors and vendor' states can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone navigates in a mountain cliff with the aim of reaching a target. Available Information: The agent’s state and the target's state can be used. Create a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 16 if (d_target < 0.3) else -math.exp(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Gymnasium. It must avoid colliding with padded wall mat or athletes. The states of the agent, the fence, and the athletes are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a busy Gymnasium. Detect treadmill and aviod it. The states of the agent, the treadmill are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_treadmill, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_treadmill = calculate_distance_min(s_agent, s_treadmill) \n    penalty_collision = -16 if (d_agent_treadmil < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Gymnasium common room. \n Objectives: It must avoid colliding with plyometric boxes. \n Conditions: States of the agent and the plyometric box are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_box, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_box = calculate_distance_min(s_agent, s_box) \n    penalty_collision = -16 if (d_agent_box < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a sideline walkway, the drone must maneuver around punching bag. All entity states (agent, punching bag) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_punching_bag, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_punching_bag = calculate_distance_min(s_agent, s_punching_bag) \n    penalty_collision = -16 if (d_agent_punching_bag < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a gym floor. \n Requirement: Avoid collisions with stacked gym mats. \n Given: The states for the agent and many gym mats are known. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone operates near a court intersection during an event. So not collide with padded wall mat. The states of the agent, the padded wall mat are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_wall_mat, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall_mat = calculate_distance_min(s_agent, s_wall_mat) \n    penalty_collision = -16 if (d_agent_wall_mat < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in a Gymnasium. \n Objectives: Do not collide hit with hanging rings, \n Conditions: States of the agent, hanging rings are available. \n Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_ring, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ring = calculate_distance_min(s_agent, s_ring) \n    penalty_collision = -16 if (d_agent_ring < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a gym floor, the drone must steer clear of volleyball poles. All states (agent, volleyball pole) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_volleyball_pole, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_volleyball_pole = calculate_distance_min(s_agent, s_volleyball_pole) \n    penalty_collision = -16 if (d_agent_volleyball_pole < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: \n Environment: sideline area. \n Avoid: Hitting with Gymnasium worker. \n Known: Agent, Gymnasium worker states. \n Create the Python reward function.",
        "chosen": "def reward(s_agent, s_gymnasium_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_gymnasium_worker = calculate_distance_min(s_agent, s_gymnasium_worker) \n    penalty_collision = -16 if (d_agent_gymnasium_worker < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Gymnasium. \n Objectives: It must avoid colliding with equipment racks. \n Conditions: The states of the agent and the equipment rack are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_equipment_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_equipment_rack = calculate_distance_min(s_agent, s_equipment_rack) \n    penalty_collision = -19 if (d_agent_equipment_rack < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Gymnasium. \n Objectives: Avoid collision with divider curtain. \n Conditions: The states of the agent, divider curtain and staff are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Gymnasium. \n Objectives: It must avoid colliding with any medicine ball rack, floor marker and gate. \n Conditions: Agent's and all obstacles' states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Gymnasium. \n Objectives: Do not collide with any pull-up bar. \n Conditions: The states of the agent and the pull-up bar are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bar = calculate_distance_min(s_agent, s_bar) \n    penalty = -19 if (d_agent_bar < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Gymnasium and must avoid contact with any tumbling block. The states of the drone and the tumbling block are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_tumbling_block, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tumbling_block = calculate_distance_min(s_agent, s_tumbling_block) \n    penalty = -19 if (d_agent_tumbling_block < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Gymnasium. It must avoid colliding with storefronts and glass doors placed in the study area. The states of the drone and multiple boxes are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Gymnasium. It must avoid colliding with queue stanchions and athletes. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Gymnasium. It must avoid colliding with Gymnasium crew. Agent's state, target's state and Gymnasium crew's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_crew, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_crew = calculate_distance_min(s_agent, s_crew) \n    penalty_collision = -19 if (d_agent_crew < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Gymnasium. \n Goal: It must avoid colliding with rope. \n Assumptions: The states of the agent and the rope area are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_rope, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rope = calculate_distance_min(s_agent, s_rope) \n    penalty_collision = -19 if (d_agent_rope < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Gymnasium. \n Goal: It must avoid colliding with Gymnasium crew, athletes and RA maintenance carts. \n Assumptions: The Agent's state, target's state and Gymnasium crew's state, tumbling block's state and athletes' states are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Gymnasium. \n Goal: Detect athletes and pillars and avoid court corners. \n Assumptions: The states of the agent and gates, equipment racks and barricades state are availalbe. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Gymnasium. \n Goal: Drone target is to reach the target. \n Assumptions: The states of the agent and the target are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 16 if (d_target < 0.3) else -math.exp(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Amusement Park. It must avoid colliding with roller coaster supports or park visitors. The states of the agent, the ride ride fence, and the park visitors are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a busy Amusement Park. Detect spinning teacup platform and aviod it. The states of the agent, the spinning teacup platform are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_treadmill, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_treadmill = calculate_distance_min(s_agent, s_treadmill) \n    penalty_collision = -16 if (d_agent_treadmill < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Amusement Park common room. \n Objectives: It must avoid colliding with carnival game counters. \n Conditions: States of the agent and the carnival game counter are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_game_counter, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_game_counter = calculate_distance_min(s_agent, s_game_counter) \n    penalty_collision = -16 if (d_agent_game_counter < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a midway, the drone must maneuver around inflatable mascot. All entity states (agent, inflatable mascot) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_mascot, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_mascot = calculate_distance_min(s_agent, s_mascot) \n    penalty_collision = -16 if (d_agent_mascot < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a central plaza. \n Requirement: Avoid collisions with multiple stroller parking racks. \n Given: The states for the agent and many stroller parking racks are known. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone operates near a ride concourse during an event. So not collide with roller coaster supports. The states of the agent, the roller coaster supports are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_support, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_support = calculate_distance_min(s_agent, s_support) \n    penalty_collision = -16 if (d_agent_support < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in a Amusement Park. \n Objectives: Do not collide hit with zipline cable, \n Conditions: States of the agent, zipline cable are available. \n Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_zipline_cable, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_zipline_cable = calculate_distance_min(s_agent, s_zipline_cable) \n    penalty_collision = -16 if (d_agent_zipline_cable < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a central plaza, the drone must steer clear of banner pylons. All states (agent, banner pylon) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_banner_pylon, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_banner_pylon = calculate_distance_min(s_agent, s_banner_pylon) \n    penalty_collision = -16 if (d_agent_banner_pylon < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: \n Environment: midway zone. \n Avoid: Hitting with Amusement Park worker. \n Known: Agent, Amusement Park worker states. \n Create the Python reward function.",
        "chosen": "def reward(s_agent, s_worker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_worker = calculate_distance_min(s_agent, s_worker) \n    penalty_collision = -16 if (d_agent_worker < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Amusement Park. \n Objectives: It must avoid colliding with souvenir stands. \n Conditions: The states of the agent and the souvenir stand are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_souvenir_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_souvenir_stand = calculate_distance_min(s_agent, s_souvenir_stand) \n    penalty_collision = -19 if (d_agent_souvenir_stand < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Amusement Park. \n Objectives: Avoid collision with parade float. \n Conditions: The states of the agent, parade float and staff are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Amusement Park. \n Objectives: It must avoid colliding with any popcorn machine, balloon cluster and turnstile. \n Conditions: Agent's, food counter and cars states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Amusement Park. \n Objectives: Do not collide with any photo booth frame. \n Conditions: The states of the agent and the photo booth frame are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_photo_booth, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_photo_booth = calculate_distance_min(s_agent, s_photo_booth) \n    penalty = -19 if (d_agent_photo_booth < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Amusement Park and must avoid contact with any park bench. The states of the agent and the park bench are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_park_bench, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_park_bench = calculate_distance_min(s_agent, s_park_bench) \n    penalty = -19 if (d_agent_park_bench < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Amusement Park. It must avoid colliding with arcade facades and glass displays placed in the study area. The states of the agent, game_counteres and crew are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Amusement Park. It must avoid colliding with queue stanchions and park visitors. The states of the agent and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Amusement Park. It must avoid colliding with Amusement Park animatronic figures. Agent's state, target's state and Amusement Park animatronic figures's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_figure, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_figure = calculate_distance_min(s_agent, s_figure) \n    penalty_collision = -19 if (d_agent_figure < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Amusement Park. \n Goal: It must avoid colliding with suspension bridge rope. \n Assumptions: The states of the agent and the suspension bridge rope area are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_rope, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_rope = calculate_distance_min(s_agent, s_rope) \n    penalty_collision = -19 if (d_agent_rope < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Amusement Park. \n Goal: It must avoid colliding with Amusement Park animatronic figures, park visitors and RA maintenance carts. \n Assumptions: The Agent's state, target's state and Amusement Park animatronic figures's state, park bench's state and park visitors' states are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Amusement Park. \n Goal: Detect park visitors and topiary sculptures and avoid court corners. \n Assumptions: The states of the agent and gates, souvenir stands and barricades state are availalbe. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Amusement Park. \n Goal: Drone target is to reach the target. \n Assumptions: The states of the agent and the target are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 16 if (d_target < 0.3) else -math.exp(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent is operating along Greenhouse. It must avoid colliding with trellis frames or hanging baskets. The states of the agent, the ride plant trelli_frame fence, and the hanging baskets are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent is operating inside a busy Greenhouse. Detect rotating irrigation boom and aviod it. The states of the agent, the rotating irrigation boom are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_irrigation, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_irrigation = calculate_distance_min(s_agent, s_irrigation) \n    penalty_collision = -16 if (d_agent_irrigation < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A agent is navigating in a Greenhouse. \n Objectives: It must avoid colliding with potting benches. \n Conditions: States of the agent and the potting bench are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bench, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bench = calculate_distance_min(s_agent, s_bench) \n    penalty_collision = -16 if (d_agent_bench < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a aisle, the agent must maneuver around shade cloth drape. All entity states (agent, shade cloth drape) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_cloth_drape, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cloth_drape = calculate_distance_min(s_agent, s_cloth_drape) \n    penalty_collision = -16 if (d_agent_cloth_drape < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A agent is flying through a central aisle. \n Requirement: Detect the three plant racks and do not collide with it. \n Given: The states for the agent and many plant racks are known. \n Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent operates near a service corridor during an event. So not collide with trellis frame. The states of the agent, the trellis frames are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_trelli_frame, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_trelli_frame = calculate_distance_min(s_agent, s_trelli_frame) \n    penalty_collision = -16 if (d_agent_trelli_frame < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in a Greenhouse. \n Objectives: Do not collide hit with overhead mist line, \n Conditions: States of the agent, overhead mist line are available. \n Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_mist_line, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_mist_line = calculate_distance_min(s_agent, s_mist_line) \n    penalty_collision = -16 if (d_agent_mist_line < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a central aisle, the agent must steer clear of sensor masts. All states (agent, sensor mast) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_sensor_mast, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sensor_mast = calculate_distance_min(s_agent, s_sensor_mast) \n    penalty_collision = -16 if (d_agent_sensor_mast < 0.6) else 16 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: \n Environment: propagation zone. \n Avoid: Hitting with Greenhouse maintenance robot. \n Known: Agent, Greenhouse maintenance robot states. \n Create the Python reward function.",
        "chosen": "def reward(s_agent, s_robot, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_robot = calculate_distance_min(s_agent, s_robot) \n    penalty_collision = -16 if (d_agent_robot < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A agent is navigating Greenhouse. \n Objectives: It must avoid colliding with supply shelves. \n Conditions: The states of the agent and the supply shelf are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_supply_shelf, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_supply_shelf = calculate_distance_min(s_agent, s_supply_shelf) \n    penalty_collision = -19 if (d_agent_supply_shelf < 0.6) else 4 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A agent is flying through Greenhouse. \n Objectives: Avoid collision with two rolling benchs. \n Conditions: The states of the agent, rolling bench and staff are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A agent is navigating Greenhouse. \n Objectives: It must avoid colliding with any fertilizer mixer, hanging vines bundle and sliding partition. \n Conditions: Agent's, food counter and cars states are accessible. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A agent is flying through Greenhouse. \n Objectives: Do not collide with any hose reel frame. \n Conditions: The states of the agent and the hose reel frame are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_frame, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_frame = calculate_distance_min(s_agent, s_frame) \n    penalty = -19 if (d_agent_frame < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent is operating along Greenhouse and must avoid contact with any water barrel. The states of the agent and the water barrel are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_water_barrel, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_water_barrel = calculate_distance_min(s_agent, s_water_barrel) \n    penalty = -19 if (d_agent_water_barrel < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent is operating in Greenhouse. It must avoid colliding with polycarbonate panels and glass panes placed in the study area. The states of the agent, benches and crew are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent is operating in Greenhouse. It must avoid colliding with row markers and hanging baskets. The states of the agent and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A agent is inspecting in Greenhouse. It must avoid colliding with Greenhouse humidifier towers. Agent's state, target's state and Greenhouse humidifier towers's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_tower, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tower = calculate_distance_min(s_agent, s_tower) \n    penalty_collision = -19 if (d_agent_tower < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A agent is navigating Greenhouse. \n Goal: It must avoid colliding with drip irrigation hose. \n Assumptions: The states of the agent and the drip irrigation hose area are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_hose_area, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_hose_area = calculate_distance_min(s_agent, s_hose_area) \n    penalty_collision = -19 if (d_agent_hose_area < 0.6) else 16 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A agent is navigating Greenhouse. \n Goal: It must avoid colliding with Greenhouse humidifier towers, hanging baskets and RA maintenance carts. \n Assumptions: The Agent's state, target's state and Greenhouse humidifier towers's state, water barrel's state and hanging baskets' states are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A agent is navigating Greenhouse. \n Goal: Detect hanging baskets and tall planters and avoid court corners. \n Assumptions: The states of the agent and gates, supply shelves and barricades state are availalbe. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 16 \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A agent is navigating Greenhouse. \n Goal: Drone target is to reach the target. \n Assumptions: The states of the agent and the target are known. \n Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 16 if (d_agent_target < 0.3) else -d_agent_target*3 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Parking Garage. It must avoid colliding with concrete columns or wayfinding signs. The states of the agent, the ride guardrail, and the wayfinding signs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a busy Parking Garage. Detect ramp guardrail and aviod it. The states of the agent, the ramp guardrail are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ramp, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ramp = calculate_distance_min(s_agent, s_ramp) \n    penalty_collision = -17 if (d_agent_ramp < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Parking Garage. Objectives: It must avoid colliding with payment kiosks. Conditions: States of the drone and the payment kiosk are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_payment_kiosk, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_payment_kiosk = calculate_distance_min(s_agent, s_payment_kiosk) \n    penalty_collision = -17 if (d_agent_payment_kiosk < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a drive aisle, the drone must maneuver around roll-up door. All entity states (agent, roll-up door) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_door, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_door = calculate_distance_min(s_agent, s_door) \n    penalty_collision = -17 if (d_agent_door < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a drive aisle. Requirement: Detect the three wheel stops and do not collide with it. Given: The states for the drone and many wheel stops are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone operates near a service corridor during an event. So not collide with car. The states of the agent, the car are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_car, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_car = calculate_distance_min(s_agent, s_car) \n    penalty_collision = -17 if (d_agent_car < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in a Parking Garage. Objectives: Do not collide hit with truck. Conditions: States of the agent, truck are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_truck, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_truck = calculate_distance_min(s_agent, s_truck) \n    penalty_collision = -17 if (d_agent_truck < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a drive aisle, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (d_agent_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: parking level. Avoid: Hitting with Parking Garage's vehicle. Known: Agent, Parking Garage vehicle states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_vehicle, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_vehicle = calculate_distance_min(s_agent, s_vehicle) \n    penalty_collision = -17 if (d_agent_vehicle < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Parking Garage. Objectives: It must avoid colliding with pay stations human. Conditions: The states of the drone and human are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -19 if (d_agent_human < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Parking Garage. Objectives: Avoid collision with two traffic cone lines. Conditions: The states of the agent, traffic cone lines and attendants are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Parking Garage. Objectives: It must avoid colliding with any truck, overhead cable bundle and gate arm. Conditions: Agent's, truck and gate arm states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Parking Garage. Objectives: Do not collide with any EV charging station. Conditions: The states of the drone and the EV charging station are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_charging_station, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_charging_station = calculate_distance_min(s_agent, s_charging_station) \n    penalty = -19 if (d_agent_charging_station < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Parking Garage and must avoid contact with any gate. The states of the drone and the gate are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_gate, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_gate = calculate_distance_min(s_agent, s_gate) \n    penalty = -19 if (d_agent_gate < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Parking Garage. It must avoid colliding with the motorcycle, attendant and sign. The states of the agent, motorcycle, attendant and sign are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Parking Garage. It must avoid colliding with bollards and wayfinding signs. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Parking Garage. It must avoid colliding with Parking Garage ventilation ducts. Agent's state, target's state and Parking Garage ventilation ducts's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ventilation, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ventilation = calculate_distance_min(s_agent, s_ventilation) \n    penalty_collision = -19 if (d_agent_ventilation < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Parking Garage. Goal: It must avoid colliding with overhead conduit. Assumptions: The states of the drone and the overhead conduit area are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_conduit, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_conduit = calculate_distance_min(s_agent, s_conduit) \n    penalty_collision = -19 if (d_agent_conduit < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Parking Garage. Goal: It must avoid colliding with Parking Garage ventilation ducts, wayfinding signs and wheel stops. Assumptions: The Agent's state, target's state and Parking Garage ventilation ducts's state, wheel stop's state and wayfinding signs' states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Parking Garage. Goal: Detect wayfinding signs and structural brackets and avoid drive aisle corners. Assumptions: The states of the drone and gates, pay stations and barricades state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Parking Garage. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a rainforest. The drone must detect and avoid concrete columns or wayfinding signs while heading toward a target. You are provided with the agents state, the targets state, and multiple obstcales: the positions of concrete columns or wayfinding signs in the rainforest.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 18 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a rainforest. The drone must detect and avoid fallen log while heading toward a target. You are provided with the agents state, the targets state, and the positions of fallen log in the rainforest.",
        "chosen": "def reward(s_agent, s_fallen_log, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_fallen_log = calculate_distance_min(s_agent, s_fallen_log) \n    penalty_collision = -18 if (d_agent_fallen_log < 0.6) else 18 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a rainforest. The drone must detect and avoid termite mound while heading toward a target. You are provided with the agents state, the targets state, and the positions of termite mound in the rainforest.",
        "chosen": "def reward(s_agent, s_termite_mound, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_termite_mound = calculate_distance_min(s_agent, s_termite_mound) \n    penalty_collision = -18 if (d_agent_termite_mound < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a rainforest. The drone must detect and avoid vine curtain while heading toward a target. You are provided with the agents state, the targets state, and the positions of vine curtain in the rainforest.",
        "chosen": "def reward(s_agent, s_vine_curtain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_vine_curtain = calculate_distance_min(s_agent, s_vine_curtain) \n    penalty_collision = -18 if (d_agent_vine_curtain < 0.6) else 18 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a rainforest. The drone must detect and avoid it while heading toward a target. You are provided with the agents state, the targets state, and the positions of multiple obstcales in the rainforest.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 18 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a rainforest. The drone must detect and avoid boar while heading toward a target. You are provided with the agents state, the targets state, and the positions of boar in the rainforest.",
        "chosen": "def reward(s_agent, s_boar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_boar = calculate_distance_min(s_agent, s_boar) \n    penalty_collision = -18 if (d_agent_boar < 0.6) else 18 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a rainforest. The drone must detect and avoid elephant while heading toward a target. You are provided with the agents state, the targets state, and the positions of elephant in the rainforest.",
        "chosen": "def reward(s_agent, s_elephant, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_elephant = calculate_distance_min(s_agent, s_elephant) \n    penalty_collision = -18 if (d_agent_elephant < 0.6) else 4 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a rainforest. The drone must detect and avoid cliff while heading toward a target. You are provided with the agents state, the targets state, and the positions of cliff in the rainforest.",
        "chosen": "def reward(s_agent, s_cliff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cliff = calculate_distance_min(s_agent, s_cliff) \n    penalty_collision = -18 if (d_agent_cliff < 0.6) else 18 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a rainforest. The drone must detect and avoid Hitting with wild_animal while heading toward a target. You are provided with the agents state, the targets state, and the positions of wild_animal in the rainforest.",
        "chosen": "def reward(s_agent, s_wild_animal, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wild_animal = calculate_distance_min(s_agent, s_wild_animal) \n    penalty_collision = -18 if (d_agent_wild_animal < 0.6) else 4 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a rainforest. The drone must detect and avoid human while heading toward a target. You are provided with the agents state, the targets state, and the positions of human in the rainforest.",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -19 if (d_agent_human < 0.6) else 4 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a rainforest. The drone must detect and avoid collision with two trees while heading toward a target. You are provided with the agents state, the targets state, and the positions of multiple obstcales: two trees in the rainforest.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a rainforest. The drone must detect and avoid elephant, branches and monkey while heading toward a target. You are provided with the agents state, the targets state, and the positions of multiple obstcales: elephant, branches and monkey in the rainforest.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a rainforest. The drone must detect and avoid ranger while heading toward a target. You are provided with the agents state, the targets state, and the positions of ranger in the rainforest.",
        "chosen": "def reward(s_agent, s_ranger, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ranger = calculate_distance_min(s_agent, s_ranger) \n    penalty = -19 if (d_agent_ranger < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a rainforest. The drone must detect and avoid gate while heading toward a target. You are provided with the agents state, the targets state, and the positions of gate in the rainforest.",
        "chosen": "def reward(s_agent, s_gate, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_gate = calculate_distance_min(s_agent, s_gate) \n    penalty = -19 if (d_agent_gate < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a rainforest. The drone must detect and avoid birds, ranger and sign while heading toward a target. You are provided with the agents state, the targets state, and the positions of multiple obstcales: birds, ranger and sign in the rainforest.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a rainforest. The drone must detect and avoid hills and wayfinding signs while heading toward a target. You are provided with the agents state, the targets state, and the positions of multiple obstcales: hills and wayfinding signs in the rainforest.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a rainforest. The drone must detect and avoid cliff  while heading toward a target. You are provided with the agents state, the targets state, and the positions of cliff  in the rainforest.",
        "chosen": "def reward(s_agent, s_ventilation, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ventilation = calculate_distance_min(s_agent, s_ventilation) \n    penalty_collision = -19 if (d_agent_ventilation < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a rainforest. The drone must detect and avoid overhead conduit while heading toward a target. You are provided with the agents state, the targets state, and the positions of overhead conduit in the rainforest.",
        "chosen": "def reward(s_agent, s_conduit, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_conduit = calculate_distance_min(s_agent, s_conduit) \n    penalty_collision = -19 if (d_agent_conduit < 0.6) else 18 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a rainforest. The drone must detect and avoid cliff , wayfinding signs and wheel stops while heading toward a target. You are provided with the agents state, the targets state, and the positions of multiple obstcales: cliff , wayfinding signs and wheel stops in the rainforest.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a rainforest. The drone must detect and avoid cave, huamn while heading toward a target. You are provided with the agents state, the targets state, and the positions of multiple obstcales: cave in the rainforest.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Design a Python reward function for a drone tasked with navigating inside a rainforest. The drone must heading toward a target. You are provided with the agents state, the targets state",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Science Lab. \n Objectives: It must avoid colliding with concrete columns or wayfinding signs. \n Conditions: The states of the agent, the ride guardrail, and the wayfinding signs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Science Lab. \n Objectives: Detect centrifuge machine and aviod it. \n Conditions: The states of the agent, the centrifuge machine are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_centrifuge_machine, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_centrifuge_machine = calculate_distance_min(s_agent, s_centrifuge_machine) \n    penalty_collision = -17 if (d_agent_centrifuge_machine < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Science Lab open workspace. Objectives: It must avoid colliding with lab equipment. Conditions: States of the drone and the lab equipment are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_lab_equipment, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lab_equipment = calculate_distance_min(s_agent, s_lab_equipment) \n    penalty_collision = -17 if (d_agent_lab_equipment < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a lab corridor, the drone must maneuver around lab curtain. All entity states (agent, lab curtain) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_lab_curtain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lab_curtain = calculate_distance_min(s_agent, s_lab_curtain) \n    penalty_collision = -17 if (d_agent_lab_curtain < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a lab corridor. Requirement: Detect the three lab technicians and do not collide with it. Given: The states for the drone and many lab technicians are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a chemical storage area during experiments. \n Objectives: So not collide with robot. \n Conditions: The states of the agent, the robot are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_lab_robot, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lab_robot = calculate_distance_min(s_agent, s_lab_robot) \n    penalty_collision = -17 if (d_agent_lab_robot < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in a Science Lab. Objectives: Do not collide hit with large apparatus. Conditions: States of the agent, apparatus are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_apparatus, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_apparatus = calculate_distance_min(s_agent, s_apparatus) \n    penalty_collision = -17 if (d_agent_apparatus < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a lab corridor, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_lab_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lab_wall = calculate_distance_min(s_agent, s_lab_wall) \n    penalty_collision = -17 if (d_agent_lab_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: lab floor. Avoid: Hitting with robotic arm. Known: Agent, robotic arm states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_robotic_arm, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_robotic_arm = calculate_distance_min(s_agent, s_robotic_arm) \n    penalty_collision = -17 if (d_agent_robotic_arm < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Science Lab. Objectives: It must avoid colliding with technician. Conditions: The states of the drone and technician are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_technician, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_technician = calculate_distance_min(s_agent, s_technician) \n    penalty_collision = -19 if (d_agent_technician < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Science Lab. Objectives: Avoid collision with two traffic cone lines. Conditions: The states of the agent, wires, technician and warning signs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Science Lab. Objectives: It must avoid colliding with any truck, overhead cable bundle and gate arm. Conditions: Agent's, wall and dropped tool states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Science Lab. Objectives: Do not collide with any centrifuge. Conditions: The states of the drone and the centrifuge are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_centrifuge, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_centrifuge = calculate_distance_min(s_agent, s_centrifuge) \n    penalty = -19 if (d_agent_centrifuge < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Science Lab and must avoid contact with any secure_door. The states of the drone and the secure_door are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_secure_door, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_secure_door = calculate_distance_min(s_agent, s_secure_door) \n    penalty = -19 if (d_agent_secure_door < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Science Lab. It must avoid colliding with the motorcycle, attendant and sign. The states of the agent, cables, storage racks and safety barricades are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Science Lab. It must avoid colliding with bollards and wayfinding signs. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Science Lab. It must avoid colliding with Science Lab pipes. Agent's state, target's state and Science Lab pipes's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_pipe, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_pipe = calculate_distance_min(s_agent, s_pipe) \n    penalty_collision = -19 if (d_agent_pipe < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Science Lab. Goal: It must avoid colliding with cable. Assumptions: The states of the drone and the cable's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_cable, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cable = calculate_distance_min(s_agent, s_cable) \n    penalty_collision = -19 if (d_agent_cable < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Science Lab. Goal: It must avoid colliding with Science Lab's chair, cabinet and student. Assumptions: The Agent's state, target's state and chair, cabinet and student' states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Science Lab. Goal: Detect waste containers and gas cylindes and avoid lab door and do not collide. Assumptions: The states of the drone, target  waste containers, gas cylindes and avoid lab door state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Science Lab. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Church. \n Objectives: It must avoid colliding with concrete columns or hymn boards. \n Conditions: The states of the agent, the altar rail, and the hymn boards are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Church. \n Objectives: Detect candelabra and aviod it. \n Conditions: The states of the agent, the candelabra are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_candelabra, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_centrifuge_machine = calculate_distance_min(s_agent, s_candelabra) \n    penalty_collision = -17 if (d_agent_centrifuge_machine < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Church open workspace. Objectives: It must avoid colliding with church equipment. Conditions: States of the drone and the church equipment are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_church_fixtures, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lab_equipment = calculate_distance_min(s_agent, s_church_fixtures) \n    penalty_collision = -17 if (d_agent_church_fixtures < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a church corridor, the drone must maneuver around church curtain. All entity states (agent, church curtain) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_choir_curtain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lab_curtain = calculate_distance_min(s_agent, s_choir_curtain) \n    penalty_collision = -17 if (d_agent_lab_curtain < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a church corridor. Requirement: Detect the three church parishioners and do not collide with it. Given: The states for the drone and many church parishioners are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a sacristy area during experiments. \n Objectives: So not collide with processional stand. \n Conditions: The states of the agent, the processional stand are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_processional_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_processional_stand = calculate_distance_min(s_agent, s_processional_stand) \n    penalty_collision = -17 if (d_agent_processional_stand < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in a Church. Objectives: Do not collide hit with baptismal font. Conditions: States of the agent, baptismal font are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_baptismal_font, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_baptismal_font = calculate_distance_min(s_agent, s_baptismal_font) \n    penalty_collision = -17 if (d_agent_baptismal_font < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a church corridor, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_church_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_church_wall = calculate_distance_min(s_agent, s_church_wall) \n    penalty_collision = -17 if (d_agent_church_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: church floor. Avoid: Hitting with processional stand. Known: Agent, processional stand states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_processional_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_robotic_arm = calculate_distance_min(s_agent, s_processional_stand) \n    penalty_collision = -17 if (d_agent_robotic_arm < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Church. Objectives: It must avoid colliding with parishioner. Conditions: The states of the drone and parishioner are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_parishioner, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_technician = calculate_distance_min(s_agent, s_parishioner) \n    penalty_collision = -19 if (d_agent_technician < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Church. Objectives: Avoid collision with two traffic cone lines. Conditions: The states of the agent, wires, parishioner and warning signs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Church. Objectives: It must avoid colliding with any truck, overhead hanging lamp chains and entrance barrier. Conditions: Agent's, wall and pastor are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Church. Objectives: Do not collide with any bible. Conditions: The states of the drone and the bible are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bible, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bible = calculate_distance_min(s_agent, s_bible) \n    penalty = -19 if (d_agent_bible < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Church and must avoid contact with any sacristy  door. The states of the drone and the sacristy door are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_sacristy_door, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sacristy_door = calculate_distance_min(s_agent, s_sacristy_door) \n    penalty = -19 if (d_agent_sacristy_door < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Church. It must avoid colliding with the confessional booth, usher and hymn board. The states of the agent, hanging lamp chains, storage racks and safety barricades are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Church. It must avoid colliding with altar rails and hymn boards. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Church. It must avoid colliding with Church organ pipess. Agent's state, target's state and Church organ pipess's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_organ_pipes, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_pipe = calculate_distance_min(s_agent, s_organ_pipes) \n    penalty_collision = -19 if (d_agent_pipe < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Church. Goal: It must avoid colliding with hanging lamp chain. Assumptions: The states of the drone and the hanging lamp chain's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_lamp_chain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lamp_chain = calculate_distance_min(s_agent, s_lamp_chain) \n    penalty_collision = -19 if (d_agent_lamp_chain < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Church. Goal: It must avoid colliding with Church's pew, pulpit and parishioner. Assumptions: The Agent's state, target's state and pew, pulpit and parishioner' states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Church. Goal: Detect altars and benchs and avoid church door and do not collide. Assumptions: The states of the drone, target  altar, benches and avoid church door state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Church. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Art Museum. \n Objectives: It must avoid colliding with concrete columns or exhibit placards. \n Conditions: The states of the agent, the stanchion, and the exhibit placards are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Art Museum. \n Objectives: Detect sculpture pedestal and aviod it. \n Conditions: The states of the agent, the sculpture pedestal are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_sculpture_pedestal, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sculpture_pedestal = calculate_distance_min(s_agent, s_sculpture_pedestal) \n    penalty_collision = -17 if (d_agent_sculpture_pedestal < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Art Museum open workspace. Objectives: It must avoid colliding with gallery fixtures. Conditions: States of the drone and the gallery fixturest are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_gallery_fixtures, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_gallery_fixtures = calculate_distance_min(s_agent, s_gallery_fixtures) \n    penalty_collision = -17 if (d_agent_gallery_fixtures < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a art museum corridor, the drone must maneuver around art museum curtain. All entity states (agent, art museum curtain) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_gallery_curtain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_gallery_curtain = calculate_distance_min(s_agent, s_gallery_curtain) \n    penalty_collision = -17 if (d_agent_gallery_curtain < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a art museum corridor. Requirement: Detect the three art museum visitors and do not collide with it. Given: The states for the drone and many art museum visitors are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a staff service area during experiments. \n Objectives: So not collide with display_stand. \n Conditions: The states of the agent, the display_stand are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_display_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_display_stand = calculate_distance_min(s_agent, s_display_stand) \n    penalty_collision = -17 if (d_agent_display_stand < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in a Art Museum. Objectives: Do not collide hit with sculpture plinth. Conditions: States of the agent, sculpture plinth are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_sculpture_plinth, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sculpture_plinth = calculate_distance_min(s_agent, s_sculpture_plinth) \n    penalty_collision = -17 if (d_agent_sculpture_plinth < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a art museum corridor, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_gallery_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_gallery_wall = calculate_distance_min(s_agent, s_gallery_wall) \n    penalty_collision = -17 if (d_agent_gallery_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: art museum floor. Avoid: Hitting with display stand. Known: Agent, display stand states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_display_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_display_stand = calculate_distance_min(s_agent, s_display_stand) \n    penalty_collision = -17 if (d_agent_display_stand < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Art Museum. Objectives: It must avoid colliding with visitor. Conditions: The states of the drone and visitor are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_visitor, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_visitor = calculate_distance_min(s_agent, s_visitor) \n    penalty_collision = -19 if (d_agent_visitor < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Art Museum. Objectives: Avoid collision with two cone lines. Conditions: The states of the agent and two cone lines are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Art Museum. Objectives: It must avoid colliding with any picture, overhead wires and ticket barrier. Conditions: Agent's, picture's, wire's and ticket barrier's states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Art Museum. Objectives: Do not collide with any sculpture pedestal. Conditions: The states of the drone and the sculpture pedestal are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_sculpture_pedestal, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sculpture_pedestal = calculate_distance_min(s_agent, s_sculpture_pedestal) \n    penalty = -19 if (d_agent_sculpture_pedestal < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Art Museum and must avoid contact with any staff only door. The states of the drone and the staff only door are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_staff_door, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_door = calculate_distance_min(s_agent, s_staff_door) \n    penalty = -19 if (d_agent_door < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Art Museum. It must avoid colliding with the freestanding partition, docent and exhibit placard. The states of the agent, suspension wires, storage racks and safety barricades are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Art Museum. It must avoid colliding with stanchions and exhibit placards. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Art Museum. It must avoid colliding with Art Museum organ hanging installations. Agent's state, target's state and Art Museum organ hanging installations's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_installation, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_installation = calculate_distance_min(s_agent, s_installation) \n    penalty_collision = -19 if (d_agent_installation < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Art Museum. Goal: It must avoid colliding with suspension wire. Assumptions: The states of the drone and the suspension wire's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_suspension_wire, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_suspension_wire = calculate_distance_min(s_agent, s_suspension_wire) \n    penalty_collision = -19 if (d_agent_suspension_wire < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Art Museum. Goal: It must avoid colliding with Art Museum's pew, pulpit and visitor. Assumptions: The Agent's state, target's state and pew, pulpit and visitor' states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Art Museum. Goal: Detect waste containers and gas cylindes and avoid art museum door and do not collide. Assumptions: The states of the drone, target  freestanding partition, docent and exhibit placard state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Art Museum. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target*3 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move a drone within a Parking garage so that it avoids collisions with the car and truck  while reaching a specified target. The drones state, the multipe obstacles positions, and the targets state are all available.  Implement a reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move a drone within a Parking garage so that it avoids collisions with the car while reaching a specified target. The drones state, the cars positions, and the targets state are all available.  Implement a reward function.",
        "chosen": "def reward(s_agent, s_ramp, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ramp = calculate_distance_min(s_agent, s_ramp) \n    penalty_collision = -17 if (d_agent_ramp < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move a drone within a Parking garage so that it avoids collisions with the payment kiosk while reaching a specified target. The drones state, the payment kiosk positions, and the targets state are all available.  Implement a reward function.",
        "chosen": "def reward(s_agent, s_payment_kiosk, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_payment_kiosk = calculate_distance_min(s_agent, s_payment_kiosk) \n    penalty_collision = -17 if (d_agent_payment_kiosk < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move a drone within a Parking garage so that it avoids collisions with the door while reaching a specified target. The drones state, the door positions, and the targets state are all available.  Implement a reward function.",
        "chosen": "def reward(s_agent, s_door, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_door = calculate_distance_min(s_agent, s_door) \n    penalty_collision = -17 if (d_agent_door < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move a drone within a Parking garage so that it avoids collisions with the ramp and pillar and door while reaching a specified target. The drones state, the multipe obstacles positions, and the targets state are all available.  Implement a reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move a drone within a Parking garage so that it avoids collisions with the car while reaching a specified target. The drones state, the cars positions, and the targets state are all available.  Implement a reward function.",
        "chosen": "def reward(s_agent, s_car, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_car = calculate_distance_min(s_agent, s_car) \n    penalty_collision = -17 if (d_agent_car < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move a drone within a Parking garage so that it avoids collisions with the truck while reaching a specified target. The drones state, the truck positions, and the targets state are all available.  Implement a reward function.",
        "chosen": "def reward(s_agent, s_truck, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_truck = calculate_distance_min(s_agent, s_truck) \n    penalty_collision = -17 if (d_agent_truck < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move a drone within a Parking garage so that it avoids collisions with the wall while reaching a specified target. The drones state, the wall positions, and the targets state are all available.  Implement a reward function.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (d_agent_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move a drone within a Parking garage so that it avoids collisions with the vehicle while reaching a specified target. The drones state, the vehicle positions, and the targets state are all available.  Implement a reward function.",
        "chosen": "def reward(s_agent, s_vehicle, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_vehicle = calculate_distance_min(s_agent, s_vehicle) \n    penalty_collision = -17 if (d_agent_vehicle < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move a drone within a Parking garage so that it avoids collisions with the human while reaching a specified target. The drones state, the human positions, and the targets state are all available.  Implement a reward function.",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -19 if (d_agent_human < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move a drone within a Parking garage so that it avoids collisions with the human, vehicle while reaching a specified target. The drones state, the multipe obstacles positions, and the targets state are all available.  Implement a reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move a drone within a Parking garage so that it avoids collisions with the ramp and pillar while reaching a specified target. The drones state, the multipe obstacles positions, and the targets state are all available.  Implement a reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move a drone within a Parking garage so that it avoids collisions with the charging station while reaching a specified target. The drones state, the charging station positions, and the targets state are all available.  Implement a reward function.",
        "chosen": "def reward(s_agent, s_charging_station, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_charging_station = calculate_distance_min(s_agent, s_charging_station) \n    penalty = -19 if (d_agent_charging_station < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move a drone within a Parking garage so that it avoids collisions with the gate while reaching a specified target. The drones state, the gate positions, and the targets state are all available.  Implement a reward function.",
        "chosen": "def reward(s_agent, s_gate, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_gate = calculate_distance_min(s_agent, s_gate) \n    penalty = -19 if (d_agent_gate < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move a drone within a Parking garage so that it avoids collisions with the person and wall and car while reaching a specified target. The drones state, the multipe obstacles positions, and the targets state are all available.  Implement a reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move a drone within a Parking garage so that it avoids collisions with the truck, taxi and entrance while reaching a specified target. The drones state, the multipe obstacles positions, and the targets state are all available.  Implement a reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move a drone within a Parking garage so that it avoids collisions with the ventilation while reaching a specified target. The drones state, the ventilation positions, and the targets state are all available.  Implement a reward function.",
        "chosen": "def reward(s_agent, s_ventilation, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ventilation = calculate_distance_min(s_agent, s_ventilation) \n    penalty_collision = -19 if (d_agent_ventilation < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move a drone within a Parking garage so that it avoids collisions with the conduit while reaching a specified target. The drones state, the conduit positions, and the targets state are all available.  Implement a reward function.",
        "chosen": "def reward(s_agent, s_conduit, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_conduit = calculate_distance_min(s_agent, s_conduit) \n    penalty_collision = -19 if (d_agent_conduit < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move a drone within a Parking garage so that it avoids collisions with the car, person and lift while reaching a specified target. The drones state, the multipe obstacles positions, and the targets state are all available.  Implement a reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move a drone within a Parking garage so that it avoids collisions with the stirs, camera and ramp while reaching a specified target. The drones state, the multipe obstacles positions, and the targets state are all available.  Implement a reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "The task is to move a drone within a Parking garage so that it reaches a specified target. The drones state and the targets state are all available.  Implement a reward function.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is operating along Rainforest. \n Objectives: It must avoid colliding with tree trunks or hanging vines. \n Conditions: The states of the agent, the ride guardrail, and the trail markers are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is operating inside a busy Rainforest. \n Objectives: Detect fallen log barrier and avoid it. \n Conditions: The states of the agent, the fallen log barrier are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_fallen_log, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_fallen_log = calculate_distance_min(s_agent, s_fallen_log) \n    penalty_collision = -15 if (d_agent_fallen_log < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Rainforest clearing. Objectives: It must avoid colliding with termite mounds. Conditions: States of the drone and the payment kiosk are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_termite_mound, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_termite_mound = calculate_distance_min(s_agent, s_termite_mound) \n    penalty_collision = -15 if (d_agent_termite_mound < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a forest trail, the drone must maneuver around thick vine curtain. All entity states (agent, thick vine curtain) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_vine_curtain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_vine_curtain = calculate_distance_min(s_agent, s_vine_curtain) \n    penalty_collision = -15 if (d_agent_vine_curtain < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a forest trail. Requirement: Detect the three boulders and do not collide with it. Given: The states for the drone and many boulders are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone operates near a riverbank during the monsoon. \n Objectives: Do not collide with wild boar. \n Conditions: The states of the agent, the wild boar are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_boar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_boar = calculate_distance_min(s_agent, s_boar) \n    penalty_collision = -15 if (d_agent_boar < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navibamboo gate Rainforest. \n Objectives: Do not collide with with elephant. \n Conditions: States of the agent, elephant are obtainable. \n Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_elephant, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_elephant = calculate_distance_min(s_agent, s_elephant) \n    penalty_collision = -15 if (d_agent_elephant < 0.5) else 5 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a forest trail, the drone must steer clear of cliff face. All states (agent, cliff face) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_cliff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cliff = calculate_distance_min(s_agent, s_cliff) \n    penalty_collision = -15 if (d_agent_cliff < 0.5) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: forest floor. Avoid: Hitting Rainforest's vehicle. Known: Agent, Rainforest vehicle states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_wild_animal, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wild_animal = calculate_distance_min(s_agent, s_wild_animal) \n    penalty_collision = -15 if (d_agent_wild_animal < 0.5) else 5 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Rainforest. Objectives: It must avoid colliding with forest ranger. Conditions: The states of the drone and human are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ranger, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ranger = calculate_distance_min(s_agent, s_ranger) \n    penalty_collision = -19 if (d_agent_ranger < 0.5) else 5 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Rainforest. Objectives: Avoid collision with two thorny bush lines. Conditions: The states of the agent, thorny bush lines and attendants are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Rainforest. Objectives: It must avoid colliding with any elephant, overhanging liana bundle and fallen branch. Conditions: Agent's, elephant and fallen branch states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Rainforest. \n Objectives: Do not collide with any large tree stump. \n Conditions: The states of the drone and the large tree stump are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_tree_stump, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tree_stump = calculate_distance_min(s_agent, s_tree_stump) \n    penalty = -19 if (d_agent_tree_stump < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is operating along Rainforest and must avoid contact with any bamboo gate. \n Conditions: The states of the drone and the bamboo gate are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_bamboo_gate, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bamboo_gate = calculate_distance_min(s_agent, s_bamboo_gate) \n    penalty = -19 if (d_agent_bamboo_gate < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Rainforest. \n Objectives: It must avoid colliding with the vine tangles, ranger and signpost. \n Conditions: The states of the agent, vine tangles, ranger and signpost are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Rainforest. It must avoid colliding with bollards and trail markers. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Rainforest. It must avoid colliding with Rainforest buttress roots. Agent's state, target's state and Rainforest buttress roots's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_buttress_roots, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_buttress_roots = calculate_distance_min(s_agent, s_buttress_roots) \n    penalty_collision = -19 if (d_agent_buttress_roots < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Rainforest. Goal: It must avoid colliding with overhanging lianas. Assumptions: The states of the drone and the overhanging lianas area are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_lianas, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lianas = calculate_distance_min(s_agent, s_lianas) \n    penalty_collision = -19 if (d_agent_lianas < 0.5) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Rainforest. Goal: It must avoid colliding with Rainforest buttress roots, trail markers and boulders. Assumptions: The Agent's state, target's state and Rainforest buttress roots's state, wheel stop's state and trail markers' states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Rainforest. Goal: Detect trail markers and tree buttresses and avoid forest trail corners. Assumptions: The states of the drone and bamboo gates, pay stations and thorny barriers state are available. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Rainforest. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (concrete columns or wayfinding signs) while progressing toward the target. \n The environment provides access to the agent state, target state, and multiple obstacles: concrete columns or wayfinding signs states.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (centrifuge machine) while progressing toward the target. \n The environment provides access to the agent state, target state, and centrifuge machine states.",
        "chosen": "def reward(s_agent, s_centrifuge_machine, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_centrifuge_machine = calculate_distance_min(s_agent, s_centrifuge_machine) \n    penalty_collision = -17 if (d_agent_centrifuge_machine < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (lab equipmentt) while progressing toward the target. \n The environment provides access to the agent state, target state, and lab equipmentt states.",
        "chosen": "def reward(s_agent, s_lab_equipment, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lab_equipment = calculate_distance_min(s_agent, s_lab_equipment) \n    penalty_collision = -17 if (d_agent_lab_equipmen < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (lab curtian) while progressing toward the target. \n The environment provides access to the agent state, target state, and lab curtain states.",
        "chosen": "def reward(s_agent, s_lab_curtain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lab_curtain = calculate_distance_min(s_agent, s_lab_curtain) \n    penalty_collision = -17 if (d_agent_lab_curtain < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (centrifuge machine, equipment and curtain) while progressing toward the target. \n The environment provides access to the agent state, target state, and multiple obstacles: centrifuge machine, equipment and curtain states.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (lab robot) while progressing toward the target. \n The environment provides access to the agent state, target state, and lab robot states.",
        "chosen": "def reward(s_agent, s_lab_robot, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lab_robot = calculate_distance_min(s_agent, s_lab_robot) \n    penalty_collision = -17 if (d_agent_lab_robot < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (apparatus) while progressing toward the target. \n The environment provides access to the agent state, target state, and apparatus states.",
        "chosen": "def reward(s_agent, s_apparatus, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_apparatus = calculate_distance_min(s_agent, s_apparatus) \n    penalty_collision = -17 if (d_agent_apparatus < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (lab wall) while progressing toward the target. \n The environment provides access to the agent state, target state, and lab wall states.",
        "chosen": "def reward(s_agent, s_lab_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lab_wall = calculate_distance_min(s_agent, s_lab_wall) \n    penalty_collision = -17 if (d_agent_lab_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (robotic arm) while progressing toward the target. \n The environment provides access to the agent state, target state, and robotic arm states.",
        "chosen": "def reward(s_agent, s_robotic_arm, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_robotic_arm = calculate_distance_min(s_agent, s_robotic_arm) \n    penalty_collision = -17 if (d_agent_robotic_arm < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (technician) while progressing toward the target. \n The environment provides access to the agent state, target state, and technician states.",
        "chosen": "def reward(s_agent, s_technician, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_technician = calculate_distance_min(s_agent, s_technician) \n    penalty_collision = -19 if (d_agent_technician < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (table, chair) while progressing toward the target. \n The environment provides access to the agent state, target state, and multiple obstacles: table and chair states.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (acid bottle, overhead cable bundle and chair) while progressing toward the target. \n The environment provides access to the agent state, target state, and multiple obstacles: acid bottle, overhead cable bundle and chair states.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (centrifuge) while progressing toward the target. \n The environment provides access to the agent state, target state, and centrifuge states.",
        "chosen": "def reward(s_agent, s_centrifuge, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_centrifuge = calculate_distance_min(s_agent, s_centrifuge) \n    penalty = -19 if (d_agent_centrifuge < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (secure door) while progressing toward the target. \n The environment provides access to the agent state, target state, and door states.",
        "chosen": "def reward(s_agent, s_secure_door, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_secure_door = calculate_distance_min(s_agent, s_secure_door) \n    penalty = -19 if (d_agent_secure_door < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (students, attendant and sign) while progressing toward the target. \n The environment provides access to the agent state, target state, and students, attendant and sign states.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (blackboard, signs) while progressing toward the target. \n The environment provides access to the agent state, target state, and  balckboard and sign signs states.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (pipes) while progressing toward the target. \n The environment provides access to the agent state, target state, and pipes states.",
        "chosen": "def reward(s_agent, s_pipe, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_pipe = calculate_distance_min(s_agent, s_pipe) \n    penalty_collision = -19 if (d_agent_pipe < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (cable) while progressing toward the target. \n The environment provides access to the agent state, target state, and cable states.",
        "chosen": "def reward(s_agent, s_cable, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cable = calculate_distance_min(s_agent, s_cable) \n    penalty_collision = -19 if (d_agent_cable < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (Science Lab's chair, cabinet and student) while progressing toward the target. \n The environment provides access to the agent state, target state, and Science Lab's chair, cabinet and student states.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (waste containers and gas cylinders) while progressing toward the target. \n The environment provides access to the agent state, target state, and waste containers and gas cylinders states.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (table) while progressing toward the target. \n The environment provides access to the agent state, target state, and table states.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Church. \n Objectives: It must avoid colliding with concrete columns or hymn boards. \n Conditions: The states of the agent, the altar rail, and the hymn boards are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Church. \n Objectives: Detect candelabra and aviod it. \n Conditions: The states of the agent, the candelabra are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_candelabra, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_centrifuge_machine = calculate_distance_min(s_agent, s_candelabra) \n    penalty_collision = -17 if (d_agent_centrifuge_machine < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Church open workspace. Objectives: It must avoid colliding with church equipment. Conditions: States of the drone and the church equipment are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_church_fixtures, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lab_equipment = calculate_distance_min(s_agent, s_church_fixtures) \n    penalty_collision = -17 if (d_agent_church_fixtures < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a church corridor, the drone must maneuver around church curtain. All entity states (agent, church curtain) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_choir_curtain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lab_curtain = calculate_distance_min(s_agent, s_choir_curtain) \n    penalty_collision = -17 if (d_agent_lab_curtain < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a church corridor. Requirement: Detect the three church parishioners and do not collide with it. Given: The states for the drone and many church parishioners are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a sacristy area during experiments. \n Objectives: So not collide with processional stand. \n Conditions: The states of the agent, the processional stand are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_processional_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_processional_stand = calculate_distance_min(s_agent, s_processional_stand) \n    penalty_collision = -17 if (d_agent_processional_stand < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in a Church. Objectives: Do not collide hit with baptismal font. Conditions: States of the agent, baptismal font are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_baptismal_font, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_baptismal_font = calculate_distance_min(s_agent, s_baptismal_font) \n    penalty_collision = -17 if (d_agent_baptismal_font < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a church corridor, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_church_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_church_wall = calculate_distance_min(s_agent, s_church_wall) \n    penalty_collision = -17 if (d_agent_church_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: church floor. Avoid: Hitting with processional stand. Known: Agent, processional stand states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_processional_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_robotic_arm = calculate_distance_min(s_agent, s_processional_stand) \n    penalty_collision = -17 if (d_agent_robotic_arm < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Church. Objectives: It must avoid colliding with parishioner. Conditions: The states of the drone and parishioner are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_parishioner, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_technician = calculate_distance_min(s_agent, s_parishioner) \n    penalty_collision = -19 if (d_agent_technician < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Church. Objectives: Avoid collision with two traffic cone lines. Conditions: The states of the agent, wires, parishioner and warning signs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Church. Objectives: It must avoid colliding with any truck, overhead hanging lamp chains and entrance barrier. Conditions: Agent's, wall and pastor are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Church. Objectives: Do not collide with any bible. Conditions: The states of the drone and the bible are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bible, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bible = calculate_distance_min(s_agent, s_bible) \n    penalty = -19 if (d_agent_bible < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Church and must avoid contact with any sacristy  door. The states of the drone and the sacristy door are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_sacristy_door, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sacristy_door = calculate_distance_min(s_agent, s_sacristy_door) \n    penalty = -19 if (d_agent_sacristy_door < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Church. It must avoid colliding with the confessional booth, usher and hymn board. The states of the agent, hanging lamp chains, storage racks and safety barricades are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Church. It must avoid colliding with altar rails and hymn boards. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Church. It must avoid colliding with Church organ pipess. Agent's state, target's state and Church organ pipess's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_organ_pipes, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_pipe = calculate_distance_min(s_agent, s_organ_pipes) \n    penalty_collision = -19 if (d_agent_pipe < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Church. Goal: It must avoid colliding with hanging lamp chain. Assumptions: The states of the drone and the hanging lamp chain's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_lamp_chain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lamp_chain = calculate_distance_min(s_agent, s_lamp_chain) \n    penalty_collision = -19 if (d_agent_lamp_chain < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Church. Goal: It must avoid colliding with Church's pew, pulpit and parishioner. Assumptions: The Agent's state, target's state and pew, pulpit and parishioner' states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Church. Goal: Detect altars and benchs and avoid church door and do not collide. Assumptions: The states of the drone, target  altar, benches and avoid church door state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Church. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Art Museum. \n Objectives: It must avoid colliding with concrete columns or exhibit placards. \n Conditions: The states of the agent, the stanchion, and the exhibit placards are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Art Museum. \n Objectives: Detect sculpture pedestal and aviod it. \n Conditions: The states of the agent, the sculpture pedestal are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_sculpture_pedestal, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sculpture_pedestal = calculate_distance_min(s_agent, s_sculpture_pedestal) \n    penalty_collision = -17 if (d_agent_sculpture_pedestal < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Art Museum open workspace. Objectives: It must avoid colliding with gallery fixtures. Conditions: States of the drone and the gallery fixturest are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_gallery_fixtures, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_gallery_fixtures = calculate_distance_min(s_agent, s_gallery_fixtures) \n    penalty_collision = -17 if (d_agent_gallery_fixtures < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a art museum corridor, the drone must maneuver around art museum curtain. All entity states (agent, art museum curtain) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_gallery_curtain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_gallery_curtain = calculate_distance_min(s_agent, s_gallery_curtain) \n    penalty_collision = -17 if (d_agent_gallery_curtain < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a art museum corridor. Requirement: Detect the three art museum visitors and do not collide with it. Given: The states for the drone and many art museum visitors are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a staff service area during experiments. \n Objectives: So not collide with display_stand. \n Conditions: The states of the agent, the display_stand are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_display_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_display_stand = calculate_distance_min(s_agent, s_display_stand) \n    penalty_collision = -17 if (d_agent_display_stand < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in a Art Museum. Objectives: Do not collide hit with sculpture plinth. Conditions: States of the agent, sculpture plinth are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_sculpture_plinth, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sculpture_plinth = calculate_distance_min(s_agent, s_sculpture_plinth) \n    penalty_collision = -17 if (d_agent_sculpture_plinth < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a art museum corridor, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_gallery_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_gallery_wall = calculate_distance_min(s_agent, s_gallery_wall) \n    penalty_collision = -17 if (d_agent_gallery_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: art museum floor. Avoid: Hitting with display stand. Known: Agent, display stand states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_display_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_display_stand = calculate_distance_min(s_agent, s_display_stand) \n    penalty_collision = -17 if (d_agent_display_stand < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Art Museum. Objectives: It must avoid colliding with visitor. Conditions: The states of the drone and visitor are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_visitor, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_visitor = calculate_distance_min(s_agent, s_visitor) \n    penalty_collision = -19 if (d_agent_visitor < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Art Museum. Objectives: Avoid collision with two cone lines. Conditions: The states of the agent and two cone lines are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Art Museum. Objectives: It must avoid colliding with any picture, overhead wires and ticket barrier. Conditions: Agent's, picture's, wire's and ticket barrier's states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Art Museum. Objectives: Do not collide with any sculpture pedestal. Conditions: The states of the drone and the sculpture pedestal are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_sculpture_pedestal, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sculpture_pedestal = calculate_distance_min(s_agent, s_sculpture_pedestal) \n    penalty = -19 if (d_agent_sculpture_pedestal < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Art Museum and must avoid contact with any staff only door. The states of the drone and the staff only door are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_staff_door, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_door = calculate_distance_min(s_agent, s_staff_door) \n    penalty = -19 if (d_agent_door < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Art Museum. It must avoid colliding with the freestanding partition, docent and exhibit placard. The states of the agent, suspension wires, storage racks and safety barricades are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Art Museum. It must avoid colliding with stanchions and exhibit placards. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Art Museum. It must avoid colliding with Art Museum organ hanging installations. Agent's state, target's state and Art Museum organ hanging installations's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_installation, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_installation = calculate_distance_min(s_agent, s_installation) \n    penalty_collision = -19 if (d_agent_installation < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Art Museum. Goal: It must avoid colliding with suspension wire. Assumptions: The states of the drone and the suspension wire's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_suspension_wire, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_suspension_wire = calculate_distance_min(s_agent, s_suspension_wire) \n    penalty_collision = -19 if (d_agent_suspension_wire < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Art Museum. Goal: It must avoid colliding with Art Museum's pew, pulpit and visitor. Assumptions: The Agent's state, target's state and pew, pulpit and visitor' states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Art Museum. Goal: Detect waste containers and gas cylindes and avoid art museum door and do not collide. Assumptions: The states of the drone, target  freestanding partition, docent and exhibit placard state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Art Museum. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -d_agent_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Parking Garage. It must avoid colliding with concrete columns or wayfinding signs. The states of the agent, the ride guardrail, and the wayfinding signs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 18 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating inside a busy Parking Garage. Detect ramp guardrail and aviod it. The states of the agent, the ramp guardrail are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ramp, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ramp = calculate_distance_min(s_agent, s_ramp) \n    penalty_collision = -18 if (d_agent_ramp < 0.6) else 18 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Parking Garage. Objectives: It must avoid colliding with payment kiosks. Conditions: States of the drone and the payment kiosk are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_payment_kiosk, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_payment_kiosk = calculate_distance_min(s_agent, s_payment_kiosk) \n    penalty_collision = -18 if (d_payment_kiosk < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a drive aisle, the drone must maneuver around roll-up door. All entity states (agent, roll-up door) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_door, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_door = calculate_distance_min(s_agent, s_door) \n    penalty_collision = -18 if (d_agent_door < 0.6) else 18 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a drive aisle. Requirement: Detect the three wheel stops and do not collide with it. Given: The states for the drone and many wheel stops are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 18 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone operates near a service corridor during an event. So not collide with car. The states of the agent, the car are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_car, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_car = calculate_distance_min(s_agent, s_car) \n    penalty_collision = -18 if (d_agent_car < 0.6) else 18 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in a Parking Garage. Objectives: Do not collide hit with truck. Conditions: States of the agent, truck are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_truck, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_truck = calculate_distance_min(s_agent, s_truck) \n    penalty_collision = -18 if (d_agent_truck < 0.6) else 4 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a drive aisle, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -18 if (d_agent_wall < 0.6) else 18 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: parking level. Avoid: Hitting with Parking Garage's vehicle. Known: Agent, Parking Garage vehicle states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_vehicle, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_vehicle = calculate_distance_min(s_agent, s_vehicle) \n    penalty_collision = -18 if (d_agent_vehicle < 0.6) else 4 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Parking Garage. Objectives: It must avoid colliding with pay stations human. Conditions: The states of the drone and human are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -19 if (d_agent_human < 0.6) else 4 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Parking Garage. Objectives: Avoid collision with two traffic cone lines. Conditions: The states of the agent, traffic cone lines and attendants are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Parking Garage. Objectives: It must avoid colliding with any truck, overhead cable bundle and gate arm. Conditions: Agent's, truck and gate arm states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Parking Garage. Objectives: Do not collide with any EV charging station. Conditions: The states of the drone and the EV charging station are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_charging_station, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_charging_station = calculate_distance_min(s_agent, s_charging_station) \n    penalty = -19 if (d_agent_charging_station < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Parking Garage and must avoid contact with any gate. The states of the drone and the gate are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_gate, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_gate = calculate_distance_min(s_agent, s_gate) \n    penalty = -19 if (d_agent_gate < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Parking Garage. It must avoid colliding with the motorcycle, attendant and sign. The states of the agent, motorcycle, attendant and sign are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Parking Garage. It must avoid colliding with bollards and wayfinding signs. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Parking Garage. It must avoid colliding with Parking Garage ventilation ducts. Agent's state, target's state and Parking Garage ventilation ducts's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ventilation, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ventilation = calculate_distance_min(s_agent, s_ventilation) \n    penalty_collision = -19 if (d_agent_ventilation < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Parking Garage. Goal: It must avoid colliding with overhead conduit. Assumptions: The states of the drone and the overhead conduit area are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_conduit, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_conduit = calculate_distance_min(s_agent, s_conduit) \n    penalty_collision = -19 if (d_agent_conduit < 0.6) else 18 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Parking Garage. Goal: It must avoid colliding with Parking Garage ventilation ducts, wayfinding signs and wheel stops. Assumptions: The Agent's state, target's state and Parking Garage ventilation ducts's state, wheel stop's state and wayfinding signs' states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Parking Garage. Goal: Detect wayfinding signs and structural brackets and avoid drive aisle corners. Assumptions: The states of the drone and gates, pay stations and barricades state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Parking Garage. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is operating along Rainforest. \n Objectives: It must avoid colliding with tree trunks or hanging vines. \n Conditions: The states of the agent, the ride guardrail, and the trail markers are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.65) else 20 \n    reward_reaching_target = 19 if (d_agent_target < 0.62) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is operating inside a busy Rainforest. \n Objectives: Detect fallen log barrier and avoid it. \n Conditions: The states of the agent, the fallen log barrier are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_fallen_log, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_fallen_log = calculate_distance_min(s_agent, s_fallen_log) \n    penalty_collision = -20 if (d_agent_fallen_log < 0.65) else 20 \n    reward_reaching_target = 20 if (d_agent_target < 0.62) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Rainforest clearing. Objectives: It must avoid colliding with termite mounds. Conditions: States of the drone and the payment kiosk are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_termite_mound, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_termite_mound = calculate_distance_min(s_agent, s_termite_mound) \n    penalty_collision = -20 if (d_agent_termite_mound < 0.65) else 20 \n    reward_reaching_target = 20 if (d_agent_target < 0.62) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a forest trail, the drone must maneuver around thick vine curtain. All entity states (agent, thick vine curtain) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_vine_curtain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_vine_curtain = calculate_distance_min(s_agent, s_vine_curtain) \n    penalty_collision = -20 if (d_agent_vine_curtain < 0.65) else 20 \n    reward_reaching_target = 20 if (d_agent_target < 0.62) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a forest trail. Requirement: Detect the three boulders and do not collide with it. Given: The states for the drone and many boulders are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.65) else 20 \n    reward_reaching_target = 19 if (d_agent_target < 0.62) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone operates near a riverbank during the monsoon. \n Objectives: Do not collide with wild boar. \n Conditions: The states of the agent, the wild boar are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_boar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_boar = calculate_distance_min(s_agent, s_boar) \n    penalty_collision = -20 if (d_agent_boar < 0.65) else 20 \n    reward_reaching_target = 20 if (d_agent_target < 0.62) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navibamboo gate Rainforest. \n Objectives: Do not collide with with elephant. \n Conditions: States of the agent, elephant are obtainable. \n Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_elephant, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_elephant = calculate_distance_min(s_agent, s_elephant) \n    penalty_collision = -20 if (d_agent_elephant < 0.65) else 5 \n    reward_reaching_target = 20 if (d_agent_target < 0.62) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a forest trail, the drone must steer clear of cliff face. All states (agent, cliff face) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_cliff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cliff = calculate_distance_min(s_agent, s_cliff) \n    penalty_collision = -20 if (d_agent_cliff < 0.65) else 20 \n    reward_reaching_target = 20 if (d_agent_target < 0.62) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: forest floor. Avoid: Hitting Rainforest's vehicle. Known: Agent, Rainforest vehicle states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_wild_animal, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wild_animal = calculate_distance_min(s_agent, s_wild_animal) \n    penalty_collision = -20 if (d_agent_wild_animal < 0.65) else 5 \n    reward_reaching_target = 20 if (d_agent_target < 0.62) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Rainforest. Objectives: It must avoid colliding with forest ranger. Conditions: The states of the drone and human are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ranger, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ranger = calculate_distance_min(s_agent, s_ranger) \n    penalty_collision = -19 if (d_agent_ranger < 0.65) else 5 \n    reward_reaching_target = 20 if (d_agent_target < 0.62) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Rainforest. Objectives: Avoid collision with two thorny bush lines. Conditions: The states of the agent, thorny bush lines and attendants are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.65) else 20 \n    reward_reaching_target = 20 if (d_agent_target < 0.62) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Rainforest. Objectives: It must avoid colliding with any elephant, overhanging liana bundle and fallen branch. Conditions: Agent's, elephant and fallen branch states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.65) else 20 \n    reward_reaching_target = 20 if (d_agent_target < 0.62) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Rainforest. \n Objectives: Do not collide with any large tree stump. \n Conditions: The states of the drone and the large tree stump are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_tree_stump, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tree_stump = calculate_distance_min(s_agent, s_tree_stump) \n    penalty = -19 if (d_agent_tree_stump < 0.65) else 20 \n    reward_reaching_target = 20 if (d_agent_target < 0.62) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is operating along Rainforest and must avoid contact with any bamboo gate. \n Conditions: The states of the drone and the bamboo gate are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_bamboo_gate, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bamboo_gate = calculate_distance_min(s_agent, s_bamboo_gate) \n    penalty = -19 if (d_agent_bamboo_gate < 0.65) else 20 \n    reward_reaching_target = 20 if (d_agent_target < 0.62) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Rainforest. \n Objectives: It must avoid colliding with the vine tangles, ranger and signpost. \n Conditions: The states of the agent, vine tangles, ranger and signpost are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.65) else 20 \n    reward_reaching_target = 20 if (d_agent_target < 0.62) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Rainforest. It must avoid colliding with bollards and trail markers. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.65) else 20 \n    reward_reaching_target = 20 if (d_agent_target < 0.62) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Rainforest. It must avoid colliding with Rainforest buttress roots. Agent's state, target's state and Rainforest buttress roots's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_buttress_roots, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_buttress_roots = calculate_distance_min(s_agent, s_buttress_roots) \n    penalty_collision = -19 if (d_agent_buttress_roots < 0.65) else 20 \n    reward_reaching_target = 20 if (d_agent_target < 0.62) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Rainforest. Goal: It must avoid colliding with overhanging lianas. Assumptions: The states of the drone and the overhanging lianas area are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_lianas, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lianas = calculate_distance_min(s_agent, s_lianas) \n    penalty_collision = -19 if (d_agent_lianas < 0.65) else 20 \n    reward_reaching_target = 19 if (d_agent_target < 0.62) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Rainforest. Goal: It must avoid colliding with Rainforest buttress roots, trail markers and boulders. Assumptions: The Agent's state, target's state and Rainforest buttress roots's state, wheel stop's state and trail markers' states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.65) else 20 \n    reward_reaching_target = 20 if (d_agent_target < 0.62) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Rainforest. Goal: Detect trail markers and tree buttresses and avoid forest trail corners. Assumptions: The states of the drone and bamboo gates, pay stations and thorny barriers state are available. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.65) else 20 \n    reward_reaching_target = 20 if (d_agent_target < 0.62) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Rainforest. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 20 if (d_agent_target < 0.62) else -math.exp(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (concrete columns or wayfinding signs) while progressing toward the target. \n The environment provides access to the agent state, target state, and multiple obstacles: concrete columns or wayfinding signs states.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (centrifuge machine) while progressing toward the target. \n The environment provides access to the agent state, target state, and centrifuge machine states.",
        "chosen": "def reward(s_agent, s_centrifuge_machine, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_centrifuge_machine = calculate_distance_min(s_agent, s_centrifuge_machine) \n    penalty_collision = -17 if (d_agent_centrifuge_machine < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (lab equipmentt) while progressing toward the target. \n The environment provides access to the agent state, target state, and lab equipmentt states.",
        "chosen": "def reward(s_agent, s_lab_equipment, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lab_equipment = calculate_distance_min(s_agent, s_lab_equipment) \n    penalty_collision = -17 if (d_agent_lab_equipmen < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (lab curtian) while progressing toward the target. \n The environment provides access to the agent state, target state, and lab curtain states.",
        "chosen": "def reward(s_agent, s_lab_curtain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lab_curtain = calculate_distance_min(s_agent, s_lab_curtain) \n    penalty_collision = -17 if (d_agent_lab_curtain < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (centrifuge machine, equipment and curtain) while progressing toward the target. \n The environment provides access to the agent state, target state, and multiple obstacles: centrifuge machine, equipment and curtain states.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (lab robot) while progressing toward the target. \n The environment provides access to the agent state, target state, and lab robot states.",
        "chosen": "def reward(s_agent, s_lab_robot, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lab_robot = calculate_distance_min(s_agent, s_lab_robot) \n    penalty_collision = -17 if (d_agent_lab_robot < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (apparatus) while progressing toward the target. \n The environment provides access to the agent state, target state, and apparatus states.",
        "chosen": "def reward(s_agent, s_apparatus, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_apparatus = calculate_distance_min(s_agent, s_apparatus) \n    penalty_collision = -17 if (d_agent_apparatus < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (lab wall) while progressing toward the target. \n The environment provides access to the agent state, target state, and lab wall states.",
        "chosen": "def reward(s_agent, s_lab_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lab_wall = calculate_distance_min(s_agent, s_lab_wall) \n    penalty_collision = -17 if (d_agent_lab_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (robotic arm) while progressing toward the target. \n The environment provides access to the agent state, target state, and robotic arm states.",
        "chosen": "def reward(s_agent, s_robotic_arm, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_robotic_arm = calculate_distance_min(s_agent, s_robotic_arm) \n    penalty_collision = -17 if (d_agent_robotic_arm < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (technician) while progressing toward the target. \n The environment provides access to the agent state, target state, and technician states.",
        "chosen": "def reward(s_agent, s_technician, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_technician = calculate_distance_min(s_agent, s_technician) \n    penalty_collision = -19 if (d_agent_technician < 0.5) else 4 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (table, chair) while progressing toward the target. \n The environment provides access to the agent state, target state, and multiple obstacles: table and chair states.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (acid bottle, overhead cable bundle and chair) while progressing toward the target. \n The environment provides access to the agent state, target state, and multiple obstacles: acid bottle, overhead cable bundle and chair states.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (centrifuge) while progressing toward the target. \n The environment provides access to the agent state, target state, and centrifuge states.",
        "chosen": "def reward(s_agent, s_centrifuge, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_centrifuge = calculate_distance_min(s_agent, s_centrifuge) \n    penalty = -19 if (d_agent_centrifuge < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (secure door) while progressing toward the target. \n The environment provides access to the agent state, target state, and door states.",
        "chosen": "def reward(s_agent, s_secure_door, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_secure_door = calculate_distance_min(s_agent, s_secure_door) \n    penalty = -19 if (d_agent_secure_door < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (students, attendant and sign) while progressing toward the target. \n The environment provides access to the agent state, target state, and students, attendant and sign states.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (blackboard, signs) while progressing toward the target. \n The environment provides access to the agent state, target state, and  balckboard and sign signs states.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (pipes) while progressing toward the target. \n The environment provides access to the agent state, target state, and pipes states.",
        "chosen": "def reward(s_agent, s_pipe, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_pipe = calculate_distance_min(s_agent, s_pipe) \n    penalty_collision = -19 if (d_agent_pipe < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (cable) while progressing toward the target. \n The environment provides access to the agent state, target state, and cable states.",
        "chosen": "def reward(s_agent, s_cable, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cable = calculate_distance_min(s_agent, s_cable) \n    penalty_collision = -19 if (d_agent_cable < 0.5) else 17 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (Science Lab's chair, cabinet and student) while progressing toward the target. \n The environment provides access to the agent state, target state, and Science Lab's chair, cabinet and student states.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (waste containers and gas cylinders) while progressing toward the target. \n The environment provides access to the agent state, target state, and waste containers and gas cylinders states.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.5) else 17 \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (table) while progressing toward the target. \n The environment provides access to the agent state, target state, and table states.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (d_agent_target < 0.2) else -math.exp(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Church. \n Objectives: It must avoid colliding with concrete columns or hymn boards. \n Conditions: The states of the agent, the altar rail, and the hymn boards are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 18 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Church. \n Objectives: Detect candelabra and aviod it. \n Conditions: The states of the agent, the candelabra are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_candelabra, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_centrifuge_machine = calculate_distance_min(s_agent, s_candelabra) \n    penalty_collision = -18 if (d_agent_centrifuge_machine < 0.6) else 18 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Church open workspace. Objectives: It must avoid colliding with church equipment. Conditions: States of the drone and the church equipment are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_church_fixtures, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lab_equipment = calculate_distance_min(s_agent, s_church_fixtures) \n    penalty_collision = -18 if (d_agent_church_fixtures < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a church corridor, the drone must maneuver around church curtain. All entity states (agent, church curtain) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_choir_curtain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lab_curtain = calculate_distance_min(s_agent, s_choir_curtain) \n    penalty_collision = -18 if (d_agent_lab_curtain < 0.6) else 18 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a church corridor. Requirement: Detect the three church parishioners and do not collide with it. Given: The states for the drone and many church parishioners are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 18 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a sacristy area during experiments. \n Objectives: So not collide with processional stand. \n Conditions: The states of the agent, the processional stand are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_processional_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_processional_stand = calculate_distance_min(s_agent, s_processional_stand) \n    penalty_collision = -18 if (d_agent_processional_stand < 0.6) else 18 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in a Church. Objectives: Do not collide hit with baptismal font. Conditions: States of the agent, baptismal font are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_baptismal_font, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_baptismal_font = calculate_distance_min(s_agent, s_baptismal_font) \n    penalty_collision = -18 if (d_agent_baptismal_font < 0.6) else 4 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a church corridor, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_church_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_church_wall = calculate_distance_min(s_agent, s_church_wall) \n    penalty_collision = -18 if (d_agent_church_wall < 0.6) else 18 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: church floor. Avoid: Hitting with processional stand. Known: Agent, processional stand states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_processional_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_robotic_arm = calculate_distance_min(s_agent, s_processional_stand) \n    penalty_collision = -18 if (d_agent_robotic_arm < 0.6) else 4 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Church. Objectives: It must avoid colliding with parishioner. Conditions: The states of the drone and parishioner are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_parishioner, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_technician = calculate_distance_min(s_agent, s_parishioner) \n    penalty_collision = -19 if (d_agent_technician < 0.6) else 4 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Church. Objectives: Avoid collision with two traffic cone lines. Conditions: The states of the agent, wires, parishioner and warning signs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Church. Objectives: It must avoid colliding with any truck, overhead hanging lamp chains and entrance barrier. Conditions: Agent's, wall and pastor are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Church. Objectives: Do not collide with any bible. Conditions: The states of the drone and the bible are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bible, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bible = calculate_distance_min(s_agent, s_bible) \n    penalty = -19 if (d_agent_bible < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Church and must avoid contact with any sacristy  door. The states of the drone and the sacristy door are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_sacristy_door, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sacristy_door = calculate_distance_min(s_agent, s_sacristy_door) \n    penalty = -19 if (d_agent_sacristy_door < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Church. It must avoid colliding with the confessional booth, usher and hymn board. The states of the agent, hanging lamp chains, storage racks and safety barricades are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Church. It must avoid colliding with altar rails and hymn boards. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Church. It must avoid colliding with Church organ pipess. Agent's state, target's state and Church organ pipess's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_organ_pipes, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_pipe = calculate_distance_min(s_agent, s_organ_pipes) \n    penalty_collision = -19 if (d_agent_pipe < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Church. Goal: It must avoid colliding with hanging lamp chain. Assumptions: The states of the drone and the hanging lamp chain's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_lamp_chain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lamp_chain = calculate_distance_min(s_agent, s_lamp_chain) \n    penalty_collision = -19 if (d_agent_lamp_chain < 0.6) else 18 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Church. Goal: It must avoid colliding with Church's pew, pulpit and parishioner. Assumptions: The Agent's state, target's state and pew, pulpit and parishioner' states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Church. Goal: Detect altars and benchs and avoid church door and do not collide. Assumptions: The states of the drone, target  altar, benches and avoid church door state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Church. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Art Museum. \n Objectives: It must avoid colliding with concrete columns or exhibit placards. \n Conditions: The states of the agent, the stanchion, and the exhibit placards are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 18 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Art Museum. \n Objectives: Detect sculpture pedestal and aviod it. \n Conditions: The states of the agent, the sculpture pedestal are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_sculpture_pedestal, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sculpture_pedestal = calculate_distance_min(s_agent, s_sculpture_pedestal) \n    penalty_collision = -18 if (d_agent_sculpture_pedestal < 0.6) else 18 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Art Museum open workspace. Objectives: It must avoid colliding with gallery fixtures. Conditions: States of the drone and the gallery fixturest are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_gallery_fixtures, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_gallery_fixtures = calculate_distance_min(s_agent, s_gallery_fixtures) \n    penalty_collision = -18 if (d_agent_gallery_fixtures < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a art museum corridor, the drone must maneuver around art museum curtain. All entity states (agent, art museum curtain) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_gallery_curtain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_gallery_curtain = calculate_distance_min(s_agent, s_gallery_curtain) \n    penalty_collision = -18 if (d_agent_gallery_curtain < 0.6) else 18 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a art museum corridor. Requirement: Detect the three art museum visitors and do not collide with it. Given: The states for the drone and many art museum visitors are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 18 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a staff service area during experiments. \n Objectives: So not collide with display_stand. \n Conditions: The states of the agent, the display_stand are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_display_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_display_stand = calculate_distance_min(s_agent, s_display_stand) \n    penalty_collision = -18 if (d_agent_display_stand < 0.6) else 18 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in a Art Museum. Objectives: Do not collide hit with sculpture plinth. Conditions: States of the agent, sculpture plinth are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_sculpture_plinth, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sculpture_plinth = calculate_distance_min(s_agent, s_sculpture_plinth) \n    penalty_collision = -18 if (d_agent_sculpture_plinth < 0.6) else 4 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a art museum corridor, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_gallery_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_gallery_wall = calculate_distance_min(s_agent, s_gallery_wall) \n    penalty_collision = -18 if (d_agent_gallery_wall < 0.6) else 18 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: art museum floor. Avoid: Hitting with display stand. Known: Agent, display stand states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_display_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_display_stand = calculate_distance_min(s_agent, s_display_stand) \n    penalty_collision = -18 if (d_agent_display_stand < 0.6) else 4 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Art Museum. Objectives: It must avoid colliding with visitor. Conditions: The states of the drone and visitor are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_visitor, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_visitor = calculate_distance_min(s_agent, s_visitor) \n    penalty_collision = -19 if (d_agent_visitor < 0.6) else 4 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Art Museum. Objectives: Avoid collision with two cone lines. Conditions: The states of the agent and two cone lines are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Art Museum. Objectives: It must avoid colliding with any picture, overhead wires and ticket barrier. Conditions: Agent's, picture's, wire's and ticket barrier's states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Art Museum. Objectives: Do not collide with any sculpture pedestal. Conditions: The states of the drone and the sculpture pedestal are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_sculpture_pedestal, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sculpture_pedestal = calculate_distance_min(s_agent, s_sculpture_pedestal) \n    penalty = -19 if (d_agent_sculpture_pedestal < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Art Museum and must avoid contact with any staff only door. The states of the drone and the staff only door are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_staff_door, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_door = calculate_distance_min(s_agent, s_staff_door) \n    penalty = -19 if (d_agent_door < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Art Museum. It must avoid colliding with the freestanding partition, docent and exhibit placard. The states of the agent, suspension wires, storage racks and safety barricades are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Art Museum. It must avoid colliding with stanchions and exhibit placards. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Art Museum. It must avoid colliding with Art Museum organ hanging installations. Agent's state, target's state and Art Museum organ hanging installations's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_installation, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_installation = calculate_distance_min(s_agent, s_installation) \n    penalty_collision = -19 if (d_agent_installation < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Art Museum. Goal: It must avoid colliding with suspension wire. Assumptions: The states of the drone and the suspension wire's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_suspension_wire, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_suspension_wire = calculate_distance_min(s_agent, s_suspension_wire) \n    penalty_collision = -19 if (d_agent_suspension_wire < 0.6) else 18 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Art Museum. Goal: It must avoid colliding with Art Museum's pew, pulpit and visitor. Assumptions: The Agent's state, target's state and pew, pulpit and visitor' states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Art Museum. Goal: Detect waste containers and gas cylindes and avoid art museum door and do not collide. Assumptions: The states of the drone, target  freestanding partition, docent and exhibit placard state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Art Museum. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -2**d_agent_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the concrete columns or wayfinding signs and moving toward a designated target. The system provides access to the drones state, the concrete columns or wayfinding signs information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 18 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the ramp guardrail and moving toward a designated target. The system provides access to the drones state, the ramp guardrail information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_ramp, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ramp = calculate_distance_min(s_agent, s_ramp) \n    penalty_collision = -18 if (d_agent_ramp < 0.6) else 18 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the payment kiosks and moving toward a designated target. The system provides access to the drones state, the payment kiosks information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_payment_kiosk, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_payment_kiosk = calculate_distance_min(s_agent, s_payment_kiosk) \n    penalty_collision = -18 if (d_agent_payment_kiosk < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the roll-up door and moving toward a designated target. The system provides access to the drones state, the roll-up door information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_door, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_door = calculate_distance_min(s_agent, s_door) \n    penalty_collision = -18 if (d_agent_door < 0.6) else 18 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the car and moving toward a designated target. The system provides access to the drones state, the many obstacles information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 18 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the car and moving toward a designated target. The system provides access to the drones state, the car information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_car, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_car = calculate_distance_min(s_agent, s_car) \n    penalty_collision = -18 if (d_agent_car < 0.6) else 18 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the truck and moving toward a designated target. The system provides access to the drones state, the truck information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_truck, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_truck = calculate_distance_min(s_agent, s_truck) \n    penalty_collision = -18 if (d_agent_truck < 0.6) else 4 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the wall and moving toward a designated target. The system provides access to the drones state, the wall information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -18 if (d_agent_wall < 0.6) else 18 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the vehicle and moving toward a designated target. The system provides access to the drones state, the vehicle information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_vehicle, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_vehicle = calculate_distance_min(s_agent, s_vehicle) \n    penalty_collision = -18 if (d_agent_vehicle < 0.6) else 4 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the human and moving toward a designated target. The system provides access to the drones state, the human information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -19 if (d_agent_human < 0.6) else 4 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the two traffic cone lines and moving toward a designated target. The system provides access to the drones state, the many obstacles: two traffic cone lines information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the truck, overhead cable bundle and gate arm and moving toward a designated target. The system provides access to the drones state, the truck, many obstacles: overhead cable bundle and gate arm information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the EV charging station and moving toward a designated target. The system provides access to the drones state, the EV charging station information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_charging_station, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_charging_station = calculate_distance_min(s_agent, s_charging_station) \n    penalty = -19 if (d_agent_charging_station < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the gate and moving toward a designated target. The system provides access to the drones state, the gate information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_gate, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_gate = calculate_distance_min(s_agent, s_gate) \n    penalty = -19 if (d_agent_gate < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the motorcycle, attendant and sign and moving toward a designated target. The system provides access to the drones state, the many obstacles: motorcycle, attendant and sign information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the bollards and wayfinding signs and moving toward a designated target. The system provides access to the drones state, the many obstacles:  bollards and wayfinding signs information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the Parking Garage ventilation ducts and moving toward a designated target. The system provides access to the drones state, the Parking Garage ventilation ducts information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_ventilation, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ventilation = calculate_distance_min(s_agent, s_ventilation) \n    penalty_collision = -19 if (d_agent_ventilation < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the overhead conduit and moving toward a designated target. The system provides access to the drones state, the overhead conduit information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_conduit, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_conduit = calculate_distance_min(s_agent, s_conduit) \n    penalty_collision = -19 if (d_agent_conduit < 0.6) else 18 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the Parking Garage ventilation ducts, wayfinding signs and wheel stops and moving toward a designated target. The system provides access to the drones state, the many obstacles: Parking Garage ventilation ducts, wayfinding signs and wheel stops information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the wayfinding signs and structural brackets and moving toward a designated target. The system provides access to the drones state, the many obstacles: wayfinding signs and structural brackets information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage and moving toward a designated target. The system provides access to the drones state, the ramp information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is operating along Rainforest. \n Objectives: It must avoid colliding with tree trunks or hanging vines. \n Conditions: The states of the agent, the ride guardrail, and the trail markers are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.65) else 10 \n    reward_reaching_target = 19 if (d_agent_target < 0.62) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is operating inside a busy Rainforest. \n Objectives: Detect fallen log barrier and avoid it. \n Conditions: The states of the agent, the fallen log barrier are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_fallen_log, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_fallen_log = calculate_distance_min(s_agent, s_fallen_log) \n    penalty_collision = -10 if (d_agent_fallen_log < 0.65) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.62) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Rainforest clearing. Objectives: It must avoid colliding with termite mounds. Conditions: States of the drone and the payment kiosk are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_termite_mound, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_termite_mound = calculate_distance_min(s_agent, s_termite_mound) \n    penalty_collision = -10 if (d_agent_termite_mound < 0.65) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.62) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a forest trail, the drone must maneuver around thick vine curtain. All entity states (agent, thick vine curtain) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_vine_curtain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_vine_curtain = calculate_distance_min(s_agent, s_vine_curtain) \n    penalty_collision = -10 if (d_agent_vine_curtain < 0.65) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.62) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a forest trail. Requirement: Detect the three boulders and do not collide with it. Given: The states for the drone and many boulders are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.65) else 10 \n    reward_reaching_target = 19 if (d_agent_target < 0.62) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone operates near a riverbank during the monsoon. \n Objectives: Do not collide with wild boar. \n Conditions: The states of the agent, the wild boar are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_boar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_boar = calculate_distance_min(s_agent, s_boar) \n    penalty_collision = -10 if (d_agent_boar < 0.65) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.62) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navibamboo gate Rainforest. \n Objectives: Do not collide with with elephant. \n Conditions: States of the agent, elephant are obtainable. \n Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_elephant, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_elephant = calculate_distance_min(s_agent, s_elephant) \n    penalty_collision = -10 if (d_agent_elephant < 0.65) else 5 \n    reward_reaching_target = 10 if (d_agent_target < 0.62) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a forest trail, the drone must steer clear of cliff face. All states (agent, cliff face) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_cliff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cliff = calculate_distance_min(s_agent, s_cliff) \n    penalty_collision = -10 if (d_agent_cliff < 0.65) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.62) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: forest floor. Avoid: Hitting Rainforest's vehicle. Known: Agent, Rainforest vehicle states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_wild_animal, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wild_animal = calculate_distance_min(s_agent, s_wild_animal) \n    penalty_collision = -10 if (d_agent_wild_animal < 0.65) else 5 \n    reward_reaching_target = 10 if (d_agent_target < 0.62) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Rainforest. Objectives: It must avoid colliding with forest ranger. Conditions: The states of the drone and human are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ranger, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ranger = calculate_distance_min(s_agent, s_ranger) \n    penalty_collision = -19 if (d_agent_ranger < 0.65) else 5 \n    reward_reaching_target = 10 if (d_agent_target < 0.62) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Rainforest. Objectives: Avoid collision with two thorny bush lines. Conditions: The states of the agent, thorny bush lines and attendants are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.65) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.62) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Rainforest. Objectives: It must avoid colliding with any elephant, overhanging liana bundle and fallen branch. Conditions: Agent's, elephant and fallen branch states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.65) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.62) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Rainforest. \n Objectives: Do not collide with any large tree stump. \n Conditions: The states of the drone and the large tree stump are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_tree_stump, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tree_stump = calculate_distance_min(s_agent, s_tree_stump) \n    penalty = -19 if (d_agent_tree_stump < 0.65) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.62) else -d_agent_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is operating along Rainforest and must avoid contact with any bamboo gate. \n Conditions: The states of the drone and the bamboo gate are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_bamboo_gate, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bamboo_gate = calculate_distance_min(s_agent, s_bamboo_gate) \n    penalty = -19 if (d_agent_bamboo_gate < 0.65) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.62) else -d_agent_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Rainforest. \n Objectives: It must avoid colliding with the vine tangles, ranger and signpost. \n Conditions: The states of the agent, vine tangles, ranger and signpost are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.65) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.62) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Rainforest. It must avoid colliding with bollards and trail markers. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.65) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.62) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Rainforest. It must avoid colliding with Rainforest buttress roots. Agent's state, target's state and Rainforest buttress roots's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_buttress_roots, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_buttress_roots = calculate_distance_min(s_agent, s_buttress_roots) \n    penalty_collision = -19 if (d_agent_buttress_roots < 0.65) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.62) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Rainforest. Goal: It must avoid colliding with overhanging lianas. Assumptions: The states of the drone and the overhanging lianas area are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_lianas, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lianas = calculate_distance_min(s_agent, s_lianas) \n    penalty_collision = -19 if (d_agent_lianas < 0.65) else 10 \n    reward_reaching_target = 19 if (d_agent_target < 0.62) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Rainforest. Goal: It must avoid colliding with Rainforest buttress roots, trail markers and boulders. Assumptions: The Agent's state, target's state and Rainforest buttress roots's state, wheel stop's state and trail markers' states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.65) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.62) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Rainforest. Goal: Detect trail markers and tree buttresses and avoid forest trail corners. Assumptions: The states of the drone and bamboo gates, pay stations and thorny barriers state are available. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.65) else 10 \n    reward_reaching_target = 10 if (d_agent_target < 0.62) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Rainforest. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 10 if (d_agent_target < 0.62) else -d_agent_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (concrete columns or wayfinding signs) while progressing toward the target. \n The environment provides access to the agent state, target state, and multiple obstacles: concrete columns or wayfinding signs states.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.4) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (centrifuge machine) while progressing toward the target. \n The environment provides access to the agent state, target state, and centrifuge machine states.",
        "chosen": "def reward(s_agent, s_centrifuge_machine, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_centrifuge_machine = calculate_distance_min(s_agent, s_centrifuge_machine) \n    penalty_collision = -15 if (d_agent_centrifuge_machine < 0.4) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (lab equipmentt) while progressing toward the target. \n The environment provides access to the agent state, target state, and lab equipmentt states.",
        "chosen": "def reward(s_agent, s_lab_equipment, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lab_equipment = calculate_distance_min(s_agent, s_lab_equipment) \n    penalty_collision = -15 if (d_agent_lab_equipmen < 0.4) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (lab curtian) while progressing toward the target. \n The environment provides access to the agent state, target state, and lab curtain states.",
        "chosen": "def reward(s_agent, s_lab_curtain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lab_curtain = calculate_distance_min(s_agent, s_lab_curtain) \n    penalty_collision = -15 if (d_agent_lab_curtain < 0.4) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (centrifuge machine, equipment and curtain) while progressing toward the target. \n The environment provides access to the agent state, target state, and multiple obstacles: centrifuge machine, equipment and curtain states.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.4) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (lab robot) while progressing toward the target. \n The environment provides access to the agent state, target state, and lab robot states.",
        "chosen": "def reward(s_agent, s_lab_robot, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lab_robot = calculate_distance_min(s_agent, s_lab_robot) \n    penalty_collision = -15 if (d_agent_lab_robot < 0.4) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (apparatus) while progressing toward the target. \n The environment provides access to the agent state, target state, and apparatus states.",
        "chosen": "def reward(s_agent, s_apparatus, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_apparatus = calculate_distance_min(s_agent, s_apparatus) \n    penalty_collision = -15 if (d_agent_apparatus < 0.4) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (lab wall) while progressing toward the target. \n The environment provides access to the agent state, target state, and lab wall states.",
        "chosen": "def reward(s_agent, s_lab_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lab_wall = calculate_distance_min(s_agent, s_lab_wall) \n    penalty_collision = -15 if (d_agent_lab_wall < 0.4) else 15 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (robotic arm) while progressing toward the target. \n The environment provides access to the agent state, target state, and robotic arm states.",
        "chosen": "def reward(s_agent, s_robotic_arm, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_robotic_arm = calculate_distance_min(s_agent, s_robotic_arm) \n    penalty_collision = -15 if (d_agent_robotic_arm < 0.4) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (technician) while progressing toward the target. \n The environment provides access to the agent state, target state, and technician states.",
        "chosen": "def reward(s_agent, s_technician, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_technician = calculate_distance_min(s_agent, s_technician) \n    penalty_collision = -19 if (d_agent_technician < 0.4) else 4 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (table, chair) while progressing toward the target. \n The environment provides access to the agent state, target state, and multiple obstacles: table and chair states.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.4) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (acid bottle, overhead cable bundle and chair) while progressing toward the target. \n The environment provides access to the agent state, target state, and multiple obstacles: acid bottle, overhead cable bundle and chair states.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.4) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (centrifuge) while progressing toward the target. \n The environment provides access to the agent state, target state, and centrifuge states.",
        "chosen": "def reward(s_agent, s_centrifuge, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_centrifuge = calculate_distance_min(s_agent, s_centrifuge) \n    penalty = -19 if (d_agent_centrifuge < 0.4) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -2**d_agent_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (secure door) while progressing toward the target. \n The environment provides access to the agent state, target state, and door states.",
        "chosen": "def reward(s_agent, s_secure_door, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_secure_door = calculate_distance_min(s_agent, s_secure_door) \n    penalty = -19 if (d_agent_secure_door < 0.4) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -2**d_agent_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (students, attendant and sign) while progressing toward the target. \n The environment provides access to the agent state, target state, and students, attendant and sign states.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.4) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (blackboard, signs) while progressing toward the target. \n The environment provides access to the agent state, target state, and  balckboard and sign signs states.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.4) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (pipes) while progressing toward the target. \n The environment provides access to the agent state, target state, and pipes states.",
        "chosen": "def reward(s_agent, s_pipe, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_pipe = calculate_distance_min(s_agent, s_pipe) \n    penalty_collision = -19 if (d_agent_pipe < 0.4) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (cable) while progressing toward the target. \n The environment provides access to the agent state, target state, and cable states.",
        "chosen": "def reward(s_agent, s_cable, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cable = calculate_distance_min(s_agent, s_cable) \n    penalty_collision = -19 if (d_agent_cable < 0.4) else 15 \n    reward_reaching_target = 19 if (d_agent_target < 0.2) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (Science Lab's chair, cabinet and student) while progressing toward the target. \n The environment provides access to the agent state, target state, and Science Lab's chair, cabinet and student states.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.4) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (waste containers and gas cylinders) while progressing toward the target. \n The environment provides access to the agent state, target state, and waste containers and gas cylinders states.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.4) else 15 \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (table) while progressing toward the target. \n The environment provides access to the agent state, target state, and table states.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 15 if (d_agent_target < 0.2) else -2**d_agent_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Church. \n Objectives: It must avoid colliding with concrete columns or hymn boards. \n Conditions: The states of the agent, the altar rail, and the hymn boards are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 18 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Church. \n Objectives: Detect candelabra and aviod it. \n Conditions: The states of the agent, the candelabra are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_candelabra, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_centrifuge_machine = calculate_distance_min(s_agent, s_candelabra) \n    penalty_collision = -18 if (d_agent_centrifuge_machine < 0.6) else 18 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Church open workspace. Objectives: It must avoid colliding with church equipment. Conditions: States of the drone and the church equipment are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_church_fixtures, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lab_equipment = calculate_distance_min(s_agent, s_church_fixtures) \n    penalty_collision = -18 if (d_agent_church_fixtures < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a church corridor, the drone must maneuver around church curtain. All entity states (agent, church curtain) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_choir_curtain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lab_curtain = calculate_distance_min(s_agent, s_choir_curtain) \n    penalty_collision = -18 if (d_agent_lab_curtain < 0.6) else 18 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a church corridor. Requirement: Detect the three church parishioners and do not collide with it. Given: The states for the drone and many church parishioners are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 18 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a sacristy area during experiments. \n Objectives: So not collide with processional stand. \n Conditions: The states of the agent, the processional stand are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_processional_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_processional_stand = calculate_distance_min(s_agent, s_processional_stand) \n    penalty_collision = -18 if (d_agent_processional_stand < 0.6) else 18 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Church. Objectives: Do not collide hit with baptismal font. Conditions: States of the agent, baptismal font are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_baptismal_font, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_baptismal_font = calculate_distance_min(s_agent, s_baptismal_font) \n    penalty_collision = -18 if (d_agent_baptismal_font < 0.6) else 4 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a church corridor, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_church_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_church_wall = calculate_distance_min(s_agent, s_church_wall) \n    penalty_collision = -18 if (d_agent_church_wall < 0.6) else 18 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: church floor. Avoid: Hitting with processional stand. Known: Agent, processional stand states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_processional_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_robotic_arm = calculate_distance_min(s_agent, s_processional_stand) \n    penalty_collision = -18 if (d_agent_robotic_arm < 0.6) else 4 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Church. Objectives: It must avoid colliding with parishioner. Conditions: The states of the drone and parishioner are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_parishioner, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_technician = calculate_distance_min(s_agent, s_parishioner) \n    penalty_collision = -19 if (d_agent_technician < 0.6) else 4 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Church. Objectives: Avoid collision with two traffic cone lines. Conditions: The states of the agent, wires, parishioner and warning signs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Church. Objectives: It must avoid colliding with any truck, overhead hanging lamp chains and entrance barrier. Conditions: Agent's, wall and pastor are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Church. Objectives: Do not collide with any bible. Conditions: The states of the drone and the bible are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bible, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bible = calculate_distance_min(s_agent, s_bible) \n    penalty = -19 if (d_agent_bible < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Church and must avoid contact with any sacristy  door. The states of the drone and the sacristy door are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_sacristy_door, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sacristy_door = calculate_distance_min(s_agent, s_sacristy_door) \n    penalty = -19 if (d_agent_sacristy_door < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Church. It must avoid colliding with the confessional booth, usher and hymn board. The states of the agent, hanging lamp chains, storage racks and safety barricades are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Church. It must avoid colliding with altar rails and hymn boards. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Church. It must avoid colliding with Church organ pipess. Agent's state, target's state and Church organ pipess's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_organ_pipes, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_pipe = calculate_distance_min(s_agent, s_organ_pipes) \n    penalty_collision = -19 if (d_agent_pipe < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Church. Goal: It must avoid colliding with hanging lamp chain. Assumptions: The states of the drone and the hanging lamp chain's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_lamp_chain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lamp_chain = calculate_distance_min(s_agent, s_lamp_chain) \n    penalty_collision = -19 if (d_agent_lamp_chain < 0.6) else 18 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Church. Goal: It must avoid colliding with Church's pew, pulpit and parishioner. Assumptions: The Agent's state, target's state and pew, pulpit and parishioner' states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Church. Goal: Detect altars and benchs and avoid church door and do not collide. Assumptions: The states of the drone, target  altar, benches and avoid church door state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Church. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Art Museum. \n Objectives: It must avoid colliding with concrete columns or exhibit placards. \n Conditions: The states of the agent, the stanchion, and the exhibit placards are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 18 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Art Museum. \n Objectives: Detect sculpture pedestal and aviod it. \n Conditions: The states of the agent, the sculpture pedestal are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_sculpture_pedestal, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sculpture_pedestal = calculate_distance_min(s_agent, s_sculpture_pedestal) \n    penalty_collision = -18 if (d_agent_sculpture_pedestal < 0.6) else 18 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Art Museum open workspace. Objectives: It must avoid colliding with gallery fixtures. Conditions: States of the drone and the gallery fixturest are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_gallery_fixtures, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_gallery_fixtures = calculate_distance_min(s_agent, s_gallery_fixtures) \n    penalty_collision = -18 if (d_agent_gallery_fixtures < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a art museum corridor, the drone must maneuver around art museum curtain. All entity states (agent, art museum curtain) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_gallery_curtain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_gallery_curtain = calculate_distance_min(s_agent, s_gallery_curtain) \n    penalty_collision = -18 if (d_agent_gallery_curtain < 0.6) else 18 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a art museum corridor. Requirement: Detect the three art museum visitors and do not collide with it. Given: The states for the drone and many art museum visitors are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 18 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a staff service area during experiments. \n Objectives: So not collide with display_stand. \n Conditions: The states of the agent, the display_stand are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_display_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_display_stand = calculate_distance_min(s_agent, s_display_stand) \n    penalty_collision = -18 if (d_agent_display_stand < 0.6) else 18 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Art Museum. Objectives: Do not collide hit with sculpture plinth. Conditions: States of the agent, sculpture plinth are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_sculpture_plinth, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sculpture_plinth = calculate_distance_min(s_agent, s_sculpture_plinth) \n    penalty_collision = -18 if (d_agent_sculpture_plinth < 0.6) else 4 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a art museum corridor, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_gallery_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_gallery_wall = calculate_distance_min(s_agent, s_gallery_wall) \n    penalty_collision = -18 if (d_agent_gallery_wall < 0.6) else 18 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: art museum floor. Avoid: Hitting with display stand. Known: Agent, display stand states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_display_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_display_stand = calculate_distance_min(s_agent, s_display_stand) \n    penalty_collision = -18 if (d_agent_display_stand < 0.6) else 4 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Art Museum. Objectives: It must avoid colliding with visitor. Conditions: The states of the drone and visitor are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_visitor, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_visitor = calculate_distance_min(s_agent, s_visitor) \n    penalty_collision = -19 if (d_agent_visitor < 0.6) else 4 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Art Museum. Objectives: Avoid collision with two cone lines. Conditions: The states of the agent and two cone lines are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Art Museum. Objectives: It must avoid colliding with any picture, overhead wires and ticket barrier. Conditions: Agent's, picture's, wire's and ticket barrier's states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Art Museum. Objectives: Do not collide with any sculpture pedestal. Conditions: The states of the drone and the sculpture pedestal are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_sculpture_pedestal, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sculpture_pedestal = calculate_distance_min(s_agent, s_sculpture_pedestal) \n    penalty = -19 if (d_agent_sculpture_pedestal < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Art Museum and must avoid contact with any staff only door. The states of the drone and the staff only door are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_staff_door, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_door = calculate_distance_min(s_agent, s_staff_door) \n    penalty = -19 if (d_agent_door < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Art Museum. It must avoid colliding with the freestanding partition, docent and exhibit placard. The states of the agent, suspension wires, storage racks and safety barricades are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Art Museum. It must avoid colliding with stanchions and exhibit placards. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Art Museum. It must avoid colliding with Art Museum organ hanging installations. Agent's state, target's state and Art Museum organ hanging installations's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_installation, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_installation = calculate_distance_min(s_agent, s_installation) \n    penalty_collision = -19 if (d_agent_installation < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Art Museum. Goal: It must avoid colliding with suspension wire. Assumptions: The states of the drone and the suspension wire's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_suspension_wire, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_suspension_wire = calculate_distance_min(s_agent, s_suspension_wire) \n    penalty_collision = -19 if (d_agent_suspension_wire < 0.6) else 18 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Art Museum. Goal: It must avoid colliding with Art Museum's pew, pulpit and visitor. Assumptions: The Agent's state, target's state and pew, pulpit and visitor' states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Art Museum. Goal: Detect waste containers and gas cylindes and avoid art museum door and do not collide. Assumptions: The states of the drone, target  freestanding partition, docent and exhibit placard state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Art Museum. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 18 if (d_agent_target < 0.3) else -math.exp(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the concrete columns or wayfinding signs and moving toward a designated target. The system provides access to the drones state, the concrete columns or wayfinding signs information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_multiple_obstacles_min < 0.6) else 20 \n    reward_reaching_target = 25 if (d_agent_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the ramp guardrail and moving toward a designated target. The system provides access to the drones state, the ramp guardrail information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_ramp, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ramp = calculate_distance_min(s_agent, s_ramp) \n    penalty_collision = -20 if (d_agent_ramp < 0.6) else 20 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the payment kiosks and moving toward a designated target. The system provides access to the drones state, the payment kiosks information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_payment_kiosk, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_payment_kiosk = calculate_distance_min(s_agent, s_payment_kiosk) \n    penalty_collision = -20 if (d_agent_payment_kiosk < 0.6) else 20 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the roll-up door and moving toward a designated target. The system provides access to the drones state, the roll-up door information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_door, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_door = calculate_distance_min(s_agent, s_door) \n    penalty_collision = -20 if (d_agent_door < 0.6) else 20 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the car and moving toward a designated target. The system provides access to the drones state, the many obstacles information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_multiple_obstacles_min < 0.6) else 20 \n    reward_reaching_target = 25 if (d_agent_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the car and moving toward a designated target. The system provides access to the drones state, the car information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_car, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_car = calculate_distance_min(s_agent, s_car) \n    penalty_collision = -20 if (d_agent_car < 0.6) else 20 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the truck and moving toward a designated target. The system provides access to the drones state, the truck information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_truck, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_truck = calculate_distance_min(s_agent, s_truck) \n    penalty_collision = -20 if (d_agent_truck < 0.6) else 4 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the wall and moving toward a designated target. The system provides access to the drones state, the wall information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -20 if (d_agent_wall < 0.6) else 20 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the vehicle and moving toward a designated target. The system provides access to the drones state, the vehicle information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_vehicle, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_vehicle = calculate_distance_min(s_agent, s_vehicle) \n    penalty_collision = -20 if (d_agent_vehicle < 0.6) else 4 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the human and moving toward a designated target. The system provides access to the drones state, the human information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_human, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_human = calculate_distance_min(s_agent, s_human) \n    penalty_collision = -25 if (d_agent_human < 0.6) else 4 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the two traffic cone lines and moving toward a designated target. The system provides access to the drones state, the many obstacles: two traffic cone lines information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_multiple_obstacles < 0.6) else 20 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the truck, overhead cable bundle and gate arm and moving toward a designated target. The system provides access to the drones state, the truck, many obstacles: overhead cable bundle and gate arm information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_multiple_obstacles < 0.6) else 20 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the EV charging station and moving toward a designated target. The system provides access to the drones state, the EV charging station information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_charging_station, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_charging_station = calculate_distance_min(s_agent, s_charging_station) \n    penalty = -25 if (d_agent_charging_station < 0.6) else 20 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -10 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the gate and moving toward a designated target. The system provides access to the drones state, the gate information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_gate, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_gate = calculate_distance_min(s_agent, s_gate) \n    penalty = -25 if (d_agent_gate < 0.6) else 20 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -10 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the motorcycle, attendant and sign and moving toward a designated target. The system provides access to the drones state, the many obstacles: motorcycle, attendant and sign information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_multiple_obstacles < 0.6) else 20 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the bollards and wayfinding signs and moving toward a designated target. The system provides access to the drones state, the many obstacles:  bollards and wayfinding signs information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_multiple_obstacles < 0.6) else 20 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the Parking Garage ventilation ducts and moving toward a designated target. The system provides access to the drones state, the Parking Garage ventilation ducts information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_ventilation, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ventilation = calculate_distance_min(s_agent, s_ventilation) \n    penalty_collision = -25 if (d_agent_ventilation < 0.6) else 20 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the overhead conduit and moving toward a designated target. The system provides access to the drones state, the overhead conduit information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_conduit, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_conduit = calculate_distance_min(s_agent, s_conduit) \n    penalty_collision = -25 if (d_agent_conduit < 0.6) else 20 \n    reward_reaching_target = 25 if (d_agent_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the Parking Garage ventilation ducts, wayfinding signs and wheel stops and moving toward a designated target. The system provides access to the drones state, the many obstacles: Parking Garage ventilation ducts, wayfinding signs and wheel stops information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_multiple_obstacles < 0.6) else 20 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage, avoiding any contact with the wayfinding signs and structural brackets and moving toward a designated target. The system provides access to the drones state, the many obstacles: wayfinding signs and structural brackets information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -25 if (d_agent_multiple_obstacles < 0.6) else 20 \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -10 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone flying within a parking garage and moving toward a designated target. The system provides access to the drones state, the ramp information, and the target state. Write a pyhton code for reward function.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 20 if (d_agent_target < 0.2) else -10 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is operating along Rainforest. \n Objectives: It must avoid colliding with tree trunks or hanging vines. \n Conditions: The states of the agent, the ride guardrail, and the trail markers are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 13 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is operating inside a busy Rainforest. \n Objectives: Detect fallen log barrier and avoid it. \n Conditions: The states of the agent, the fallen log barrier are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_fallen_log, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_fallen_log = calculate_distance_min(s_agent, s_fallen_log) \n    penalty_collision = -13 if (d_agent_fallen_log < 0.6) else 13 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Rainforest clearing. Objectives: It must avoid colliding with termite mounds. Conditions: States of the drone and the payment kiosk are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_termite_mound, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_termite_mound = calculate_distance_min(s_agent, s_termite_mound) \n    penalty_collision = -13 if (d_agent_termite_mound < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a forest trail, the drone must maneuver around thick vine curtain. All entity states (agent, thick vine curtain) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_vine_curtain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_vine_curtain = calculate_distance_min(s_agent, s_vine_curtain) \n    penalty_collision = -13 if (d_agent_vine_curtain < 0.6) else 13 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a forest trail. Requirement: Detect the three boulders and do not collide with it. Given: The states for the drone and many boulders are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 13 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone operates near a riverbank during the monsoon. \n Objectives: Do not collide with wild boar. \n Conditions: The states of the agent, the wild boar are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_boar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_boar = calculate_distance_min(s_agent, s_boar) \n    penalty_collision = -13 if (d_agent_boar < 0.6) else 13 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navibamboo gate Rainforest. \n Objectives: Do not collide with with elephant. \n Conditions: States of the agent, elephant are obtainable. \n Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_elephant, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_elephant = calculate_distance_min(s_agent, s_elephant) \n    penalty_collision = -13 if (d_agent_elephant < 0.6) else 4 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a forest trail, the drone must steer clear of cliff face. All states (agent, cliff face) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_cliff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cliff = calculate_distance_min(s_agent, s_cliff) \n    penalty_collision = -13 if (d_agent_cliff < 0.6) else 13 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: forest floor. Avoid: Hitting Rainforest's vehicle. Known: Agent, Rainforest vehicle states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_wild_animal, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wild_animal = calculate_distance_min(s_agent, s_wild_animal) \n    penalty_collision = -13 if (d_agent_wild_animal < 0.6) else 4 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Rainforest. Objectives: It must avoid colliding with forest ranger. Conditions: The states of the drone and human are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ranger, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ranger = calculate_distance_min(s_agent, s_ranger) \n    penalty_collision = -19 if (d_agent_ranger < 0.6) else 4 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Rainforest. Objectives: Avoid collision with two thorny bush lines. Conditions: The states of the agent, thorny bush lines and attendants are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Rainforest. Objectives: It must avoid colliding with any elephant, overhanging liana bundle and fallen branch. Conditions: Agent's, elephant and fallen branch states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Rainforest. \n Objectives: Do not collide with any large tree stump. \n Conditions: The states of the drone and the large tree stump are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_tree_stump, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tree_stump = calculate_distance_min(s_agent, s_tree_stump) \n    penalty = -19 if (d_agent_tree_stump < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is operating along Rainforest and must avoid contact with any bamboo gate. \n Conditions: The states of the drone and the bamboo gate are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_bamboo_gate, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bamboo_gate = calculate_distance_min(s_agent, s_bamboo_gate) \n    penalty = -19 if (d_agent_bamboo_gate < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Rainforest. \n Objectives: It must avoid colliding with the vine tangles, ranger and signpost. \n Conditions: The states of the agent, vine tangles, ranger and signpost are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Rainforest. It must avoid colliding with bollards and trail markers. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Rainforest. It must avoid colliding with Rainforest buttress roots. Agent's state, target's state and Rainforest buttress roots's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_buttress_roots, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_buttress_roots = calculate_distance_min(s_agent, s_buttress_roots) \n    penalty_collision = -19 if (d_agent_buttress_roots < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Rainforest. Goal: It must avoid colliding with overhanging lianas. Assumptions: The states of the drone and the overhanging lianas area are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_lianas, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lianas = calculate_distance_min(s_agent, s_lianas) \n    penalty_collision = -19 if (d_agent_lianas < 0.6) else 13 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Rainforest. Goal: It must avoid colliding with Rainforest buttress roots, trail markers and boulders. Assumptions: The Agent's state, target's state and Rainforest buttress roots's state, wheel stop's state and trail markers' states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Rainforest. Goal: Detect trail markers and tree buttresses and avoid forest trail corners. Assumptions: The states of the drone and bamboo gates, pay stations and thorny barriers state are available. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Rainforest. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is operating along Rainforest. \n Objectives: It must avoid colliding with tree trunks or hanging vines. \n Conditions: The states of the agent, the ride guardrail, and the trail markers are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 13 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is operating inside a busy Rainforest. \n Objectives: Detect fallen log barrier and avoid it. \n Conditions: The states of the agent, the fallen log barrier are known. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_fallen_log, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_fallen_log = calculate_distance_min(s_agent, s_fallen_log) \n    penalty_collision = -13 if (d_agent_fallen_log < 0.6) else 13 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Rainforest clearing. Objectives: It must avoid colliding with termite mounds. Conditions: States of the drone and the payment kiosk are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_termite_mound, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_termite_mound = calculate_distance_min(s_agent, s_termite_mound) \n    penalty_collision = -13 if (d_agent_termite_mound < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a forest trail, the drone must maneuver around thick vine curtain. All entity states (agent, thick vine curtain) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_vine_curtain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_vine_curtain = calculate_distance_min(s_agent, s_vine_curtain) \n    penalty_collision = -13 if (d_agent_vine_curtain < 0.6) else 13 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a forest trail. Requirement: Detect the three boulders and do not collide with it. Given: The states for the drone and many boulders are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 13 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone operates near a riverbank during the monsoon. \n Objectives: Do not collide with wild boar. \n Conditions: The states of the agent, the wild boar are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_boar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_boar = calculate_distance_min(s_agent, s_boar) \n    penalty_collision = -13 if (d_agent_boar < 0.6) else 13 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navibamboo gate Rainforest. \n Objectives: Do not collide with with elephant. \n Conditions: States of the agent, elephant are obtainable. \n Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_elephant, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_elephant = calculate_distance_min(s_agent, s_elephant) \n    penalty_collision = -13 if (d_agent_elephant < 0.6) else 4 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a forest trail, the drone must steer clear of cliff face. All states (agent, cliff face) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_cliff, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cliff = calculate_distance_min(s_agent, s_cliff) \n    penalty_collision = -13 if (d_agent_cliff < 0.6) else 13 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: forest floor. Avoid: Hitting Rainforest's vehicle. Known: Agent, Rainforest vehicle states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_wild_animal, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wild_animal = calculate_distance_min(s_agent, s_wild_animal) \n    penalty_collision = -13 if (d_agent_wild_animal < 0.6) else 4 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Rainforest. Objectives: It must avoid colliding with forest ranger. Conditions: The states of the drone and human are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ranger, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ranger = calculate_distance_min(s_agent, s_ranger) \n    penalty_collision = -19 if (d_agent_ranger < 0.6) else 4 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Rainforest. Objectives: Avoid collision with two thorny bush lines. Conditions: The states of the agent, thorny bush lines and attendants are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Rainforest. Objectives: It must avoid colliding with any elephant, overhanging liana bundle and fallen branch. Conditions: Agent's, elephant and fallen branch states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Rainforest. \n Objectives: Do not collide with any large tree stump. \n Conditions: The states of the drone and the large tree stump are obtainable. \n Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_tree_stump, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tree_stump = calculate_distance_min(s_agent, s_tree_stump) \n    penalty = -19 if (d_agent_tree_stump < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is operating along Rainforest and must avoid contact with any bamboo gate. \n Conditions: The states of the drone and the bamboo gate are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_bamboo_gate, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bamboo_gate = calculate_distance_min(s_agent, s_bamboo_gate) \n    penalty = -19 if (d_agent_bamboo_gate < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Rainforest. \n Objectives: It must avoid colliding with the vine tangles, ranger and signpost. \n Conditions: The states of the agent, vine tangles, ranger and signpost are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Rainforest. It must avoid colliding with bollards and trail markers. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Rainforest. It must avoid colliding with Rainforest buttress roots. Agent's state, target's state and Rainforest buttress roots's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_buttress_roots, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_buttress_roots = calculate_distance_min(s_agent, s_buttress_roots) \n    penalty_collision = -19 if (d_agent_buttress_roots < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Rainforest. Goal: It must avoid colliding with overhanging lianas. Assumptions: The states of the drone and the overhanging lianas area are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_lianas, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lianas = calculate_distance_min(s_agent, s_lianas) \n    penalty_collision = -19 if (d_agent_lianas < 0.6) else 13 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Rainforest. Goal: It must avoid colliding with Rainforest buttress roots, trail markers and boulders. Assumptions: The Agent's state, target's state and Rainforest buttress roots's state, wheel stop's state and trail markers' states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Rainforest. Goal: Detect trail markers and tree buttresses and avoid forest trail corners. Assumptions: The states of the drone and bamboo gates, pay stations and thorny barriers state are available. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Rainforest. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (concrete columns or wayfinding signs) while progressing toward the target. \n The environment provides access to the agent state, target state, and multiple obstacles: concrete columns or wayfinding signs states.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.4) else 18 \n    reward_reaching_target = 19 if (d_agent_target < 0.1) else -math.log(d_agent_target)t \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (centrifuge machine) while progressing toward the target. \n The environment provides access to the agent state, target state, and centrifuge machine states.",
        "chosen": "def reward(s_agent, s_centrifuge_machine, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_centrifuge_machine = calculate_distance_min(s_agent, s_centrifuge_machine) \n    penalty_collision = -18 if (d_agent_centrifuge_machine < 0.4) else 18 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -math.log(d_agent_target)t \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (lab equipmentt) while progressing toward the target. \n The environment provides access to the agent state, target state, and lab equipmentt states.",
        "chosen": "def reward(s_agent, s_lab_equipment, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lab_equipment = calculate_distance_min(s_agent, s_lab_equipment) \n    penalty_collision = -18 if (d_agent_lab_equipmen < 0.4) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.1) else -math.log(d_agent_target)t \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (lab curtian) while progressing toward the target. \n The environment provides access to the agent state, target state, and lab curtain states.",
        "chosen": "def reward(s_agent, s_lab_curtain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lab_curtain = calculate_distance_min(s_agent, s_lab_curtain) \n    penalty_collision = -18 if (d_agent_lab_curtain < 0.4) else 18 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -math.log(d_agent_target)t \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (centrifuge machine, equipment and curtain) while progressing toward the target. \n The environment provides access to the agent state, target state, and multiple obstacles: centrifuge machine, equipment and curtain states.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.4) else 18 \n    reward_reaching_target = 19 if (d_agent_target < 0.1) else -math.log(d_agent_target)t \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (lab robot) while progressing toward the target. \n The environment provides access to the agent state, target state, and lab robot states.",
        "chosen": "def reward(s_agent, s_lab_robot, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lab_robot = calculate_distance_min(s_agent, s_lab_robot) \n    penalty_collision = -18 if (d_agent_lab_robot < 0.4) else 18 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -math.log(d_agent_target)t \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (apparatus) while progressing toward the target. \n The environment provides access to the agent state, target state, and apparatus states.",
        "chosen": "def reward(s_agent, s_apparatus, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_apparatus = calculate_distance_min(s_agent, s_apparatus) \n    penalty_collision = -18 if (d_agent_apparatus < 0.4) else 4 \n    reward_reaching_target = 18 if (d_agent_target < 0.1) else -math.log(d_agent_target)t \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (lab wall) while progressing toward the target. \n The environment provides access to the agent state, target state, and lab wall states.",
        "chosen": "def reward(s_agent, s_lab_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lab_wall = calculate_distance_min(s_agent, s_lab_wall) \n    penalty_collision = -18 if (d_agent_lab_wall < 0.4) else 18 \n    reward_reaching_target = 20 if (d_agent_target < 0.1) else -math.log(d_agent_target)t \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (robotic arm) while progressing toward the target. \n The environment provides access to the agent state, target state, and robotic arm states.",
        "chosen": "def reward(s_agent, s_robotic_arm, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_robotic_arm = calculate_distance_min(s_agent, s_robotic_arm) \n    penalty_collision = -18 if (d_agent_robotic_arm < 0.4) else 4 \n    reward_reaching_target = 18 if (d_agent_target < 0.1) else -math.log(d_agent_target)t \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (technician) while progressing toward the target. \n The environment provides access to the agent state, target state, and technician states.",
        "chosen": "def reward(s_agent, s_technician, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_technician = calculate_distance_min(s_agent, s_technician) \n    penalty_collision = -19 if (d_agent_technician < 0.4) else 4 \n    reward_reaching_target = 18 if (d_agent_target < 0.1) else -math.log(d_agent_target)t \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (table, chair) while progressing toward the target. \n The environment provides access to the agent state, target state, and multiple obstacles: table and chair states.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.4) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.1) else -math.log(d_agent_target)t \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (acid bottle, overhead cable bundle and chair) while progressing toward the target. \n The environment provides access to the agent state, target state, and multiple obstacles: acid bottle, overhead cable bundle and chair states.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.4) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.1) else -math.log(d_agent_target)t \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (centrifuge) while progressing toward the target. \n The environment provides access to the agent state, target state, and centrifuge states.",
        "chosen": "def reward(s_agent, s_centrifuge, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_centrifuge = calculate_distance_min(s_agent, s_centrifuge) \n    penalty = -19 if (d_agent_centrifuge < 0.4) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.1) else -math.log(d_agent_target)t \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (secure door) while progressing toward the target. \n The environment provides access to the agent state, target state, and door states.",
        "chosen": "def reward(s_agent, s_secure_door, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_secure_door = calculate_distance_min(s_agent, s_secure_door) \n    penalty = -19 if (d_agent_secure_door < 0.4) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.1) else -math.log(d_agent_target)t \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (students, attendant and sign) while progressing toward the target. \n The environment provides access to the agent state, target state, and students, attendant and sign states.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.4) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.1) else -math.log(d_agent_target)t \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (blackboard, signs) while progressing toward the target. \n The environment provides access to the agent state, target state, and  balckboard and sign signs states.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.4) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.1) else -math.log(d_agent_target)t \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (pipes) while progressing toward the target. \n The environment provides access to the agent state, target state, and pipes states.",
        "chosen": "def reward(s_agent, s_pipe, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_pipe = calculate_distance_min(s_agent, s_pipe) \n    penalty_collision = -19 if (d_agent_pipe < 0.4) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.1) else -math.log(d_agent_target)t \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (cable) while progressing toward the target. \n The environment provides access to the agent state, target state, and cable states.",
        "chosen": "def reward(s_agent, s_cable, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cable = calculate_distance_min(s_agent, s_cable) \n    penalty_collision = -19 if (d_agent_cable < 0.4) else 18 \n    reward_reaching_target = 19 if (d_agent_target < 0.1) else -math.log(d_agent_target)t \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (Science Lab's chair, cabinet and student) while progressing toward the target. \n The environment provides access to the agent state, target state, and Science Lab's chair, cabinet and student states.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.4) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.1) else -math.log(d_agent_target)t \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (waste containers and gas cylinders) while progressing toward the target. \n The environment provides access to the agent state, target state, and waste containers and gas cylinders states.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.4) else 18 \n    reward_reaching_target = 18 if (d_agent_target < 0.1) else -math.log(d_agent_target)t \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Implement in Python a reward function suitable. \n Scenario: an indoor drone explores a Science Lab. \n Requirements: the drone should identify and avoid obstacles (table) while progressing toward the target. \n The environment provides access to the agent state, target state, and table states.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 18 if (d_agent_target < 0.1) else -math.log(d_agent_target)t \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Church. \n Objectives: It must avoid colliding with concrete columns or hymn boards. \n Conditions: The states of the agent, the altar rail, and the hymn boards are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 13 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Church. \n Objectives: Detect candelabra and aviod it. \n Conditions: The states of the agent, the candelabra are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_candelabra, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_centrifuge_machine = calculate_distance_min(s_agent, s_candelabra) \n    penalty_collision = -13 if (d_agent_centrifuge_machine < 0.6) else 13 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Church open workspace. Objectives: It must avoid colliding with church equipment. Conditions: States of the drone and the church equipment are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_church_fixtures, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lab_equipment = calculate_distance_min(s_agent, s_church_fixtures) \n    penalty_collision = -13 if (d_agent_church_fixtures < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a church corridor, the drone must maneuver around church curtain. All entity states (agent, church curtain) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_choir_curtain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lab_curtain = calculate_distance_min(s_agent, s_choir_curtain) \n    penalty_collision = -13 if (d_agent_lab_curtain < 0.6) else 13 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a church corridor. Requirement: Detect the three church parishioners and do not collide with it. Given: The states for the drone and many church parishioners are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 13 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a sacristy area during logeriments. \n Objectives: So not collide with processional stand. \n Conditions: The states of the agent, the processional stand are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_processional_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_processional_stand = calculate_distance_min(s_agent, s_processional_stand) \n    penalty_collision = -13 if (d_agent_processional_stand < 0.6) else 13 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Church. Objectives: Do not collide hit with baptismal font. Conditions: States of the agent, baptismal font are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_baptismal_font, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_baptismal_font = calculate_distance_min(s_agent, s_baptismal_font) \n    penalty_collision = -13 if (d_agent_baptismal_font < 0.6) else 4 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a church corridor, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_church_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_church_wall = calculate_distance_min(s_agent, s_church_wall) \n    penalty_collision = -13 if (d_agent_church_wall < 0.6) else 13 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: church floor. Avoid: Hitting with processional stand. Known: Agent, processional stand states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_processional_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_robotic_arm = calculate_distance_min(s_agent, s_processional_stand) \n    penalty_collision = -13 if (d_agent_robotic_arm < 0.6) else 4 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Church. Objectives: It must avoid colliding with parishioner. Conditions: The states of the drone and parishioner are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_parishioner, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_technician = calculate_distance_min(s_agent, s_parishioner) \n    penalty_collision = -19 if (d_agent_technician < 0.6) else 4 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Church. Objectives: Avoid collision with two traffic cone lines. Conditions: The states of the agent, wires, parishioner and warning signs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Church. Objectives: It must avoid colliding with any truck, overhead hanging lamp chains and entrance barrier. Conditions: Agent's, wall and pastor are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Church. Objectives: Do not collide with any bible. Conditions: The states of the drone and the bible are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bible, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bible = calculate_distance_min(s_agent, s_bible) \n    penalty = -19 if (d_agent_bible < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Church and must avoid contact with any sacristy  door. The states of the drone and the sacristy door are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_sacristy_door, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sacristy_door = calculate_distance_min(s_agent, s_sacristy_door) \n    penalty = -19 if (d_agent_sacristy_door < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Church. It must avoid colliding with the confessional booth, usher and hymn board. The states of the agent, hanging lamp chains, storage racks and safety barricades are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Church. It must avoid colliding with altar rails and hymn boards. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Church. It must avoid colliding with Church organ pipess. Agent's state, target's state and Church organ pipess's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_organ_pipes, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_pipe = calculate_distance_min(s_agent, s_organ_pipes) \n    penalty_collision = -19 if (d_agent_pipe < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Church. Goal: It must avoid colliding with hanging lamp chain. Assumptions: The states of the drone and the hanging lamp chain's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_lamp_chain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_lamp_chain = calculate_distance_min(s_agent, s_lamp_chain) \n    penalty_collision = -19 if (d_agent_lamp_chain < 0.6) else 13 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Church. Goal: It must avoid colliding with Church's pew, pulpit and parishioner. Assumptions: The Agent's state, target's state and pew, pulpit and parishioner' states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Church. Goal: Detect altars and benchs and avoid church door and do not collide. Assumptions: The states of the drone, target  altar, benches and avoid church door state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Church. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Art Museum. \n Objectives: It must avoid colliding with concrete columns or exhibit placards. \n Conditions: The states of the agent, the stanchion, and the exhibit placards are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 13 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Art Museum. \n Objectives: Detect sculpture pedestal and aviod it. \n Conditions: The states of the agent, the sculpture pedestal are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_sculpture_pedestal, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sculpture_pedestal = calculate_distance_min(s_agent, s_sculpture_pedestal) \n    penalty_collision = -13 if (d_agent_sculpture_pedestal < 0.6) else 13 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Art Museum open workspace. Objectives: It must avoid colliding with gallery fixtures. Conditions: States of the drone and the gallery fixturest are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_gallery_fixtures, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_gallery_fixtures = calculate_distance_min(s_agent, s_gallery_fixtures) \n    penalty_collision = -13 if (d_agent_gallery_fixtures < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a art museum corridor, the drone must maneuver around art museum curtain. All entity states (agent, art museum curtain) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_gallery_curtain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_gallery_curtain = calculate_distance_min(s_agent, s_gallery_curtain) \n    penalty_collision = -13 if (d_agent_gallery_curtain < 0.6) else 13 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a art museum corridor. Requirement: Detect the three art museum visitors and do not collide with it. Given: The states for the drone and many art museum visitors are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles_min < 0.6) else 13 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a staff service area during logeriments. \n Objectives: So not collide with display_stand. \n Conditions: The states of the agent, the display_stand are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_display_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_display_stand = calculate_distance_min(s_agent, s_display_stand) \n    penalty_collision = -13 if (d_agent_display_stand < 0.6) else 13 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Art Museum. Objectives: Do not collide hit with sculpture plinth. Conditions: States of the agent, sculpture plinth are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_sculpture_plinth, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sculpture_plinth = calculate_distance_min(s_agent, s_sculpture_plinth) \n    penalty_collision = -13 if (d_agent_sculpture_plinth < 0.6) else 4 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a art museum corridor, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_gallery_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_gallery_wall = calculate_distance_min(s_agent, s_gallery_wall) \n    penalty_collision = -13 if (d_agent_gallery_wall < 0.6) else 13 \n    reward_reaching_target = 20 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: art museum floor. Avoid: Hitting with display stand. Known: Agent, display stand states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_display_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_display_stand = calculate_distance_min(s_agent, s_display_stand) \n    penalty_collision = -13 if (d_agent_display_stand < 0.6) else 4 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Art Museum. Objectives: It must avoid colliding with visitor. Conditions: The states of the drone and visitor are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_visitor, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_visitor = calculate_distance_min(s_agent, s_visitor) \n    penalty_collision = -19 if (d_agent_visitor < 0.6) else 4 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Art Museum. Objectives: Avoid collision with two cone lines. Conditions: The states of the agent and two cone lines are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Art Museum. Objectives: It must avoid colliding with any picture, overhead wires and ticket barrier. Conditions: Agent's, picture's, wire's and ticket barrier's states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Art Museum. Objectives: Do not collide with any sculpture pedestal. Conditions: The states of the drone and the sculpture pedestal are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_sculpture_pedestal, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sculpture_pedestal = calculate_distance_min(s_agent, s_sculpture_pedestal) \n    penalty = -19 if (d_agent_sculpture_pedestal < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Art Museum and must avoid contact with any staff only door. The states of the drone and the staff only door are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_staff_door, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_door = calculate_distance_min(s_agent, s_staff_door) \n    penalty = -19 if (d_agent_door < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Art Museum. It must avoid colliding with the freestanding partition, docent and exhibit placard. The states of the agent, suspension wires, storage racks and safety barricades are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Art Museum. It must avoid colliding with stanchions and exhibit placards. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Art Museum. It must avoid colliding with Art Museum organ hanging installations. Agent's state, target's state and Art Museum organ hanging installations's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_installation, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_installation = calculate_distance_min(s_agent, s_installation) \n    penalty_collision = -19 if (d_agent_installation < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Art Museum. Goal: It must avoid colliding with suspension wire. Assumptions: The states of the drone and the suspension wire's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_suspension_wire, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_suspension_wire = calculate_distance_min(s_agent, s_suspension_wire) \n    penalty_collision = -19 if (d_agent_suspension_wire < 0.6) else 13 \n    reward_reaching_target = 19 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Art Museum. Goal: It must avoid colliding with Art Museum's pew, pulpit and visitor. Assumptions: The Agent's state, target's state and pew, pulpit and visitor' states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Art Museum. Goal: Detect waste containers and gas cylindes and avoid art museum door and do not collide. Assumptions: The states of the drone, target  freestanding partition, docent and exhibit placard state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (d_agent_multiple_obstacles < 0.6) else 13 \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Art Museum. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 13 if (d_agent_target < 0.3) else -math.log(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Kindergarten Playground. \n Objectives: It must avoid colliding with concrete columns or play signs. \n Conditions: The states of the agent, the playground fence, and the play signs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Kindergarten Playground. \n Objectives: Detect slide and aviod it. \n Conditions: The states of the agent, the slide are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_slide, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_slide = calculate_distance_min(s_agent, s_slide) \n    penalty_collision = -17 if (dist_slide < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Kindergarten Playground open workspace. Objectives: It must avoid colliding with playground equipment. Conditions: States of the drone and the playground equipment are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_playground_equipment, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_playground_equipment = calculate_distance_min(s_agent, s_playground_equipment) \n    penalty_collision = -17 if (dist_playground_equipment < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a playground path, the drone must maneuver around play tunnel curtain. All entity states (agent, play tunnel curtain) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_play_tunnel_curtain, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_play_tunnel_curtain = calculate_distance_min(s_agent, s_play_tunnel_curtain) \n    penalty_collision = -17 if (dist_play_tunnel_curtain < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a playground path. Requirement: Detect the three children and do not collide with it. Given: The states for the drone and many children are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a toy storage area during experiments. \n Objectives: So not collide with playhouse. \n Conditions: The states of the agent, the playhouse are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_playhouse, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_playhouse = calculate_distance_min(s_agent, s_playhouse) \n    penalty_collision = -17 if (dist_playhouse < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Kindergarten Playground. Objectives: Do not collide hit with sandbox. Conditions: States of the agent, sandbox are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_sandbox, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_sandbox = calculate_distance_min(s_agent, s_sandbox) \n    penalty_collision = -17 if (dist_sandbox < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a playground path, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: playground ground. Avoid: Hitting with climbing frame. Known: Agent, climbing frame states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_climbing_frame, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_climbing_frame = calculate_distance_min(s_agent, s_climbing_frame) \n    penalty_collision = -17 if (dist_climbing_frame < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Kindergarten Playground. Objectives: It must avoid colliding with child. Conditions: The states of the drone and child are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_child, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_child = calculate_distance_min(s_agent, s_child) \n    penalty_collision = -19 if (dist_child < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Kindergarten Playground. Objectives: Avoid collision with toy shelves, child and toy boxs. Conditions: The states of the agent, toy shelves, child and toy boxs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Kindergarten Playground. Objectives: It must avoid colliding with any tricycle, teacher and play sign. Conditions: Agent's, tricycle, teacher and play sign states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Kindergarten Playground. Objectives: Do not collide with any swing. Conditions: The states of the drone and the swing are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_swing, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_swing = calculate_distance_min(s_agent, s_swing) \n    penalty = -19 if (dist_swing < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Kindergarten Playground and must avoid contact with any storage shed door. The states of the drone and the storage shed door are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_storage_shed_door, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_storage_shed_door = calculate_distance_min(s_agent, s_storage_shed_door) \n    penalty = -19 if (dist_storage_shed_door < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Kindergarten Playground. It must avoid colliding with the tricycle, teacher and play sign. The states of the agent, swing chains, storage racks and safety barricades are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Kindergarten Playground. It must avoid colliding with safety cones and play signs. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Kindergarten Playground. It must avoid colliding with Kindergarten Playground hanging ropes. Agent's state, target's state and Kindergarten Playground hanging ropes's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_hanging_rope, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle = calculate_distance_min(s_agent, s_hanging_rope) \n    penalty_collision = -19 if (dist_obstacle < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Kindergarten Playground. Goal: It must avoid colliding with swing chain. Assumptions: The states of the drone and the swing chain's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_swing_chain, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_swing_chain = calculate_distance_min(s_agent, s_swing_chain) \n    penalty_collision = -19 if (dist_swing_chain < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Kindergarten Playground. Goal: It must avoid colliding with Kindergarten Playground's bench, toy box and child. Assumptions: The Agent's state, target's state and bench, toy box and child' states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Kindergarten Playground. Goal: Detect waste containers and gas cylindes and avoid storage shed door and do not collide. Assumptions: The states of the drone, target  waste containers, gas cylindes and avoid storage shed door state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Kindergarten Playground. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along University Campus. \n Objectives: It must avoid colliding with concrete columns or notice boards. \n Conditions: The states of the agent, the campus fence, and the notice boards are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy University Campus. \n Objectives: Detect bike rack and aviod it. \n Conditions: The states of the agent, the bike rack are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bike_rack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_bike_rack = calculate_distance_min(s_agent, s_bike_rack) \n    penalty_collision = -17 if (dist_bike_rack < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating University Campus open workspace. Objectives: It must avoid colliding with campus facilities. Conditions: States of the drone and the campus facilities are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_campus_facilities, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_campus_facilities = calculate_distance_min(s_agent, s_campus_facilities) \n    penalty_collision = -17 if (dist_campus_facilities < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a campus walkway, the drone must maneuver around construction tarp. All entity states (agent, construction tarp) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_construction_tarp, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_play_construction_tarp = calculate_distance_min(s_agent, s_construction_tarp) \n    penalty_collision = -17 if (dist_play_construction_tarp < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a campus walkway. Requirement: Detect the three students and do not collide with it. Given: The states for the drone and many students are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a library lobby during experiments. \n Objectives: So not collide with kiosk. \n Conditions: The states of the agent, the kiosk are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_kiosk, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_playhouse = calculate_distance_min(s_agent, s_kiosk) \n    penalty_collision = -17 if (dist_playhouse < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in University Campus. Objectives: Do not collide hit with fountain. Conditions: States of the agent, fountain are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_fountain, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_fountain = calculate_distance_min(s_agent, s_fountain) \n    penalty_collision = -17 if (dist_fountain < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a campus walkway, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: campus quad. Avoid: Hitting with statue. Known: Agent, statue states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_statue, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_statue = calculate_distance_min(s_agent, s_statue) \n    penalty_collision = -17 if (dist_statue < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating University Campus. Objectives: It must avoid colliding with student. Conditions: The states of the drone and student are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_student, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_student = calculate_distance_min(s_agent, s_student) \n    penalty_collision = -19 if (dist_student < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through University Campus. Objectives: Avoid collision with toy shelves, student and toy boxs. Conditions: The states of the agent, toy shelves, student and toy boxs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating University Campus. Objectives: It must avoid colliding with any bicycle, professor and play sign. Conditions: Agent's, bicycle, professor and play sign states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through University Campus. Objectives: Do not collide with any flag pole. Conditions: The states of the drone and the flag pole are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_flag_pole, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_flag_pole = calculate_distance_min(s_agent, s_flag_pole) \n    penalty = -19 if (dist_flag_pole < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along University Campus and must avoid contact with any maintenance room door. The states of the drone and the maintenance room door are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_maintenance_room_door, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_maintenance_room_door = calculate_distance_min(s_agent, s_maintenance_room_door) \n    penalty = -19 if (dist_maintenance_room_door < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in University Campus. It must avoid colliding with the bicycle, professor and play sign. The states of the agent, flag poles, storage racks and safety barricades are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in University Campus. It must avoid colliding with barricades and notice boards. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in University Campus. It must avoid colliding with University Campus hanging banners. Agent's state, target's state and University Campus hanging banners's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_banner, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_banner = calculate_distance_min(s_agent, s_banner) \n    penalty_collision = -19 if (dist_banner < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating University Campus. Goal: It must avoid colliding with professor. Assumptions: The states of the drone and the professor's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_professor, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_professor = calculate_distance_min(s_agent, s_professor) \n    penalty_collision = -19 if (dist_professor < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating University Campus. Goal: It must avoid colliding with University Campus's flag poles, newspaper stands and barricades. Assumptions: The Agent's state, target's state and flag poles, newspaper stands and barricades's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating University Campus. Goal: Detect bench, bike rack and student and do not collide. Assumptions: The states of the drone, target  bench, bike rack and student state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating University Campus. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Fire Station. \n Objectives: It must avoid colliding with concrete columns or safety signs. \n Conditions: The states of the agent, the station fence, and the safety signs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Fire Station. \n Objectives: Detect fire hydrant and aviod it. \n Conditions: The states of the agent, the fire hydrant are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_fire_hydrant, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_fire_hydrant = calculate_distance_min(s_agent, s_fire_hydrant) \n    penalty_collision = -17 if (dist_fire_hydrant < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Fire Station open workspace. Objectives: It must avoid colliding with fire equipment. Conditions: States of the drone and the fire equipment are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_fire_equipment, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_fire_equipment = calculate_distance_min(s_agent, s_fire_equipment) \n    penalty_collision = -17 if (dist_fire_equipment < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a garage bay, the drone must maneuver around fire hose line. All entity states (agent, fire hose line) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_fire_hose_line, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_fire_hose_line = calculate_distance_min(s_agent, s_fire_hose_line) \n    penalty_collision = -17 if (dist_fire_hose_line < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a garage bay. Requirement: Detect the three firefighters and do not collide with it. Given: The states for the drone and many firefighters are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a control room during experiments. \n Objectives: So not collide with locker. \n Conditions: The states of the agent, the locker are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_locker, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_locker = calculate_distance_min(s_agent, s_locker) \n    penalty_collision = -17 if (dist_locker < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Fire Station. Objectives: Do not collide hit with water tank. Conditions: States of the agent, water tank are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_water_tank, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_water_tank = calculate_distance_min(s_agent, s_water_tank) \n    penalty_collision = -17 if (dist_water_tank < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a garage bay, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: station yard. Avoid: Hitting with fire truck. Known: Agent, fire truck states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_fire_truck, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_fire_truck = calculate_distance_min(s_agent, s_fire_truck) \n    penalty_collision = -17 if (dist_fire_truck < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Fire Station. Objectives: It must avoid colliding with firefighter. Conditions: The states of the drone and firefighter are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_firefighter, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_firefighter = calculate_distance_min(s_agent, s_firefighter) \n    penalty_collision = -19 if (dist_firefighter < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Fire Station. Objectives: Avoid collision with toy shelves, firefighter and toy boxs. Conditions: The states of the agent, toy shelves, firefighter and toy boxs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Fire Station. Objectives: It must avoid colliding with any fire engine, chief and play sign. Conditions: Agent's, fire engine, chief and play sign states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Fire Station. Objectives: Do not collide with any antenna mast. Conditions: The states of the drone and the antenna mast are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_antenna_mast, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_antenna_mast = calculate_distance_min(s_agent, s_antenna_mast) \n    penalty = -19 if (dist_antenna_mast < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Fire Station and must avoid contact with any equipment room door. The states of the drone and the equipment room door are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_equipment_room_door, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_equipment_room_door = calculate_distance_min(s_agent, s_equipment_room_door) \n    penalty = -19 if (dist_equipment_room_door < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Fire Station. It must avoid colliding with the fire engine, chief and play sign. The states of the agent, antenna masts, storage racks and safety barriers are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Fire Station. It must avoid colliding with barriers and safety signs. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Fire Station. It must avoid colliding with Fire Station hanging ladders. Agent's state, target's state and Fire Station hanging ladders's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ladder, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_ladder = calculate_distance_min(s_agent, s_ladder) \n    penalty_collision = -19 if (dist_ladder < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Fire Station. Goal: It must avoid colliding with chief. Assumptions: The states of the drone and the chief's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_chief, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_chief = calculate_distance_min(s_agent, s_chief) \n    penalty_collision = -19 if (dist_chief < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Fire Station. Goal: It must avoid colliding with Fire Station's antenna masts, newspaper stands and barriers. Assumptions: The Agent's state, target's state and antenna masts, newspaper stands and barriers's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Fire Station. Goal: Detect bench, fire hydrant and firefighter and do not collide. Assumptions: The states of the drone, target  bench, fire hydrant and firefighter state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Fire Station. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Recycling Facility. \n Objectives: It must avoid colliding with concrete columns or hazard signs. \n Conditions: The states of the agent, the facility fence, and the hazard signs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Recycling Facility. \n Objectives: Detect waste bin and aviod it. \n Conditions: The states of the agent, the waste bin are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_waste_bin, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_waste_bin = calculate_distance_min(s_agent, s_waste_bin) \n    penalty_collision = -17 if (dist_waste_bin < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Recycling Facility open workspace. Objectives: It must avoid colliding with recycling machinery. Conditions: States of the drone and the recycling machinery are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_recycling_machinery, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_recycling_machinery = calculate_distance_min(s_agent, s_recycling_machinery) \n    penalty_collision = -17 if (dist_recycling_machinery < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a processing bay, the drone must maneuver around fire hose line. All entity states (agent, fire hose line) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_conveyor_belt, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_conveyor_belt = calculate_distance_min(s_agent, s_conveyor_belt) \n    penalty_collision = -17 if (dist_conveyor_belt < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a processing bay. Requirement: Detect the three workers and do not collide with it. Given: The states for the drone and many workers are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a control office during experiments. \n Objectives: So not collide with storage bin. \n Conditions: The states of the agent, the storage bin are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_storage_bin, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_storage_bin = calculate_distance_min(s_agent, s_storage_bin) \n    penalty_collision = -17 if (dist_storage_bin < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Recycling Facility. Objectives: Do not collide hit with liquid container. Conditions: States of the agent, liquid container are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_liquid_container, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_liquid_container = calculate_distance_min(s_agent, s_liquid_container) \n    penalty_collision = -17 if (dist_liquid_container < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a processing bay, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: sorting yard. Avoid: Hitting with forklift. Known: Agent, forklift states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_forklift, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_forklift = calculate_distance_min(s_agent, s_forklift) \n    penalty_collision = -17 if (dist_forklift < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Recycling Facility. Objectives: It must avoid colliding with worker. Conditions: The states of the drone and worker are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_worker, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_worker = calculate_distance_min(s_agent, s_worker) \n    penalty_collision = -19 if (dist_worker < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Recycling Facility. Objectives: Avoid collision with sorting shelves, worker and plastic crates. Conditions: The states of the agent, sorting shelves, worker and plastic crates are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Recycling Facility. Objectives: It must avoid colliding with any compactor truck, supervisor and play sign. Conditions: Agent's, compactor truck, supervisor and play sign states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Recycling Facility. Objectives: Do not collide with any crane. Conditions: The states of the drone and the crane are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_crane, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_crane = calculate_distance_min(s_agent, s_crane) \n    penalty = -19 if (dist_crane < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Recycling Facility and must avoid contact with any storage room door. The states of the drone and the storage room door are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_storage_room_door, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_storage_room_door = calculate_distance_min(s_agent, s_storage_room_door) \n    penalty = -19 if (dist_storage_room_door < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Recycling Facility. It must avoid colliding with the compactor truck, supervisor and play sign. The states of the agent, cranes, storage racks and safety safety safety barriers are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Recycling Facility. It must avoid colliding with safety safety barriers and hazard signs. The states of the drone and the water tuck and forklift are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Recycling Facility. It must avoid colliding with Recycling Facility conveyor belts. Agent's state, target's state and Recycling Facility conveyor belts's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_belt, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_belt = calculate_distance_min(s_agent, s_belt) \n    penalty_collision = -19 if (dist_belt < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Recycling Facility. Goal: It must avoid colliding with supervisor. Assumptions: The states of the drone and the supervisor's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_supervisor, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_supervisor = calculate_distance_min(s_agent, s_supervisor) \n    penalty_collision = -19 if (dist_supervisor < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Recycling Facility. Goal: It must avoid colliding with Recycling Facility's cranes, newspaper stands and safety safety barriers. Assumptions: The Agent's state, target's state and cranes, newspaper stands and safety safety barriers's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Recycling Facility. Goal: Detect bench, waste bin and worker and do not collide. Assumptions: The states of the drone, target  bench, waste bin and worker state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Recycling Facility. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Botanical Garden. \n Objectives: It must avoid colliding with concrete columns or garden signs. \n Conditions: The states of the agent, the garden fence, and the garden signs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Botanical Garden. \n Objectives: Detect garden pot and aviod it. \n Conditions: The states of the agent, the garden pot are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_garden_pot, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_garden_pot = calculate_distance_min(s_agent, s_garden_pot) \n    penalty_collision = -17 if (dist_garden_pot < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through a Botanical Garden. Objectives: It must avoid colliding with fountain. Conditions: States of the drone and the fountain are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_fountain, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_fountain = calculate_distance_min(s_agent, s_fountain) \n    penalty_collision = -17 if (dist_fountain < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a greenhouse, the drone must maneuver around fire hose line. All entity states (agent, fire hose line) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_vine_trellis, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_vine_trellis = calculate_distance_min(s_agent, s_vine_trellis) \n    penalty_collision = -17 if (dist_vine_trellis < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a greenhouse. Requirement: Detect the three statues and do not collide with it. Given: The states for the drone and many statues are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a gazebo during experiments. \n Objectives: So not collide with planter box. \n Conditions: The states of the agent, the planter box are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_planter_box, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_planter_box = calculate_distance_min(s_agent, s_planter_box) \n    penalty_collision = -17 if (dist_planter_box < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Botanical Garden. Objectives: Do not collide hit with pond. Conditions: States of the agent, pond are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_pond, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_pond = calculate_distance_min(s_agent, s_pond) \n    penalty_collision = -17 if (dist_pond < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a greenhouse, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: flower beds. Avoid: Hitting with lawn mower. Known: Agent, lawn mower states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_lawn_mower, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_lawn_mower = calculate_distance_min(s_agent, s_lawn_mower) \n    penalty_collision = -17 if (dist_lawn_mower < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Botanical Garden. Objectives: It must avoid colliding with statue. Conditions: The states of the drone and statue are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_statue, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_statue = calculate_distance_min(s_agent, s_statue) \n    penalty_collision = -19 if (dist_statue < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Botanical Garden. Objectives: Avoid collision with flower stands, statue and rock formations. Conditions: The states of the agent, flower stands, statue and rock formations are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Botanical Garden. Objectives: It must avoid colliding with any garden cart, statue and play sign. Conditions: Agent's, garden cart, statue and play sign states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Botanical Garden. Objectives: Do not collide with any tree. Conditions: The states of the drone and the tree are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_tree, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_tree = calculate_distance_min(s_agent, s_tree) \n    penalty = -19 if (dist_tree < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Botanical Garden and must avoid contact with any stone arch. The states of the drone and the stone arch are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_stone_arch, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_stone_arch = calculate_distance_min(s_agent, s_stone_arch) \n    penalty = -19 if (dist_stone_arch < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Botanical Garden. It must avoid colliding with the garden cart, statue and play sign. The states of the agent, trees, storage racks and safety safety hedges are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Botanical Garden. It must avoid colliding with safety hedges and garden signs. The states of the drone and the water tuck and lawn mower are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Botanical Garden. It must avoid colliding with Botanical Garden vine trelliss. Agent's state, target's state and Botanical Garden bush's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bush, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_bush = calculate_distance_min(s_agent, s_bush) \n    penalty_collision = -19 if (dist_bush < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Botanical Garden. Goal: It must avoid colliding with statue. Assumptions: The states of the drone and the statue's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_statue, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_supervisor = calculate_distance_min(s_agent, s_statue) \n    penalty_collision = -19 if (dist_supervisor < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Botanical Garden. Goal: It must avoid colliding with Botanical Garden's trees, newspaper stands and safety hedges. Assumptions: The Agent's state, target's state and trees, newspaper stands and safety hedges's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Botanical Garden. Goal: Detect bench, garden pot and statue and do not collide. Assumptions: The states of the drone, target  bench, garden pot and statue state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Botanical Garden. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Hotel. \n Objectives: It must avoid colliding with concrete columns or signboards. \n Conditions: The states of the agent, the hotel facade, and the signboards are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Hotel. \n Objectives: Detect luggage cart and aviod it. \n Conditions: The states of the agent, the luggage cart are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_luggage_cart, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_luggage_cart = calculate_distance_min(s_agent, s_luggage_cart) \n    penalty_collision = -17 if (dist_luggage_cart < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Hotel open workspace. Objectives: It must avoid colliding with chandelier. Conditions: States of the drone and the chandelier are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_chandelier, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_chandelier = calculate_distance_min(s_agent, s_chandelier) \n    penalty_collision = -17 if (dist_chandelier < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a reception hall, the drone must maneuver around fire hose line. All entity states (agent, fire hose line) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_decorative_partition, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_decorative_partition = calculate_distance_min(s_agent, s_decorative_partition) \n    penalty_collision = -17 if (dist_decorative_partition < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a reception hall. Requirement: Detect the three statues and do not collide with it. Given: The states for the drone and many statues are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a front desk during experiments. \n Objectives: So not collide with service counter. \n Conditions: The states of the agent, the service counter are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_service_counter, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_service_counter = calculate_distance_min(s_agent, s_service_counter) \n    penalty_collision = -17 if (dist_service_counter < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Hotel. Objectives: Do not collide hit with indoor fountain. Conditions: States of the agent, indoor fountain are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_indoor_fountain, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_indoor_fountain = calculate_distance_min(s_agent, s_indoor_fountain) \n    penalty_collision = -17 if (dist_indoor_fountain < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a reception hall, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: lobby seating. Avoid: Hitting with elevator. Known: Agent, elevator states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_elevator, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_elevator = calculate_distance_min(s_agent, s_elevator) \n    penalty_collision = -17 if (dist_elevator < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Hotel. Objectives: It must avoid colliding with statue. Conditions: The states of the drone and statue are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_statue, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_statue = calculate_distance_min(s_agent, s_statue) \n    penalty_collision = -19 if (dist_statue < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Hotel. Objectives: Avoid collision with brochure stands, statue and decor columns. Conditions: The states of the agent, brochure stands, statue and decor columns are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Hotel. Objectives: It must avoid colliding with any room service trolley, statue and play sign. Conditions: Agent's, room service trolley, statue and play sign states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Hotel. Objectives: Do not collide with any pillar. Conditions: The states of the drone and the pillar are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_pillar, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_pillar = calculate_distance_min(s_agent, s_pillar) \n    penalty = -19 if (dist_pillar < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Hotel and must avoid contact with any archway. The states of the drone and the archway are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_archway, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_archway = calculate_distance_min(s_agent, s_archway) \n    penalty = -19 if (dist_archway < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Hotel. It must avoid colliding with the room service trolley, statue and play sign. The states of the agent, pillars, storage racks and safety safety railings are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Hotel. It must avoid colliding with safety railings and signboards. The states of the drone and the water tuck and elevator are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Hotel. It must avoid colliding with Hotel decorative partitions. Agent's state, target's state and Hotel decorative partitions's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_deco, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_deco = calculate_distance_min(s_agent, s_deco) \n    penalty_collision = -19 if (dist_deco < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Hotel. Goal: It must avoid colliding with entrance. Assumptions: The states of the drone and the entrance's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_entrance, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_entrance = calculate_distance_min(s_agent, s_entrance) \n    penalty_collision = -19 if (dist_entrance < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Hotel. Goal: It must avoid colliding with Hotel's pillars, newspaper stands and safety railings. Assumptions: The Agent's state, target's state and pillars, newspaper stands and safety railings's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Hotel. Goal: Detect bench, luggage cart and statue and do not collide. Assumptions: The states of the drone, target  bench, luggage cart and statue state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Hotel. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Kindergarten Playground. \n Objectives: It must avoid colliding with concrete columns or play signs. \n Conditions: The states of the agent, the playground fence, and the play signs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Kindergarten Playground. \n Objectives: Detect slide and aviod it. \n Conditions: The states of the agent, the slide are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_slide, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_slide = calculate_distance_min(s_agent, s_slide) \n    penalty_collision = -17 if (dist_slide < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Kindergarten Playground open workspace. Objectives: It must avoid colliding with playground equipment. Conditions: States of the drone and the playground equipment are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_playground_equipment, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_playground_equipment = calculate_distance_min(s_agent, s_playground_equipment) \n    penalty_collision = -17 if (dist_playground_equipment < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a playground path, the drone must maneuver around play tunnel curtain. All entity states (agent, play tunnel curtain) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_play_tunnel_curtain, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_play_tunnel_curtain = calculate_distance_min(s_agent, s_play_tunnel_curtain) \n    penalty_collision = -17 if (dist_play_tunnel_curtain < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a playground path. Requirement: Detect the three children and do not collide with it. Given: The states for the drone and many children are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a toy storage area during experiments. \n Objectives: So not collide with playhouse. \n Conditions: The states of the agent, the playhouse are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_playhouse, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_playhouse = calculate_distance_min(s_agent, s_playhouse) \n    penalty_collision = -17 if (dist_playhouse < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Kindergarten Playground. Objectives: Do not collide hit with sandbox. Conditions: States of the agent, sandbox are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_sandbox, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_sandbox = calculate_distance_min(s_agent, s_sandbox) \n    penalty_collision = -17 if (dist_sandbox < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a playground path, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: playground ground. Avoid: Hitting with climbing frame. Known: Agent, climbing frame states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_climbing_frame, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_climbing_frame = calculate_distance_min(s_agent, s_climbing_frame) \n    penalty_collision = -17 if (dist_climbing_frame < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Kindergarten Playground. Objectives: It must avoid colliding with child. Conditions: The states of the drone and child are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_child, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_child = calculate_distance_min(s_agent, s_child) \n    penalty_collision = -19 if (dist_child < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Kindergarten Playground. Objectives: Avoid collision with toy shelves, child and toy boxs. Conditions: The states of the agent, toy shelves, child and toy boxs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Kindergarten Playground. Objectives: It must avoid colliding with any tricycle, teacher and play sign. Conditions: Agent's, tricycle, teacher and play sign states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Kindergarten Playground. Objectives: Do not collide with any swing. Conditions: The states of the drone and the swing are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_swing, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_swing = calculate_distance_min(s_agent, s_swing) \n    penalty = -19 if (dist_swing < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Kindergarten Playground and must avoid contact with any storage shed door. The states of the drone and the storage shed door are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_storage_shed_door, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_storage_shed_door = calculate_distance_min(s_agent, s_storage_shed_door) \n    penalty = -19 if (dist_storage_shed_door < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Kindergarten Playground. It must avoid colliding with the tricycle, teacher and play sign. The states of the agent, swing chains, storage racks and safety barricades are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Kindergarten Playground. It must avoid colliding with safety cones and play signs. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Kindergarten Playground. It must avoid colliding with Kindergarten Playground hanging ropes. Agent's state, target's state and Kindergarten Playground hanging ropes's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_hanging_rope, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle = calculate_distance_min(s_agent, s_hanging_rope) \n    penalty_collision = -19 if (dist_obstacle < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Kindergarten Playground. Goal: It must avoid colliding with swing chain. Assumptions: The states of the drone and the swing chain's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_swing_chain, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_swing_chain = calculate_distance_min(s_agent, s_swing_chain) \n    penalty_collision = -19 if (dist_swing_chain < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Kindergarten Playground. Goal: It must avoid colliding with Kindergarten Playground's bench, toy box and child. Assumptions: The Agent's state, target's state and bench, toy box and child' states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Kindergarten Playground. Goal: Detect waste containers and gas cylindes and avoid storage shed door and do not collide. Assumptions: The states of the drone, target  waste containers, gas cylindes and avoid storage shed door state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Kindergarten Playground. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along University Campus. \n Objectives: It must avoid colliding with concrete columns or notice boards. \n Conditions: The states of the agent, the campus fence, and the notice boards are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy University Campus. \n Objectives: Detect bike rack and aviod it. \n Conditions: The states of the agent, the bike rack are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bike_rack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_bike_rack = calculate_distance_min(s_agent, s_bike_rack) \n    penalty_collision = -17 if (dist_bike_rack < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating University Campus open workspace. Objectives: It must avoid colliding with campus facilities. Conditions: States of the drone and the campus facilities are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_campus_facilities, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_campus_facilities = calculate_distance_min(s_agent, s_campus_facilities) \n    penalty_collision = -17 if (dist_campus_facilities < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a campus walkway, the drone must maneuver around construction tarp. All entity states (agent, construction tarp) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_construction_tarp, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_play_construction_tarp = calculate_distance_min(s_agent, s_construction_tarp) \n    penalty_collision = -17 if (dist_play_construction_tarp < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a campus walkway. Requirement: Detect the three students and do not collide with it. Given: The states for the drone and many students are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a library lobby during experiments. \n Objectives: So not collide with kiosk. \n Conditions: The states of the agent, the kiosk are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_kiosk, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_playhouse = calculate_distance_min(s_agent, s_kiosk) \n    penalty_collision = -17 if (dist_playhouse < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in University Campus. Objectives: Do not collide hit with fountain. Conditions: States of the agent, fountain are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_fountain, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_fountain = calculate_distance_min(s_agent, s_fountain) \n    penalty_collision = -17 if (dist_fountain < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a campus walkway, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: campus quad. Avoid: Hitting with statue. Known: Agent, statue states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_statue, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_statue = calculate_distance_min(s_agent, s_statue) \n    penalty_collision = -17 if (dist_statue < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating University Campus. Objectives: It must avoid colliding with student. Conditions: The states of the drone and student are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_student, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_student = calculate_distance_min(s_agent, s_student) \n    penalty_collision = -19 if (dist_student < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through University Campus. Objectives: Avoid collision with toy shelves, student and toy boxs. Conditions: The states of the agent, toy shelves, student and toy boxs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating University Campus. Objectives: It must avoid colliding with any bicycle, professor and play sign. Conditions: Agent's, bicycle, professor and play sign states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through University Campus. Objectives: Do not collide with any flag pole. Conditions: The states of the drone and the flag pole are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_flag_pole, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_flag_pole = calculate_distance_min(s_agent, s_flag_pole) \n    penalty = -19 if (dist_flag_pole < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along University Campus and must avoid contact with any maintenance room door. The states of the drone and the maintenance room door are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_maintenance_room_door, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_maintenance_room_door = calculate_distance_min(s_agent, s_maintenance_room_door) \n    penalty = -19 if (dist_maintenance_room_door < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in University Campus. It must avoid colliding with the bicycle, professor and play sign. The states of the agent, flag poles, storage racks and safety barricades are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in University Campus. It must avoid colliding with barricades and notice boards. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in University Campus. It must avoid colliding with University Campus hanging banners. Agent's state, target's state and University Campus hanging banners's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_banner, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_banner = calculate_distance_min(s_agent, s_banner) \n    penalty_collision = -19 if (dist_banner < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating University Campus. Goal: It must avoid colliding with professor. Assumptions: The states of the drone and the professor's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_professor, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_professor = calculate_distance_min(s_agent, s_professor) \n    penalty_collision = -19 if (dist_professor < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating University Campus. Goal: It must avoid colliding with University Campus's flag poles, newspaper stands and barricades. Assumptions: The Agent's state, target's state and flag poles, newspaper stands and barricades's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating University Campus. Goal: Detect bench, bike rack and student and do not collide. Assumptions: The states of the drone, target  bench, bike rack and student state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating University Campus. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Fire Station. \n Objectives: It must avoid colliding with concrete columns or safety signs. \n Conditions: The states of the agent, the station fence, and the safety signs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Fire Station. \n Objectives: Detect fire hydrant and aviod it. \n Conditions: The states of the agent, the fire hydrant are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_fire_hydrant, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_fire_hydrant = calculate_distance_min(s_agent, s_fire_hydrant) \n    penalty_collision = -17 if (dist_fire_hydrant < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Fire Station open workspace. Objectives: It must avoid colliding with fire equipment. Conditions: States of the drone and the fire equipment are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_fire_equipment, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_fire_equipment = calculate_distance_min(s_agent, s_fire_equipment) \n    penalty_collision = -17 if (dist_fire_equipment < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a garage bay, the drone must maneuver around fire hose line. All entity states (agent, fire hose line) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_fire_hose_line, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_fire_hose_line = calculate_distance_min(s_agent, s_fire_hose_line) \n    penalty_collision = -17 if (dist_fire_hose_line < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a garage bay. Requirement: Detect the three firefighters and do not collide with it. Given: The states for the drone and many firefighters are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a control room during experiments. \n Objectives: So not collide with locker. \n Conditions: The states of the agent, the locker are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_locker, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_locker = calculate_distance_min(s_agent, s_locker) \n    penalty_collision = -17 if (dist_locker < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Fire Station. Objectives: Do not collide hit with water tank. Conditions: States of the agent, water tank are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_water_tank, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_water_tank = calculate_distance_min(s_agent, s_water_tank) \n    penalty_collision = -17 if (dist_water_tank < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a garage bay, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: station yard. Avoid: Hitting with fire truck. Known: Agent, fire truck states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_fire_truck, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_fire_truck = calculate_distance_min(s_agent, s_fire_truck) \n    penalty_collision = -17 if (dist_fire_truck < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Fire Station. Objectives: It must avoid colliding with firefighter. Conditions: The states of the drone and firefighter are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_firefighter, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_firefighter = calculate_distance_min(s_agent, s_firefighter) \n    penalty_collision = -19 if (dist_firefighter < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Fire Station. Objectives: Avoid collision with toy shelves, firefighter and toy boxs. Conditions: The states of the agent, toy shelves, firefighter and toy boxs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Fire Station. Objectives: It must avoid colliding with any fire engine, chief and play sign. Conditions: Agent's, fire engine, chief and play sign states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Fire Station. Objectives: Do not collide with any antenna mast. Conditions: The states of the drone and the antenna mast are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_antenna_mast, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_antenna_mast = calculate_distance_min(s_agent, s_antenna_mast) \n    penalty = -19 if (dist_antenna_mast < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Fire Station and must avoid contact with any equipment room door. The states of the drone and the equipment room door are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_equipment_room_door, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_equipment_room_door = calculate_distance_min(s_agent, s_equipment_room_door) \n    penalty = -19 if (dist_equipment_room_door < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Fire Station. It must avoid colliding with the fire engine, chief and play sign. The states of the agent, antenna masts, storage racks and safety barriers are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Fire Station. It must avoid colliding with barriers and safety signs. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Fire Station. It must avoid colliding with Fire Station hanging ladders. Agent's state, target's state and Fire Station hanging ladders's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ladder, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_ladder = calculate_distance_min(s_agent, s_ladder) \n    penalty_collision = -19 if (dist_ladder < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Fire Station. Goal: It must avoid colliding with chief. Assumptions: The states of the drone and the chief's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_chief, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_chief = calculate_distance_min(s_agent, s_chief) \n    penalty_collision = -19 if (dist_chief < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Fire Station. Goal: It must avoid colliding with Fire Station's antenna masts, newspaper stands and barriers. Assumptions: The Agent's state, target's state and antenna masts, newspaper stands and barriers's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Fire Station. Goal: Detect bench, fire hydrant and firefighter and do not collide. Assumptions: The states of the drone, target  bench, fire hydrant and firefighter state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Fire Station. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Recycling Facility. \n Objectives: It must avoid colliding with concrete columns or hazard signs. \n Conditions: The states of the agent, the facility fence, and the hazard signs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Recycling Facility. \n Objectives: Detect waste bin and aviod it. \n Conditions: The states of the agent, the waste bin are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_waste_bin, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_waste_bin = calculate_distance_min(s_agent, s_waste_bin) \n    penalty_collision = -17 if (dist_waste_bin < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Recycling Facility open workspace. Objectives: It must avoid colliding with recycling machinery. Conditions: States of the drone and the recycling machinery are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_recycling_machinery, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_recycling_machinery = calculate_distance_min(s_agent, s_recycling_machinery) \n    penalty_collision = -17 if (dist_recycling_machinery < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a processing bay, the drone must maneuver around fire hose line. All entity states (agent, fire hose line) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_conveyor_belt, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_conveyor_belt = calculate_distance_min(s_agent, s_conveyor_belt) \n    penalty_collision = -17 if (dist_conveyor_belt < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a processing bay. Requirement: Detect the three workers and do not collide with it. Given: The states for the drone and many workers are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a control office during experiments. \n Objectives: So not collide with storage bin. \n Conditions: The states of the agent, the storage bin are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_storage_bin, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_storage_bin = calculate_distance_min(s_agent, s_storage_bin) \n    penalty_collision = -17 if (dist_storage_bin < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Recycling Facility. Objectives: Do not collide hit with liquid container. Conditions: States of the agent, liquid container are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_liquid_container, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_liquid_container = calculate_distance_min(s_agent, s_liquid_container) \n    penalty_collision = -17 if (dist_liquid_container < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a processing bay, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: sorting yard. Avoid: Hitting with forklift. Known: Agent, forklift states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_forklift, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_forklift = calculate_distance_min(s_agent, s_forklift) \n    penalty_collision = -17 if (dist_forklift < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Recycling Facility. Objectives: It must avoid colliding with worker. Conditions: The states of the drone and worker are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_worker, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_worker = calculate_distance_min(s_agent, s_worker) \n    penalty_collision = -19 if (dist_worker < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Recycling Facility. Objectives: Avoid collision with sorting shelves, worker and plastic crates. Conditions: The states of the agent, sorting shelves, worker and plastic crates are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Recycling Facility. Objectives: It must avoid colliding with any compactor truck, supervisor and play sign. Conditions: Agent's, compactor truck, supervisor and play sign states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Recycling Facility. Objectives: Do not collide with any crane. Conditions: The states of the drone and the crane are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_crane, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_crane = calculate_distance_min(s_agent, s_crane) \n    penalty = -19 if (dist_crane < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Recycling Facility and must avoid contact with any storage room door. The states of the drone and the storage room door are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_storage_room_door, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_storage_room_door = calculate_distance_min(s_agent, s_storage_room_door) \n    penalty = -19 if (dist_storage_room_door < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Recycling Facility. It must avoid colliding with the compactor truck, supervisor and play sign. The states of the agent, cranes, storage racks and safety safety safety barriers are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Recycling Facility. It must avoid colliding with safety safety barriers and hazard signs. The states of the drone and the water tuck and forklift are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Recycling Facility. It must avoid colliding with Recycling Facility conveyor belts. Agent's state, target's state and Recycling Facility conveyor belts's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_belt, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_belt = calculate_distance_min(s_agent, s_belt) \n    penalty_collision = -19 if (dist_belt < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Recycling Facility. Goal: It must avoid colliding with supervisor. Assumptions: The states of the drone and the supervisor's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_supervisor, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_supervisor = calculate_distance_min(s_agent, s_supervisor) \n    penalty_collision = -19 if (dist_supervisor < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Recycling Facility. Goal: It must avoid colliding with Recycling Facility's cranes, newspaper stands and safety safety barriers. Assumptions: The Agent's state, target's state and cranes, newspaper stands and safety safety barriers's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Recycling Facility. Goal: Detect bench, waste bin and worker and do not collide. Assumptions: The states of the drone, target  bench, waste bin and worker state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Recycling Facility. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Botanical Garden. \n Objectives: It must avoid colliding with concrete columns or garden signs. \n Conditions: The states of the agent, the garden fence, and the garden signs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Botanical Garden. \n Objectives: Detect garden pot and aviod it. \n Conditions: The states of the agent, the garden pot are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_garden_pot, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_garden_pot = calculate_distance_min(s_agent, s_garden_pot) \n    penalty_collision = -17 if (dist_garden_pot < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through a Botanical Garden. Objectives: It must avoid colliding with fountain. Conditions: States of the drone and the fountain are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_fountain, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_fountain = calculate_distance_min(s_agent, s_fountain) \n    penalty_collision = -17 if (dist_fountain < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a greenhouse, the drone must maneuver around fire hose line. All entity states (agent, fire hose line) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_vine_trellis, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_vine_trellis = calculate_distance_min(s_agent, s_vine_trellis) \n    penalty_collision = -17 if (dist_vine_trellis < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a greenhouse. Requirement: Detect the three statues and do not collide with it. Given: The states for the drone and many statues are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a gazebo during experiments. \n Objectives: So not collide with planter box. \n Conditions: The states of the agent, the planter box are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_planter_box, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_planter_box = calculate_distance_min(s_agent, s_planter_box) \n    penalty_collision = -17 if (dist_planter_box < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Botanical Garden. Objectives: Do not collide hit with pond. Conditions: States of the agent, pond are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_pond, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_pond = calculate_distance_min(s_agent, s_pond) \n    penalty_collision = -17 if (dist_pond < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a greenhouse, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: flower beds. Avoid: Hitting with lawn mower. Known: Agent, lawn mower states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_lawn_mower, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_lawn_mower = calculate_distance_min(s_agent, s_lawn_mower) \n    penalty_collision = -17 if (dist_lawn_mower < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Botanical Garden. Objectives: It must avoid colliding with statue. Conditions: The states of the drone and statue are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_statue, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_statue = calculate_distance_min(s_agent, s_statue) \n    penalty_collision = -19 if (dist_statue < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Botanical Garden. Objectives: Avoid collision with flower stands, statue and rock formations. Conditions: The states of the agent, flower stands, statue and rock formations are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Botanical Garden. Objectives: It must avoid colliding with any garden cart, statue and play sign. Conditions: Agent's, garden cart, statue and play sign states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Botanical Garden. Objectives: Do not collide with any tree. Conditions: The states of the drone and the tree are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_tree, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_tree = calculate_distance_min(s_agent, s_tree) \n    penalty = -19 if (dist_tree < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Botanical Garden and must avoid contact with any stone arch. The states of the drone and the stone arch are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_stone_arch, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_stone_arch = calculate_distance_min(s_agent, s_stone_arch) \n    penalty = -19 if (dist_stone_arch < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Botanical Garden. It must avoid colliding with the garden cart, statue and play sign. The states of the agent, trees, storage racks and safety safety hedges are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Botanical Garden. It must avoid colliding with safety hedges and garden signs. The states of the drone and the water tuck and lawn mower are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Botanical Garden. It must avoid colliding with Botanical Garden vine trelliss. Agent's state, target's state and Botanical Garden bush's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bush, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_bush = calculate_distance_min(s_agent, s_bush) \n    penalty_collision = -19 if (dist_bush < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Botanical Garden. Goal: It must avoid colliding with statue. Assumptions: The states of the drone and the statue's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_statue, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_supervisor = calculate_distance_min(s_agent, s_statue) \n    penalty_collision = -19 if (dist_supervisor < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Botanical Garden. Goal: It must avoid colliding with Botanical Garden's trees, newspaper stands and safety hedges. Assumptions: The Agent's state, target's state and trees, newspaper stands and safety hedges's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Botanical Garden. Goal: Detect bench, garden pot and statue and do not collide. Assumptions: The states of the drone, target  bench, garden pot and statue state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Botanical Garden. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Hotel. \n Objectives: It must avoid colliding with concrete columns or signboards. \n Conditions: The states of the agent, the hotel facade, and the signboards are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Hotel. \n Objectives: Detect luggage cart and aviod it. \n Conditions: The states of the agent, the luggage cart are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_luggage_cart, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_luggage_cart = calculate_distance_min(s_agent, s_luggage_cart) \n    penalty_collision = -17 if (dist_luggage_cart < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Hotel open workspace. Objectives: It must avoid colliding with chandelier. Conditions: States of the drone and the chandelier are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_chandelier, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_chandelier = calculate_distance_min(s_agent, s_chandelier) \n    penalty_collision = -17 if (dist_chandelier < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a reception hall, the drone must maneuver around fire hose line. All entity states (agent, fire hose line) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_decorative_partition, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_decorative_partition = calculate_distance_min(s_agent, s_decorative_partition) \n    penalty_collision = -17 if (dist_decorative_partition < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a reception hall. Requirement: Detect the three statues and do not collide with it. Given: The states for the drone and many statues are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a front desk during experiments. \n Objectives: So not collide with service counter. \n Conditions: The states of the agent, the service counter are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_service_counter, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_service_counter = calculate_distance_min(s_agent, s_service_counter) \n    penalty_collision = -17 if (dist_service_counter < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Hotel. Objectives: Do not collide hit with indoor fountain. Conditions: States of the agent, indoor fountain are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_indoor_fountain, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_indoor_fountain = calculate_distance_min(s_agent, s_indoor_fountain) \n    penalty_collision = -17 if (dist_indoor_fountain < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a reception hall, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: lobby seating. Avoid: Hitting with elevator. Known: Agent, elevator states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_elevator, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_elevator = calculate_distance_min(s_agent, s_elevator) \n    penalty_collision = -17 if (dist_elevator < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Hotel. Objectives: It must avoid colliding with statue. Conditions: The states of the drone and statue are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_statue, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_statue = calculate_distance_min(s_agent, s_statue) \n    penalty_collision = -19 if (dist_statue < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Hotel. Objectives: Avoid collision with brochure stands, statue and decor columns. Conditions: The states of the agent, brochure stands, statue and decor columns are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Hotel. Objectives: It must avoid colliding with any room service trolley, statue and play sign. Conditions: Agent's, room service trolley, statue and play sign states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Hotel. Objectives: Do not collide with any pillar. Conditions: The states of the drone and the pillar are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_pillar, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_pillar = calculate_distance_min(s_agent, s_pillar) \n    penalty = -19 if (dist_pillar < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Hotel and must avoid contact with any archway. The states of the drone and the archway are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_archway, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_archway = calculate_distance_min(s_agent, s_archway) \n    penalty = -19 if (dist_archway < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Hotel. It must avoid colliding with the room service trolley, statue and play sign. The states of the agent, pillars, storage racks and safety safety railings are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Hotel. It must avoid colliding with safety railings and signboards. The states of the drone and the water tuck and elevator are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Hotel. It must avoid colliding with Hotel decorative partitions. Agent's state, target's state and Hotel decorative partitions's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_deco, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_deco = calculate_distance_min(s_agent, s_deco) \n    penalty_collision = -19 if (dist_deco < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Hotel. Goal: It must avoid colliding with entrance. Assumptions: The states of the drone and the entrance's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_entrance, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_entrance = calculate_distance_min(s_agent, s_entrance) \n    penalty_collision = -19 if (dist_entrance < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Hotel. Goal: It must avoid colliding with Hotel's pillars, newspaper stands and safety railings. Assumptions: The Agent's state, target's state and pillars, newspaper stands and safety railings's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Hotel. Goal: Detect bench, luggage cart and statue and do not collide. Assumptions: The states of the drone, target  bench, luggage cart and statue state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Hotel. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target*3 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Kindergarten Playground. \n Objectives: It must avoid colliding with concrete columns or play signs. \n Conditions: The states of the agent, the playground fence, and the play signs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_obstacle_min < 0.4) else 20 \n    reward_reaching_target = 11 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Kindergarten Playground. \n Objectives: Detect slide and aviod it. \n Conditions: The states of the agent, the slide are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_slide, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_slide = calculate_distance_min(s_agent, s_slide) \n    penalty_collision = -20 if (dist_slide < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Kindergarten Playground open workspace. Objectives: It must avoid colliding with playground equipment. Conditions: States of the drone and the playground equipment are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_playground_equipment, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_playground_equipment = calculate_distance_min(s_agent, s_playground_equipment) \n    penalty_collision = -20 if (dist_playground_equipment < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a playground path, the drone must maneuver around play tunnel curtain. All entity states (agent, play tunnel curtain) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_play_tunnel_curtain, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_play_tunnel_curtain = calculate_distance_min(s_agent, s_play_tunnel_curtain) \n    penalty_collision = -20 if (dist_play_tunnel_curtain < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a playground path. Requirement: Detect the three children and do not collide with it. Given: The states for the drone and many children are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_obstacle_min < 0.4) else 20 \n    reward_reaching_target = 11 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a toy storage area during experiments. \n Objectives: So not collide with playhouse. \n Conditions: The states of the agent, the playhouse are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_playhouse, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_playhouse = calculate_distance_min(s_agent, s_playhouse) \n    penalty_collision = -20 if (dist_playhouse < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Kindergarten Playground. Objectives: Do not collide hit with sandbox. Conditions: States of the agent, sandbox are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_sandbox, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_sandbox = calculate_distance_min(s_agent, s_sandbox) \n    penalty_collision = -20 if (dist_sandbox < 0.4) else 4 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a playground path, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -20 if (dist_wall < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: playground ground. Avoid: Hitting with climbing frame. Known: Agent, climbing frame states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_climbing_frame, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_climbing_frame = calculate_distance_min(s_agent, s_climbing_frame) \n    penalty_collision = -20 if (dist_climbing_frame < 0.4) else 4 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Kindergarten Playground. Objectives: It must avoid colliding with child. Conditions: The states of the drone and child are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_child, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_child = calculate_distance_min(s_agent, s_child) \n    penalty_collision = -11 if (dist_child < 0.4) else 4 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Kindergarten Playground. Objectives: Avoid collision with toy shelves, child and toy boxs. Conditions: The states of the agent, toy shelves, child and toy boxs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_obstacle < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Kindergarten Playground. Objectives: It must avoid colliding with any tricycle, teacher and play sign. Conditions: Agent's, tricycle, teacher and play sign states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_obstacle < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Kindergarten Playground. Objectives: Do not collide with any swing. Conditions: The states of the drone and the swing are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_swing, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_swing = calculate_distance_min(s_agent, s_swing) \n    penalty = -11 if (dist_swing < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Kindergarten Playground and must avoid contact with any storage shed door. The states of the drone and the storage shed door are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_storage_shed_door, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_storage_shed_door = calculate_distance_min(s_agent, s_storage_shed_door) \n    penalty = -11 if (dist_storage_shed_door < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Kindergarten Playground. It must avoid colliding with the tricycle, teacher and play sign. The states of the agent, swing chains, storage racks and safety barricades are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_obstacle < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Kindergarten Playground. It must avoid colliding with safety cones and play signs. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_obstacle < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Kindergarten Playground. It must avoid colliding with Kindergarten Playground hanging ropes. Agent's state, target's state and Kindergarten Playground hanging ropes's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_hanging_rope, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle = calculate_distance_min(s_agent, s_hanging_rope) \n    penalty_collision = -11 if (dist_obstacle < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Kindergarten Playground. Goal: It must avoid colliding with swing chain. Assumptions: The states of the drone and the swing chain's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_swing_chain, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_swing_chain = calculate_distance_min(s_agent, s_swing_chain) \n    penalty_collision = -11 if (dist_swing_chain < 0.4) else 20 \n    reward_reaching_target = 11 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Kindergarten Playground. Goal: It must avoid colliding with Kindergarten Playground's bench, toy box and child. Assumptions: The Agent's state, target's state and bench, toy box and child' states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_obstacle < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Kindergarten Playground. Goal: Detect waste containers and gas cylindes and avoid storage shed door and do not collide. Assumptions: The states of the drone, target  waste containers, gas cylindes and avoid storage shed door state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_obstacle < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Kindergarten Playground. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along University Campus. \n Objectives: It must avoid colliding with concrete columns or notice boards. \n Conditions: The states of the agent, the campus fence, and the notice boards are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_obstacle_min < 0.4) else 20 \n    reward_reaching_target = 11 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy University Campus. \n Objectives: Detect bike rack and aviod it. \n Conditions: The states of the agent, the bike rack are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bike_rack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_bike_rack = calculate_distance_min(s_agent, s_bike_rack) \n    penalty_collision = -20 if (dist_bike_rack < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating University Campus open workspace. Objectives: It must avoid colliding with campus facilities. Conditions: States of the drone and the campus facilities are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_campus_facilities, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_campus_facilities = calculate_distance_min(s_agent, s_campus_facilities) \n    penalty_collision = -20 if (dist_campus_facilities < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a campus walkway, the drone must maneuver around construction tarp. All entity states (agent, construction tarp) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_construction_tarp, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_play_construction_tarp = calculate_distance_min(s_agent, s_construction_tarp) \n    penalty_collision = -20 if (dist_play_construction_tarp < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a campus walkway. Requirement: Detect the three students and do not collide with it. Given: The states for the drone and many students are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_obstacle_min < 0.4) else 20 \n    reward_reaching_target = 11 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a library lobby during experiments. \n Objectives: So not collide with kiosk. \n Conditions: The states of the agent, the kiosk are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_kiosk, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_playhouse = calculate_distance_min(s_agent, s_kiosk) \n    penalty_collision = -20 if (dist_playhouse < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in University Campus. Objectives: Do not collide hit with fountain. Conditions: States of the agent, fountain are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_fountain, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_fountain = calculate_distance_min(s_agent, s_fountain) \n    penalty_collision = -20 if (dist_fountain < 0.4) else 4 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a campus walkway, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -20 if (dist_wall < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: campus quad. Avoid: Hitting with statue. Known: Agent, statue states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_statue, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_statue = calculate_distance_min(s_agent, s_statue) \n    penalty_collision = -20 if (dist_statue < 0.4) else 4 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating University Campus. Objectives: It must avoid colliding with student. Conditions: The states of the drone and student are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_student, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_student = calculate_distance_min(s_agent, s_student) \n    penalty_collision = -11 if (dist_student < 0.4) else 4 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through University Campus. Objectives: Avoid collision with toy shelves, student and toy boxs. Conditions: The states of the agent, toy shelves, student and toy boxs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_multiple_obstacles_min < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating University Campus. Objectives: It must avoid colliding with any bicycle, professor and play sign. Conditions: Agent's, bicycle, professor and play sign states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_multiple_obstacles_min < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through University Campus. Objectives: Do not collide with any flag pole. Conditions: The states of the drone and the flag pole are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_flag_pole, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_flag_pole = calculate_distance_min(s_agent, s_flag_pole) \n    penalty = -11 if (dist_flag_pole < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along University Campus and must avoid contact with any maintenance room door. The states of the drone and the maintenance room door are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_maintenance_room_door, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_maintenance_room_door = calculate_distance_min(s_agent, s_maintenance_room_door) \n    penalty = -11 if (dist_maintenance_room_door < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in University Campus. It must avoid colliding with the bicycle, professor and play sign. The states of the agent, flag poles, storage racks and safety barricades are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_multiple_obstacles_min < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in University Campus. It must avoid colliding with barricades and notice boards. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_multiple_obstacles_min < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in University Campus. It must avoid colliding with University Campus hanging banners. Agent's state, target's state and University Campus hanging banners's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_banner, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_banner = calculate_distance_min(s_agent, s_banner) \n    penalty_collision = -11 if (dist_banner < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating University Campus. Goal: It must avoid colliding with professor. Assumptions: The states of the drone and the professor's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_professor, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_professor = calculate_distance_min(s_agent, s_professor) \n    penalty_collision = -11 if (dist_professor < 0.4) else 20 \n    reward_reaching_target = 11 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating University Campus. Goal: It must avoid colliding with University Campus's flag poles, newspaper stands and barricades. Assumptions: The Agent's state, target's state and flag poles, newspaper stands and barricades's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_multiple_obstacles_min < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating University Campus. Goal: Detect bench, bike rack and student and do not collide. Assumptions: The states of the drone, target  bench, bike rack and student state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_multiple_obstacles_min < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating University Campus. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Fire Station. \n Objectives: It must avoid colliding with concrete columns or safety signs. \n Conditions: The states of the agent, the station fence, and the safety signs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_multiple_obstacle_min < 0.4) else 20 \n    reward_reaching_target = 11 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Fire Station. \n Objectives: Detect fire hydrant and aviod it. \n Conditions: The states of the agent, the fire hydrant are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_fire_hydrant, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_fire_hydrant = calculate_distance_min(s_agent, s_fire_hydrant) \n    penalty_collision = -20 if (dist_fire_hydrant < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Fire Station open workspace. Objectives: It must avoid colliding with fire equipment. Conditions: States of the drone and the fire equipment are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_fire_equipment, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_fire_equipment = calculate_distance_min(s_agent, s_fire_equipment) \n    penalty_collision = -20 if (dist_fire_equipment < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a garage bay, the drone must maneuver around fire hose line. All entity states (agent, fire hose line) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_fire_hose_line, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_fire_hose_line = calculate_distance_min(s_agent, s_fire_hose_line) \n    penalty_collision = -20 if (dist_fire_hose_line < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a garage bay. Requirement: Detect the three firefighters and do not collide with it. Given: The states for the drone and many firefighters are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_multiple_obstacle_min < 0.4) else 20 \n    reward_reaching_target = 11 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a control room during experiments. \n Objectives: So not collide with locker. \n Conditions: The states of the agent, the locker are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_locker, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_locker = calculate_distance_min(s_agent, s_locker) \n    penalty_collision = -20 if (dist_locker < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Fire Station. Objectives: Do not collide hit with water tank. Conditions: States of the agent, water tank are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_water_tank, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_water_tank = calculate_distance_min(s_agent, s_water_tank) \n    penalty_collision = -20 if (dist_water_tank < 0.4) else 4 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a garage bay, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -20 if (dist_wall < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: station yard. Avoid: Hitting with fire truck. Known: Agent, fire truck states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_fire_truck, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_fire_truck = calculate_distance_min(s_agent, s_fire_truck) \n    penalty_collision = -20 if (dist_fire_truck < 0.4) else 4 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Fire Station. Objectives: It must avoid colliding with firefighter. Conditions: The states of the drone and firefighter are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_firefighter, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_firefighter = calculate_distance_min(s_agent, s_firefighter) \n    penalty_collision = -11 if (dist_firefighter < 0.4) else 4 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Fire Station. Objectives: Avoid collision with toy shelves, firefighter and toy boxs. Conditions: The states of the agent, toy shelves, firefighter and toy boxs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_multiple_obstacles_min < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Fire Station. Objectives: It must avoid colliding with any fire engine, chief and play sign. Conditions: Agent's, fire engine, chief and play sign states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_multiple_obstacles_min < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Fire Station. Objectives: Do not collide with any antenna mast. Conditions: The states of the drone and the antenna mast are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_antenna_mast, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_antenna_mast = calculate_distance_min(s_agent, s_antenna_mast) \n    penalty = -11 if (dist_antenna_mast < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Fire Station and must avoid contact with any equipment room door. The states of the drone and the equipment room door are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_equipment_room_door, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_equipment_room_door = calculate_distance_min(s_agent, s_equipment_room_door) \n    penalty = -11 if (dist_equipment_room_door < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Fire Station. It must avoid colliding with the fire engine, chief and play sign. The states of the agent, antenna masts, storage racks and safety barriers are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_multiple_obstacles_min < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Fire Station. It must avoid colliding with barriers and safety signs. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_multiple_obstacles_min < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Fire Station. It must avoid colliding with Fire Station hanging ladders. Agent's state, target's state and Fire Station hanging ladders's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ladder, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_ladder = calculate_distance_min(s_agent, s_ladder) \n    penalty_collision = -11 if (dist_ladder < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Fire Station. Goal: It must avoid colliding with chief. Assumptions: The states of the drone and the chief's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_chief, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_chief = calculate_distance_min(s_agent, s_chief) \n    penalty_collision = -11 if (dist_chief < 0.4) else 20 \n    reward_reaching_target = 11 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Fire Station. Goal: It must avoid colliding with Fire Station's antenna masts, newspaper stands and barriers. Assumptions: The Agent's state, target's state and antenna masts, newspaper stands and barriers's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_multiple_obstacles_min < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Fire Station. Goal: Detect bench, fire hydrant and firefighter and do not collide. Assumptions: The states of the drone, target  bench, fire hydrant and firefighter state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_multiple_obstacles_min < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Fire Station. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Recycling Facility. \n Objectives: It must avoid colliding with concrete columns or hazard signs. \n Conditions: The states of the agent, the facility fence, and the hazard signs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_multiple_obstacle_min < 0.4) else 20 \n    reward_reaching_target = 11 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Recycling Facility. \n Objectives: Detect waste bin and aviod it. \n Conditions: The states of the agent, the waste bin are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_waste_bin, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_waste_bin = calculate_distance_min(s_agent, s_waste_bin) \n    penalty_collision = -20 if (dist_waste_bin < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Recycling Facility open workspace. Objectives: It must avoid colliding with recycling machinery. Conditions: States of the drone and the recycling machinery are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_recycling_machinery, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_recycling_machinery = calculate_distance_min(s_agent, s_recycling_machinery) \n    penalty_collision = -20 if (dist_recycling_machinery < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a processing bay, the drone must maneuver around fire hose line. All entity states (agent, fire hose line) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_conveyor_belt, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_conveyor_belt = calculate_distance_min(s_agent, s_conveyor_belt) \n    penalty_collision = -20 if (dist_conveyor_belt < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a processing bay. Requirement: Detect the three workers and do not collide with it. Given: The states for the drone and many workers are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_multiple_obstacle_min < 0.4) else 20 \n    reward_reaching_target = 11 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a control office during experiments. \n Objectives: So not collide with storage bin. \n Conditions: The states of the agent, the storage bin are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_storage_bin, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_storage_bin = calculate_distance_min(s_agent, s_storage_bin) \n    penalty_collision = -20 if (dist_storage_bin < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Recycling Facility. Objectives: Do not collide hit with liquid container. Conditions: States of the agent, liquid container are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_liquid_container, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_liquid_container = calculate_distance_min(s_agent, s_liquid_container) \n    penalty_collision = -20 if (dist_liquid_container < 0.4) else 4 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a processing bay, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -20 if (dist_wall < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: sorting yard. Avoid: Hitting with forklift. Known: Agent, forklift states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_forklift, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_forklift = calculate_distance_min(s_agent, s_forklift) \n    penalty_collision = -20 if (dist_forklift < 0.4) else 4 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Recycling Facility. Objectives: It must avoid colliding with worker. Conditions: The states of the drone and worker are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_worker, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_worker = calculate_distance_min(s_agent, s_worker) \n    penalty_collision = -11 if (dist_worker < 0.4) else 4 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Recycling Facility. Objectives: Avoid collision with sorting shelves, worker and plastic crates. Conditions: The states of the agent, sorting shelves, worker and plastic crates are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_multiple_obstacles_min < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Recycling Facility. Objectives: It must avoid colliding with any compactor truck, supervisor and play sign. Conditions: Agent's, compactor truck, supervisor and play sign states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_multiple_obstacles_min < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Recycling Facility. Objectives: Do not collide with any crane. Conditions: The states of the drone and the crane are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_crane, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_crane = calculate_distance_min(s_agent, s_crane) \n    penalty = -11 if (dist_crane < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Recycling Facility and must avoid contact with any storage room door. The states of the drone and the storage room door are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_storage_room_door, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_storage_room_door = calculate_distance_min(s_agent, s_storage_room_door) \n    penalty = -11 if (dist_storage_room_door < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Recycling Facility. It must avoid colliding with the compactor truck, supervisor and play sign. The states of the agent, cranes, storage racks and safety safety safety barriers are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_multiple_obstacles_min < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Recycling Facility. It must avoid colliding with safety safety barriers and hazard signs. The states of the drone and the water tuck and forklift are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_multiple_obstacles_min < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Recycling Facility. It must avoid colliding with Recycling Facility conveyor belts. Agent's state, target's state and Recycling Facility conveyor belts's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_belt, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_belt = calculate_distance_min(s_agent, s_belt) \n    penalty_collision = -11 if (dist_belt < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Recycling Facility. Goal: It must avoid colliding with supervisor. Assumptions: The states of the drone and the supervisor's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_supervisor, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_supervisor = calculate_distance_min(s_agent, s_supervisor) \n    penalty_collision = -11 if (dist_supervisor < 0.4) else 20 \n    reward_reaching_target = 11 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Recycling Facility. Goal: It must avoid colliding with Recycling Facility's cranes, newspaper stands and safety safety barriers. Assumptions: The Agent's state, target's state and cranes, newspaper stands and safety safety barriers's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_multiple_obstacles_min < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Recycling Facility. Goal: Detect bench, waste bin and worker and do not collide. Assumptions: The states of the drone, target  bench, waste bin and worker state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_multiple_obstacles_min < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Recycling Facility. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Botanical Garden. \n Objectives: It must avoid colliding with concrete columns or garden signs. \n Conditions: The states of the agent, the garden fence, and the garden signs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_multiple_obstacle_min < 0.4) else 20 \n    reward_reaching_target = 11 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Botanical Garden. \n Objectives: Detect garden pot and aviod it. \n Conditions: The states of the agent, the garden pot are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_garden_pot, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_garden_pot = calculate_distance_min(s_agent, s_garden_pot) \n    penalty_collision = -20 if (dist_garden_pot < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through a Botanical Garden. Objectives: It must avoid colliding with fountain. Conditions: States of the drone and the fountain are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_fountain, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_fountain = calculate_distance_min(s_agent, s_fountain) \n    penalty_collision = -20 if (dist_fountain < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a greenhouse, the drone must maneuver around fire hose line. All entity states (agent, fire hose line) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_vine_trellis, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_vine_trellis = calculate_distance_min(s_agent, s_vine_trellis) \n    penalty_collision = -20 if (dist_vine_trellis < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a greenhouse. Requirement: Detect the three statues and do not collide with it. Given: The states for the drone and many statues are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_multiple_obstacle_min < 0.4) else 20 \n    reward_reaching_target = 11 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a gazebo during experiments. \n Objectives: So not collide with planter box. \n Conditions: The states of the agent, the planter box are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_planter_box, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_planter_box = calculate_distance_min(s_agent, s_planter_box) \n    penalty_collision = -20 if (dist_planter_box < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Botanical Garden. Objectives: Do not collide hit with pond. Conditions: States of the agent, pond are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_pond, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_pond = calculate_distance_min(s_agent, s_pond) \n    penalty_collision = -20 if (dist_pond < 0.4) else 4 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a greenhouse, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -20 if (dist_wall < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: flower beds. Avoid: Hitting with lawn mower. Known: Agent, lawn mower states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_lawn_mower, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_lawn_mower = calculate_distance_min(s_agent, s_lawn_mower) \n    penalty_collision = -20 if (dist_lawn_mower < 0.4) else 4 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Botanical Garden. Objectives: It must avoid colliding with statue. Conditions: The states of the drone and statue are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_statue, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_statue = calculate_distance_min(s_agent, s_statue) \n    penalty_collision = -11 if (dist_statue < 0.4) else 4 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Botanical Garden. Objectives: Avoid collision with flower stands, statue and rock formations. Conditions: The states of the agent, flower stands, statue and rock formations are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_multiple_obstacles_min < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Botanical Garden. Objectives: It must avoid colliding with any garden cart, statue and play sign. Conditions: Agent's, garden cart, statue and play sign states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_multiple_obstacles_min < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Botanical Garden. Objectives: Do not collide with any tree. Conditions: The states of the drone and the tree are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_tree, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_tree = calculate_distance_min(s_agent, s_tree) \n    penalty = -11 if (dist_tree < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Botanical Garden and must avoid contact with any stone arch. The states of the drone and the stone arch are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_stone_arch, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_stone_arch = calculate_distance_min(s_agent, s_stone_arch) \n    penalty = -11 if (dist_stone_arch < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Botanical Garden. It must avoid colliding with the garden cart, statue and play sign. The states of the agent, trees, storage racks and safety safety hedges are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_multiple_obstacles_min < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Botanical Garden. It must avoid colliding with safety hedges and garden signs. The states of the drone and the water tuck and lawn mower are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_multiple_obstacles_min < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Botanical Garden. It must avoid colliding with Botanical Garden vine trelliss. Agent's state, target's state and Botanical Garden bush's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bush, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_bush = calculate_distance_min(s_agent, s_bush) \n    penalty_collision = -11 if (dist_bush < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Botanical Garden. Goal: It must avoid colliding with statue. Assumptions: The states of the drone and the statue's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_statue, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_supervisor = calculate_distance_min(s_agent, s_statue) \n    penalty_collision = -11 if (dist_supervisor < 0.4) else 20 \n    reward_reaching_target = 11 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Botanical Garden. Goal: It must avoid colliding with Botanical Garden's trees, newspaper stands and safety hedges. Assumptions: The Agent's state, target's state and trees, newspaper stands and safety hedges's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_multiple_obstacles_min < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Botanical Garden. Goal: Detect bench, garden pot and statue and do not collide. Assumptions: The states of the drone, target  bench, garden pot and statue state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_multiple_obstacles_min < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Botanical Garden. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Hotel. \n Objectives: It must avoid colliding with concrete columns or signboards. \n Conditions: The states of the agent, the hotel facade, and the signboards are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_multiple_obstacle_min < 0.4) else 20 \n    reward_reaching_target = 11 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Hotel. \n Objectives: Detect luggage cart and aviod it. \n Conditions: The states of the agent, the luggage cart are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_luggage_cart, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_luggage_cart = calculate_distance_min(s_agent, s_luggage_cart) \n    penalty_collision = -20 if (dist_luggage_cart < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Hotel open workspace. Objectives: It must avoid colliding with chandelier. Conditions: States of the drone and the chandelier are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_chandelier, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_chandelier = calculate_distance_min(s_agent, s_chandelier) \n    penalty_collision = -20 if (dist_chandelier < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a reception hall, the drone must maneuver around fire hose line. All entity states (agent, fire hose line) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_decorative_partition, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_decorative_partition = calculate_distance_min(s_agent, s_decorative_partition) \n    penalty_collision = -20 if (dist_decorative_partition < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a reception hall. Requirement: Detect the three statues and do not collide with it. Given: The states for the drone and many statues are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_multiple_obstacle_min < 0.4) else 20 \n    reward_reaching_target = 11 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a front desk during experiments. \n Objectives: So not collide with service counter. \n Conditions: The states of the agent, the service counter are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_service_counter, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_service_counter = calculate_distance_min(s_agent, s_service_counter) \n    penalty_collision = -20 if (dist_service_counter < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Hotel. Objectives: Do not collide hit with indoor fountain. Conditions: States of the agent, indoor fountain are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_indoor_fountain, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_indoor_fountain = calculate_distance_min(s_agent, s_indoor_fountain) \n    penalty_collision = -20 if (dist_indoor_fountain < 0.4) else 4 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a reception hall, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -20 if (dist_wall < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: lobby seating. Avoid: Hitting with elevator. Known: Agent, elevator states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_elevator, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_elevator = calculate_distance_min(s_agent, s_elevator) \n    penalty_collision = -20 if (dist_elevator < 0.4) else 4 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Hotel. Objectives: It must avoid colliding with statue. Conditions: The states of the drone and statue are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_statue, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_statue = calculate_distance_min(s_agent, s_statue) \n    penalty_collision = -11 if (dist_statue < 0.4) else 4 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Hotel. Objectives: Avoid collision with brochure stands, statue and decor columns. Conditions: The states of the agent, brochure stands, statue and decor columns are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_multiple_obstacles_min < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Hotel. Objectives: It must avoid colliding with any room service trolley, statue and play sign. Conditions: Agent's, room service trolley, statue and play sign states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_multiple_obstacles_min < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Hotel. Objectives: Do not collide with any pillar. Conditions: The states of the drone and the pillar are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_pillar, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_pillar = calculate_distance_min(s_agent, s_pillar) \n    penalty = -11 if (dist_pillar < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Hotel and must avoid contact with any archway. The states of the drone and the archway are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_archway, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_archway = calculate_distance_min(s_agent, s_archway) \n    penalty = -11 if (dist_archway < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Hotel. It must avoid colliding with the room service trolley, statue and play sign. The states of the agent, pillars, storage racks and safety safety railings are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_multiple_obstacles_min < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Hotel. It must avoid colliding with safety railings and signboards. The states of the drone and the water tuck and elevator are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_multiple_obstacles_min < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Hotel. It must avoid colliding with Hotel decorative partitions. Agent's state, target's state and Hotel decorative partitions's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_deco, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_deco = calculate_distance_min(s_agent, s_deco) \n    penalty_collision = -11 if (dist_deco < 20) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Hotel. Goal: It must avoid colliding with entrance. Assumptions: The states of the drone and the entrance's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_entrance, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_entrance = calculate_distance_min(s_agent, s_entrance) \n    penalty_collision = -11 if (dist_entrance < 0.4) else 20 \n    reward_reaching_target = 11 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Hotel. Goal: It must avoid colliding with Hotel's pillars, newspaper stands and safety railings. Assumptions: The Agent's state, target's state and pillars, newspaper stands and safety railings's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_multiple_obstacles_min < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Hotel. Goal: Detect bench, luggage cart and statue and do not collide. Assumptions: The states of the drone, target  bench, luggage cart and statue state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -11 if (dist_multiple_obstacles_min < 0.4) else 20 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Hotel. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Kindergarten Playground. \n Objectives: It must avoid colliding with concrete columns or play signs. \n Conditions: The states of the agent, the playground fence, and the play signs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Kindergarten Playground. \n Objectives: Detect slide and aviod it. \n Conditions: The states of the agent, the slide are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_slide, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_slide = calculate_distance_min(s_agent, s_slide) \n    penalty_collision = -17 if (dist_slide < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Kindergarten Playground open workspace. Objectives: It must avoid colliding with playground equipment. Conditions: States of the drone and the playground equipment are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_playground_equipment, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_playground_equipment = calculate_distance_min(s_agent, s_playground_equipment) \n    penalty_collision = -17 if (dist_playground_equipment < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a playground path, the drone must maneuver around play tunnel curtain. All entity states (agent, play tunnel curtain) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_play_tunnel_curtain, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_play_tunnel_curtain = calculate_distance_min(s_agent, s_play_tunnel_curtain) \n    penalty_collision = -17 if (dist_play_tunnel_curtain < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a playground path. Requirement: Detect the three children and do not collide with it. Given: The states for the drone and many children are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a toy storage area during experiments. \n Objectives: So not collide with playhouse. \n Conditions: The states of the agent, the playhouse are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_playhouse, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_playhouse = calculate_distance_min(s_agent, s_playhouse) \n    penalty_collision = -17 if (dist_playhouse < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Kindergarten Playground. Objectives: Do not collide hit with sandbox. Conditions: States of the agent, sandbox are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_sandbox, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_sandbox = calculate_distance_min(s_agent, s_sandbox) \n    penalty_collision = -17 if (dist_sandbox < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a playground path, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: playground ground. Avoid: Hitting with climbing frame. Known: Agent, climbing frame states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_climbing_frame, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_climbing_frame = calculate_distance_min(s_agent, s_climbing_frame) \n    penalty_collision = -17 if (dist_climbing_frame < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Kindergarten Playground. Objectives: It must avoid colliding with child. Conditions: The states of the drone and child are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_child, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_child = calculate_distance_min(s_agent, s_child) \n    penalty_collision = -19 if (dist_child < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Kindergarten Playground. Objectives: Avoid collision with toy shelves, child and toy boxs. Conditions: The states of the agent, toy shelves, child and toy boxs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Kindergarten Playground. Objectives: It must avoid colliding with any tricycle, teacher and play sign. Conditions: Agent's, tricycle, teacher and play sign states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Kindergarten Playground. Objectives: Do not collide with any swing. Conditions: The states of the drone and the swing are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_swing, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_swing = calculate_distance_min(s_agent, s_swing) \n    penalty = -19 if (dist_swing < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Kindergarten Playground and must avoid contact with any storage shed door. The states of the drone and the storage shed door are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_storage_shed_door, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_storage_shed_door = calculate_distance_min(s_agent, s_storage_shed_door) \n    penalty = -19 if (dist_storage_shed_door < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Kindergarten Playground. It must avoid colliding with the tricycle, teacher and play sign. The states of the agent, swing chains, storage racks and safety barricades are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Kindergarten Playground. It must avoid colliding with safety cones and play signs. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Kindergarten Playground. It must avoid colliding with Kindergarten Playground hanging ropes. Agent's state, target's state and Kindergarten Playground hanging ropes's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_hanging_rope, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle = calculate_distance_min(s_agent, s_hanging_rope) \n    penalty_collision = -19 if (dist_obstacle < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Kindergarten Playground. Goal: It must avoid colliding with swing chain. Assumptions: The states of the drone and the swing chain's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_swing_chain, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_swing_chain = calculate_distance_min(s_agent, s_swing_chain) \n    penalty_collision = -19 if (dist_swing_chain < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Kindergarten Playground. Goal: It must avoid colliding with Kindergarten Playground's bench, toy box and child. Assumptions: The Agent's state, target's state and bench, toy box and child' states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Kindergarten Playground. Goal: Detect waste containers and gas cylindes and avoid storage shed door and do not collide. Assumptions: The states of the drone, target  waste containers, gas cylindes and avoid storage shed door state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Kindergarten Playground. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along University Campus. \n Objectives: It must avoid colliding with concrete columns or notice boards. \n Conditions: The states of the agent, the campus fence, and the notice boards are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy University Campus. \n Objectives: Detect bike rack and aviod it. \n Conditions: The states of the agent, the bike rack are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bike_rack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_bike_rack = calculate_distance_min(s_agent, s_bike_rack) \n    penalty_collision = -17 if (dist_bike_rack < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating University Campus open workspace. Objectives: It must avoid colliding with campus facilities. Conditions: States of the drone and the campus facilities are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_campus_facilities, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_campus_facilities = calculate_distance_min(s_agent, s_campus_facilities) \n    penalty_collision = -17 if (dist_campus_facilities < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a campus walkway, the drone must maneuver around construction tarp. All entity states (agent, construction tarp) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_construction_tarp, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_play_construction_tarp = calculate_distance_min(s_agent, s_construction_tarp) \n    penalty_collision = -17 if (dist_play_construction_tarp < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a campus walkway. Requirement: Detect the three students and do not collide with it. Given: The states for the drone and many students are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a library lobby during experiments. \n Objectives: So not collide with kiosk. \n Conditions: The states of the agent, the kiosk are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_kiosk, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_playhouse = calculate_distance_min(s_agent, s_kiosk) \n    penalty_collision = -17 if (dist_playhouse < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in University Campus. Objectives: Do not collide hit with fountain. Conditions: States of the agent, fountain are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_fountain, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_fountain = calculate_distance_min(s_agent, s_fountain) \n    penalty_collision = -17 if (dist_fountain < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a campus walkway, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: campus quad. Avoid: Hitting with statue. Known: Agent, statue states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_statue, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_statue = calculate_distance_min(s_agent, s_statue) \n    penalty_collision = -17 if (dist_statue < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating University Campus. Objectives: It must avoid colliding with student. Conditions: The states of the drone and student are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_student, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_student = calculate_distance_min(s_agent, s_student) \n    penalty_collision = -19 if (dist_student < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through University Campus. Objectives: Avoid collision with toy shelves, student and toy boxs. Conditions: The states of the agent, toy shelves, student and toy boxs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating University Campus. Objectives: It must avoid colliding with any bicycle, professor and play sign. Conditions: Agent's, bicycle, professor and play sign states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through University Campus. Objectives: Do not collide with any flag pole. Conditions: The states of the drone and the flag pole are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_flag_pole, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_flag_pole = calculate_distance_min(s_agent, s_flag_pole) \n    penalty = -19 if (dist_flag_pole < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along University Campus and must avoid contact with any maintenance room door. The states of the drone and the maintenance room door are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_maintenance_room_door, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_maintenance_room_door = calculate_distance_min(s_agent, s_maintenance_room_door) \n    penalty = -19 if (dist_maintenance_room_door < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in University Campus. It must avoid colliding with the bicycle, professor and play sign. The states of the agent, flag poles, storage racks and safety barricades are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in University Campus. It must avoid colliding with barricades and notice boards. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in University Campus. It must avoid colliding with University Campus hanging banners. Agent's state, target's state and University Campus hanging banners's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_banner, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_banner = calculate_distance_min(s_agent, s_banner) \n    penalty_collision = -19 if (dist_banner < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating University Campus. Goal: It must avoid colliding with professor. Assumptions: The states of the drone and the professor's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_professor, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_professor = calculate_distance_min(s_agent, s_professor) \n    penalty_collision = -19 if (dist_professor < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating University Campus. Goal: It must avoid colliding with University Campus's flag poles, newspaper stands and barricades. Assumptions: The Agent's state, target's state and flag poles, newspaper stands and barricades's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating University Campus. Goal: Detect bench, bike rack and student and do not collide. Assumptions: The states of the drone, target  bench, bike rack and student state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating University Campus. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Fire Station. \n Objectives: It must avoid colliding with concrete columns or safety signs. \n Conditions: The states of the agent, the station fence, and the safety signs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Fire Station. \n Objectives: Detect fire hydrant and aviod it. \n Conditions: The states of the agent, the fire hydrant are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_fire_hydrant, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_fire_hydrant = calculate_distance_min(s_agent, s_fire_hydrant) \n    penalty_collision = -17 if (dist_fire_hydrant < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Fire Station open workspace. Objectives: It must avoid colliding with fire equipment. Conditions: States of the drone and the fire equipment are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_fire_equipment, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_fire_equipment = calculate_distance_min(s_agent, s_fire_equipment) \n    penalty_collision = -17 if (dist_fire_equipment < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a garage bay, the drone must maneuver around fire hose line. All entity states (agent, fire hose line) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_fire_hose_line, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_fire_hose_line = calculate_distance_min(s_agent, s_fire_hose_line) \n    penalty_collision = -17 if (dist_fire_hose_line < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a garage bay. Requirement: Detect the three firefighters and do not collide with it. Given: The states for the drone and many firefighters are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a control room during experiments. \n Objectives: So not collide with locker. \n Conditions: The states of the agent, the locker are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_locker, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_locker = calculate_distance_min(s_agent, s_locker) \n    penalty_collision = -17 if (dist_locker < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Fire Station. Objectives: Do not collide hit with water tank. Conditions: States of the agent, water tank are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_water_tank, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_water_tank = calculate_distance_min(s_agent, s_water_tank) \n    penalty_collision = -17 if (dist_water_tank < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a garage bay, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: station yard. Avoid: Hitting with fire truck. Known: Agent, fire truck states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_fire_truck, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_fire_truck = calculate_distance_min(s_agent, s_fire_truck) \n    penalty_collision = -17 if (dist_fire_truck < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Fire Station. Objectives: It must avoid colliding with firefighter. Conditions: The states of the drone and firefighter are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_firefighter, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_firefighter = calculate_distance_min(s_agent, s_firefighter) \n    penalty_collision = -19 if (dist_firefighter < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Fire Station. Objectives: Avoid collision with toy shelves, firefighter and toy boxs. Conditions: The states of the agent, toy shelves, firefighter and toy boxs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Fire Station. Objectives: It must avoid colliding with any fire engine, chief and play sign. Conditions: Agent's, fire engine, chief and play sign states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Fire Station. Objectives: Do not collide with any antenna mast. Conditions: The states of the drone and the antenna mast are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_antenna_mast, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_antenna_mast = calculate_distance_min(s_agent, s_antenna_mast) \n    penalty = -19 if (dist_antenna_mast < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Fire Station and must avoid contact with any equipment room door. The states of the drone and the equipment room door are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_equipment_room_door, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_equipment_room_door = calculate_distance_min(s_agent, s_equipment_room_door) \n    penalty = -19 if (dist_equipment_room_door < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Fire Station. It must avoid colliding with the fire engine, chief and play sign. The states of the agent, antenna masts, storage racks and safety barriers are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Fire Station. It must avoid colliding with barriers and safety signs. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Fire Station. It must avoid colliding with Fire Station hanging ladders. Agent's state, target's state and Fire Station hanging ladders's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ladder, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_ladder = calculate_distance_min(s_agent, s_ladder) \n    penalty_collision = -19 if (dist_ladder < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Fire Station. Goal: It must avoid colliding with chief. Assumptions: The states of the drone and the chief's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_chief, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_chief = calculate_distance_min(s_agent, s_chief) \n    penalty_collision = -19 if (dist_chief < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Fire Station. Goal: It must avoid colliding with Fire Station's antenna masts, newspaper stands and barriers. Assumptions: The Agent's state, target's state and antenna masts, newspaper stands and barriers's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Fire Station. Goal: Detect bench, fire hydrant and firefighter and do not collide. Assumptions: The states of the drone, target  bench, fire hydrant and firefighter state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Fire Station. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Recycling Facility. \n Objectives: It must avoid colliding with concrete columns or hazard signs. \n Conditions: The states of the agent, the facility fence, and the hazard signs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Recycling Facility. \n Objectives: Detect waste bin and aviod it. \n Conditions: The states of the agent, the waste bin are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_waste_bin, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_waste_bin = calculate_distance_min(s_agent, s_waste_bin) \n    penalty_collision = -17 if (dist_waste_bin < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Recycling Facility open workspace. Objectives: It must avoid colliding with recycling machinery. Conditions: States of the drone and the recycling machinery are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_recycling_machinery, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_recycling_machinery = calculate_distance_min(s_agent, s_recycling_machinery) \n    penalty_collision = -17 if (dist_recycling_machinery < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a processing bay, the drone must maneuver around fire hose line. All entity states (agent, fire hose line) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_conveyor_belt, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_conveyor_belt = calculate_distance_min(s_agent, s_conveyor_belt) \n    penalty_collision = -17 if (dist_conveyor_belt < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a processing bay. Requirement: Detect the three workers and do not collide with it. Given: The states for the drone and many workers are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a control office during experiments. \n Objectives: So not collide with storage bin. \n Conditions: The states of the agent, the storage bin are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_storage_bin, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_storage_bin = calculate_distance_min(s_agent, s_storage_bin) \n    penalty_collision = -17 if (dist_storage_bin < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Recycling Facility. Objectives: Do not collide hit with liquid container. Conditions: States of the agent, liquid container are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_liquid_container, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_liquid_container = calculate_distance_min(s_agent, s_liquid_container) \n    penalty_collision = -17 if (dist_liquid_container < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a processing bay, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: sorting yard. Avoid: Hitting with forklift. Known: Agent, forklift states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_forklift, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_forklift = calculate_distance_min(s_agent, s_forklift) \n    penalty_collision = -17 if (dist_forklift < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Recycling Facility. Objectives: It must avoid colliding with worker. Conditions: The states of the drone and worker are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_worker, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_worker = calculate_distance_min(s_agent, s_worker) \n    penalty_collision = -19 if (dist_worker < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Recycling Facility. Objectives: Avoid collision with sorting shelves, worker and plastic crates. Conditions: The states of the agent, sorting shelves, worker and plastic crates are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Recycling Facility. Objectives: It must avoid colliding with any compactor truck, supervisor and play sign. Conditions: Agent's, compactor truck, supervisor and play sign states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Recycling Facility. Objectives: Do not collide with any crane. Conditions: The states of the drone and the crane are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_crane, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_crane = calculate_distance_min(s_agent, s_crane) \n    penalty = -19 if (dist_crane < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Recycling Facility and must avoid contact with any storage room door. The states of the drone and the storage room door are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_storage_room_door, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_storage_room_door = calculate_distance_min(s_agent, s_storage_room_door) \n    penalty = -19 if (dist_storage_room_door < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Recycling Facility. It must avoid colliding with the compactor truck, supervisor and play sign. The states of the agent, cranes, storage racks and safety safety safety barriers are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Recycling Facility. It must avoid colliding with safety safety barriers and hazard signs. The states of the drone and the water tuck and forklift are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Recycling Facility. It must avoid colliding with Recycling Facility conveyor belts. Agent's state, target's state and Recycling Facility conveyor belts's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_belt, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_belt = calculate_distance_min(s_agent, s_belt) \n    penalty_collision = -19 if (dist_belt < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Recycling Facility. Goal: It must avoid colliding with supervisor. Assumptions: The states of the drone and the supervisor's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_supervisor, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_supervisor = calculate_distance_min(s_agent, s_supervisor) \n    penalty_collision = -19 if (dist_supervisor < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Recycling Facility. Goal: It must avoid colliding with Recycling Facility's cranes, newspaper stands and safety safety barriers. Assumptions: The Agent's state, target's state and cranes, newspaper stands and safety safety barriers's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Recycling Facility. Goal: Detect bench, waste bin and worker and do not collide. Assumptions: The states of the drone, target  bench, waste bin and worker state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Recycling Facility. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Botanical Garden. \n Objectives: It must avoid colliding with concrete columns or garden signs. \n Conditions: The states of the agent, the garden fence, and the garden signs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Botanical Garden. \n Objectives: Detect garden pot and aviod it. \n Conditions: The states of the agent, the garden pot are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_garden_pot, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_garden_pot = calculate_distance_min(s_agent, s_garden_pot) \n    penalty_collision = -17 if (dist_garden_pot < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through a Botanical Garden. Objectives: It must avoid colliding with fountain. Conditions: States of the drone and the fountain are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_fountain, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_fountain = calculate_distance_min(s_agent, s_fountain) \n    penalty_collision = -17 if (dist_fountain < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a greenhouse, the drone must maneuver around fire hose line. All entity states (agent, fire hose line) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_vine_trellis, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_vine_trellis = calculate_distance_min(s_agent, s_vine_trellis) \n    penalty_collision = -17 if (dist_vine_trellis < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a greenhouse. Requirement: Detect the three statues and do not collide with it. Given: The states for the drone and many statues are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a gazebo during experiments. \n Objectives: So not collide with planter box. \n Conditions: The states of the agent, the planter box are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_planter_box, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_planter_box = calculate_distance_min(s_agent, s_planter_box) \n    penalty_collision = -17 if (dist_planter_box < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Botanical Garden. Objectives: Do not collide hit with pond. Conditions: States of the agent, pond are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_pond, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_pond = calculate_distance_min(s_agent, s_pond) \n    penalty_collision = -17 if (dist_pond < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a greenhouse, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: flower beds. Avoid: Hitting with lawn mower. Known: Agent, lawn mower states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_lawn_mower, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_lawn_mower = calculate_distance_min(s_agent, s_lawn_mower) \n    penalty_collision = -17 if (dist_lawn_mower < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Botanical Garden. Objectives: It must avoid colliding with statue. Conditions: The states of the drone and statue are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_statue, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_statue = calculate_distance_min(s_agent, s_statue) \n    penalty_collision = -19 if (dist_statue < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Botanical Garden. Objectives: Avoid collision with flower stands, statue and rock formations. Conditions: The states of the agent, flower stands, statue and rock formations are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Botanical Garden. Objectives: It must avoid colliding with any garden cart, statue and play sign. Conditions: Agent's, garden cart, statue and play sign states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Botanical Garden. Objectives: Do not collide with any tree. Conditions: The states of the drone and the tree are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_tree, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_tree = calculate_distance_min(s_agent, s_tree) \n    penalty = -19 if (dist_tree < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Botanical Garden and must avoid contact with any stone arch. The states of the drone and the stone arch are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_stone_arch, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_stone_arch = calculate_distance_min(s_agent, s_stone_arch) \n    penalty = -19 if (dist_stone_arch < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Botanical Garden. It must avoid colliding with the garden cart, statue and play sign. The states of the agent, trees, storage racks and safety safety hedges are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Botanical Garden. It must avoid colliding with safety hedges and garden signs. The states of the drone and the water tuck and lawn mower are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Botanical Garden. It must avoid colliding with Botanical Garden vine trelliss. Agent's state, target's state and Botanical Garden bush's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bush, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_bush = calculate_distance_min(s_agent, s_bush) \n    penalty_collision = -19 if (dist_bush < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Botanical Garden. Goal: It must avoid colliding with statue. Assumptions: The states of the drone and the statue's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_statue, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_supervisor = calculate_distance_min(s_agent, s_statue) \n    penalty_collision = -19 if (dist_supervisor < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Botanical Garden. Goal: It must avoid colliding with Botanical Garden's trees, newspaper stands and safety hedges. Assumptions: The Agent's state, target's state and trees, newspaper stands and safety hedges's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Botanical Garden. Goal: Detect bench, garden pot and statue and do not collide. Assumptions: The states of the drone, target  bench, garden pot and statue state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Botanical Garden. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Hotel. \n Objectives: It must avoid colliding with concrete columns or signboards. \n Conditions: The states of the agent, the hotel facade, and the signboards are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Hotel. \n Objectives: Detect luggage cart and aviod it. \n Conditions: The states of the agent, the luggage cart are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_luggage_cart, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_luggage_cart = calculate_distance_min(s_agent, s_luggage_cart) \n    penalty_collision = -17 if (dist_luggage_cart < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Hotel open workspace. Objectives: It must avoid colliding with chandelier. Conditions: States of the drone and the chandelier are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_chandelier, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_chandelier = calculate_distance_min(s_agent, s_chandelier) \n    penalty_collision = -17 if (dist_chandelier < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a reception hall, the drone must maneuver around fire hose line. All entity states (agent, fire hose line) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_decorative_partition, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_decorative_partition = calculate_distance_min(s_agent, s_decorative_partition) \n    penalty_collision = -17 if (dist_decorative_partition < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a reception hall. Requirement: Detect the three statues and do not collide with it. Given: The states for the drone and many statues are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a front desk during experiments. \n Objectives: So not collide with service counter. \n Conditions: The states of the agent, the service counter are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_service_counter, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_service_counter = calculate_distance_min(s_agent, s_service_counter) \n    penalty_collision = -17 if (dist_service_counter < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Hotel. Objectives: Do not collide hit with indoor fountain. Conditions: States of the agent, indoor fountain are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_indoor_fountain, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_indoor_fountain = calculate_distance_min(s_agent, s_indoor_fountain) \n    penalty_collision = -17 if (dist_indoor_fountain < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a reception hall, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: lobby seating. Avoid: Hitting with elevator. Known: Agent, elevator states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_elevator, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_elevator = calculate_distance_min(s_agent, s_elevator) \n    penalty_collision = -17 if (dist_elevator < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Hotel. Objectives: It must avoid colliding with statue. Conditions: The states of the drone and statue are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_statue, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_statue = calculate_distance_min(s_agent, s_statue) \n    penalty_collision = -19 if (dist_statue < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Hotel. Objectives: Avoid collision with brochure stands, statue and decor columns. Conditions: The states of the agent, brochure stands, statue and decor columns are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Hotel. Objectives: It must avoid colliding with any room service trolley, statue and play sign. Conditions: Agent's, room service trolley, statue and play sign states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Hotel. Objectives: Do not collide with any pillar. Conditions: The states of the drone and the pillar are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_pillar, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_pillar = calculate_distance_min(s_agent, s_pillar) \n    penalty = -19 if (dist_pillar < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Hotel and must avoid contact with any archway. The states of the drone and the archway are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_archway, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_archway = calculate_distance_min(s_agent, s_archway) \n    penalty = -19 if (dist_archway < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Hotel. It must avoid colliding with the room service trolley, statue and play sign. The states of the agent, pillars, storage racks and safety safety railings are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Hotel. It must avoid colliding with safety railings and signboards. The states of the drone and the water tuck and elevator are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Hotel. It must avoid colliding with Hotel decorative partitions. Agent's state, target's state and Hotel decorative partitions's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_deco, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_deco = calculate_distance_min(s_agent, s_deco) \n    penalty_collision = -19 if (dist_deco < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Hotel. Goal: It must avoid colliding with entrance. Assumptions: The states of the drone and the entrance's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_entrance, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_entrance = calculate_distance_min(s_agent, s_entrance) \n    penalty_collision = -19 if (dist_entrance < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Hotel. Goal: It must avoid colliding with Hotel's pillars, newspaper stands and safety railings. Assumptions: The Agent's state, target's state and pillars, newspaper stands and safety railings's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Hotel. Goal: Detect bench, luggage cart and statue and do not collide. Assumptions: The states of the drone, target  bench, luggage cart and statue state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Hotel. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -math.exp(dist_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Kindergarten Playground. \n Objectives: It must avoid colliding with concrete columns or play signs. \n Conditions: The states of the agent, the playground fence, and the play signs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Kindergarten Playground. \n Objectives: Detect slide and aviod it. \n Conditions: The states of the agent, the slide are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_slide, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_slide = calculate_distance_min(s_agent, s_slide) \n    penalty_collision = -17 if (dist_slide < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Kindergarten Playground open workspace. Objectives: It must avoid colliding with playground equipment. Conditions: States of the drone and the playground equipment are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_playground_equipment, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_playground_equipment = calculate_distance_min(s_agent, s_playground_equipment) \n    penalty_collision = -17 if (dist_playground_equipment < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a playground path, the drone must maneuver around play tunnel curtain. All entity states (agent, play tunnel curtain) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_play_tunnel_curtain, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_play_tunnel_curtain = calculate_distance_min(s_agent, s_play_tunnel_curtain) \n    penalty_collision = -17 if (dist_play_tunnel_curtain < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a playground path. Requirement: Detect the three children and do not collide with it. Given: The states for the drone and many children are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a toy storage area during experiments. \n Objectives: So not collide with playhouse. \n Conditions: The states of the agent, the playhouse are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_playhouse, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_playhouse = calculate_distance_min(s_agent, s_playhouse) \n    penalty_collision = -17 if (dist_playhouse < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Kindergarten Playground. Objectives: Do not collide hit with sandbox. Conditions: States of the agent, sandbox are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_sandbox, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_sandbox = calculate_distance_min(s_agent, s_sandbox) \n    penalty_collision = -17 if (dist_sandbox < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a playground path, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: playground ground. Avoid: Hitting with climbing frame. Known: Agent, climbing frame states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_climbing_frame, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_climbing_frame = calculate_distance_min(s_agent, s_climbing_frame) \n    penalty_collision = -17 if (dist_climbing_frame < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Kindergarten Playground. Objectives: It must avoid colliding with child. Conditions: The states of the drone and child are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_child, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_child = calculate_distance_min(s_agent, s_child) \n    penalty_collision = -19 if (dist_child < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Kindergarten Playground. Objectives: Avoid collision with toy shelves, child and toy boxs. Conditions: The states of the agent, toy shelves, child and toy boxs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Kindergarten Playground. Objectives: It must avoid colliding with any tricycle, teacher and play sign. Conditions: Agent's, tricycle, teacher and play sign states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Kindergarten Playground. Objectives: Do not collide with any swing. Conditions: The states of the drone and the swing are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_swing, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_swing = calculate_distance_min(s_agent, s_swing) \n    penalty = -19 if (dist_swing < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Kindergarten Playground and must avoid contact with any storage shed door. The states of the drone and the storage shed door are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_storage_shed_door, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_storage_shed_door = calculate_distance_min(s_agent, s_storage_shed_door) \n    penalty = -19 if (dist_storage_shed_door < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Kindergarten Playground. It must avoid colliding with the tricycle, teacher and play sign. The states of the agent, swing chains, storage racks and safety barricades are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Kindergarten Playground. It must avoid colliding with safety cones and play signs. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Kindergarten Playground. It must avoid colliding with Kindergarten Playground hanging ropes. Agent's state, target's state and Kindergarten Playground hanging ropes's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_hanging_rope, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle = calculate_distance_min(s_agent, s_hanging_rope) \n    penalty_collision = -19 if (dist_obstacle < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Kindergarten Playground. Goal: It must avoid colliding with swing chain. Assumptions: The states of the drone and the swing chain's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_swing_chain, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_swing_chain = calculate_distance_min(s_agent, s_swing_chain) \n    penalty_collision = -19 if (dist_swing_chain < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Kindergarten Playground. Goal: It must avoid colliding with Kindergarten Playground's bench, toy box and child. Assumptions: The Agent's state, target's state and bench, toy box and child' states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Kindergarten Playground. Goal: Detect waste containers and gas cylindes and avoid storage shed door and do not collide. Assumptions: The states of the drone, target  waste containers, gas cylindes and avoid storage shed door state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Kindergarten Playground. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along University Campus. \n Objectives: It must avoid colliding with concrete columns or notice boards. \n Conditions: The states of the agent, the campus fence, and the notice boards are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy University Campus. \n Objectives: Detect bike rack and aviod it. \n Conditions: The states of the agent, the bike rack are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bike_rack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_bike_rack = calculate_distance_min(s_agent, s_bike_rack) \n    penalty_collision = -17 if (dist_bike_rack < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating University Campus open workspace. Objectives: It must avoid colliding with campus facilities. Conditions: States of the drone and the campus facilities are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_campus_facilities, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_campus_facilities = calculate_distance_min(s_agent, s_campus_facilities) \n    penalty_collision = -17 if (dist_campus_facilities < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a campus walkway, the drone must maneuver around construction tarp. All entity states (agent, construction tarp) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_construction_tarp, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_play_construction_tarp = calculate_distance_min(s_agent, s_construction_tarp) \n    penalty_collision = -17 if (dist_play_construction_tarp < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a campus walkway. Requirement: Detect the three students and do not collide with it. Given: The states for the drone and many students are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a library lobby during experiments. \n Objectives: So not collide with kiosk. \n Conditions: The states of the agent, the kiosk are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_kiosk, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_playhouse = calculate_distance_min(s_agent, s_kiosk) \n    penalty_collision = -17 if (dist_playhouse < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in University Campus. Objectives: Do not collide hit with fountain. Conditions: States of the agent, fountain are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_fountain, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_fountain = calculate_distance_min(s_agent, s_fountain) \n    penalty_collision = -17 if (dist_fountain < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a campus walkway, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: campus quad. Avoid: Hitting with statue. Known: Agent, statue states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_statue, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_statue = calculate_distance_min(s_agent, s_statue) \n    penalty_collision = -17 if (dist_statue < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating University Campus. Objectives: It must avoid colliding with student. Conditions: The states of the drone and student are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_student, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_student = calculate_distance_min(s_agent, s_student) \n    penalty_collision = -19 if (dist_student < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through University Campus. Objectives: Avoid collision with toy shelves, student and toy boxs. Conditions: The states of the agent, toy shelves, student and toy boxs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating University Campus. Objectives: It must avoid colliding with any bicycle, professor and play sign. Conditions: Agent's, bicycle, professor and play sign states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through University Campus. Objectives: Do not collide with any flag pole. Conditions: The states of the drone and the flag pole are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_flag_pole, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_flag_pole = calculate_distance_min(s_agent, s_flag_pole) \n    penalty = -19 if (dist_flag_pole < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along University Campus and must avoid contact with any maintenance room door. The states of the drone and the maintenance room door are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_maintenance_room_door, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_maintenance_room_door = calculate_distance_min(s_agent, s_maintenance_room_door) \n    penalty = -19 if (dist_maintenance_room_door < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in University Campus. It must avoid colliding with the bicycle, professor and play sign. The states of the agent, flag poles, storage racks and safety barricades are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in University Campus. It must avoid colliding with barricades and notice boards. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in University Campus. It must avoid colliding with University Campus hanging banners. Agent's state, target's state and University Campus hanging banners's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_banner, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_banner = calculate_distance_min(s_agent, s_banner) \n    penalty_collision = -19 if (dist_banner < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating University Campus. Goal: It must avoid colliding with professor. Assumptions: The states of the drone and the professor's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_professor, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_professor = calculate_distance_min(s_agent, s_professor) \n    penalty_collision = -19 if (dist_professor < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating University Campus. Goal: It must avoid colliding with University Campus's flag poles, newspaper stands and barricades. Assumptions: The Agent's state, target's state and flag poles, newspaper stands and barricades's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating University Campus. Goal: Detect bench, bike rack and student and do not collide. Assumptions: The states of the drone, target  bench, bike rack and student state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating University Campus. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Fire Station. \n Objectives: It must avoid colliding with concrete columns or safety signs. \n Conditions: The states of the agent, the station fence, and the safety signs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Fire Station. \n Objectives: Detect fire hydrant and aviod it. \n Conditions: The states of the agent, the fire hydrant are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_fire_hydrant, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_fire_hydrant = calculate_distance_min(s_agent, s_fire_hydrant) \n    penalty_collision = -17 if (dist_fire_hydrant < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Fire Station open workspace. Objectives: It must avoid colliding with fire equipment. Conditions: States of the drone and the fire equipment are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_fire_equipment, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_fire_equipment = calculate_distance_min(s_agent, s_fire_equipment) \n    penalty_collision = -17 if (dist_fire_equipment < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a garage bay, the drone must maneuver around fire hose line. All entity states (agent, fire hose line) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_fire_hose_line, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_fire_hose_line = calculate_distance_min(s_agent, s_fire_hose_line) \n    penalty_collision = -17 if (dist_fire_hose_line < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a garage bay. Requirement: Detect the three firefighters and do not collide with it. Given: The states for the drone and many firefighters are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a control room during experiments. \n Objectives: So not collide with locker. \n Conditions: The states of the agent, the locker are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_locker, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_locker = calculate_distance_min(s_agent, s_locker) \n    penalty_collision = -17 if (dist_locker < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Fire Station. Objectives: Do not collide hit with water tank. Conditions: States of the agent, water tank are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_water_tank, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_water_tank = calculate_distance_min(s_agent, s_water_tank) \n    penalty_collision = -17 if (dist_water_tank < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a garage bay, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: station yard. Avoid: Hitting with fire truck. Known: Agent, fire truck states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_fire_truck, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_fire_truck = calculate_distance_min(s_agent, s_fire_truck) \n    penalty_collision = -17 if (dist_fire_truck < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Fire Station. Objectives: It must avoid colliding with firefighter. Conditions: The states of the drone and firefighter are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_firefighter, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_firefighter = calculate_distance_min(s_agent, s_firefighter) \n    penalty_collision = -19 if (dist_firefighter < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Fire Station. Objectives: Avoid collision with toy shelves, firefighter and toy boxs. Conditions: The states of the agent, toy shelves, firefighter and toy boxs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Fire Station. Objectives: It must avoid colliding with any fire engine, chief and play sign. Conditions: Agent's, fire engine, chief and play sign states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Fire Station. Objectives: Do not collide with any antenna mast. Conditions: The states of the drone and the antenna mast are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_antenna_mast, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_antenna_mast = calculate_distance_min(s_agent, s_antenna_mast) \n    penalty = -19 if (dist_antenna_mast < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Fire Station and must avoid contact with any equipment room door. The states of the drone and the equipment room door are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_equipment_room_door, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_equipment_room_door = calculate_distance_min(s_agent, s_equipment_room_door) \n    penalty = -19 if (dist_equipment_room_door < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Fire Station. It must avoid colliding with the fire engine, chief and play sign. The states of the agent, antenna masts, storage racks and safety barriers are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Fire Station. It must avoid colliding with barriers and safety signs. The states of the drone and the multiple obstacles are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Fire Station. It must avoid colliding with Fire Station hanging ladders. Agent's state, target's state and Fire Station hanging ladders's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ladder, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_ladder = calculate_distance_min(s_agent, s_ladder) \n    penalty_collision = -19 if (dist_ladder < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Fire Station. Goal: It must avoid colliding with chief. Assumptions: The states of the drone and the chief's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_chief, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_chief = calculate_distance_min(s_agent, s_chief) \n    penalty_collision = -19 if (dist_chief < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Fire Station. Goal: It must avoid colliding with Fire Station's antenna masts, newspaper stands and barriers. Assumptions: The Agent's state, target's state and antenna masts, newspaper stands and barriers's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Fire Station. Goal: Detect bench, fire hydrant and firefighter and do not collide. Assumptions: The states of the drone, target  bench, fire hydrant and firefighter state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Fire Station. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Recycling Facility. \n Objectives: It must avoid colliding with concrete columns or hazard signs. \n Conditions: The states of the agent, the facility fence, and the hazard signs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Recycling Facility. \n Objectives: Detect waste bin and aviod it. \n Conditions: The states of the agent, the waste bin are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_waste_bin, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_waste_bin = calculate_distance_min(s_agent, s_waste_bin) \n    penalty_collision = -17 if (dist_waste_bin < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Recycling Facility open workspace. Objectives: It must avoid colliding with recycling machinery. Conditions: States of the drone and the recycling machinery are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_recycling_machinery, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_recycling_machinery = calculate_distance_min(s_agent, s_recycling_machinery) \n    penalty_collision = -17 if (dist_recycling_machinery < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a processing bay, the drone must maneuver around fire hose line. All entity states (agent, fire hose line) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_conveyor_belt, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_conveyor_belt = calculate_distance_min(s_agent, s_conveyor_belt) \n    penalty_collision = -17 if (dist_conveyor_belt < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a processing bay. Requirement: Detect the three workers and do not collide with it. Given: The states for the drone and many workers are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a control office during experiments. \n Objectives: So not collide with storage bin. \n Conditions: The states of the agent, the storage bin are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_storage_bin, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_storage_bin = calculate_distance_min(s_agent, s_storage_bin) \n    penalty_collision = -17 if (dist_storage_bin < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Recycling Facility. Objectives: Do not collide hit with liquid container. Conditions: States of the agent, liquid container are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_liquid_container, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_liquid_container = calculate_distance_min(s_agent, s_liquid_container) \n    penalty_collision = -17 if (dist_liquid_container < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a processing bay, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: sorting yard. Avoid: Hitting with forklift. Known: Agent, forklift states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_forklift, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_forklift = calculate_distance_min(s_agent, s_forklift) \n    penalty_collision = -17 if (dist_forklift < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Recycling Facility. Objectives: It must avoid colliding with worker. Conditions: The states of the drone and worker are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_worker, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_worker = calculate_distance_min(s_agent, s_worker) \n    penalty_collision = -19 if (dist_worker < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Recycling Facility. Objectives: Avoid collision with sorting shelves, worker and plastic crates. Conditions: The states of the agent, sorting shelves, worker and plastic crates are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Recycling Facility. Objectives: It must avoid colliding with any compactor truck, supervisor and play sign. Conditions: Agent's, compactor truck, supervisor and play sign states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Recycling Facility. Objectives: Do not collide with any crane. Conditions: The states of the drone and the crane are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_crane, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_crane = calculate_distance_min(s_agent, s_crane) \n    penalty = -19 if (dist_crane < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Recycling Facility and must avoid contact with any storage room door. The states of the drone and the storage room door are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_storage_room_door, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_storage_room_door = calculate_distance_min(s_agent, s_storage_room_door) \n    penalty = -19 if (dist_storage_room_door < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Recycling Facility. It must avoid colliding with the compactor truck, supervisor and play sign. The states of the agent, cranes, storage racks and safety safety safety barriers are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Recycling Facility. It must avoid colliding with safety safety barriers and hazard signs. The states of the drone and the water tuck and forklift are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Recycling Facility. It must avoid colliding with Recycling Facility conveyor belts. Agent's state, target's state and Recycling Facility conveyor belts's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_belt, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_belt = calculate_distance_min(s_agent, s_belt) \n    penalty_collision = -19 if (dist_belt < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Recycling Facility. Goal: It must avoid colliding with supervisor. Assumptions: The states of the drone and the supervisor's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_supervisor, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_supervisor = calculate_distance_min(s_agent, s_supervisor) \n    penalty_collision = -19 if (dist_supervisor < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Recycling Facility. Goal: It must avoid colliding with Recycling Facility's cranes, newspaper stands and safety safety barriers. Assumptions: The Agent's state, target's state and cranes, newspaper stands and safety safety barriers's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Recycling Facility. Goal: Detect bench, waste bin and worker and do not collide. Assumptions: The states of the drone, target  bench, waste bin and worker state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Recycling Facility. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Botanical Garden. \n Objectives: It must avoid colliding with concrete columns or garden signs. \n Conditions: The states of the agent, the garden fence, and the garden signs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Botanical Garden. \n Objectives: Detect garden pot and aviod it. \n Conditions: The states of the agent, the garden pot are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_garden_pot, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_garden_pot = calculate_distance_min(s_agent, s_garden_pot) \n    penalty_collision = -17 if (dist_garden_pot < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through a Botanical Garden. Objectives: It must avoid colliding with fountain. Conditions: States of the drone and the fountain are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_fountain, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_fountain = calculate_distance_min(s_agent, s_fountain) \n    penalty_collision = -17 if (dist_fountain < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a greenhouse, the drone must maneuver around fire hose line. All entity states (agent, fire hose line) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_vine_trellis, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_vine_trellis = calculate_distance_min(s_agent, s_vine_trellis) \n    penalty_collision = -17 if (dist_vine_trellis < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a greenhouse. Requirement: Detect the three statues and do not collide with it. Given: The states for the drone and many statues are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a gazebo during experiments. \n Objectives: So not collide with planter box. \n Conditions: The states of the agent, the planter box are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_planter_box, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_planter_box = calculate_distance_min(s_agent, s_planter_box) \n    penalty_collision = -17 if (dist_planter_box < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Botanical Garden. Objectives: Do not collide hit with pond. Conditions: States of the agent, pond are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_pond, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_pond = calculate_distance_min(s_agent, s_pond) \n    penalty_collision = -17 if (dist_pond < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a greenhouse, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: flower beds. Avoid: Hitting with lawn mower. Known: Agent, lawn mower states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_lawn_mower, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_lawn_mower = calculate_distance_min(s_agent, s_lawn_mower) \n    penalty_collision = -17 if (dist_lawn_mower < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Botanical Garden. Objectives: It must avoid colliding with statue. Conditions: The states of the drone and statue are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_statue, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_statue = calculate_distance_min(s_agent, s_statue) \n    penalty_collision = -19 if (dist_statue < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Botanical Garden. Objectives: Avoid collision with flower stands, statue and rock formations. Conditions: The states of the agent, flower stands, statue and rock formations are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Botanical Garden. Objectives: It must avoid colliding with any garden cart, statue and play sign. Conditions: Agent's, garden cart, statue and play sign states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Botanical Garden. Objectives: Do not collide with any tree. Conditions: The states of the drone and the tree are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_tree, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_tree = calculate_distance_min(s_agent, s_tree) \n    penalty = -19 if (dist_tree < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Botanical Garden and must avoid contact with any stone arch. The states of the drone and the stone arch are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_stone_arch, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_stone_arch = calculate_distance_min(s_agent, s_stone_arch) \n    penalty = -19 if (dist_stone_arch < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Botanical Garden. It must avoid colliding with the garden cart, statue and play sign. The states of the agent, trees, storage racks and safety safety hedges are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Botanical Garden. It must avoid colliding with safety hedges and garden signs. The states of the drone and the water tuck and lawn mower are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Botanical Garden. It must avoid colliding with Botanical Garden vine trelliss. Agent's state, target's state and Botanical Garden bush's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bush, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_bush = calculate_distance_min(s_agent, s_bush) \n    penalty_collision = -19 if (dist_bush < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Botanical Garden. Goal: It must avoid colliding with statue. Assumptions: The states of the drone and the statue's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_statue, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_supervisor = calculate_distance_min(s_agent, s_statue) \n    penalty_collision = -19 if (dist_supervisor < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Botanical Garden. Goal: It must avoid colliding with Botanical Garden's trees, newspaper stands and safety hedges. Assumptions: The Agent's state, target's state and trees, newspaper stands and safety hedges's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Botanical Garden. Goal: Detect bench, garden pot and statue and do not collide. Assumptions: The states of the drone, target  bench, garden pot and statue state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Botanical Garden. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Hotel. \n Objectives: It must avoid colliding with concrete columns or signboards. \n Conditions: The states of the agent, the hotel facade, and the signboards are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Hotel. \n Objectives: Detect luggage cart and aviod it. \n Conditions: The states of the agent, the luggage cart are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_luggage_cart, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_luggage_cart = calculate_distance_min(s_agent, s_luggage_cart) \n    penalty_collision = -17 if (dist_luggage_cart < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Hotel open workspace. Objectives: It must avoid colliding with chandelier. Conditions: States of the drone and the chandelier are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_chandelier, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_chandelier = calculate_distance_min(s_agent, s_chandelier) \n    penalty_collision = -17 if (dist_chandelier < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a reception hall, the drone must maneuver around fire hose line. All entity states (agent, fire hose line) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_decorative_partition, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_decorative_partition = calculate_distance_min(s_agent, s_decorative_partition) \n    penalty_collision = -17 if (dist_decorative_partition < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a reception hall. Requirement: Detect the three statues and do not collide with it. Given: The states for the drone and many statues are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a front desk during experiments. \n Objectives: So not collide with service counter. \n Conditions: The states of the agent, the service counter are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_service_counter, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_service_counter = calculate_distance_min(s_agent, s_service_counter) \n    penalty_collision = -17 if (dist_service_counter < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Hotel. Objectives: Do not collide hit with indoor fountain. Conditions: States of the agent, indoor fountain are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_indoor_fountain, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_indoor_fountain = calculate_distance_min(s_agent, s_indoor_fountain) \n    penalty_collision = -17 if (dist_indoor_fountain < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a reception hall, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: lobby seating. Avoid: Hitting with elevator. Known: Agent, elevator states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_elevator, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_elevator = calculate_distance_min(s_agent, s_elevator) \n    penalty_collision = -17 if (dist_elevator < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Hotel. Objectives: It must avoid colliding with statue. Conditions: The states of the drone and statue are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_statue, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_statue = calculate_distance_min(s_agent, s_statue) \n    penalty_collision = -19 if (dist_statue < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Hotel. Objectives: Avoid collision with brochure stands, statue and decor columns. Conditions: The states of the agent, brochure stands, statue and decor columns are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Hotel. Objectives: It must avoid colliding with any room service trolley, statue and play sign. Conditions: Agent's, room service trolley, statue and play sign states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Hotel. Objectives: Do not collide with any pillar. Conditions: The states of the drone and the pillar are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_pillar, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_pillar = calculate_distance_min(s_agent, s_pillar) \n    penalty = -19 if (dist_pillar < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Hotel and must avoid contact with any archway. The states of the drone and the archway are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_archway, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_archway = calculate_distance_min(s_agent, s_archway) \n    penalty = -19 if (dist_archway < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Hotel. It must avoid colliding with the room service trolley, statue and play sign. The states of the agent, pillars, storage racks and safety safety railings are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Hotel. It must avoid colliding with safety railings and signboards. The states of the drone and the water tuck and elevator are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Hotel. It must avoid colliding with Hotel decorative partitions. Agent's state, target's state and Hotel decorative partitions's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_deco, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_deco = calculate_distance_min(s_agent, s_deco) \n    penalty_collision = -19 if (dist_deco < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Hotel. Goal: It must avoid colliding with entrance. Assumptions: The states of the drone and the entrance's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_entrance, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_entrance = calculate_distance_min(s_agent, s_entrance) \n    penalty_collision = -19 if (dist_entrance < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Hotel. Goal: It must avoid colliding with Hotel's pillars, newspaper stands and safety railings. Assumptions: The Agent's state, target's state and pillars, newspaper stands and safety railings's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Hotel. Goal: Detect bench, luggage cart and statue and do not collide. Assumptions: The states of the drone, target  bench, luggage cart and statue state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Hotel. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -c_1*dist_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Data Center. \n Objectives: It must avoid colliding with concrete columns or warning placards. \n Conditions: The states of the agent, the server room wall, and the warning placards are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Data Center. \n Objectives: Detect server rack and aviod it. \n Conditions: The states of the agent, the server rack are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_server_rack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_server_rack = calculate_distance_min(s_agent, s_server_rack) \n    penalty_collision = -17 if (dist_server_rack < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Data Center open workspace. Objectives: It must avoid colliding with overhead cable tray. Conditions: States of the drone and the overhead cable tray are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_cable_tray, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_cable_tray = calculate_distance_min(s_agent, s_cable_tray) \n    penalty_collision = -17 if (dist_cable_tray < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a data hall, the drone must maneuver around fire hose line. All entity states (agent, containment_panel) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_containment_panel, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_containment_panel = calculate_distance_min(s_agent, s_containment_panel) \n    penalty_collision = -17 if (dist_containment_panel < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a data hall. Requirement: Detect the three computers and do not collide with it. Given: The states for the drone and many computers are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a NOC console during experiments. \n Objectives: So not collide with PDU cabinet. \n Conditions: The states of the agent, the PDU cabinet are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_pdu_cabinet, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_pdu_cabinet = calculate_distance_min(s_agent, s_pdu_cabinet) \n    penalty_collision = -17 if (dist_pdu_cabinet < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Data Center. Objectives: Do not collide hit with CRAC unit. Conditions: States of the agent, CRAC unit are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_crac_unit, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_crac_unit = calculate_distance_min(s_agent, s_crac_unit) \n    penalty_collision = -17 if (dist_crac_unit < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a data hall, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: staging area. Avoid: Hitting with rack_lift. Known: Agent, rack_lift states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_rack_lift, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_rack_lift = calculate_distance_min(s_agent, s_rack_lift) \n    penalty_collision = -17 if (dist_rack_lift < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Data Center. Objectives: It must avoid colliding with ups cabinet. Conditions: The states of the drone and ups cabinet are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ups_cabinet, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_ups_cabinet = calculate_distance_min(s_agent, s_ups_cabinet) \n    penalty_collision = -19 if (dist_ups_cabinet < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Data Center. Objectives: Avoid collision with patch panel racks, switch and busway drops. Conditions: The states of the agent, patch panel racks, switch and busway drops are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Data Center. Objectives: It must avoid colliding with any KVM cart, units and computers. Conditions: Agent's, KVM cart, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Data Center. Objectives: Do not collide with any pillar. Conditions: The states of the drone and the pillar are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_pillar, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_pillar = calculate_distance_min(s_agent, s_pillar) \n    penalty = -19 if (dist_pillar < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Data Center and must avoid contact with any ladder rack. The states of the drone and the ladder rack are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_ladder_rack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_archway = calculate_distance_min(s_agent, s_ladder_rack) \n    penalty = -19 if (dist_archway < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Data Center. It must avoid colliding with the KVM cart, statue and play sign. The states of the agent, pillars, storage racks and safety cable guards are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Data Center. It must avoid colliding with cable guards and warning placards. The states of the drone and the water tuck and rack_lift are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Data Center. It must avoid colliding with Data Center aisle camera. Agent's state, target's state and Data Center aisle camera's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_camera, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_camera = calculate_distance_min(s_agent, s_camera) \n    penalty_collision = -19 if (dist_camera < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Data Center. Goal: It must avoid colliding with power station. Assumptions: The states of the drone and the power station's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_power_station, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_power_station = calculate_distance_min(s_agent, s_power_station) \n    penalty_collision = -19 if (dist_power_station < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Data Center. Goal: It must avoid colliding with Data Center's pillars, tool cabinets and cable guards. Assumptions: The Agent's state, target's state and pillars, tool cabinets and cable guards's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Data Center. Goal: Detect bench, server rack and statue and do not collide. Assumptions: The states of the drone, target  bench, server rack and statue state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Data Center. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Yoga Studio. \n Objectives: It must avoid colliding with concrete columns or yoga mats. \n Conditions: The states of the agent, the studio mirror, and the yoga mats are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Yoga Studio. \n Objectives: Detect yoga block stack and aviod it. \n Conditions: The states of the agent, the yoga block stack are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_block_stack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_block_stack = calculate_distance_min(s_agent, s_block_stack) \n    penalty_collision = -17 if (dist_block_stack < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Yoga Studio open workspace. Objectives: It must avoid colliding with ceiling fan. Conditions: States of the drone and the ceiling fan are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ceiling_fan, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_ceiling_fan = calculate_distance_min(s_agent, s_ceiling_fan) \n    penalty_collision = -17 if (dist_ceiling_fan < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a yoga hall, the drone must maneuver around yoga mat. All entity states (agent, yoga_mat) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_yoga_mat, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_yoga_mat = calculate_distance_min(s_agent, s_yoga_mat) \n    penalty_collision = -17 if (dist_yoga_mat < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a yoga hall. Requirement: Detect the three air purifiers and do not collide with it. Given: The states for the drone and many air purifiers are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a instructor podium during experiments. \n Objectives: So not collide with shelf. \n Conditions: The states of the agent, the shelf are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_shelf, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_shelf = calculate_distance_min(s_agent, s_shelf) \n    penalty_collision = -17 if (dist_shelf < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Yoga Studio. Objectives: Do not collide hit with air purifier. Conditions: States of the agent, air purifier are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_air_purifier, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_air_purifier = calculate_distance_min(s_agent, s_air_purifier) \n    penalty_collision = -17 if (dist_air_purifier < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a yoga hall, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: reception area. Avoid: Hitting with rack lift. Known: Agent, rack lift states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_rack_lift, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_rack_lift = calculate_distance_min(s_agent, s_rack_lift) \n    penalty_collision = -17 if (dist_rack_lift < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Yoga Studio. Objectives: It must avoid colliding with speaker stand. Conditions: The states of the drone and speaker stand are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_speaker_stand, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_speaker_stand = calculate_distance_min(s_agent, s_speaker_stand) \n    penalty_collision = -19 if (dist_speaker_stand < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Yoga Studio. Objectives: Avoid collision with prop shelves, switch and sound system. Conditions: The states of the agent, prop shelves, switch and sound system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Yoga Studio. Objectives: It must avoid colliding with any equipment trolley, units and computers. Conditions: Agent's, equipment trolley, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Yoga Studio. Objectives: Do not collide with any prop shelve. Conditions: The states of the drone and the prop shelve are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_prop_shelve, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_prop_shelve = calculate_distance_min(s_agent, s_prop_shelve) \n    penalty = -19 if (dist_prop_shelve < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Yoga Studio and must avoid contact with any rope hanger. The states of the drone and the rope hanger are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_rope_hanger, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_rope_hanger = calculate_distance_min(s_agent, s_rope_hanger) \n    penalty = -19 if (dist_rope_hanger < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Yoga Studio. It must avoid colliding with the equipment trolley, plant pot and play sign. The states of the agent, prop_shelves, storage racks and safety floor cushions are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Yoga Studio. It must avoid colliding with floor cushions and yoga mats. The states of the drone and the water tuck and rack_lift are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Yoga Studio. It must avoid colliding with Yoga Studio aisle CCTV camera. Agent's state, target's state and Yoga Studio aisle CCTV camera's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_camera, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_camera = calculate_distance_min(s_agent, s_camera) \n    penalty_collision = -19 if (dist_camera < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Yoga Studio. Goal: It must avoid colliding with water dispenser. Assumptions: The states of the drone and the water dispenser's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_water_dispenser, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_water_dispenser = calculate_distance_min(s_agent, s_water_dispenser) \n    penalty_collision = -19 if (dist_water_dispenser < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Yoga Studio. Goal: It must avoid colliding with Yoga Studio's prop_shelves, storage lockers and floor cushions. Assumptions: The Agent's state, target's state and prop_shelves, storage lockers and floor cushions's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Yoga Studio. Goal: Detect bench, yoga block stack and plant pot and do not collide. Assumptions: The states of the drone, target  bench, yoga block stack and plant pot state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Yoga Studio. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Bakery. \n Objectives: It must avoid colliding with concrete columns or bread trays. \n Conditions: The states of the agent, the display glass, and the bread trays are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Bakery. \n Objectives: Detect flour sack stack and aviod it. \n Conditions: The states of the agent, the flour sack stack are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_flour_sack_stack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_flour_sack_stack = calculate_distance_min(s_agent, s_flour_sack_stack) \n    penalty_collision = -17 if (dist_flour_sack_stack < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Bakery open workspace. Objectives: It must avoid colliding with ventilation fan. Conditions: States of the drone and the ventilation fan are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ventilation_fan, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_ventilation_fan = calculate_distance_min(s_agent, s_ventilation_fan) \n    penalty_collision = -17 if (dist_ventilation_fan < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a baking area, the drone must maneuver around bread tray. All entity states (agent, bread_tray) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_bread_tray, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_bread_tray = calculate_distance_min(s_agent, s_bread_tray) \n    penalty_collision = -17 if (dist_bread_tray < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a baking area. Requirement: Detect the three oven units and do not collide with it. Given: The states for the drone and many oven units are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a baker's table during experiments. \n Objectives: So not collide with shelf. \n Conditions: The states of the agent, the ingredient shelf are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_ingredient_shelf, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_ingredient_shelf = calculate_distance_min(s_agent, s_ingredient_shelf) \n    penalty_collision = -17 if (dist_ingredient_shelf < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Bakery. Objectives: Do not collide hit with oven unit. Conditions: States of the agent, oven unit are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_oven_unit, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_oven_unit = calculate_distance_min(s_agent, s_oven_unit) \n    penalty_collision = -17 if (dist_oven_unit < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a baking area, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: cashier counter. Avoid: Hitting with rack lift. Known: Agent, baking trolley states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_baking_trolley, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_baking_trolley = calculate_distance_min(s_agent, s_baking_trolley) \n    penalty_collision = -17 if (dist_baking_trolley < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Bakery. Objectives: It must avoid colliding with stand mixer. Conditions: The states of the drone and stand mixer are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_stand_mixer, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_speaker_stand = calculate_distance_min(s_agent, s_stand_mixer) \n    penalty_collision = -19 if (dist_speaker_stand < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Bakery. Objectives: Avoid collision with utensil racks, switch and dough mixer. Conditions: The states of the agent, utensil racks, switch and dough mixer are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Bakery. Objectives: It must avoid colliding with any baking trolley, units and computers. Conditions: Agent's, baking trolley, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Bakery. Objectives: Do not collide with any prop shelve. Conditions: The states of the drone and the prop shelve are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_utensil_rack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_utensil_rack = calculate_distance_min(s_agent, s_utensil_rack) \n    penalty = -19 if (dist_utensil_rack < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Bakery and must avoid contact with any hanging baskets. The states of the drone and the hanging baskets are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_hanging_basket, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_hanging_basket = calculate_distance_min(s_agent, s_hanging_basket) \n    penalty = -19 if (dist_hanging_basket < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Bakery. It must avoid colliding with the baking trolley, sugar container and play sign. The states of the agent, utensil_racks, storage racks and safety flour bags are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Bakery. It must avoid colliding with flour bags and bread trays. The states of the drone and the water tuck and baking_trolley are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Bakery. It must avoid colliding with Bakery aisle security camera. Agent's state, target's state and Bakery aisle security camera's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_security_camera, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_security_camera = calculate_distance_min(s_agent, s_security_security_camera) \n    penalty_collision = -19 if (dist_security_camera < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Bakery. Goal: It must avoid colliding with water tank. Assumptions: The states of the drone and the water tank's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_water_tank, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_water_tank = calculate_distance_min(s_agent, s_water_tank) \n    penalty_collision = -19 if (dist_water_tank < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Bakery. Goal: It must avoid colliding with Bakery's utensil_racks, cold storage unit and flour bags. Assumptions: The Agent's state, target's state and utensil_racks, cold storage unit and flour bags's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Bakery. Goal: Detect wooden bench, flour sack stack and sugar container and do not collide. Assumptions: The states of the drone, target  wooden bench, flour sack stack and sugar container state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Bakery. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Stadium. \n Objectives: It must avoid colliding with concrete columns or seat rows. \n Conditions: The states of the agent, the scoreboard, and the seat rows are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Stadium. \n Objectives: Detect concession stand and aviod it. \n Conditions: The states of the agent, the concession stand are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_concession_stand, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_concession_stand = calculate_distance_min(s_agent, s_concession_stand) \n    penalty_collision = -17 if (dist_concession_stand < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Stadium open workspace. Objectives: It must avoid colliding with stadium lights. Conditions: States of the drone and the stadium lights are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_stadium_light, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_stadium_light = calculate_distance_min(s_agent, s_stadium_light) \n    penalty_collision = -17 if (dist_stadium_light < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a field area, the drone must maneuver around seat row. All entity states (agent, seat row) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_seat_row, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_seat_row = calculate_distance_min(s_agent, s_seat_row) \n    penalty_collision = -17 if (dist_seat_row < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a field area. Requirement: Detect the three target posts and do not collide with it. Given: The states for the drone and many target posts are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a dugout bench during experiments. \n Objectives: So not collide with shelf. \n Conditions: The states of the agent, the shelf are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_shelf, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_shelf = calculate_distance_min(s_agent, s_shelf) \n    penalty_collision = -17 if (dist_shelf < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Stadium. Objectives: Do not collide hit with target post. Conditions: States of the agent, target post are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_target_post, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_target_post = calculate_distance_min(s_agent, s_target_post) \n    penalty_collision = -17 if (dist_target_post < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a field area, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: ticket counter. Avoid: Hitting with rack lift. Known: Agent, rack lift states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_refreshment_cart, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_refreshment_cart = calculate_distance_min(s_agent, s_refreshment_cart) \n    penalty_collision = -17 if (dist_refreshment_cart < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Stadium. Objectives: It must avoid colliding with PA system. Conditions: The states of the drone and PA system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_pa_system, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_pa_system = calculate_distance_min(s_agent, s_pa_system) \n    penalty_collision = -19 if (dist_pa_system < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Stadium. Objectives: Avoid collision with jersey racks, switch and sound system. Conditions: The states of the agent, jersey racks, switch and sound system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Stadium. Objectives: It must avoid colliding with any refreshment cart, units and computers. Conditions: Agent's, refreshment cart, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Stadium. Objectives: Do not collide with any prop shelve. Conditions: The states of the drone and the prop shelve are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_jersey_rack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_jersey_rack = calculate_distance_min(s_agent, s_jersey_rack) \n    penalty = -19 if (dist_jersey_rack < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Stadium and must avoid contact with any banner hangers. The states of the drone and the banner hangers are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_banner_hanger, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_banner_hanger = calculate_distance_min(s_agent, s_banner_hanger) \n    penalty = -19 if (dist_banner_hanger < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Stadium. It must avoid colliding with the refreshment cart, trophy stand and play sign. The states of the agent, prop_shelves, storage racks and safety cones are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Stadium. It must avoid colliding with cones and seat rows. The states of the drone and the water tuck and refreshment_cart are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Stadium. It must avoid colliding with Stadium aisle CCTV camera. Agent's state, target's state and Stadium aisle CCTV camera's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_camera, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_camera = calculate_distance_min(s_agent, s_camera) \n    penalty_collision = -19 if (dist_camera < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Stadium. Goal: It must avoid colliding with hydration station. Assumptions: The states of the drone and the hydration station's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_hydration_station, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_hydration_station = calculate_distance_min(s_agent, s_hydration_station) \n    penalty_collision = -19 if (dist_hydration_station < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Stadium. Goal: It must avoid colliding with Stadium's prop_shelves, locker room and cones. Assumptions: The Agent's state, target's state and prop_shelves, locker room and cones's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Stadium. Goal: Detect bleacher bench, concession stand and trophy stand and do not collide. Assumptions: The states of the drone, target  bleacher bench, concession stand and trophy stand state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Stadium. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Skatepark. \n Objectives: It must avoid colliding with concrete columns or ramps. \n Conditions: The states of the agent, the graffiti wall, and the ramps are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Skatepark. \n Objectives: Detect half-pipe and aviod it. \n Conditions: The states of the agent, the half-pipe are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_half_pipe, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_half_pipek = calculate_distance_min(s_agent, s_half_pipe) \n    penalty_collision = -17 if (dist_half_pipek < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Skatepark open workspace. Objectives: It must avoid colliding with floodlight. Conditions: States of the drone and the floodlight are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_floodlight, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_floodlight = calculate_distance_min(s_agent, s_floodlight) \n    penalty_collision = -17 if (dist_floodlight < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a skating area, the drone must maneuver around ramp. All entity states (agent, ramp) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_ramp, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_ramp = calculate_distance_min(s_agent, s_ramp) \n    penalty_collision = -17 if (dist_ramp < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a skating area. Requirement: Detect the three funboxs and do not collide with it. Given: The states for the drone and many funboxs are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a grind rail during experiments. \n Objectives: So not collide with skateboard_rack. \n Conditions: The states of the agent, the skateboard_rack are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_skateboard_rack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_skateboard_rack = calculate_distance_min(s_agent, s_skateboard_rack) \n    penalty_collision = -17 if (dist_skateboard_rack < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Skatepark. Objectives: Do not collide hit with funbox. Conditions: States of the agent, funbox are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_funbox, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_funbox = calculate_distance_min(s_agent, s_funbox) \n    penalty_collision = -17 if (dist_funbox < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a skating area, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: snack kiosk. Avoid: Hitting with rack lift. Known: Agent, rack lift states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_gear_cart, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_gear_cart = calculate_distance_min(s_agent, s_gear_cart) \n    penalty_collision = -17 if (dist_gear_cart < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Skatepark. Objectives: It must avoid colliding with announcement system. Conditions: The states of the drone and announcement system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_announcement_system, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_announcement_system = calculate_distance_min(s_agent, s_announcement_system) \n    penalty_collision = -19 if (dist_announcement_system < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Skatepark. Objectives: Avoid collision with helmet stand, switch and speaker system. Conditions: The states of the agent, helmet stand, switch and speaker system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Skatepark. Objectives: It must avoid colliding with any gear cart, units and computers. Conditions: Agent's, gear cart, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Skatepark. Objectives: Do not collide with any prop shelve. Conditions: The states of the drone and the prop shelve are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_helmet_stand, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_helmet_stand = calculate_distance_min(s_agent, s_helmet_stand) \n    penalty = -19 if (dist_helmet_stand < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Skatepark and must avoid contact with any skate banners. The states of the drone and the skate banners are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_skate_banner, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_skate_banner = calculate_distance_min(s_agent, s_skate_banner) \n    penalty = -19 if (dist_skate_banner < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Skatepark. It must avoid colliding with the gear cart, trick podium and play sign. The states of the agent, helmet_stands, storage racks and safety traffic cones are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Skatepark. It must avoid colliding with traffic cones and ramps. The states of the drone and the water tuck and gear_cart are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Skatepark. It must avoid colliding with a child. Agent's state, target's state and child's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_child, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_child = calculate_distance_min(s_agent, s_child) \n    penalty_collision = -19 if (dist_child < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Skatepark. Goal: It must avoid colliding with water fountain. Assumptions: The states of the drone and the water fountain's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_water_fountain, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_water_fountain = calculate_distance_min(s_agent, s_water_fountain) \n    penalty_collision = -19 if (dist_water_fountain < 0.5) else 17 \n    reward_reaching_target = 19 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Skatepark. Goal: It must avoid colliding with Skatepark's helmet_stands, equipment shed and traffic cones. Assumptions: The Agent's state, target's state and helmet_stands, equipment shed and traffic cones's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Skatepark. Goal: Detect seating bench, half-pipe and trick podium and do not collide. Assumptions: The states of the drone, target  seating bench, half-pipe and trick podium state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -19 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Skatepark. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -dist_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Data Center. \n Objectives: It must avoid colliding with concrete columns or warning placards. \n Conditions: The states of the agent, the server room wall, and the warning placards are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Data Center. \n Objectives: Detect server rack and aviod it. \n Conditions: The states of the agent, the server rack are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_server_rack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_server_rack = calculate_distance_min(s_agent, s_server_rack) \n    penalty_collision = -17 if (dist_server_rack < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Data Center open workspace. Objectives: It must avoid colliding with overhead cable tray. Conditions: States of the drone and the overhead cable tray are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_cable_tray, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_cable_tray = calculate_distance_min(s_agent, s_cable_tray) \n    penalty_collision = -17 if (dist_cable_tray < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a data hall, the drone must maneuver around fire hose line. All entity states (agent, containment_panel) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_containment_panel, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_containment_panel = calculate_distance_min(s_agent, s_containment_panel) \n    penalty_collision = -17 if (dist_containment_panel < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a data hall. Requirement: Detect the three computers and do not collide with it. Given: The states for the drone and many computers are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a NOC console during experiments. \n Objectives: So not collide with PDU cabinet. \n Conditions: The states of the agent, the PDU cabinet are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_pdu_cabinet, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_pdu_cabinet = calculate_distance_min(s_agent, s_pdu_cabinet) \n    penalty_collision = -17 if (dist_pdu_cabinet < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Data Center. Objectives: Do not collide hit with CRAC unit. Conditions: States of the agent, CRAC unit are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_crac_unit, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_crac_unit = calculate_distance_min(s_agent, s_crac_unit) \n    penalty_collision = -17 if (dist_crac_unit < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a data hall, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: staging area. Avoid: Hitting with rack_lift. Known: Agent, rack_lift states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_rack_lift, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_rack_lift = calculate_distance_min(s_agent, s_rack_lift) \n    penalty_collision = -17 if (dist_rack_lift < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Data Center. Objectives: It must avoid colliding with ups cabinet. Conditions: The states of the drone and ups cabinet are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ups_cabinet, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_ups_cabinet = calculate_distance_min(s_agent, s_ups_cabinet) \n    penalty_collision = -16 if (dist_ups_cabinet < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Data Center. Objectives: Avoid collision with patch panel racks, switch and busway drops. Conditions: The states of the agent, patch panel racks, switch and busway drops are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Data Center. Objectives: It must avoid colliding with any KVM cart, units and computers. Conditions: Agent's, KVM cart, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Data Center. Objectives: Do not collide with any pillar. Conditions: The states of the drone and the pillar are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_pillar, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_pillar = calculate_distance_min(s_agent, s_pillar) \n    penalty = -16 if (dist_pillar < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Data Center and must avoid contact with any ladder rack. The states of the drone and the ladder rack are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_ladder_rack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_archway = calculate_distance_min(s_agent, s_ladder_rack) \n    penalty = -16 if (dist_archway < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Data Center. It must avoid colliding with the KVM cart, statue and play sign. The states of the agent, pillars, storage racks and safety cable guards are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Data Center. It must avoid colliding with cable guards and warning placards. The states of the drone and the water tuck and rack_lift are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Data Center. It must avoid colliding with Data Center aisle camera. Agent's state, target's state and Data Center aisle camera's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_camera, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_camera = calculate_distance_min(s_agent, s_camera) \n    penalty_collision = -16 if (dist_camera < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Data Center. Goal: It must avoid colliding with power station. Assumptions: The states of the drone and the power station's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_power_station, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_power_station = calculate_distance_min(s_agent, s_power_station) \n    penalty_collision = -16 if (dist_power_station < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Data Center. Goal: It must avoid colliding with Data Center's pillars, tool cabinets and cable guards. Assumptions: The Agent's state, target's state and pillars, tool cabinets and cable guards's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Data Center. Goal: Detect bench, server rack and statue and do not collide. Assumptions: The states of the drone, target  bench, server rack and statue state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Data Center. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Yoga Studio. \n Objectives: It must avoid colliding with concrete columns or yoga mats. \n Conditions: The states of the agent, the studio mirror, and the yoga mats are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Yoga Studio. \n Objectives: Detect yoga block stack and aviod it. \n Conditions: The states of the agent, the yoga block stack are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_block_stack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_block_stack = calculate_distance_min(s_agent, s_block_stack) \n    penalty_collision = -17 if (dist_block_stack < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Yoga Studio open workspace. Objectives: It must avoid colliding with ceiling fan. Conditions: States of the drone and the ceiling fan are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ceiling_fan, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_ceiling_fan = calculate_distance_min(s_agent, s_ceiling_fan) \n    penalty_collision = -17 if (dist_ceiling_fan < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a yoga hall, the drone must maneuver around yoga mat. All entity states (agent, yoga_mat) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_yoga_mat, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_yoga_mat = calculate_distance_min(s_agent, s_yoga_mat) \n    penalty_collision = -17 if (dist_yoga_mat < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a yoga hall. Requirement: Detect the three air purifiers and do not collide with it. Given: The states for the drone and many air purifiers are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a instructor podium during experiments. \n Objectives: So not collide with shelf. \n Conditions: The states of the agent, the shelf are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_shelf, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_shelf = calculate_distance_min(s_agent, s_shelf) \n    penalty_collision = -17 if (dist_shelf < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Yoga Studio. Objectives: Do not collide hit with air purifier. Conditions: States of the agent, air purifier are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_air_purifier, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_air_purifier = calculate_distance_min(s_agent, s_air_purifier) \n    penalty_collision = -17 if (dist_air_purifier < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a yoga hall, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: reception area. Avoid: Hitting with rack lift. Known: Agent, rack lift states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_rack_lift, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_rack_lift = calculate_distance_min(s_agent, s_rack_lift) \n    penalty_collision = -17 if (dist_rack_lift < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Yoga Studio. Objectives: It must avoid colliding with speaker stand. Conditions: The states of the drone and speaker stand are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_speaker_stand, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_speaker_stand = calculate_distance_min(s_agent, s_speaker_stand) \n    penalty_collision = -16 if (dist_speaker_stand < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Yoga Studio. Objectives: Avoid collision with prop shelves, switch and sound system. Conditions: The states of the agent, prop shelves, switch and sound system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Yoga Studio. Objectives: It must avoid colliding with any equipment trolley, units and computers. Conditions: Agent's, equipment trolley, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Yoga Studio. Objectives: Do not collide with any prop shelve. Conditions: The states of the drone and the prop shelve are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_prop_shelve, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_prop_shelve = calculate_distance_min(s_agent, s_prop_shelve) \n    penalty = -16 if (dist_prop_shelve < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Yoga Studio and must avoid contact with any rope hanger. The states of the drone and the rope hanger are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_rope_hanger, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_rope_hanger = calculate_distance_min(s_agent, s_rope_hanger) \n    penalty = -16 if (dist_rope_hanger < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Yoga Studio. It must avoid colliding with the equipment trolley, plant pot and play sign. The states of the agent, prop_shelves, storage racks and safety floor cushions are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Yoga Studio. It must avoid colliding with floor cushions and yoga mats. The states of the drone and the water tuck and rack_lift are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Yoga Studio. It must avoid colliding with Yoga Studio aisle CCTV camera. Agent's state, target's state and Yoga Studio aisle CCTV camera's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_camera, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_camera = calculate_distance_min(s_agent, s_camera) \n    penalty_collision = -16 if (dist_camera < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Yoga Studio. Goal: It must avoid colliding with water dispenser. Assumptions: The states of the drone and the water dispenser's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_water_dispenser, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_water_dispenser = calculate_distance_min(s_agent, s_water_dispenser) \n    penalty_collision = -16 if (dist_water_dispenser < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Yoga Studio. Goal: It must avoid colliding with Yoga Studio's prop_shelves, storage lockers and floor cushions. Assumptions: The Agent's state, target's state and prop_shelves, storage lockers and floor cushions's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Yoga Studio. Goal: Detect bench, yoga block stack and plant pot and do not collide. Assumptions: The states of the drone, target  bench, yoga block stack and plant pot state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Yoga Studio. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Bakery. \n Objectives: It must avoid colliding with concrete columns or bread trays. \n Conditions: The states of the agent, the display glass, and the bread trays are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Bakery. \n Objectives: Detect flour sack stack and aviod it. \n Conditions: The states of the agent, the flour sack stack are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_flour_sack_stack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_flour_sack_stack = calculate_distance_min(s_agent, s_flour_sack_stack) \n    penalty_collision = -17 if (dist_flour_sack_stack < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Bakery open workspace. Objectives: It must avoid colliding with ventilation fan. Conditions: States of the drone and the ventilation fan are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ventilation_fan, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_ventilation_fan = calculate_distance_min(s_agent, s_ventilation_fan) \n    penalty_collision = -17 if (dist_ventilation_fan < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a baking area, the drone must maneuver around bread tray. All entity states (agent, bread_tray) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_bread_tray, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_bread_tray = calculate_distance_min(s_agent, s_bread_tray) \n    penalty_collision = -17 if (dist_bread_tray < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a baking area. Requirement: Detect the three oven units and do not collide with it. Given: The states for the drone and many oven units are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a baker's table during experiments. \n Objectives: So not collide with shelf. \n Conditions: The states of the agent, the ingredient shelf are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_ingredient_shelf, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_ingredient_shelf = calculate_distance_min(s_agent, s_ingredient_shelf) \n    penalty_collision = -17 if (dist_ingredient_shelf < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Bakery. Objectives: Do not collide hit with oven unit. Conditions: States of the agent, oven unit are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_oven_unit, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_oven_unit = calculate_distance_min(s_agent, s_oven_unit) \n    penalty_collision = -17 if (dist_oven_unit < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a baking area, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: cashier counter. Avoid: Hitting with rack lift. Known: Agent, baking trolley states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_baking_trolley, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_baking_trolley = calculate_distance_min(s_agent, s_baking_trolley) \n    penalty_collision = -17 if (dist_baking_trolley < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Bakery. Objectives: It must avoid colliding with stand mixer. Conditions: The states of the drone and stand mixer are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_stand_mixer, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_speaker_stand = calculate_distance_min(s_agent, s_stand_mixer) \n    penalty_collision = -16 if (dist_speaker_stand < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Bakery. Objectives: Avoid collision with utensil racks, switch and dough mixer. Conditions: The states of the agent, utensil racks, switch and dough mixer are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Bakery. Objectives: It must avoid colliding with any baking trolley, units and computers. Conditions: Agent's, baking trolley, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Bakery. Objectives: Do not collide with any prop shelve. Conditions: The states of the drone and the prop shelve are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_utensil_rack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_utensil_rack = calculate_distance_min(s_agent, s_utensil_rack) \n    penalty = -16 if (dist_utensil_rack < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Bakery and must avoid contact with any hanging baskets. The states of the drone and the hanging baskets are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_hanging_basket, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_hanging_basket = calculate_distance_min(s_agent, s_hanging_basket) \n    penalty = -16 if (dist_hanging_basket < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Bakery. It must avoid colliding with the baking trolley, sugar container and play sign. The states of the agent, utensil_racks, storage racks and safety flour bags are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Bakery. It must avoid colliding with flour bags and bread trays. The states of the drone and the water tuck and baking_trolley are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Bakery. It must avoid colliding with Bakery aisle security camera. Agent's state, target's state and Bakery aisle security camera's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_security_camera, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_security_camera = calculate_distance_min(s_agent, s_security_security_camera) \n    penalty_collision = -16 if (dist_security_camera < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Bakery. Goal: It must avoid colliding with water tank. Assumptions: The states of the drone and the water tank's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_water_tank, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_water_tank = calculate_distance_min(s_agent, s_water_tank) \n    penalty_collision = -16 if (dist_water_tank < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Bakery. Goal: It must avoid colliding with Bakery's utensil_racks, cold storage unit and flour bags. Assumptions: The Agent's state, target's state and utensil_racks, cold storage unit and flour bags's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Bakery. Goal: Detect wooden bench, flour sack stack and sugar container and do not collide. Assumptions: The states of the drone, target  wooden bench, flour sack stack and sugar container state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Bakery. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Stadium. \n Objectives: It must avoid colliding with concrete columns or seat rows. \n Conditions: The states of the agent, the scoreboard, and the seat rows are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Stadium. \n Objectives: Detect concession stand and aviod it. \n Conditions: The states of the agent, the concession stand are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_concession_stand, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_concession_stand = calculate_distance_min(s_agent, s_concession_stand) \n    penalty_collision = -17 if (dist_concession_stand < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Stadium open workspace. Objectives: It must avoid colliding with stadium lights. Conditions: States of the drone and the stadium lights are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_stadium_light, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_stadium_light = calculate_distance_min(s_agent, s_stadium_light) \n    penalty_collision = -17 if (dist_stadium_light < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a field area, the drone must maneuver around seat row. All entity states (agent, seat row) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_seat_row, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_seat_row = calculate_distance_min(s_agent, s_seat_row) \n    penalty_collision = -17 if (dist_seat_row < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a field area. Requirement: Detect the three target posts and do not collide with it. Given: The states for the drone and many target posts are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a dugout bench during experiments. \n Objectives: So not collide with shelf. \n Conditions: The states of the agent, the shelf are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_shelf, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_shelf = calculate_distance_min(s_agent, s_shelf) \n    penalty_collision = -17 if (dist_shelf < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Stadium. Objectives: Do not collide hit with target post. Conditions: States of the agent, target post are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_target_post, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_target_post = calculate_distance_min(s_agent, s_target_post) \n    penalty_collision = -17 if (dist_target_post < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a field area, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: ticket counter. Avoid: Hitting with rack lift. Known: Agent, rack lift states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_refreshment_cart, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_refreshment_cart = calculate_distance_min(s_agent, s_refreshment_cart) \n    penalty_collision = -17 if (dist_refreshment_cart < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Stadium. Objectives: It must avoid colliding with PA system. Conditions: The states of the drone and PA system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_pa_system, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_pa_system = calculate_distance_min(s_agent, s_pa_system) \n    penalty_collision = -16 if (dist_pa_system < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Stadium. Objectives: Avoid collision with jersey racks, switch and sound system. Conditions: The states of the agent, jersey racks, switch and sound system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Stadium. Objectives: It must avoid colliding with any refreshment cart, units and computers. Conditions: Agent's, refreshment cart, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Stadium. Objectives: Do not collide with any prop shelve. Conditions: The states of the drone and the prop shelve are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_jersey_rack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_jersey_rack = calculate_distance_min(s_agent, s_jersey_rack) \n    penalty = -16 if (dist_jersey_rack < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Stadium and must avoid contact with any banner hangers. The states of the drone and the banner hangers are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_banner_hanger, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_banner_hanger = calculate_distance_min(s_agent, s_banner_hanger) \n    penalty = -16 if (dist_banner_hanger < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Stadium. It must avoid colliding with the refreshment cart, trophy stand and play sign. The states of the agent, prop_shelves, storage racks and safety cones are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Stadium. It must avoid colliding with cones and seat rows. The states of the drone and the water tuck and refreshment_cart are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Stadium. It must avoid colliding with Stadium aisle CCTV camera. Agent's state, target's state and Stadium aisle CCTV camera's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_camera, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_camera = calculate_distance_min(s_agent, s_camera) \n    penalty_collision = -16 if (dist_camera < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Stadium. Goal: It must avoid colliding with hydration station. Assumptions: The states of the drone and the hydration station's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_hydration_station, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_hydration_station = calculate_distance_min(s_agent, s_hydration_station) \n    penalty_collision = -16 if (dist_hydration_station < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Stadium. Goal: It must avoid colliding with Stadium's prop_shelves, locker room and cones. Assumptions: The Agent's state, target's state and prop_shelves, locker room and cones's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Stadium. Goal: Detect bleacher bench, concession stand and trophy stand and do not collide. Assumptions: The states of the drone, target  bleacher bench, concession stand and trophy stand state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Stadium. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Skatepark. \n Objectives: It must avoid colliding with concrete columns or ramps. \n Conditions: The states of the agent, the graffiti wall, and the ramps are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Skatepark. \n Objectives: Detect half-pipe and aviod it. \n Conditions: The states of the agent, the half-pipe are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_half_pipe, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_half_pipek = calculate_distance_min(s_agent, s_half_pipe) \n    penalty_collision = -17 if (dist_half_pipek < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Skatepark open workspace. Objectives: It must avoid colliding with floodlight. Conditions: States of the drone and the floodlight are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_floodlight, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_floodlight = calculate_distance_min(s_agent, s_floodlight) \n    penalty_collision = -17 if (dist_floodlight < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a skating area, the drone must maneuver around ramp. All entity states (agent, ramp) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_ramp, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_ramp = calculate_distance_min(s_agent, s_ramp) \n    penalty_collision = -17 if (dist_ramp < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a skating area. Requirement: Detect the three funboxs and do not collide with it. Given: The states for the drone and many funboxs are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a grind rail during experiments. \n Objectives: So not collide with skateboard_rack. \n Conditions: The states of the agent, the skateboard_rack are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_skateboard_rack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_skateboard_rack = calculate_distance_min(s_agent, s_skateboard_rack) \n    penalty_collision = -17 if (dist_skateboard_rack < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Skatepark. Objectives: Do not collide hit with funbox. Conditions: States of the agent, funbox are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_funbox, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_funbox = calculate_distance_min(s_agent, s_funbox) \n    penalty_collision = -17 if (dist_funbox < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a skating area, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: snack kiosk. Avoid: Hitting with rack lift. Known: Agent, rack lift states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_gear_cart, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_gear_cart = calculate_distance_min(s_agent, s_gear_cart) \n    penalty_collision = -17 if (dist_gear_cart < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Skatepark. Objectives: It must avoid colliding with announcement system. Conditions: The states of the drone and announcement system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_announcement_system, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_announcement_system = calculate_distance_min(s_agent, s_announcement_system) \n    penalty_collision = -16 if (dist_announcement_system < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Skatepark. Objectives: Avoid collision with helmet stand, switch and speaker system. Conditions: The states of the agent, helmet stand, switch and speaker system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Skatepark. Objectives: It must avoid colliding with any gear cart, units and computers. Conditions: Agent's, gear cart, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Skatepark. Objectives: Do not collide with any prop shelve. Conditions: The states of the drone and the prop shelve are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_helmet_stand, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_helmet_stand = calculate_distance_min(s_agent, s_helmet_stand) \n    penalty = -16 if (dist_helmet_stand < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Skatepark and must avoid contact with any skate banners. The states of the drone and the skate banners are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_skate_banner, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_skate_banner = calculate_distance_min(s_agent, s_skate_banner) \n    penalty = -16 if (dist_skate_banner < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Skatepark. It must avoid colliding with the gear cart, trick podium and play sign. The states of the agent, helmet_stands, storage racks and safety traffic cones are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Skatepark. It must avoid colliding with traffic cones and ramps. The states of the drone and the water tuck and gear_cart are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Skatepark. It must avoid colliding with a child. Agent's state, target's state and child's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_child, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_child = calculate_distance_min(s_agent, s_child) \n    penalty_collision = -16 if (dist_child < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Skatepark. Goal: It must avoid colliding with water fountain. Assumptions: The states of the drone and the water fountain's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_water_fountain, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_water_fountain = calculate_distance_min(s_agent, s_water_fountain) \n    penalty_collision = -16 if (dist_water_fountain < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Skatepark. Goal: It must avoid colliding with Skatepark's helmet_stands, equipment shed and traffic cones. Assumptions: The Agent's state, target's state and helmet_stands, equipment shed and traffic cones's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Skatepark. Goal: Detect seating bench, half-pipe and trick podium and do not collide. Assumptions: The states of the drone, target  seating bench, half-pipe and trick podium state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Skatepark. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -4*dist_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Data Center. \n Objectives: It must avoid colliding with concrete columns or warning placards. \n Conditions: The states of the agent, the server room wall, and the warning placards are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Data Center. \n Objectives: Detect server rack and aviod it. \n Conditions: The states of the agent, the server rack are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_server_rack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_server_rack = calculate_distance_min(s_agent, s_server_rack) \n    penalty_collision = -17 if (dist_server_rack < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Data Center open workspace. Objectives: It must avoid colliding with overhead cable tray. Conditions: States of the drone and the overhead cable tray are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_cable_tray, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_cable_tray = calculate_distance_min(s_agent, s_cable_tray) \n    penalty_collision = -17 if (dist_cable_tray < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a data hall, the drone must maneuver around fire hose line. All entity states (agent, containment_panel) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_containment_panel, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_containment_panel = calculate_distance_min(s_agent, s_containment_panel) \n    penalty_collision = -17 if (dist_containment_panel < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a data hall. Requirement: Detect the three computers and do not collide with it. Given: The states for the drone and many computers are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a NOC console during experiments. \n Objectives: So not collide with PDU cabinet. \n Conditions: The states of the agent, the PDU cabinet are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_pdu_cabinet, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_pdu_cabinet = calculate_distance_min(s_agent, s_pdu_cabinet) \n    penalty_collision = -17 if (dist_pdu_cabinet < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Data Center. Objectives: Do not collide hit with CRAC unit. Conditions: States of the agent, CRAC unit are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_crac_unit, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_crac_unit = calculate_distance_min(s_agent, s_crac_unit) \n    penalty_collision = -17 if (dist_crac_unit < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a data hall, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: staging area. Avoid: Hitting with rack_lift. Known: Agent, rack_lift states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_rack_lift, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_rack_lift = calculate_distance_min(s_agent, s_rack_lift) \n    penalty_collision = -17 if (dist_rack_lift < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Data Center. Objectives: It must avoid colliding with ups cabinet. Conditions: The states of the drone and ups cabinet are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ups_cabinet, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_ups_cabinet = calculate_distance_min(s_agent, s_ups_cabinet) \n    penalty_collision = -16 if (dist_ups_cabinet < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Data Center. Objectives: Avoid collision with patch panel racks, switch and busway drops. Conditions: The states of the agent, patch panel racks, switch and busway drops are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Data Center. Objectives: It must avoid colliding with any KVM cart, units and computers. Conditions: Agent's, KVM cart, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Data Center. Objectives: Do not collide with any pillar. Conditions: The states of the drone and the pillar are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_pillar, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_pillar = calculate_distance_min(s_agent, s_pillar) \n    penalty = -16 if (dist_pillar < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Data Center and must avoid contact with any ladder rack. The states of the drone and the ladder rack are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_ladder_rack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_archway = calculate_distance_min(s_agent, s_ladder_rack) \n    penalty = -16 if (dist_archway < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Data Center. It must avoid colliding with the KVM cart, statue and play sign. The states of the agent, pillars, storage racks and safety cable guards are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Data Center. It must avoid colliding with cable guards and warning placards. The states of the drone and the water tuck and rack_lift are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Data Center. It must avoid colliding with Data Center aisle camera. Agent's state, target's state and Data Center aisle camera's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_camera, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_camera = calculate_distance_min(s_agent, s_camera) \n    penalty_collision = -16 if (dist_camera < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Data Center. Goal: It must avoid colliding with power station. Assumptions: The states of the drone and the power station's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_power_station, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_power_station = calculate_distance_min(s_agent, s_power_station) \n    penalty_collision = -16 if (dist_power_station < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Data Center. Goal: It must avoid colliding with Data Center's pillars, tool cabinets and cable guards. Assumptions: The Agent's state, target's state and pillars, tool cabinets and cable guards's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Data Center. Goal: Detect bench, server rack and statue and do not collide. Assumptions: The states of the drone, target  bench, server rack and statue state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Data Center. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Yoga Studio. \n Objectives: It must avoid colliding with concrete columns or yoga mats. \n Conditions: The states of the agent, the studio mirror, and the yoga mats are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Yoga Studio. \n Objectives: Detect yoga block stack and aviod it. \n Conditions: The states of the agent, the yoga block stack are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_block_stack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_block_stack = calculate_distance_min(s_agent, s_block_stack) \n    penalty_collision = -17 if (dist_block_stack < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Yoga Studio open workspace. Objectives: It must avoid colliding with ceiling fan. Conditions: States of the drone and the ceiling fan are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ceiling_fan, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_ceiling_fan = calculate_distance_min(s_agent, s_ceiling_fan) \n    penalty_collision = -17 if (dist_ceiling_fan < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a yoga hall, the drone must maneuver around yoga mat. All entity states (agent, yoga_mat) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_yoga_mat, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_yoga_mat = calculate_distance_min(s_agent, s_yoga_mat) \n    penalty_collision = -17 if (dist_yoga_mat < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a yoga hall. Requirement: Detect the three air purifiers and do not collide with it. Given: The states for the drone and many air purifiers are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a instructor podium during experiments. \n Objectives: So not collide with shelf. \n Conditions: The states of the agent, the shelf are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_shelf, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_shelf = calculate_distance_min(s_agent, s_shelf) \n    penalty_collision = -17 if (dist_shelf < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Yoga Studio. Objectives: Do not collide hit with air purifier. Conditions: States of the agent, air purifier are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_air_purifier, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_air_purifier = calculate_distance_min(s_agent, s_air_purifier) \n    penalty_collision = -17 if (dist_air_purifier < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a yoga hall, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: reception area. Avoid: Hitting with rack lift. Known: Agent, rack lift states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_rack_lift, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_rack_lift = calculate_distance_min(s_agent, s_rack_lift) \n    penalty_collision = -17 if (dist_rack_lift < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Yoga Studio. Objectives: It must avoid colliding with speaker stand. Conditions: The states of the drone and speaker stand are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_speaker_stand, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_speaker_stand = calculate_distance_min(s_agent, s_speaker_stand) \n    penalty_collision = -16 if (dist_speaker_stand < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Yoga Studio. Objectives: Avoid collision with prop shelves, switch and sound system. Conditions: The states of the agent, prop shelves, switch and sound system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Yoga Studio. Objectives: It must avoid colliding with any equipment trolley, units and computers. Conditions: Agent's, equipment trolley, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Yoga Studio. Objectives: Do not collide with any prop shelve. Conditions: The states of the drone and the prop shelve are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_prop_shelve, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_prop_shelve = calculate_distance_min(s_agent, s_prop_shelve) \n    penalty = -16 if (dist_prop_shelve < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Yoga Studio and must avoid contact with any rope hanger. The states of the drone and the rope hanger are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_rope_hanger, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_rope_hanger = calculate_distance_min(s_agent, s_rope_hanger) \n    penalty = -16 if (dist_rope_hanger < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Yoga Studio. It must avoid colliding with the equipment trolley, plant pot and play sign. The states of the agent, prop_shelves, storage racks and safety floor cushions are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Yoga Studio. It must avoid colliding with floor cushions and yoga mats. The states of the drone and the water tuck and rack_lift are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Yoga Studio. It must avoid colliding with Yoga Studio aisle CCTV camera. Agent's state, target's state and Yoga Studio aisle CCTV camera's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_camera, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_camera = calculate_distance_min(s_agent, s_camera) \n    penalty_collision = -16 if (dist_camera < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Yoga Studio. Goal: It must avoid colliding with water dispenser. Assumptions: The states of the drone and the water dispenser's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_water_dispenser, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_water_dispenser = calculate_distance_min(s_agent, s_water_dispenser) \n    penalty_collision = -16 if (dist_water_dispenser < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Yoga Studio. Goal: It must avoid colliding with Yoga Studio's prop_shelves, storage lockers and floor cushions. Assumptions: The Agent's state, target's state and prop_shelves, storage lockers and floor cushions's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Yoga Studio. Goal: Detect bench, yoga block stack and plant pot and do not collide. Assumptions: The states of the drone, target  bench, yoga block stack and plant pot state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Yoga Studio. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Bakery. \n Objectives: It must avoid colliding with concrete columns or bread trays. \n Conditions: The states of the agent, the display glass, and the bread trays are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Bakery. \n Objectives: Detect flour sack stack and aviod it. \n Conditions: The states of the agent, the flour sack stack are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_flour_sack_stack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_flour_sack_stack = calculate_distance_min(s_agent, s_flour_sack_stack) \n    penalty_collision = -17 if (dist_flour_sack_stack < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Bakery open workspace. Objectives: It must avoid colliding with ventilation fan. Conditions: States of the drone and the ventilation fan are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ventilation_fan, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_ventilation_fan = calculate_distance_min(s_agent, s_ventilation_fan) \n    penalty_collision = -17 if (dist_ventilation_fan < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a baking area, the drone must maneuver around bread tray. All entity states (agent, bread_tray) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_bread_tray, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_bread_tray = calculate_distance_min(s_agent, s_bread_tray) \n    penalty_collision = -17 if (dist_bread_tray < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a baking area. Requirement: Detect the three oven units and do not collide with it. Given: The states for the drone and many oven units are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a baker's table during experiments. \n Objectives: So not collide with shelf. \n Conditions: The states of the agent, the ingredient shelf are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_ingredient_shelf, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_ingredient_shelf = calculate_distance_min(s_agent, s_ingredient_shelf) \n    penalty_collision = -17 if (dist_ingredient_shelf < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Bakery. Objectives: Do not collide hit with oven unit. Conditions: States of the agent, oven unit are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_oven_unit, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_oven_unit = calculate_distance_min(s_agent, s_oven_unit) \n    penalty_collision = -17 if (dist_oven_unit < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a baking area, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: cashier counter. Avoid: Hitting with rack lift. Known: Agent, baking trolley states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_baking_trolley, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_baking_trolley = calculate_distance_min(s_agent, s_baking_trolley) \n    penalty_collision = -17 if (dist_baking_trolley < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Bakery. Objectives: It must avoid colliding with stand mixer. Conditions: The states of the drone and stand mixer are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_stand_mixer, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_speaker_stand = calculate_distance_min(s_agent, s_stand_mixer) \n    penalty_collision = -16 if (dist_speaker_stand < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Bakery. Objectives: Avoid collision with utensil racks, switch and dough mixer. Conditions: The states of the agent, utensil racks, switch and dough mixer are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Bakery. Objectives: It must avoid colliding with any baking trolley, units and computers. Conditions: Agent's, baking trolley, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Bakery. Objectives: Do not collide with any prop shelve. Conditions: The states of the drone and the prop shelve are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_utensil_rack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_utensil_rack = calculate_distance_min(s_agent, s_utensil_rack) \n    penalty = -16 if (dist_utensil_rack < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Bakery and must avoid contact with any hanging baskets. The states of the drone and the hanging baskets are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_hanging_basket, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_hanging_basket = calculate_distance_min(s_agent, s_hanging_basket) \n    penalty = -16 if (dist_hanging_basket < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Bakery. It must avoid colliding with the baking trolley, sugar container and play sign. The states of the agent, utensil_racks, storage racks and safety flour bags are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Bakery. It must avoid colliding with flour bags and bread trays. The states of the drone and the water tuck and baking_trolley are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Bakery. It must avoid colliding with Bakery aisle security camera. Agent's state, target's state and Bakery aisle security camera's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_security_camera, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_security_camera = calculate_distance_min(s_agent, s_security_security_camera) \n    penalty_collision = -16 if (dist_security_camera < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Bakery. Goal: It must avoid colliding with water tank. Assumptions: The states of the drone and the water tank's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_water_tank, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_water_tank = calculate_distance_min(s_agent, s_water_tank) \n    penalty_collision = -16 if (dist_water_tank < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Bakery. Goal: It must avoid colliding with Bakery's utensil_racks, cold storage unit and flour bags. Assumptions: The Agent's state, target's state and utensil_racks, cold storage unit and flour bags's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Bakery. Goal: Detect wooden bench, flour sack stack and sugar container and do not collide. Assumptions: The states of the drone, target  wooden bench, flour sack stack and sugar container state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Bakery. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Stadium. \n Objectives: It must avoid colliding with concrete columns or seat rows. \n Conditions: The states of the agent, the scoreboard, and the seat rows are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Stadium. \n Objectives: Detect concession stand and aviod it. \n Conditions: The states of the agent, the concession stand are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_concession_stand, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_concession_stand = calculate_distance_min(s_agent, s_concession_stand) \n    penalty_collision = -17 if (dist_concession_stand < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Stadium open workspace. Objectives: It must avoid colliding with stadium lights. Conditions: States of the drone and the stadium lights are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_stadium_light, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_stadium_light = calculate_distance_min(s_agent, s_stadium_light) \n    penalty_collision = -17 if (dist_stadium_light < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a field area, the drone must maneuver around seat row. All entity states (agent, seat row) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_seat_row, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_seat_row = calculate_distance_min(s_agent, s_seat_row) \n    penalty_collision = -17 if (dist_seat_row < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a field area. Requirement: Detect the three target posts and do not collide with it. Given: The states for the drone and many target posts are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a dugout bench during experiments. \n Objectives: So not collide with shelf. \n Conditions: The states of the agent, the shelf are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_shelf, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_shelf = calculate_distance_min(s_agent, s_shelf) \n    penalty_collision = -17 if (dist_shelf < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Stadium. Objectives: Do not collide hit with target post. Conditions: States of the agent, target post are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_target_post, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_target_post = calculate_distance_min(s_agent, s_target_post) \n    penalty_collision = -17 if (dist_target_post < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a field area, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: ticket counter. Avoid: Hitting with rack lift. Known: Agent, rack lift states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_refreshment_cart, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_refreshment_cart = calculate_distance_min(s_agent, s_refreshment_cart) \n    penalty_collision = -17 if (dist_refreshment_cart < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Stadium. Objectives: It must avoid colliding with PA system. Conditions: The states of the drone and PA system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_pa_system, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_pa_system = calculate_distance_min(s_agent, s_pa_system) \n    penalty_collision = -16 if (dist_pa_system < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Stadium. Objectives: Avoid collision with jersey racks, switch and sound system. Conditions: The states of the agent, jersey racks, switch and sound system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Stadium. Objectives: It must avoid colliding with any refreshment cart, units and computers. Conditions: Agent's, refreshment cart, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Stadium. Objectives: Do not collide with any prop shelve. Conditions: The states of the drone and the prop shelve are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_jersey_rack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_jersey_rack = calculate_distance_min(s_agent, s_jersey_rack) \n    penalty = -16 if (dist_jersey_rack < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Stadium and must avoid contact with any banner hangers. The states of the drone and the banner hangers are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_banner_hanger, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_banner_hanger = calculate_distance_min(s_agent, s_banner_hanger) \n    penalty = -16 if (dist_banner_hanger < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Stadium. It must avoid colliding with the refreshment cart, trophy stand and play sign. The states of the agent, prop_shelves, storage racks and safety cones are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Stadium. It must avoid colliding with cones and seat rows. The states of the drone and the water tuck and refreshment_cart are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Stadium. It must avoid colliding with Stadium aisle CCTV camera. Agent's state, target's state and Stadium aisle CCTV camera's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_camera, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_camera = calculate_distance_min(s_agent, s_camera) \n    penalty_collision = -16 if (dist_camera < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Stadium. Goal: It must avoid colliding with hydration station. Assumptions: The states of the drone and the hydration station's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_hydration_station, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_hydration_station = calculate_distance_min(s_agent, s_hydration_station) \n    penalty_collision = -16 if (dist_hydration_station < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Stadium. Goal: It must avoid colliding with Stadium's prop_shelves, locker room and cones. Assumptions: The Agent's state, target's state and prop_shelves, locker room and cones's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Stadium. Goal: Detect bleacher bench, concession stand and trophy stand and do not collide. Assumptions: The states of the drone, target  bleacher bench, concession stand and trophy stand state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Stadium. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Skatepark. \n Objectives: It must avoid colliding with concrete columns or ramps. \n Conditions: The states of the agent, the graffiti wall, and the ramps are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Skatepark. \n Objectives: Detect half-pipe and aviod it. \n Conditions: The states of the agent, the half-pipe are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_half_pipe, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_half_pipek = calculate_distance_min(s_agent, s_half_pipe) \n    penalty_collision = -17 if (dist_half_pipek < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Skatepark open workspace. Objectives: It must avoid colliding with floodlight. Conditions: States of the drone and the floodlight are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_floodlight, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_floodlight = calculate_distance_min(s_agent, s_floodlight) \n    penalty_collision = -17 if (dist_floodlight < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a skating area, the drone must maneuver around ramp. All entity states (agent, ramp) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_ramp, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_ramp = calculate_distance_min(s_agent, s_ramp) \n    penalty_collision = -17 if (dist_ramp < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a skating area. Requirement: Detect the three funboxs and do not collide with it. Given: The states for the drone and many funboxs are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a grind rail during experiments. \n Objectives: So not collide with skateboard_rack. \n Conditions: The states of the agent, the skateboard_rack are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_skateboard_rack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_skateboard_rack = calculate_distance_min(s_agent, s_skateboard_rack) \n    penalty_collision = -17 if (dist_skateboard_rack < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Skatepark. Objectives: Do not collide hit with funbox. Conditions: States of the agent, funbox are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_funbox, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_funbox = calculate_distance_min(s_agent, s_funbox) \n    penalty_collision = -17 if (dist_funbox < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a skating area, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: snack kiosk. Avoid: Hitting with rack lift. Known: Agent, rack lift states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_gear_cart, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_gear_cart = calculate_distance_min(s_agent, s_gear_cart) \n    penalty_collision = -17 if (dist_gear_cart < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Skatepark. Objectives: It must avoid colliding with announcement system. Conditions: The states of the drone and announcement system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_announcement_system, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_announcement_system = calculate_distance_min(s_agent, s_announcement_system) \n    penalty_collision = -16 if (dist_announcement_system < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Skatepark. Objectives: Avoid collision with helmet stand, switch and speaker system. Conditions: The states of the agent, helmet stand, switch and speaker system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Skatepark. Objectives: It must avoid colliding with any gear cart, units and computers. Conditions: Agent's, gear cart, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Skatepark. Objectives: Do not collide with any prop shelve. Conditions: The states of the drone and the prop shelve are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_helmet_stand, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_helmet_stand = calculate_distance_min(s_agent, s_helmet_stand) \n    penalty = -16 if (dist_helmet_stand < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Skatepark and must avoid contact with any skate banners. The states of the drone and the skate banners are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_skate_banner, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_skate_banner = calculate_distance_min(s_agent, s_skate_banner) \n    penalty = -16 if (dist_skate_banner < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Skatepark. It must avoid colliding with the gear cart, trick podium and play sign. The states of the agent, helmet_stands, storage racks and safety traffic cones are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Skatepark. It must avoid colliding with traffic cones and ramps. The states of the drone and the water tuck and gear_cart are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Skatepark. It must avoid colliding with a child. Agent's state, target's state and child's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_child, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_child = calculate_distance_min(s_agent, s_child) \n    penalty_collision = -16 if (dist_child < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Skatepark. Goal: It must avoid colliding with water fountain. Assumptions: The states of the drone and the water fountain's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_water_fountain, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_water_fountain = calculate_distance_min(s_agent, s_water_fountain) \n    penalty_collision = -16 if (dist_water_fountain < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Skatepark. Goal: It must avoid colliding with Skatepark's helmet_stands, equipment shed and traffic cones. Assumptions: The Agent's state, target's state and helmet_stands, equipment shed and traffic cones's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Skatepark. Goal: Detect seating bench, half-pipe and trick podium and do not collide. Assumptions: The states of the drone, target  seating bench, half-pipe and trick podium state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Skatepark. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -2**dist_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Data Center. \n Objectives: It must avoid colliding with concrete columns or warning placards. \n Conditions: The states of the agent, the server room wall, and the warning placards are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Data Center. \n Objectives: Detect server rack and aviod it. \n Conditions: The states of the agent, the server rack are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_server_rack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_server_rack = calculate_distance_min(s_agent, s_server_rack) \n    penalty_collision = -17 if (dist_server_rack < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Data Center open workspace. Objectives: It must avoid colliding with overhead cable tray. Conditions: States of the drone and the overhead cable tray are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_cable_tray, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_cable_tray = calculate_distance_min(s_agent, s_cable_tray) \n    penalty_collision = -17 if (dist_cable_tray < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a data hall, the drone must maneuver around fire hose line. All entity states (agent, containment_panel) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_containment_panel, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_containment_panel = calculate_distance_min(s_agent, s_containment_panel) \n    penalty_collision = -17 if (dist_containment_panel < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a data hall. Requirement: Detect the three computers and do not collide with it. Given: The states for the drone and many computers are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a NOC console during experiments. \n Objectives: So not collide with PDU cabinet. \n Conditions: The states of the agent, the PDU cabinet are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_pdu_cabinet, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_pdu_cabinet = calculate_distance_min(s_agent, s_pdu_cabinet) \n    penalty_collision = -17 if (dist_pdu_cabinet < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Data Center. Objectives: Do not collide hit with CRAC unit. Conditions: States of the agent, CRAC unit are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_crac_unit, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_crac_unit = calculate_distance_min(s_agent, s_crac_unit) \n    penalty_collision = -17 if (dist_crac_unit < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a data hall, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: staging area. Avoid: Hitting with rack_lift. Known: Agent, rack_lift states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_rack_lift, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_rack_lift = calculate_distance_min(s_agent, s_rack_lift) \n    penalty_collision = -17 if (dist_rack_lift < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Data Center. Objectives: It must avoid colliding with ups cabinet. Conditions: The states of the drone and ups cabinet are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ups_cabinet, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_ups_cabinet = calculate_distance_min(s_agent, s_ups_cabinet) \n    penalty_collision = -16 if (dist_ups_cabinet < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Data Center. Objectives: Avoid collision with patch panel racks, switch and busway drops. Conditions: The states of the agent, patch panel racks, switch and busway drops are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Data Center. Objectives: It must avoid colliding with any KVM cart, units and computers. Conditions: Agent's, KVM cart, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Data Center. Objectives: Do not collide with any pillar. Conditions: The states of the drone and the pillar are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_pillar, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_pillar = calculate_distance_min(s_agent, s_pillar) \n    penalty = -16 if (dist_pillar < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Data Center and must avoid contact with any ladder rack. The states of the drone and the ladder rack are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_ladder_rack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_archway = calculate_distance_min(s_agent, s_ladder_rack) \n    penalty = -16 if (dist_archway < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Data Center. It must avoid colliding with the KVM cart, statue and play sign. The states of the agent, pillars, storage racks and safety cable guards are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Data Center. It must avoid colliding with cable guards and warning placards. The states of the drone and the water tuck and rack_lift are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Data Center. It must avoid colliding with Data Center aisle camera. Agent's state, target's state and Data Center aisle camera's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_camera, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_camera = calculate_distance_min(s_agent, s_camera) \n    penalty_collision = -16 if (dist_camera < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Data Center. Goal: It must avoid colliding with power station. Assumptions: The states of the drone and the power station's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_power_station, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_power_station = calculate_distance_min(s_agent, s_power_station) \n    penalty_collision = -16 if (dist_power_station < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Data Center. Goal: It must avoid colliding with Data Center's pillars, tool cabinets and cable guards. Assumptions: The Agent's state, target's state and pillars, tool cabinets and cable guards's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Data Center. Goal: Detect bench, server rack and statue and do not collide. Assumptions: The states of the drone, target  bench, server rack and statue state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Data Center. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Yoga Studio. \n Objectives: It must avoid colliding with concrete columns or yoga mats. \n Conditions: The states of the agent, the studio mirror, and the yoga mats are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Yoga Studio. \n Objectives: Detect yoga block stack and aviod it. \n Conditions: The states of the agent, the yoga block stack are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_block_stack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_block_stack = calculate_distance_min(s_agent, s_block_stack) \n    penalty_collision = -17 if (dist_block_stack < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Yoga Studio open workspace. Objectives: It must avoid colliding with ceiling fan. Conditions: States of the drone and the ceiling fan are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ceiling_fan, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_ceiling_fan = calculate_distance_min(s_agent, s_ceiling_fan) \n    penalty_collision = -17 if (dist_ceiling_fan < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a yoga hall, the drone must maneuver around yoga mat. All entity states (agent, yoga_mat) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_yoga_mat, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_yoga_mat = calculate_distance_min(s_agent, s_yoga_mat) \n    penalty_collision = -17 if (dist_yoga_mat < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a yoga hall. Requirement: Detect the three air purifiers and do not collide with it. Given: The states for the drone and many air purifiers are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a instructor podium during experiments. \n Objectives: So not collide with shelf. \n Conditions: The states of the agent, the shelf are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_shelf, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_shelf = calculate_distance_min(s_agent, s_shelf) \n    penalty_collision = -17 if (dist_shelf < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Yoga Studio. Objectives: Do not collide hit with air purifier. Conditions: States of the agent, air purifier are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_air_purifier, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_air_purifier = calculate_distance_min(s_agent, s_air_purifier) \n    penalty_collision = -17 if (dist_air_purifier < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a yoga hall, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: reception area. Avoid: Hitting with rack lift. Known: Agent, rack lift states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_rack_lift, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_rack_lift = calculate_distance_min(s_agent, s_rack_lift) \n    penalty_collision = -17 if (dist_rack_lift < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Yoga Studio. Objectives: It must avoid colliding with speaker stand. Conditions: The states of the drone and speaker stand are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_speaker_stand, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_speaker_stand = calculate_distance_min(s_agent, s_speaker_stand) \n    penalty_collision = -16 if (dist_speaker_stand < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Yoga Studio. Objectives: Avoid collision with prop shelves, switch and sound system. Conditions: The states of the agent, prop shelves, switch and sound system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Yoga Studio. Objectives: It must avoid colliding with any equipment trolley, units and computers. Conditions: Agent's, equipment trolley, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Yoga Studio. Objectives: Do not collide with any prop shelve. Conditions: The states of the drone and the prop shelve are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_prop_shelve, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_prop_shelve = calculate_distance_min(s_agent, s_prop_shelve) \n    penalty = -16 if (dist_prop_shelve < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Yoga Studio and must avoid contact with any rope hanger. The states of the drone and the rope hanger are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_rope_hanger, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_rope_hanger = calculate_distance_min(s_agent, s_rope_hanger) \n    penalty = -16 if (dist_rope_hanger < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Yoga Studio. It must avoid colliding with the equipment trolley, plant pot and play sign. The states of the agent, prop_shelves, storage racks and safety floor cushions are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Yoga Studio. It must avoid colliding with floor cushions and yoga mats. The states of the drone and the water tuck and rack_lift are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Yoga Studio. It must avoid colliding with Yoga Studio aisle CCTV camera. Agent's state, target's state and Yoga Studio aisle CCTV camera's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_camera, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_camera = calculate_distance_min(s_agent, s_camera) \n    penalty_collision = -16 if (dist_camera < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Yoga Studio. Goal: It must avoid colliding with water dispenser. Assumptions: The states of the drone and the water dispenser's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_water_dispenser, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_water_dispenser = calculate_distance_min(s_agent, s_water_dispenser) \n    penalty_collision = -16 if (dist_water_dispenser < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Yoga Studio. Goal: It must avoid colliding with Yoga Studio's prop_shelves, storage lockers and floor cushions. Assumptions: The Agent's state, target's state and prop_shelves, storage lockers and floor cushions's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Yoga Studio. Goal: Detect bench, yoga block stack and plant pot and do not collide. Assumptions: The states of the drone, target  bench, yoga block stack and plant pot state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Yoga Studio. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Bakery. \n Objectives: It must avoid colliding with concrete columns or bread trays. \n Conditions: The states of the agent, the display glass, and the bread trays are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Bakery. \n Objectives: Detect flour sack stack and aviod it. \n Conditions: The states of the agent, the flour sack stack are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_flour_sack_stack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_flour_sack_stack = calculate_distance_min(s_agent, s_flour_sack_stack) \n    penalty_collision = -17 if (dist_flour_sack_stack < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Bakery open workspace. Objectives: It must avoid colliding with ventilation fan. Conditions: States of the drone and the ventilation fan are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ventilation_fan, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_ventilation_fan = calculate_distance_min(s_agent, s_ventilation_fan) \n    penalty_collision = -17 if (dist_ventilation_fan < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a baking area, the drone must maneuver around bread tray. All entity states (agent, bread_tray) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_bread_tray, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_bread_tray = calculate_distance_min(s_agent, s_bread_tray) \n    penalty_collision = -17 if (dist_bread_tray < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a baking area. Requirement: Detect the three oven units and do not collide with it. Given: The states for the drone and many oven units are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a baker's table during experiments. \n Objectives: So not collide with shelf. \n Conditions: The states of the agent, the ingredient shelf are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_ingredient_shelf, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_ingredient_shelf = calculate_distance_min(s_agent, s_ingredient_shelf) \n    penalty_collision = -17 if (dist_ingredient_shelf < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Bakery. Objectives: Do not collide hit with oven unit. Conditions: States of the agent, oven unit are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_oven_unit, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_oven_unit = calculate_distance_min(s_agent, s_oven_unit) \n    penalty_collision = -17 if (dist_oven_unit < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a baking area, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: cashier counter. Avoid: Hitting with rack lift. Known: Agent, baking trolley states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_baking_trolley, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_baking_trolley = calculate_distance_min(s_agent, s_baking_trolley) \n    penalty_collision = -17 if (dist_baking_trolley < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Bakery. Objectives: It must avoid colliding with stand mixer. Conditions: The states of the drone and stand mixer are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_stand_mixer, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_speaker_stand = calculate_distance_min(s_agent, s_stand_mixer) \n    penalty_collision = -16 if (dist_speaker_stand < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Bakery. Objectives: Avoid collision with utensil racks, switch and dough mixer. Conditions: The states of the agent, utensil racks, switch and dough mixer are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Bakery. Objectives: It must avoid colliding with any baking trolley, units and computers. Conditions: Agent's, baking trolley, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Bakery. Objectives: Do not collide with any prop shelve. Conditions: The states of the drone and the prop shelve are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_utensil_rack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_utensil_rack = calculate_distance_min(s_agent, s_utensil_rack) \n    penalty = -16 if (dist_utensil_rack < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Bakery and must avoid contact with any hanging baskets. The states of the drone and the hanging baskets are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_hanging_basket, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_hanging_basket = calculate_distance_min(s_agent, s_hanging_basket) \n    penalty = -16 if (dist_hanging_basket < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Bakery. It must avoid colliding with the baking trolley, sugar container and play sign. The states of the agent, utensil_racks, storage racks and safety flour bags are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Bakery. It must avoid colliding with flour bags and bread trays. The states of the drone and the water tuck and baking_trolley are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Bakery. It must avoid colliding with Bakery aisle security camera. Agent's state, target's state and Bakery aisle security camera's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_security_camera, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_security_camera = calculate_distance_min(s_agent, s_security_security_camera) \n    penalty_collision = -16 if (dist_security_camera < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Bakery. Goal: It must avoid colliding with water tank. Assumptions: The states of the drone and the water tank's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_water_tank, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_water_tank = calculate_distance_min(s_agent, s_water_tank) \n    penalty_collision = -16 if (dist_water_tank < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Bakery. Goal: It must avoid colliding with Bakery's utensil_racks, cold storage unit and flour bags. Assumptions: The Agent's state, target's state and utensil_racks, cold storage unit and flour bags's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Bakery. Goal: Detect wooden bench, flour sack stack and sugar container and do not collide. Assumptions: The states of the drone, target  wooden bench, flour sack stack and sugar container state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Bakery. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Stadium. \n Objectives: It must avoid colliding with concrete columns or seat rows. \n Conditions: The states of the agent, the scoreboard, and the seat rows are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Stadium. \n Objectives: Detect concession stand and aviod it. \n Conditions: The states of the agent, the concession stand are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_concession_stand, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_concession_stand = calculate_distance_min(s_agent, s_concession_stand) \n    penalty_collision = -17 if (dist_concession_stand < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Stadium open workspace. Objectives: It must avoid colliding with stadium lights. Conditions: States of the drone and the stadium lights are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_stadium_light, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_stadium_light = calculate_distance_min(s_agent, s_stadium_light) \n    penalty_collision = -17 if (dist_stadium_light < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a field area, the drone must maneuver around seat row. All entity states (agent, seat row) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_seat_row, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_seat_row = calculate_distance_min(s_agent, s_seat_row) \n    penalty_collision = -17 if (dist_seat_row < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a field area. Requirement: Detect the three target posts and do not collide with it. Given: The states for the drone and many target posts are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a dugout bench during experiments. \n Objectives: So not collide with shelf. \n Conditions: The states of the agent, the shelf are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_shelf, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_shelf = calculate_distance_min(s_agent, s_shelf) \n    penalty_collision = -17 if (dist_shelf < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Stadium. Objectives: Do not collide hit with target post. Conditions: States of the agent, target post are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_target_post, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_target_post = calculate_distance_min(s_agent, s_target_post) \n    penalty_collision = -17 if (dist_target_post < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a field area, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: ticket counter. Avoid: Hitting with rack lift. Known: Agent, rack lift states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_refreshment_cart, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_refreshment_cart = calculate_distance_min(s_agent, s_refreshment_cart) \n    penalty_collision = -17 if (dist_refreshment_cart < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Stadium. Objectives: It must avoid colliding with PA system. Conditions: The states of the drone and PA system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_pa_system, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_pa_system = calculate_distance_min(s_agent, s_pa_system) \n    penalty_collision = -16 if (dist_pa_system < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Stadium. Objectives: Avoid collision with jersey racks, switch and sound system. Conditions: The states of the agent, jersey racks, switch and sound system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Stadium. Objectives: It must avoid colliding with any refreshment cart, units and computers. Conditions: Agent's, refreshment cart, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Stadium. Objectives: Do not collide with any prop shelve. Conditions: The states of the drone and the prop shelve are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_jersey_rack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_jersey_rack = calculate_distance_min(s_agent, s_jersey_rack) \n    penalty = -16 if (dist_jersey_rack < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Stadium and must avoid contact with any banner hangers. The states of the drone and the banner hangers are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_banner_hanger, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_banner_hanger = calculate_distance_min(s_agent, s_banner_hanger) \n    penalty = -16 if (dist_banner_hanger < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Stadium. It must avoid colliding with the refreshment cart, trophy stand and play sign. The states of the agent, prop_shelves, storage racks and safety cones are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Stadium. It must avoid colliding with cones and seat rows. The states of the drone and the water tuck and refreshment_cart are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Stadium. It must avoid colliding with Stadium aisle CCTV camera. Agent's state, target's state and Stadium aisle CCTV camera's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_camera, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_camera = calculate_distance_min(s_agent, s_camera) \n    penalty_collision = -16 if (dist_camera < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Stadium. Goal: It must avoid colliding with hydration station. Assumptions: The states of the drone and the hydration station's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_hydration_station, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_hydration_station = calculate_distance_min(s_agent, s_hydration_station) \n    penalty_collision = -16 if (dist_hydration_station < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Stadium. Goal: It must avoid colliding with Stadium's prop_shelves, locker room and cones. Assumptions: The Agent's state, target's state and prop_shelves, locker room and cones's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Stadium. Goal: Detect bleacher bench, concession stand and trophy stand and do not collide. Assumptions: The states of the drone, target  bleacher bench, concession stand and trophy stand state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Stadium. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Skatepark. \n Objectives: It must avoid colliding with concrete columns or ramps. \n Conditions: The states of the agent, the graffiti wall, and the ramps are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Skatepark. \n Objectives: Detect half-pipe and aviod it. \n Conditions: The states of the agent, the half-pipe are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_half_pipe, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_half_pipek = calculate_distance_min(s_agent, s_half_pipe) \n    penalty_collision = -17 if (dist_half_pipek < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Skatepark open workspace. Objectives: It must avoid colliding with floodlight. Conditions: States of the drone and the floodlight are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_floodlight, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_floodlight = calculate_distance_min(s_agent, s_floodlight) \n    penalty_collision = -17 if (dist_floodlight < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a skating area, the drone must maneuver around ramp. All entity states (agent, ramp) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_ramp, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_ramp = calculate_distance_min(s_agent, s_ramp) \n    penalty_collision = -17 if (dist_ramp < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a skating area. Requirement: Detect the three funboxs and do not collide with it. Given: The states for the drone and many funboxs are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a grind rail during experiments. \n Objectives: So not collide with skateboard_rack. \n Conditions: The states of the agent, the skateboard_rack are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_skateboard_rack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_skateboard_rack = calculate_distance_min(s_agent, s_skateboard_rack) \n    penalty_collision = -17 if (dist_skateboard_rack < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Skatepark. Objectives: Do not collide hit with funbox. Conditions: States of the agent, funbox are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_funbox, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_funbox = calculate_distance_min(s_agent, s_funbox) \n    penalty_collision = -17 if (dist_funbox < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a skating area, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: snack kiosk. Avoid: Hitting with rack lift. Known: Agent, rack lift states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_gear_cart, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_gear_cart = calculate_distance_min(s_agent, s_gear_cart) \n    penalty_collision = -17 if (dist_gear_cart < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Skatepark. Objectives: It must avoid colliding with announcement system. Conditions: The states of the drone and announcement system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_announcement_system, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_announcement_system = calculate_distance_min(s_agent, s_announcement_system) \n    penalty_collision = -16 if (dist_announcement_system < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Skatepark. Objectives: Avoid collision with helmet stand, switch and speaker system. Conditions: The states of the agent, helmet stand, switch and speaker system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Skatepark. Objectives: It must avoid colliding with any gear cart, units and computers. Conditions: Agent's, gear cart, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Skatepark. Objectives: Do not collide with any prop shelve. Conditions: The states of the drone and the prop shelve are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_helmet_stand, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_helmet_stand = calculate_distance_min(s_agent, s_helmet_stand) \n    penalty = -16 if (dist_helmet_stand < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Skatepark and must avoid contact with any skate banners. The states of the drone and the skate banners are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_skate_banner, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_skate_banner = calculate_distance_min(s_agent, s_skate_banner) \n    penalty = -16 if (dist_skate_banner < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Skatepark. It must avoid colliding with the gear cart, trick podium and play sign. The states of the agent, helmet_stands, storage racks and safety traffic cones are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Skatepark. It must avoid colliding with traffic cones and ramps. The states of the drone and the water tuck and gear_cart are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Skatepark. It must avoid colliding with a child. Agent's state, target's state and child's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_child, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_child = calculate_distance_min(s_agent, s_child) \n    penalty_collision = -16 if (dist_child < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Skatepark. Goal: It must avoid colliding with water fountain. Assumptions: The states of the drone and the water fountain's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_water_fountain, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_water_fountain = calculate_distance_min(s_agent, s_water_fountain) \n    penalty_collision = -16 if (dist_water_fountain < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Skatepark. Goal: It must avoid colliding with Skatepark's helmet_stands, equipment shed and traffic cones. Assumptions: The Agent's state, target's state and helmet_stands, equipment shed and traffic cones's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Skatepark. Goal: Detect seating bench, half-pipe and trick podium and do not collide. Assumptions: The states of the drone, target  seating bench, half-pipe and trick podium state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Skatepark. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -k*dist_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Data Center. \n Objectives: It must avoid colliding with concrete columns or warning placards. \n Conditions: The states of the agent, the server room wall, and the warning placards are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Data Center. \n Objectives: Detect server rack and aviod it. \n Conditions: The states of the agent, the server rack are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_server_rack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_server_rack = calculate_distance_min(s_agent, s_server_rack) \n    penalty_collision = -17 if (dist_server_rack < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Data Center open workspace. Objectives: It must avoid colliding with overhead cable tray. Conditions: States of the drone and the overhead cable tray are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_cable_tray, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_cable_tray = calculate_distance_min(s_agent, s_cable_tray) \n    penalty_collision = -17 if (dist_cable_tray < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a data hall, the drone must maneuver around fire hose line. All entity states (agent, containment_panel) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_containment_panel, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_containment_panel = calculate_distance_min(s_agent, s_containment_panel) \n    penalty_collision = -17 if (dist_containment_panel < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a data hall. Requirement: Detect the three computers and do not collide with it. Given: The states for the drone and many computers are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a NOC console during experiments. \n Objectives: So not collide with PDU cabinet. \n Conditions: The states of the agent, the PDU cabinet are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_pdu_cabinet, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_pdu_cabinet = calculate_distance_min(s_agent, s_pdu_cabinet) \n    penalty_collision = -17 if (dist_pdu_cabinet < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Data Center. Objectives: Do not collide hit with CRAC unit. Conditions: States of the agent, CRAC unit are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_crac_unit, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_crac_unit = calculate_distance_min(s_agent, s_crac_unit) \n    penalty_collision = -17 if (dist_crac_unit < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a data hall, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: staging area. Avoid: Hitting with rack_lift. Known: Agent, rack_lift states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_rack_lift, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_rack_lift = calculate_distance_min(s_agent, s_rack_lift) \n    penalty_collision = -17 if (dist_rack_lift < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Data Center. Objectives: It must avoid colliding with ups cabinet. Conditions: The states of the drone and ups cabinet are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ups_cabinet, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_ups_cabinet = calculate_distance_min(s_agent, s_ups_cabinet) \n    penalty_collision = -16 if (dist_ups_cabinet < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Data Center. Objectives: Avoid collision with patch panel racks, switch and busway drops. Conditions: The states of the agent, patch panel racks, switch and busway drops are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Data Center. Objectives: It must avoid colliding with any KVM cart, units and computers. Conditions: Agent's, KVM cart, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Data Center. Objectives: Do not collide with any pillar. Conditions: The states of the drone and the pillar are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_pillar, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_pillar = calculate_distance_min(s_agent, s_pillar) \n    penalty = -16 if (dist_pillar < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Data Center and must avoid contact with any ladder rack. The states of the drone and the ladder rack are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_ladder_rack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_archway = calculate_distance_min(s_agent, s_ladder_rack) \n    penalty = -16 if (dist_archway < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Data Center. It must avoid colliding with the KVM cart, statue and play sign. The states of the agent, pillars, storage racks and safety cable guards are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Data Center. It must avoid colliding with cable guards and warning placards. The states of the drone and the water tuck and rack_lift are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Data Center. It must avoid colliding with Data Center aisle camera. Agent's state, target's state and Data Center aisle camera's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_camera, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_camera = calculate_distance_min(s_agent, s_camera) \n    penalty_collision = -16 if (dist_camera < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Data Center. Goal: It must avoid colliding with power station. Assumptions: The states of the drone and the power station's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_power_station, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_power_station = calculate_distance_min(s_agent, s_power_station) \n    penalty_collision = -16 if (dist_power_station < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Data Center. Goal: It must avoid colliding with Data Center's pillars, tool cabinets and cable guards. Assumptions: The Agent's state, target's state and pillars, tool cabinets and cable guards's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Data Center. Goal: Detect bench, server rack and statue and do not collide. Assumptions: The states of the drone, target  bench, server rack and statue state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Data Center. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Yoga Studio. \n Objectives: It must avoid colliding with concrete columns or yoga mats. \n Conditions: The states of the agent, the studio mirror, and the yoga mats are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Yoga Studio. \n Objectives: Detect yoga block stack and aviod it. \n Conditions: The states of the agent, the yoga block stack are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_block_stack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_block_stack = calculate_distance_min(s_agent, s_block_stack) \n    penalty_collision = -17 if (dist_block_stack < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Yoga Studio open workspace. Objectives: It must avoid colliding with ceiling fan. Conditions: States of the drone and the ceiling fan are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ceiling_fan, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_ceiling_fan = calculate_distance_min(s_agent, s_ceiling_fan) \n    penalty_collision = -17 if (dist_ceiling_fan < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a yoga hall, the drone must maneuver around yoga mat. All entity states (agent, yoga_mat) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_yoga_mat, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_yoga_mat = calculate_distance_min(s_agent, s_yoga_mat) \n    penalty_collision = -17 if (dist_yoga_mat < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a yoga hall. Requirement: Detect the three air purifiers and do not collide with it. Given: The states for the drone and many air purifiers are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a instructor podium during experiments. \n Objectives: So not collide with shelf. \n Conditions: The states of the agent, the shelf are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_shelf, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_shelf = calculate_distance_min(s_agent, s_shelf) \n    penalty_collision = -17 if (dist_shelf < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Yoga Studio. Objectives: Do not collide hit with air purifier. Conditions: States of the agent, air purifier are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_air_purifier, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_air_purifier = calculate_distance_min(s_agent, s_air_purifier) \n    penalty_collision = -17 if (dist_air_purifier < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a yoga hall, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: reception area. Avoid: Hitting with rack lift. Known: Agent, rack lift states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_rack_lift, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_rack_lift = calculate_distance_min(s_agent, s_rack_lift) \n    penalty_collision = -17 if (dist_rack_lift < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Yoga Studio. Objectives: It must avoid colliding with speaker stand. Conditions: The states of the drone and speaker stand are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_speaker_stand, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_speaker_stand = calculate_distance_min(s_agent, s_speaker_stand) \n    penalty_collision = -16 if (dist_speaker_stand < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Yoga Studio. Objectives: Avoid collision with prop shelves, switch and sound system. Conditions: The states of the agent, prop shelves, switch and sound system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Yoga Studio. Objectives: It must avoid colliding with any equipment trolley, units and computers. Conditions: Agent's, equipment trolley, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Yoga Studio. Objectives: Do not collide with any prop shelve. Conditions: The states of the drone and the prop shelve are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_prop_shelve, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_prop_shelve = calculate_distance_min(s_agent, s_prop_shelve) \n    penalty = -16 if (dist_prop_shelve < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Yoga Studio and must avoid contact with any rope hanger. The states of the drone and the rope hanger are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_rope_hanger, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_rope_hanger = calculate_distance_min(s_agent, s_rope_hanger) \n    penalty = -16 if (dist_rope_hanger < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Yoga Studio. It must avoid colliding with the equipment trolley, plant pot and play sign. The states of the agent, prop_shelves, storage racks and safety floor cushions are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Yoga Studio. It must avoid colliding with floor cushions and yoga mats. The states of the drone and the water tuck and rack_lift are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Yoga Studio. It must avoid colliding with Yoga Studio aisle CCTV camera. Agent's state, target's state and Yoga Studio aisle CCTV camera's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_camera, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_camera = calculate_distance_min(s_agent, s_camera) \n    penalty_collision = -16 if (dist_camera < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Yoga Studio. Goal: It must avoid colliding with water dispenser. Assumptions: The states of the drone and the water dispenser's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_water_dispenser, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_water_dispenser = calculate_distance_min(s_agent, s_water_dispenser) \n    penalty_collision = -16 if (dist_water_dispenser < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Yoga Studio. Goal: It must avoid colliding with Yoga Studio's prop_shelves, storage lockers and floor cushions. Assumptions: The Agent's state, target's state and prop_shelves, storage lockers and floor cushions's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Yoga Studio. Goal: Detect bench, yoga block stack and plant pot and do not collide. Assumptions: The states of the drone, target  bench, yoga block stack and plant pot state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Yoga Studio. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Bakery. \n Objectives: It must avoid colliding with concrete columns or bread trays. \n Conditions: The states of the agent, the display glass, and the bread trays are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Bakery. \n Objectives: Detect flour sack stack and aviod it. \n Conditions: The states of the agent, the flour sack stack are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_flour_sack_stack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_flour_sack_stack = calculate_distance_min(s_agent, s_flour_sack_stack) \n    penalty_collision = -17 if (dist_flour_sack_stack < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Bakery open workspace. Objectives: It must avoid colliding with ventilation fan. Conditions: States of the drone and the ventilation fan are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ventilation_fan, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_ventilation_fan = calculate_distance_min(s_agent, s_ventilation_fan) \n    penalty_collision = -17 if (dist_ventilation_fan < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a baking area, the drone must maneuver around bread tray. All entity states (agent, bread_tray) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_bread_tray, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_bread_tray = calculate_distance_min(s_agent, s_bread_tray) \n    penalty_collision = -17 if (dist_bread_tray < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a baking area. Requirement: Detect the three oven units and do not collide with it. Given: The states for the drone and many oven units are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a baker's table during experiments. \n Objectives: So not collide with shelf. \n Conditions: The states of the agent, the ingredient shelf are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_ingredient_shelf, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_ingredient_shelf = calculate_distance_min(s_agent, s_ingredient_shelf) \n    penalty_collision = -17 if (dist_ingredient_shelf < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Bakery. Objectives: Do not collide hit with oven unit. Conditions: States of the agent, oven unit are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_oven_unit, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_oven_unit = calculate_distance_min(s_agent, s_oven_unit) \n    penalty_collision = -17 if (dist_oven_unit < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a baking area, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: cashier counter. Avoid: Hitting with rack lift. Known: Agent, baking trolley states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_baking_trolley, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_baking_trolley = calculate_distance_min(s_agent, s_baking_trolley) \n    penalty_collision = -17 if (dist_baking_trolley < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Bakery. Objectives: It must avoid colliding with stand mixer. Conditions: The states of the drone and stand mixer are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_stand_mixer, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_speaker_stand = calculate_distance_min(s_agent, s_stand_mixer) \n    penalty_collision = -16 if (dist_speaker_stand < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Bakery. Objectives: Avoid collision with utensil racks, switch and dough mixer. Conditions: The states of the agent, utensil racks, switch and dough mixer are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Bakery. Objectives: It must avoid colliding with any baking trolley, units and computers. Conditions: Agent's, baking trolley, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Bakery. Objectives: Do not collide with any prop shelve. Conditions: The states of the drone and the prop shelve are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_utensil_rack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_utensil_rack = calculate_distance_min(s_agent, s_utensil_rack) \n    penalty = -16 if (dist_utensil_rack < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Bakery and must avoid contact with any hanging baskets. The states of the drone and the hanging baskets are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_hanging_basket, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_hanging_basket = calculate_distance_min(s_agent, s_hanging_basket) \n    penalty = -16 if (dist_hanging_basket < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Bakery. It must avoid colliding with the baking trolley, sugar container and play sign. The states of the agent, utensil_racks, storage racks and safety flour bags are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Bakery. It must avoid colliding with flour bags and bread trays. The states of the drone and the water tuck and baking_trolley are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Bakery. It must avoid colliding with Bakery aisle security camera. Agent's state, target's state and Bakery aisle security camera's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_security_camera, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_security_camera = calculate_distance_min(s_agent, s_security_security_camera) \n    penalty_collision = -16 if (dist_security_camera < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Bakery. Goal: It must avoid colliding with water tank. Assumptions: The states of the drone and the water tank's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_water_tank, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_water_tank = calculate_distance_min(s_agent, s_water_tank) \n    penalty_collision = -16 if (dist_water_tank < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Bakery. Goal: It must avoid colliding with Bakery's utensil_racks, cold storage unit and flour bags. Assumptions: The Agent's state, target's state and utensil_racks, cold storage unit and flour bags's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Bakery. Goal: Detect wooden bench, flour sack stack and sugar container and do not collide. Assumptions: The states of the drone, target  wooden bench, flour sack stack and sugar container state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Bakery. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Stadium. \n Objectives: It must avoid colliding with concrete columns or seat rows. \n Conditions: The states of the agent, the scoreboard, and the seat rows are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Stadium. \n Objectives: Detect concession stand and aviod it. \n Conditions: The states of the agent, the concession stand are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_concession_stand, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_concession_stand = calculate_distance_min(s_agent, s_concession_stand) \n    penalty_collision = -17 if (dist_concession_stand < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Stadium open workspace. Objectives: It must avoid colliding with stadium lights. Conditions: States of the drone and the stadium lights are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_stadium_light, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_stadium_light = calculate_distance_min(s_agent, s_stadium_light) \n    penalty_collision = -17 if (dist_stadium_light < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a field area, the drone must maneuver around seat row. All entity states (agent, seat row) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_seat_row, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_seat_row = calculate_distance_min(s_agent, s_seat_row) \n    penalty_collision = -17 if (dist_seat_row < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a field area. Requirement: Detect the three target posts and do not collide with it. Given: The states for the drone and many target posts are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a dugout bench during experiments. \n Objectives: So not collide with shelf. \n Conditions: The states of the agent, the shelf are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_shelf, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_shelf = calculate_distance_min(s_agent, s_shelf) \n    penalty_collision = -17 if (dist_shelf < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Stadium. Objectives: Do not collide hit with target post. Conditions: States of the agent, target post are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_target_post, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_target_post = calculate_distance_min(s_agent, s_target_post) \n    penalty_collision = -17 if (dist_target_post < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a field area, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: ticket counter. Avoid: Hitting with rack lift. Known: Agent, rack lift states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_refreshment_cart, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_refreshment_cart = calculate_distance_min(s_agent, s_refreshment_cart) \n    penalty_collision = -17 if (dist_refreshment_cart < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Stadium. Objectives: It must avoid colliding with PA system. Conditions: The states of the drone and PA system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_pa_system, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_pa_system = calculate_distance_min(s_agent, s_pa_system) \n    penalty_collision = -16 if (dist_pa_system < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Stadium. Objectives: Avoid collision with jersey racks, switch and sound system. Conditions: The states of the agent, jersey racks, switch and sound system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Stadium. Objectives: It must avoid colliding with any refreshment cart, units and computers. Conditions: Agent's, refreshment cart, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Stadium. Objectives: Do not collide with any prop shelve. Conditions: The states of the drone and the prop shelve are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_jersey_rack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_jersey_rack = calculate_distance_min(s_agent, s_jersey_rack) \n    penalty = -16 if (dist_jersey_rack < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Stadium and must avoid contact with any banner hangers. The states of the drone and the banner hangers are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_banner_hanger, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_banner_hanger = calculate_distance_min(s_agent, s_banner_hanger) \n    penalty = -16 if (dist_banner_hanger < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Stadium. It must avoid colliding with the refreshment cart, trophy stand and play sign. The states of the agent, prop_shelves, storage racks and safety cones are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Stadium. It must avoid colliding with cones and seat rows. The states of the drone and the water tuck and refreshment_cart are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Stadium. It must avoid colliding with Stadium aisle CCTV camera. Agent's state, target's state and Stadium aisle CCTV camera's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_camera, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_camera = calculate_distance_min(s_agent, s_camera) \n    penalty_collision = -16 if (dist_camera < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Stadium. Goal: It must avoid colliding with hydration station. Assumptions: The states of the drone and the hydration station's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_hydration_station, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_hydration_station = calculate_distance_min(s_agent, s_hydration_station) \n    penalty_collision = -16 if (dist_hydration_station < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Stadium. Goal: It must avoid colliding with Stadium's prop_shelves, locker room and cones. Assumptions: The Agent's state, target's state and prop_shelves, locker room and cones's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Stadium. Goal: Detect bleacher bench, concession stand and trophy stand and do not collide. Assumptions: The states of the drone, target  bleacher bench, concession stand and trophy stand state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Stadium. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Skatepark. \n Objectives: It must avoid colliding with concrete columns or ramps. \n Conditions: The states of the agent, the graffiti wall, and the ramps are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Skatepark. \n Objectives: Detect half-pipe and aviod it. \n Conditions: The states of the agent, the half-pipe are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_half_pipe, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_half_pipek = calculate_distance_min(s_agent, s_half_pipe) \n    penalty_collision = -17 if (dist_half_pipek < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Skatepark open workspace. Objectives: It must avoid colliding with floodlight. Conditions: States of the drone and the floodlight are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_floodlight, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_floodlight = calculate_distance_min(s_agent, s_floodlight) \n    penalty_collision = -17 if (dist_floodlight < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a skating area, the drone must maneuver around ramp. All entity states (agent, ramp) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_ramp, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_ramp = calculate_distance_min(s_agent, s_ramp) \n    penalty_collision = -17 if (dist_ramp < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a skating area. Requirement: Detect the three funboxs and do not collide with it. Given: The states for the drone and many funboxs are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacle_min < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a grind rail during experiments. \n Objectives: So not collide with skateboard_rack. \n Conditions: The states of the agent, the skateboard_rack are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_skateboard_rack, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_skateboard_rack = calculate_distance_min(s_agent, s_skateboard_rack) \n    penalty_collision = -17 if (dist_skateboard_rack < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Skatepark. Objectives: Do not collide hit with funbox. Conditions: States of the agent, funbox are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_funbox, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_funbox = calculate_distance_min(s_agent, s_funbox) \n    penalty_collision = -17 if (dist_funbox < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a skating area, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -17 if (dist_wall < 0.5) else 17 \n    reward_reaching_target = 20 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: snack kiosk. Avoid: Hitting with rack lift. Known: Agent, rack lift states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_gear_cart, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_gear_cart = calculate_distance_min(s_agent, s_gear_cart) \n    penalty_collision = -17 if (dist_gear_cart < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Skatepark. Objectives: It must avoid colliding with announcement system. Conditions: The states of the drone and announcement system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_announcement_system, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_announcement_system = calculate_distance_min(s_agent, s_announcement_system) \n    penalty_collision = -16 if (dist_announcement_system < 0.5) else 4 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Skatepark. Objectives: Avoid collision with helmet stand, switch and speaker system. Conditions: The states of the agent, helmet stand, switch and speaker system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating Skatepark. Objectives: It must avoid colliding with any gear cart, units and computers. Conditions: Agent's, gear cart, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Skatepark. Objectives: Do not collide with any prop shelve. Conditions: The states of the drone and the prop shelve are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_helmet_stand, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_helmet_stand = calculate_distance_min(s_agent, s_helmet_stand) \n    penalty = -16 if (dist_helmet_stand < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Skatepark and must avoid contact with any skate banners. The states of the drone and the skate banners are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_skate_banner, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_skate_banner = calculate_distance_min(s_agent, s_skate_banner) \n    penalty = -16 if (dist_skate_banner < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Skatepark. It must avoid colliding with the gear cart, trick podium and play sign. The states of the agent, helmet_stands, storage racks and safety traffic cones are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Skatepark. It must avoid colliding with traffic cones and ramps. The states of the drone and the water tuck and gear_cart are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Skatepark. It must avoid colliding with a child. Agent's state, target's state and child's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_child, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_child = calculate_distance_min(s_agent, s_child) \n    penalty_collision = -16 if (dist_child < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Skatepark. Goal: It must avoid colliding with water fountain. Assumptions: The states of the drone and the water fountain's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_water_fountain, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_water_fountain = calculate_distance_min(s_agent, s_water_fountain) \n    penalty_collision = -16 if (dist_water_fountain < 0.5) else 17 \n    reward_reaching_target = 16 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Skatepark. Goal: It must avoid colliding with Skatepark's helmet_stands, equipment shed and traffic cones. Assumptions: The Agent's state, target's state and helmet_stands, equipment shed and traffic cones's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Skatepark. Goal: Detect seating bench, half-pipe and trick podium and do not collide. Assumptions: The states of the drone, target  seating bench, half-pipe and trick podium state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    dist_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -16 if (dist_multiple_obstacles_min < 0.5) else 17 \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating Skatepark. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    dist_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 17 if (dist_target < 0.2) else -1.5*dist_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Mosque. \n Objectives: It must avoid colliding with concrete columns or prayer rows. \n Conditions: The states of the agent, the mihrab, and the prayer rows are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Mosque. \n Objectives: Detect shoe rack and aviod it. \n Conditions: The states of the agent, the shoe rack are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_shoe_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_shoe_rack = calculate_distance_min(s_agent, s_shoe_rack) \n    penalty_collision = -20 if (d_agent_shoe_rack < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through Mosque open workspace. Objectives: It must avoid colliding with chandeliers. Conditions: States of the drone and the chandeliers are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_chandelier, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chandelier = calculate_distance_min(s_agent, s_chandelier) \n    penalty_collision = -20 if (d_agent_chandelier < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a prayer hall, the drone must maneuver around prayer row. All entity states (agent, ) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_prayer_row, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_prayer_row = calculate_distance_min(s_agent, s_prayer_row) \n    penalty_collision = -20 if (d_agent_prayer_row < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a prayer hall. Requirement: Detect the three minbars and do not collide with it. Given: The states for the drone and many minbars are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a ablution Mosque during experiments. \n Objectives: So not collide with shelf. \n Conditions: The states of the agent, the shelf are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_bookshelf, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bookshelf = calculate_distance_min(s_agent, s_bookshelf) \n    penalty_collision = -20 if (d_agent_bookshelf < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Mosque. Objectives: Do not collide hit with minbar. Conditions: States of the agent, minbar are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_minbar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_target_minbart = calculate_distance_min(s_agent, s_minbar) \n    penalty_collision = -20 if (d_agent_target_minbart < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a prayer hall, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -20 if (d_agent_wall < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: shoe area. Avoid: Detect donation box and do  not hitting with donation box. Known: Agent, donation box states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_donation_box, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_donation_box = calculate_distance_min(s_agent, s_donation_box) \n    penalty_collision = -20 if (d_agent_donation_box < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through Mosque. Objectives: It must avoid colliding with loudspeaker. Conditions: The states of the drone and loudspeaker are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_loudspeaker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_loudspeaker = calculate_distance_min(s_agent, s_loudspeaker) \n    penalty_collision = -20 if (d_agent_loudspeaker < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Mosque. Objectives: Avoid collision with cloak racks, people and loudspeaker system. Conditions: The states of the agent, cloak racks, people and loudspeaker system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through Mosque. Objectives: It must avoid colliding with any donation box, units and computers. Conditions: Agent's, donation box, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Mosque. Objectives: Do not collide with any cloack rack. Conditions: The states of the drone and the cloack rack are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_cloak_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cloak_rack = calculate_distance_min(s_agent, s_cloak_rack) \n    penalty = -20 if (d_agent_cloak_rack < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Mosque and must avoid contact with any calligraphy panels. The states of the drone and the calligraphy panels are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_calligraphy_panel, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_calligraphy_panel = calculate_distance_min(s_agent, s_calligraphy_panel) \n    penalty = -20 if (d_agent_calligraphy_panel < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Mosque. It must avoid colliding with the donation box, Quran stand and play sign. The states of the agent, book shelves, shoe racks and safety divider stands are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Mosque. It must avoid colliding with divider stands and prayer rows. The states of the drone and the water tuck and donation_box are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Mosque. It must avoid colliding with Mosque aisle divider stand. Agent's state, target's state and divider stand's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_divider_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_divider_stand = calculate_distance_min(s_agent, s_divider_stand) \n    penalty_collision = -20 if (d_agent_divider_stand < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Mosque. Goal: It must avoid colliding with ablution fountain. Assumptions: The states of the drone and the ablution fountain's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_ablution_fountain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ablution_fountain = calculate_distance_min(s_agent, s_ablution_fountain) \n    penalty_collision = -20 if (d_agent_ablution_fountain < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Mosque. Goal: It must avoid colliding with Mosque's book shelves, storage room and divider stands. Assumptions: The Agent's state, target's state and book shelves, storage room and divider stands's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Mosque. Goal: Detect wooden bench, shoe rack and Quran stand and do not collide. Assumptions: The states of the drone, target  wooden bench, shoe rack and Quran stand state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Mosque. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Music Hall. \n Objectives: It must avoid colliding with concrete columns or audience rows. \n Conditions: The states of the agent, the stage, and the audience rows are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Music Hall. \n Objectives: Detect coat rack and aviod it. \n Conditions: The states of the agent, the coat rack are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_coat_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_coat_rack = calculate_distance_min(s_agent, s_coat_rack) \n    penalty_collision = -20 if (d_agent_coat_rack < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Music Hall. Objectives: It must avoid colliding with spotlights. Conditions: States of the drone and the spotlights are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_spotlight, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_spotlight = calculate_distance_min(s_agent, s_spotlight) \n    penalty_collision = -20 if (d_agent_spotlight < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a concert hall, the drone must maneuver around audience row. All entity states (agent, audience row) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_audience_row, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_audience_row = calculate_distance_min(s_agent, s_audience_row) \n    penalty_collision = -20 if (d_agent_audience_row < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a concert hall. Requirement: Detect the three podiums and do not collide with it. Given: The states for the drone and many podiums are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a ablution Music Hall during experiments. \n Objectives: So not collide with shelf. \n Conditions: The states of the agent, the shelf are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_music_shelf, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_music_shelf = calculate_distance_min(s_agent, s_music_shelf) \n    penalty_collision = -20 if (d_agent_music_shelf < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Music Hall. Objectives: Do not collide hit with podium. Conditions: States of the agent, podium are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_podium, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_podium = calculate_distance_min(s_agent, s_podium) \n    penalty_collision = -20 if (d_agent_podium < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a concert hall, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -20 if (d_agent_wall < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: ticket booth. Avoid: Detect speaker cabinet and do not hitting with speaker cabinet. Known: Agent, speaker cabinet states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_speaker_cabinet, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_speaker_cabinet = calculate_distance_min(s_agent, s_speaker_cabinet) \n    penalty_collision = -20 if (d_agent_speaker_cabinet < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through Music Hall. Objectives: It must avoid colliding with sound system. Conditions: The states of the drone and sound system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_sound_system, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sound_system = calculate_distance_min(s_agent, s_sound_system) \n    penalty_collision = -20 if (d_agent_sound_system < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Music Hall. Objectives: Avoid collision with coat stands, people and sound system. Conditions: The states of the agent, coat stands, people and sound system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through Music Hall. Objectives: It must avoid colliding with any speaker cabinet, units and computers. Conditions: Agent's, speaker cabinet, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Music Hall. Objectives: Do not collide with any cloack rack. Conditions: The states of the drone and the cloack rack are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_coat_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cloak_rack = calculate_distance_min(s_agent, s_coat_stand) \n    penalty = -20 if (d_agent_cloak_rack < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Music Hall and must avoid contact with any acoustic panels. The states of the drone and the acoustic panels are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_acoustic_panel, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_acoustic_panel = calculate_distance_min(s_agent, s_acoustic_panel) \n    penalty = -20 if (d_agent_acoustic_panel < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Music Hall. It must avoid colliding with the speaker cabinet, music stand and play sign. The states of the agent, music shelves, coat racks and safety barrier stands are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Music Hall. It must avoid colliding with barrier stands and audience rows. The states of the drone and the water tuck and speaker_cabinet are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Music Hall. It must avoid colliding with guitar. Agent's state, target's state and guitar's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_guitar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_guitar = calculate_distance_min(s_agent, s_guitar) \n    penalty_collision = -20 if (d_agent_guitar < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Music Hall. Goal: It must avoid colliding with violin. Assumptions: The states of the drone and the violin's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_violin, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_violin = calculate_distance_min(s_agent, s_violin) \n    penalty_collision = -20 if (d_agent_violin < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Music Hall. Goal: It must avoid colliding with Music Hall's music shelves, backstage room and barrier stands. Assumptions: The Agent's state, target's state and music shelves, backstage room and barrier stands's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Music Hall. Goal: Detect audience bench, coat rack and music stand and do not collide. Assumptions: The states of the drone, target  audience bench, coat rack and music stand state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Music Hall. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Hair Salon. \n Objectives: It must avoid colliding with concrete columns or styling chairs. \n Conditions: The states of the agent, the stylist station, and the styling chairs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Hair Salon. \n Objectives: Detect towel rack and aviod it. \n Conditions: The states of the agent, the towel rack are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_towel_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_towel_rack = calculate_distance_min(s_agent, s_towel_rack) \n    penalty_collision = -20 if (d_agent_towel_rack < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Hair Salon. Objectives: It must avoid colliding with hair dryers. Conditions: States of the drone and the hair dryers are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_hair_dryer, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_hair_dryer = calculate_distance_min(s_agent, s_hair_dryer) \n    penalty_collision = -20 if (d_agent_hair_dryer < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a salon floor, the drone must maneuver around styling chair. All entity states (agent, chair) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -20 if (d_agent_chair < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a salon floor. Requirement: Detect the three cashier counters and do not collide with it. Given: The states for the drone and many cashier counters are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a ablution Hair Salon during experiments. \n Objectives: So not collide with shelf. \n Conditions: The states of the agent, the shelf are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_product_shelf, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_product_shelf = calculate_distance_min(s_agent, s_product_shelf) \n    penalty_collision = -20 if (d_agent_product_shelf < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Hair Salon. Objectives: Do not collide hit with cashier counter. Conditions: States of the agent, cashier counter are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_cashier_counter, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cashier_counter = calculate_distance_min(s_agent, s_cashier_counter) \n    penalty_collision = -20 if (d_agent_cashier_counter < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a salon floor, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -20 if (d_agent_wall < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: reception desk. Avoid: Detect product display and do not hitting with product display. Known: Agent, product display states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_product_display, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_product_display = calculate_distance_min(s_agent, s_product_display) \n    penalty_collision = -20 if (d_agent_product_display < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Hair Salon. Objectives: It must avoid colliding with stereo system. Conditions: The states of the drone and stereo system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_customer, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_customer = calculate_distance_min(s_agent, s_customer) \n    penalty_collision = -20 if (d_agent_customer < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Hair Salon. Objectives: Avoid collision with mirror stands, people and stereo system. Conditions: The states of the agent, mirror stands, people and stereo system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Hair Salon. Objectives: It must avoid colliding with any product display, units and computers. Conditions: Agent's, product display, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Hair Salon. Objectives: Do not collide with any cloack rack. Conditions: The states of the drone and the cloack rack are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_mirror_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_mirror_stand = calculate_distance_min(s_agent, s_mirror_stand) \n    penalty = -20 if (d_agent_mirror_stand < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Hair Salon and must avoid contact with any mirrors. The states of the drone and the customer are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_customer, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_customer = calculate_distance_min(s_agent, s_customer) \n    penalty = -20 if (d_agent_customer < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Hair Salon. It must avoid colliding with the product display, magazine stand and play sign. The states of the agent, product shelves, towel racks and doors are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Hair Salon. It must avoid colliding with partition stands and styling chairs. The states of the drone and the basket and product_display are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Hair Salon. It must avoid colliding with hair dryer stand. Agent's state, target's state and hair dryer stand's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_hair_dryer_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_hair_dryer_stand = calculate_distance_min(s_agent, s_hair_dryer_stand) \n    penalty_collision = -20 if (d_agent_hair_dryer_stand < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Hair Salon. Goal: It must avoid colliding with shampoo chair. Assumptions: The states of the drone and the shampoo chair's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_shampoo_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_shampoo_chair = calculate_distance_min(s_agent, s_shampoo_chair) \n    penalty_collision = -20 if (d_agent_shampoo_chair < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Hair Salon. Goal: It must avoid colliding with Hair Salon's product shelves, backstylist station room and partition stands. Assumptions: The Agent's state, target's state and product shelves, backstylist station room and partition stands's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Hair Salon. Goal: Detect waiting bench, towel rack and magazine stand and do not collide. Assumptions: The states of the drone, target  waiting bench, towel rack and magazine stand state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Hair Salon. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Spa Resort. \n Objectives: It must avoid colliding with concrete columns or massage tables. \n Conditions: The states of the agent, the spa station, and the massage tables are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Spa Resort. \n Objectives: Detect towel warmer and aviod it. \n Conditions: The states of the agent, the towel warmer are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_towel_warmer, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_towel_warmer = calculate_distance_min(s_agent, s_towel_warmer) \n    penalty_collision = -20 if (d_agent_towel_warmer < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through a Spa Resort. Objectives: It must avoid colliding with sauna heater. Conditions: States of the drone and the sauna heater are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_sauna_heater, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sauna_heater = calculate_distance_min(s_agent, s_sauna_heater) \n    penalty_collision = -20 if (d_agent_sauna_heater < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a spa floor, the drone must maneuver around massage table. All entity states (agent, chair) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -20 if (d_agent_chair < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a spa floor. Requirement: Detect the three spa counters and do not collide with it. Given: The states for the drone and many spa counters are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a ablution Spa Resort during experiments. \n Objectives: So not collide with aroma diffuser. \n Conditions: The states of the agent, the aroma diffuser are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_aroma_diffuser, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_aroma_diffuser = calculate_distance_min(s_agent, s_aroma_diffuser) \n    penalty_collision = -20 if (d_agent_aroma_diffuser < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Spa Resort. Objectives: Do not collide hit with spa counter. Conditions: States of the agent, spa counter are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_spa_counter, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_spa_counter = calculate_distance_min(s_agent, s_spa_counter) \n    penalty_collision = -20 if (d_agent_spa_counter < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a spa floor, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -20 if (d_agent_wall < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: front desk. Avoid: Detect flower arrangement and do not hitting with flower arrangement. Known: Agent, flower arrangement states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_flower_arrangement, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_flower_arrangement = calculate_distance_min(s_agent, s_flower_arrangement) \n    penalty_collision = -20 if (d_agent_flower_arrangement < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Spa Resort. Objectives: It must avoid colliding with spa statue. Conditions: The states of the drone and spa statue are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_spa_statue, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_spa_statue = calculate_distance_min(s_agent, s_spa_statue) \n    penalty_collision = -20 if (d_agent_spa_statue < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Spa Resort. Objectives: Avoid collision with decorative screen, people and water fountain. Conditions: The states of the agent, decorative screen, people and water fountain are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Spa Resort. Objectives: It must avoid colliding with any flower arrangement, units and computers. Conditions: Agent's, flower arrangement, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Spa Resort. Objectives: Do not collide with sauna heater. Conditions: The states of the drone and the sauna heater are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_sauna_heater, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sauna_heater = calculate_distance_min(s_agent, s_sauna_heater) \n    penalty = -20 if (d_agent_sauna_heater < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Spa Resort and must avoid contact with the oil diffuser stand. The states of the drone and the oil stand are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_oil_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_oil_stand = calculate_distance_min(s_agent, s_oil_stand) \n    penalty = -20 if (d_agent_oil_stand < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Spa Resort. It must avoid colliding with the flower arrangement, tea table and play sign. The states of the agent, insliding screen plants, towel warmers and sliding screens are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Spa Resort. It must avoid colliding with spa divider and massage tables. The states of the drone and the stone sculpture and flower_arrangement are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Spa Resort. It must avoid colliding with the tea table. Agent's state, target's state and tea table's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_tea_table, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tea_table = calculate_distance_min(s_agent, s_tea_table) \n    penalty_collision = -20 if (d_agent_tea_table < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Spa Resort. Goal: It must avoid colliding with foot bath. Assumptions: The states of the drone and the foot bath's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_foot_bath, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_foot_bath = calculate_distance_min(s_agent, s_foot_bath) \n    penalty_collision = -20 if (d_agent_foot_bath < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Spa Resort. Goal: It must avoid colliding with Spa Resort's insliding screen plants, backspa station room and spa divider. Assumptions: The Agent's state, target's state and insliding screen plants, backspa station room and spa divider's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Spa Resort. Goal: Detect lounge chair, towel warmer and tea table and do not collide. Assumptions: The states of the drone, target  lounge chair, towel warmer and tea table state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Spa Resort. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Aquarium. \n Objectives: It must avoid colliding with concrete columns or viewing benches. \n Conditions: The states of the agent, the aquarium tank, and the viewing benches are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Aquarium. \n Objectives: Detect filter pump and aviod it. \n Conditions: The states of the agent, the filter pump are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_filter_pump, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_filter_pump = calculate_distance_min(s_agent, s_filter_pump) \n    penalty_collision = -20 if (d_agent_filter_pump < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through an aquarium. Objectives: It must avoid colliding with UV sterilizer. Conditions: States of the drone and the UV sterilizer are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bench, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bench = calculate_distance_min(s_agent, s_bench) \n    penalty_collision = -20 if (d_agent_bench < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a aquarium hall, the drone must maneuver around viewing bench. All entity states (agent, chair) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -20 if (d_agent_chair < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a aquarium hall. Requirement: Detect the three touch pools and do not collide with it. Given: The states for the drone and many touch pools are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a ablution Aquarium during experiments. \n Objectives: So not collide with air bubbler. \n Conditions: The states of the agent, the air bubbler are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_air_bubbler, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_air_bubbler = calculate_distance_min(s_agent, s_air_bubbler) \n    penalty_collision = -20 if (d_agent_air_bubbler < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Aquarium. Objectives: Do not collide hit with touch pool. Conditions: States of the agent, touch pool are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_touch_pool, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_touch_pool = calculate_distance_min(s_agent, s_touch_pool) \n    penalty_collision = -20 if (d_agent_touch_pool < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a aquarium hall, the drone must steer clear of the visitor. All states (agent, visitor) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_visitor, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_visitor = calculate_distance_min(s_agent, s_visitor) \n    penalty_collision = -20 if (d_agent_visitor < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: ticket counter. Avoid: Detect coral display and do not hitting with coral display. Known: Agent, coral display states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_coral_display, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_coral_display = calculate_distance_min(s_agent, s_coral_display) \n    penalty_collision = -20 if (d_agent_coral_display < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Aquarium. Objectives: It must avoid colliding with marine sculpture. Conditions: The states of the drone and marine sculpture are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_marine_sculpture, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_marine_sculpture = calculate_distance_min(s_agent, s_marine_sculpture) \n    penalty_collision = -20 if (d_agent_marine_sculpture < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Aquarium. Objectives: Avoid collision with glass panel, people and fish tank. Conditions: The states of the agent, glass panel, people and fish tank are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Aquarium. Objectives: It must avoid colliding with any coral display, units and computers. Conditions: Agent's, coral display, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Aquarium. Objectives: Do not collide with viewing_bench. Conditions: The states of the drone and the viewing_bench are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bench, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bench = calculate_distance_min(s_agent, s_bench) \n    penalty = -20 if (d_agent_bench < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Aquarium and must avoid contact with the oxygen pump. The states of the drone and the oxygen pump are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_oxygen_pump, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_oxygen_pump = calculate_distance_min(s_agent, s_oxygen_pump) \n    penalty = -20 if (d_agent_oxygen_pump < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Aquarium. It must avoid colliding with the coral display, info stand and play sign. The states of the agent, inglass door plants, filter pumps and glass doors are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Aquarium. It must avoid colliding with aquarium barrier and viewing benches. The states of the drone and the aquarium statue and coral_display are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Aquarium. It must avoid colliding with the info stand. Agent's state, target's state and info stand's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_info_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_info_stand = calculate_distance_min(s_agent, s_info_stand) \n    penalty_collision = -20 if (d_agent_info_stand < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Aquarium. Goal: It must avoid colliding with shallow tank. Assumptions: The states of the drone and the shallow tank's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_tank, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tank = calculate_distance_min(s_agent, s_tank) \n    penalty_collision = -20 if (d_agent_tank < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Aquarium. Goal: It must avoid colliding with Aquarium's inglass door plants, backaquarium tank room and aquarium barrier. Assumptions: The Agent's state, target's state and inglass door plants, backaquarium tank room and aquarium barrier's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Aquarium. Goal: Detect visitor bench, filter pump and info stand and do not collide. Assumptions: The states of the drone, target  visitor bench, filter pump and info stand state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Aquarium. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Tennis Court. \n Objectives: It must avoid colliding with concrete columns or spectator benches. \n Conditions: The states of the agent, the net post, and the spectator benches are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Tennis Court. \n Objectives: Detect ball basket and aviod it. \n Conditions: The states of the agent, the ball basket are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ball_basket, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ball_basket = calculate_distance_min(s_agent, s_ball_basket) \n    penalty_collision = -20 if (d_agent_ball_basket < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Tennis Court open workspace. Objectives: It must avoid colliding with ball machine. Conditions: States of the drone and the ball machine are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ball_machine, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ball_machine = calculate_distance_min(s_agent, s_ball_machine) \n    penalty_collision = -20 if (d_agent_ball_machine < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a tennis court area, the drone must maneuver around spectator bench. All entity states (agent, chair) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -20 if (d_agent_chair < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a tennis court area. Requirement: Detect the three practice walls and do not collide with it. Given: The states for the drone and many practice walls are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a ablution Tennis Court during experiments. \n Objectives: So not collide with water cooler. \n Conditions: The states of the agent, the water cooler are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_water_cooler, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_water_cooler = calculate_distance_min(s_agent, s_water_cooler) \n    penalty_collision = -20 if (d_agent_water_cooler < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Tennis Court. Objectives: Do not collide hit with practice wall. Conditions: States of the agent, practice wall are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_practice_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_practice_wall = calculate_distance_min(s_agent, s_practice_wall) \n    penalty_collision = -20 if (d_agent_practice_wall < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a tennis court area, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -20 if (d_agent_wall < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: umpire chair. Avoid: Detect scoreboard and do not hitting with scoreboard. Known: Agent, scoreboard states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_scoreboard, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_scoreboard = calculate_distance_min(s_agent, s_scoreboard) \n    penalty_collision = -20 if (d_agent_scoreboard < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Tennis Court. Objectives: It must avoid colliding with tennis monument. Conditions: The states of the drone and tennis monument are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_monument, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_monument = calculate_distance_min(s_agent, s_monument) \n    penalty_collision = -20 if (d_agent_monument < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Tennis Court. Objectives: Avoid collision with fence panel, people and ball cart. Conditions: The states of the agent, fence panel, people and ball cart are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Tennis Court. Objectives: It must avoid colliding with any scoreboard, units and computers. Conditions: Agent's, scoreboard, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Tennis Court. Objectives: Do not collide with ball machine. Conditions: The states of the drone and the ball machine are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ball_machine, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ball_machine = calculate_distance_min(s_agent, s_ball_machine) \n    penalty = -20 if (d_agent_ball_machine < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Tennis Court and must avoid contact with the lighting pole. The states of the drone and the lighting pole are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_pole, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_pole = calculate_distance_min(s_agent, s_pole) \n    penalty = -20 if (d_agent_pole < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Tennis Court. It must avoid colliding with the scoreboard, schedule board and play sign. The states of the agent, incourt gate plants, ball baskets and court gates are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Tennis Court. It must avoid colliding with boundary fence and spectator benches. The states of the drone and the court statue and scoreboard are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Tennis Court. It must avoid colliding with the schedule board. Agent's state, target's state and schedule board's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_schedule_board, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_schedule_board = calculate_distance_min(s_agent, s_schedule_board) \n    penalty_collision = -20 if (d_agent_schedule_board < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Tennis Court. Goal: It must avoid colliding with clay patch. Assumptions: The states of the drone and the clay patch's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_clay_patch, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_foot_bath = calculate_distance_min(s_agent, s_clay_patch) \n    penalty_collision = -20 if (d_agent_foot_bath < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Tennis Court. Goal: It must avoid colliding with Tennis Court's incourt gate plants, backnet post room and boundary fence. Assumptions: The Agent's state, target's state and incourt gate plants, backnet post room and boundary fence's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Tennis Court. Goal: Detect player bench, ball basket and schedule board and do not collide. Assumptions: The states of the drone, target  player bench, ball basket and schedule board state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Tennis Court. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target**2 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Mosque. \n Objectives: It must avoid colliding with concrete columns or prayer rows. \n Conditions: The states of the agent, the mihrab, and the prayer rows are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Mosque. \n Objectives: Detect shoe rack and aviod it. \n Conditions: The states of the agent, the shoe rack are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_shoe_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_shoe_rack = calculate_distance_min(s_agent, s_shoe_rack) \n    penalty_collision = -20 if (d_agent_shoe_rack < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through Mosque open workspace. Objectives: It must avoid colliding with chandeliers. Conditions: States of the drone and the chandeliers are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_chandelier, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chandelier = calculate_distance_min(s_agent, s_chandelier) \n    penalty_collision = -20 if (d_agent_chandelier < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a prayer hall, the drone must maneuver around prayer row. All entity states (agent, ) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_prayer_row, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_prayer_row = calculate_distance_min(s_agent, s_prayer_row) \n    penalty_collision = -20 if (d_agent_prayer_row < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a prayer hall. Requirement: Detect the three minbars and do not collide with it. Given: The states for the drone and many minbars are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a ablution Mosque during experiments. \n Objectives: So not collide with shelf. \n Conditions: The states of the agent, the shelf are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_bookshelf, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bookshelf = calculate_distance_min(s_agent, s_bookshelf) \n    penalty_collision = -20 if (d_agent_bookshelf < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Mosque. Objectives: Do not collide hit with minbar. Conditions: States of the agent, minbar are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_minbar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_target_minbart = calculate_distance_min(s_agent, s_minbar) \n    penalty_collision = -20 if (d_agent_target_minbart < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a prayer hall, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -20 if (d_agent_wall < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: shoe area. Avoid: Detect donation box and do  not hitting with donation box. Known: Agent, donation box states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_donation_box, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_donation_box = calculate_distance_min(s_agent, s_donation_box) \n    penalty_collision = -20 if (d_agent_donation_box < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through Mosque. Objectives: It must avoid colliding with loudspeaker. Conditions: The states of the drone and loudspeaker are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_loudspeaker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_loudspeaker = calculate_distance_min(s_agent, s_loudspeaker) \n    penalty_collision = -20 if (d_agent_loudspeaker < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Mosque. Objectives: Avoid collision with cloak racks, people and loudspeaker system. Conditions: The states of the agent, cloak racks, people and loudspeaker system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through Mosque. Objectives: It must avoid colliding with any donation box, units and computers. Conditions: Agent's, donation box, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Mosque. Objectives: Do not collide with any cloack rack. Conditions: The states of the drone and the cloack rack are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_cloak_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cloak_rack = calculate_distance_min(s_agent, s_cloak_rack) \n    penalty = -20 if (d_agent_cloak_rack < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Mosque and must avoid contact with any calligraphy panels. The states of the drone and the calligraphy panels are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_calligraphy_panel, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_calligraphy_panel = calculate_distance_min(s_agent, s_calligraphy_panel) \n    penalty = -20 if (d_agent_calligraphy_panel < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Mosque. It must avoid colliding with the donation box, Quran stand and play sign. The states of the agent, book shelves, shoe racks and safety divider stands are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Mosque. It must avoid colliding with divider stands and prayer rows. The states of the drone and the water tuck and donation_box are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Mosque. It must avoid colliding with Mosque aisle divider stand. Agent's state, target's state and divider stand's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_divider_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_divider_stand = calculate_distance_min(s_agent, s_divider_stand) \n    penalty_collision = -20 if (d_agent_divider_stand < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Mosque. Goal: It must avoid colliding with ablution fountain. Assumptions: The states of the drone and the ablution fountain's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_ablution_fountain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ablution_fountain = calculate_distance_min(s_agent, s_ablution_fountain) \n    penalty_collision = -20 if (d_agent_ablution_fountain < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Mosque. Goal: It must avoid colliding with Mosque's book shelves, storage room and divider stands. Assumptions: The Agent's state, target's state and book shelves, storage room and divider stands's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Mosque. Goal: Detect wooden bench, shoe rack and Quran stand and do not collide. Assumptions: The states of the drone, target  wooden bench, shoe rack and Quran stand state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Mosque. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Music Hall. \n Objectives: It must avoid colliding with concrete columns or audience rows. \n Conditions: The states of the agent, the stage, and the audience rows are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Music Hall. \n Objectives: Detect coat rack and aviod it. \n Conditions: The states of the agent, the coat rack are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_coat_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_coat_rack = calculate_distance_min(s_agent, s_coat_rack) \n    penalty_collision = -20 if (d_agent_coat_rack < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Music Hall. Objectives: It must avoid colliding with spotlights. Conditions: States of the drone and the spotlights are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_spotlight, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_spotlight = calculate_distance_min(s_agent, s_spotlight) \n    penalty_collision = -20 if (d_agent_spotlight < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a concert hall, the drone must maneuver around audience row. All entity states (agent, audience row) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_audience_row, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_audience_row = calculate_distance_min(s_agent, s_audience_row) \n    penalty_collision = -20 if (d_agent_audience_row < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a concert hall. Requirement: Detect the three podiums and do not collide with it. Given: The states for the drone and many podiums are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a ablution Music Hall during experiments. \n Objectives: So not collide with shelf. \n Conditions: The states of the agent, the shelf are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_music_shelf, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_music_shelf = calculate_distance_min(s_agent, s_music_shelf) \n    penalty_collision = -20 if (d_agent_music_shelf < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Music Hall. Objectives: Do not collide hit with podium. Conditions: States of the agent, podium are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_podium, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_podium = calculate_distance_min(s_agent, s_podium) \n    penalty_collision = -20 if (d_agent_podium < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a concert hall, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -20 if (d_agent_wall < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: ticket booth. Avoid: Detect speaker cabinet and do not hitting with speaker cabinet. Known: Agent, speaker cabinet states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_speaker_cabinet, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_speaker_cabinet = calculate_distance_min(s_agent, s_speaker_cabinet) \n    penalty_collision = -20 if (d_agent_speaker_cabinet < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through Music Hall. Objectives: It must avoid colliding with sound system. Conditions: The states of the drone and sound system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_sound_system, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sound_system = calculate_distance_min(s_agent, s_sound_system) \n    penalty_collision = -20 if (d_agent_sound_system < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Music Hall. Objectives: Avoid collision with coat stands, people and sound system. Conditions: The states of the agent, coat stands, people and sound system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through Music Hall. Objectives: It must avoid colliding with any speaker cabinet, units and computers. Conditions: Agent's, speaker cabinet, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Music Hall. Objectives: Do not collide with any cloack rack. Conditions: The states of the drone and the cloack rack are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_coat_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cloak_rack = calculate_distance_min(s_agent, s_coat_stand) \n    penalty = -20 if (d_agent_cloak_rack < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Music Hall and must avoid contact with any acoustic panels. The states of the drone and the acoustic panels are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_acoustic_panel, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_acoustic_panel = calculate_distance_min(s_agent, s_acoustic_panel) \n    penalty = -20 if (d_agent_acoustic_panel < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Music Hall. It must avoid colliding with the speaker cabinet, music stand and play sign. The states of the agent, music shelves, coat racks and safety barrier stands are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Music Hall. It must avoid colliding with barrier stands and audience rows. The states of the drone and the water tuck and speaker_cabinet are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Music Hall. It must avoid colliding with guitar. Agent's state, target's state and guitar's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_guitar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_guitar = calculate_distance_min(s_agent, s_guitar) \n    penalty_collision = -20 if (d_agent_guitar < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Music Hall. Goal: It must avoid colliding with violin. Assumptions: The states of the drone and the violin's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_violin, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_violin = calculate_distance_min(s_agent, s_violin) \n    penalty_collision = -20 if (d_agent_violin < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Music Hall. Goal: It must avoid colliding with Music Hall's music shelves, backstage room and barrier stands. Assumptions: The Agent's state, target's state and music shelves, backstage room and barrier stands's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Music Hall. Goal: Detect audience bench, coat rack and music stand and do not collide. Assumptions: The states of the drone, target  audience bench, coat rack and music stand state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Music Hall. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Hair Salon. \n Objectives: It must avoid colliding with concrete columns or styling chairs. \n Conditions: The states of the agent, the stylist station, and the styling chairs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Hair Salon. \n Objectives: Detect towel rack and aviod it. \n Conditions: The states of the agent, the towel rack are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_towel_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_towel_rack = calculate_distance_min(s_agent, s_towel_rack) \n    penalty_collision = -20 if (d_agent_towel_rack < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Hair Salon. Objectives: It must avoid colliding with hair dryers. Conditions: States of the drone and the hair dryers are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_hair_dryer, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_hair_dryer = calculate_distance_min(s_agent, s_hair_dryer) \n    penalty_collision = -20 if (d_agent_hair_dryer < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a salon floor, the drone must maneuver around styling chair. All entity states (agent, chair) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -20 if (d_agent_chair < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a salon floor. Requirement: Detect the three cashier counters and do not collide with it. Given: The states for the drone and many cashier counters are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a ablution Hair Salon during experiments. \n Objectives: So not collide with shelf. \n Conditions: The states of the agent, the shelf are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_product_shelf, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_product_shelf = calculate_distance_min(s_agent, s_product_shelf) \n    penalty_collision = -20 if (d_agent_product_shelf < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Hair Salon. Objectives: Do not collide hit with cashier counter. Conditions: States of the agent, cashier counter are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_cashier_counter, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cashier_counter = calculate_distance_min(s_agent, s_cashier_counter) \n    penalty_collision = -20 if (d_agent_cashier_counter < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a salon floor, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -20 if (d_agent_wall < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: reception desk. Avoid: Detect product display and do not hitting with product display. Known: Agent, product display states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_product_display, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_product_display = calculate_distance_min(s_agent, s_product_display) \n    penalty_collision = -20 if (d_agent_product_display < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Hair Salon. Objectives: It must avoid colliding with stereo system. Conditions: The states of the drone and stereo system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_customer, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_customer = calculate_distance_min(s_agent, s_customer) \n    penalty_collision = -20 if (d_agent_customer < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Hair Salon. Objectives: Avoid collision with mirror stands, people and stereo system. Conditions: The states of the agent, mirror stands, people and stereo system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Hair Salon. Objectives: It must avoid colliding with any product display, units and computers. Conditions: Agent's, product display, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Hair Salon. Objectives: Do not collide with any cloack rack. Conditions: The states of the drone and the cloack rack are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_mirror_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_mirror_stand = calculate_distance_min(s_agent, s_mirror_stand) \n    penalty = -20 if (d_agent_mirror_stand < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Hair Salon and must avoid contact with any mirrors. The states of the drone and the customer are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_customer, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_customer = calculate_distance_min(s_agent, s_customer) \n    penalty = -20 if (d_agent_customer < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Hair Salon. It must avoid colliding with the product display, magazine stand and play sign. The states of the agent, product shelves, towel racks and doors are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Hair Salon. It must avoid colliding with partition stands and styling chairs. The states of the drone and the basket and product_display are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Hair Salon. It must avoid colliding with hair dryer stand. Agent's state, target's state and hair dryer stand's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_hair_dryer_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_hair_dryer_stand = calculate_distance_min(s_agent, s_hair_dryer_stand) \n    penalty_collision = -20 if (d_agent_hair_dryer_stand < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Hair Salon. Goal: It must avoid colliding with shampoo chair. Assumptions: The states of the drone and the shampoo chair's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_shampoo_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_shampoo_chair = calculate_distance_min(s_agent, s_shampoo_chair) \n    penalty_collision = -20 if (d_agent_shampoo_chair < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Hair Salon. Goal: It must avoid colliding with Hair Salon's product shelves, backstylist station room and partition stands. Assumptions: The Agent's state, target's state and product shelves, backstylist station room and partition stands's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Hair Salon. Goal: Detect waiting bench, towel rack and magazine stand and do not collide. Assumptions: The states of the drone, target  waiting bench, towel rack and magazine stand state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Hair Salon. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Spa Resort. \n Objectives: It must avoid colliding with concrete columns or massage tables. \n Conditions: The states of the agent, the spa station, and the massage tables are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Spa Resort. \n Objectives: Detect towel warmer and aviod it. \n Conditions: The states of the agent, the towel warmer are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_towel_warmer, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_towel_warmer = calculate_distance_min(s_agent, s_towel_warmer) \n    penalty_collision = -20 if (d_agent_towel_warmer < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through a Spa Resort. Objectives: It must avoid colliding with sauna heater. Conditions: States of the drone and the sauna heater are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_sauna_heater, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sauna_heater = calculate_distance_min(s_agent, s_sauna_heater) \n    penalty_collision = -20 if (d_agent_sauna_heater < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a spa floor, the drone must maneuver around massage table. All entity states (agent, chair) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -20 if (d_agent_chair < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a spa floor. Requirement: Detect the three spa counters and do not collide with it. Given: The states for the drone and many spa counters are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a ablution Spa Resort during experiments. \n Objectives: So not collide with aroma diffuser. \n Conditions: The states of the agent, the aroma diffuser are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_aroma_diffuser, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_aroma_diffuser = calculate_distance_min(s_agent, s_aroma_diffuser) \n    penalty_collision = -20 if (d_agent_aroma_diffuser < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Spa Resort. Objectives: Do not collide hit with spa counter. Conditions: States of the agent, spa counter are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_spa_counter, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_spa_counter = calculate_distance_min(s_agent, s_spa_counter) \n    penalty_collision = -20 if (d_agent_spa_counter < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a spa floor, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -20 if (d_agent_wall < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: front desk. Avoid: Detect flower arrangement and do not hitting with flower arrangement. Known: Agent, flower arrangement states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_flower_arrangement, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_flower_arrangement = calculate_distance_min(s_agent, s_flower_arrangement) \n    penalty_collision = -20 if (d_agent_flower_arrangement < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Spa Resort. Objectives: It must avoid colliding with spa statue. Conditions: The states of the drone and spa statue are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_spa_statue, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_spa_statue = calculate_distance_min(s_agent, s_spa_statue) \n    penalty_collision = -20 if (d_agent_spa_statue < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Spa Resort. Objectives: Avoid collision with decorative screen, people and water fountain. Conditions: The states of the agent, decorative screen, people and water fountain are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Spa Resort. Objectives: It must avoid colliding with any flower arrangement, units and computers. Conditions: Agent's, flower arrangement, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Spa Resort. Objectives: Do not collide with sauna heater. Conditions: The states of the drone and the sauna heater are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_sauna_heater, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sauna_heater = calculate_distance_min(s_agent, s_sauna_heater) \n    penalty = -20 if (d_agent_sauna_heater < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Spa Resort and must avoid contact with the oil diffuser stand. The states of the drone and the oil stand are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_oil_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_oil_stand = calculate_distance_min(s_agent, s_oil_stand) \n    penalty = -20 if (d_agent_oil_stand < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Spa Resort. It must avoid colliding with the flower arrangement, tea table and play sign. The states of the agent, insliding screen plants, towel warmers and sliding screens are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Spa Resort. It must avoid colliding with spa divider and massage tables. The states of the drone and the stone sculpture and flower_arrangement are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Spa Resort. It must avoid colliding with the tea table. Agent's state, target's state and tea table's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_tea_table, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tea_table = calculate_distance_min(s_agent, s_tea_table) \n    penalty_collision = -20 if (d_agent_tea_table < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Spa Resort. Goal: It must avoid colliding with foot bath. Assumptions: The states of the drone and the foot bath's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_foot_bath, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_foot_bath = calculate_distance_min(s_agent, s_foot_bath) \n    penalty_collision = -20 if (d_agent_foot_bath < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Spa Resort. Goal: It must avoid colliding with Spa Resort's insliding screen plants, backspa station room and spa divider. Assumptions: The Agent's state, target's state and insliding screen plants, backspa station room and spa divider's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Spa Resort. Goal: Detect lounge chair, towel warmer and tea table and do not collide. Assumptions: The states of the drone, target  lounge chair, towel warmer and tea table state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Spa Resort. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Aquarium. \n Objectives: It must avoid colliding with concrete columns or viewing benches. \n Conditions: The states of the agent, the aquarium tank, and the viewing benches are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Aquarium. \n Objectives: Detect filter pump and aviod it. \n Conditions: The states of the agent, the filter pump are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_filter_pump, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_filter_pump = calculate_distance_min(s_agent, s_filter_pump) \n    penalty_collision = -20 if (d_agent_filter_pump < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through an aquarium. Objectives: It must avoid colliding with UV sterilizer. Conditions: States of the drone and the UV sterilizer are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bench, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bench = calculate_distance_min(s_agent, s_bench) \n    penalty_collision = -20 if (d_agent_bench < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a aquarium hall, the drone must maneuver around viewing bench. All entity states (agent, chair) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -20 if (d_agent_chair < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a aquarium hall. Requirement: Detect the three touch pools and do not collide with it. Given: The states for the drone and many touch pools are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a ablution Aquarium during experiments. \n Objectives: So not collide with air bubbler. \n Conditions: The states of the agent, the air bubbler are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_air_bubbler, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_air_bubbler = calculate_distance_min(s_agent, s_air_bubbler) \n    penalty_collision = -20 if (d_agent_air_bubbler < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Aquarium. Objectives: Do not collide hit with touch pool. Conditions: States of the agent, touch pool are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_touch_pool, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_touch_pool = calculate_distance_min(s_agent, s_touch_pool) \n    penalty_collision = -20 if (d_agent_touch_pool < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a aquarium hall, the drone must steer clear of the visitor. All states (agent, visitor) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_visitor, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_visitor = calculate_distance_min(s_agent, s_visitor) \n    penalty_collision = -20 if (d_agent_visitor < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: ticket counter. Avoid: Detect coral display and do not hitting with coral display. Known: Agent, coral display states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_coral_display, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_coral_display = calculate_distance_min(s_agent, s_coral_display) \n    penalty_collision = -20 if (d_agent_coral_display < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Aquarium. Objectives: It must avoid colliding with marine sculpture. Conditions: The states of the drone and marine sculpture are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_marine_sculpture, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_marine_sculpture = calculate_distance_min(s_agent, s_marine_sculpture) \n    penalty_collision = -20 if (d_agent_marine_sculpture < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Aquarium. Objectives: Avoid collision with glass panel, people and fish tank. Conditions: The states of the agent, glass panel, people and fish tank are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Aquarium. Objectives: It must avoid colliding with any coral display, units and computers. Conditions: Agent's, coral display, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Aquarium. Objectives: Do not collide with viewing_bench. Conditions: The states of the drone and the viewing_bench are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bench, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bench = calculate_distance_min(s_agent, s_bench) \n    penalty = -20 if (d_agent_bench < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Aquarium and must avoid contact with the oxygen pump. The states of the drone and the oxygen pump are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_oxygen_pump, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_oxygen_pump = calculate_distance_min(s_agent, s_oxygen_pump) \n    penalty = -20 if (d_agent_oxygen_pump < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Aquarium. It must avoid colliding with the coral display, info stand and play sign. The states of the agent, inglass door plants, filter pumps and glass doors are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Aquarium. It must avoid colliding with aquarium barrier and viewing benches. The states of the drone and the aquarium statue and coral_display are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Aquarium. It must avoid colliding with the info stand. Agent's state, target's state and info stand's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_info_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_info_stand = calculate_distance_min(s_agent, s_info_stand) \n    penalty_collision = -20 if (d_agent_info_stand < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Aquarium. Goal: It must avoid colliding with shallow tank. Assumptions: The states of the drone and the shallow tank's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_tank, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tank = calculate_distance_min(s_agent, s_tank) \n    penalty_collision = -20 if (d_agent_tank < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Aquarium. Goal: It must avoid colliding with Aquarium's inglass door plants, backaquarium tank room and aquarium barrier. Assumptions: The Agent's state, target's state and inglass door plants, backaquarium tank room and aquarium barrier's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Aquarium. Goal: Detect visitor bench, filter pump and info stand and do not collide. Assumptions: The states of the drone, target  visitor bench, filter pump and info stand state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Aquarium. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Tennis Court. \n Objectives: It must avoid colliding with concrete columns or spectator benches. \n Conditions: The states of the agent, the net post, and the spectator benches are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Tennis Court. \n Objectives: Detect ball basket and aviod it. \n Conditions: The states of the agent, the ball basket are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ball_basket, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ball_basket = calculate_distance_min(s_agent, s_ball_basket) \n    penalty_collision = -20 if (d_agent_ball_basket < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Tennis Court open workspace. Objectives: It must avoid colliding with ball machine. Conditions: States of the drone and the ball machine are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ball_machine, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ball_machine = calculate_distance_min(s_agent, s_ball_machine) \n    penalty_collision = -20 if (d_agent_ball_machine < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a tennis court area, the drone must maneuver around spectator bench. All entity states (agent, chair) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -20 if (d_agent_chair < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a tennis court area. Requirement: Detect the three practice walls and do not collide with it. Given: The states for the drone and many practice walls are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a ablution Tennis Court during experiments. \n Objectives: So not collide with water cooler. \n Conditions: The states of the agent, the water cooler are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_water_cooler, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_water_cooler = calculate_distance_min(s_agent, s_water_cooler) \n    penalty_collision = -20 if (d_agent_water_cooler < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Tennis Court. Objectives: Do not collide hit with practice wall. Conditions: States of the agent, practice wall are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_practice_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_practice_wall = calculate_distance_min(s_agent, s_practice_wall) \n    penalty_collision = -20 if (d_agent_practice_wall < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a tennis court area, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -20 if (d_agent_wall < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: umpire chair. Avoid: Detect scoreboard and do not hitting with scoreboard. Known: Agent, scoreboard states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_scoreboard, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_scoreboard = calculate_distance_min(s_agent, s_scoreboard) \n    penalty_collision = -20 if (d_agent_scoreboard < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Tennis Court. Objectives: It must avoid colliding with tennis monument. Conditions: The states of the drone and tennis monument are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_monument, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_monument = calculate_distance_min(s_agent, s_monument) \n    penalty_collision = -20 if (d_agent_monument < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Tennis Court. Objectives: Avoid collision with fence panel, people and ball cart. Conditions: The states of the agent, fence panel, people and ball cart are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Tennis Court. Objectives: It must avoid colliding with any scoreboard, units and computers. Conditions: Agent's, scoreboard, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Tennis Court. Objectives: Do not collide with ball machine. Conditions: The states of the drone and the ball machine are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ball_machine, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ball_machine = calculate_distance_min(s_agent, s_ball_machine) \n    penalty = -20 if (d_agent_ball_machine < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Tennis Court and must avoid contact with the lighting pole. The states of the drone and the lighting pole are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_pole, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_pole = calculate_distance_min(s_agent, s_pole) \n    penalty = -20 if (d_agent_pole < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Tennis Court. It must avoid colliding with the scoreboard, schedule board and play sign. The states of the agent, incourt gate plants, ball baskets and court gates are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Tennis Court. It must avoid colliding with boundary fence and spectator benches. The states of the drone and the court statue and scoreboard are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Tennis Court. It must avoid colliding with the schedule board. Agent's state, target's state and schedule board's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_schedule_board, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_schedule_board = calculate_distance_min(s_agent, s_schedule_board) \n    penalty_collision = -20 if (d_agent_schedule_board < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Tennis Court. Goal: It must avoid colliding with clay patch. Assumptions: The states of the drone and the clay patch's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_clay_patch, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_foot_bath = calculate_distance_min(s_agent, s_clay_patch) \n    penalty_collision = -20 if (d_agent_foot_bath < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Tennis Court. Goal: It must avoid colliding with Tennis Court's incourt gate plants, backnet post room and boundary fence. Assumptions: The Agent's state, target's state and incourt gate plants, backnet post room and boundary fence's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Tennis Court. Goal: Detect player bench, ball basket and schedule board and do not collide. Assumptions: The states of the drone, target  player bench, ball basket and schedule board state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Tennis Court. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target*5 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Mosque. \n Objectives: It must avoid colliding with concrete columns or prayer rows. \n Conditions: The states of the agent, the mihrab, and the prayer rows are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Mosque. \n Objectives: Detect shoe rack and aviod it. \n Conditions: The states of the agent, the shoe rack are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_shoe_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_shoe_rack = calculate_distance_min(s_agent, s_shoe_rack) \n    penalty_collision = -20 if (d_agent_shoe_rack < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through Mosque open workspace. Objectives: It must avoid colliding with chandeliers. Conditions: States of the drone and the chandeliers are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_chandelier, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chandelier = calculate_distance_min(s_agent, s_chandelier) \n    penalty_collision = -20 if (d_agent_chandelier < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a prayer hall, the drone must maneuver around prayer row. All entity states (agent, ) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_prayer_row, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_prayer_row = calculate_distance_min(s_agent, s_prayer_row) \n    penalty_collision = -20 if (d_agent_prayer_row < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a prayer hall. Requirement: Detect the three minbars and do not collide with it. Given: The states for the drone and many minbars are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a ablution Mosque during experiments. \n Objectives: So not collide with shelf. \n Conditions: The states of the agent, the shelf are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_bookshelf, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bookshelf = calculate_distance_min(s_agent, s_bookshelf) \n    penalty_collision = -20 if (d_agent_bookshelf < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Mosque. Objectives: Do not collide hit with minbar. Conditions: States of the agent, minbar are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_minbar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_target_minbart = calculate_distance_min(s_agent, s_minbar) \n    penalty_collision = -20 if (d_agent_target_minbart < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a prayer hall, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -20 if (d_agent_wall < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: shoe area. Avoid: Detect donation box and do  not hitting with donation box. Known: Agent, donation box states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_donation_box, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_donation_box = calculate_distance_min(s_agent, s_donation_box) \n    penalty_collision = -20 if (d_agent_donation_box < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through Mosque. Objectives: It must avoid colliding with loudspeaker. Conditions: The states of the drone and loudspeaker are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_loudspeaker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_loudspeaker = calculate_distance_min(s_agent, s_loudspeaker) \n    penalty_collision = -20 if (d_agent_loudspeaker < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Mosque. Objectives: Avoid collision with cloak racks, people and loudspeaker system. Conditions: The states of the agent, cloak racks, people and loudspeaker system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through Mosque. Objectives: It must avoid colliding with any donation box, units and computers. Conditions: Agent's, donation box, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Mosque. Objectives: Do not collide with any cloack rack. Conditions: The states of the drone and the cloack rack are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_cloak_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cloak_rack = calculate_distance_min(s_agent, s_cloak_rack) \n    penalty = -20 if (d_agent_cloak_rack < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Mosque and must avoid contact with any calligraphy panels. The states of the drone and the calligraphy panels are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_calligraphy_panel, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_calligraphy_panel = calculate_distance_min(s_agent, s_calligraphy_panel) \n    penalty = -20 if (d_agent_calligraphy_panel < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Mosque. It must avoid colliding with the donation box, Quran stand and play sign. The states of the agent, book shelves, shoe racks and safety divider stands are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Mosque. It must avoid colliding with divider stands and prayer rows. The states of the drone and the water tuck and donation_box are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Mosque. It must avoid colliding with Mosque aisle divider stand. Agent's state, target's state and divider stand's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_divider_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_divider_stand = calculate_distance_min(s_agent, s_divider_stand) \n    penalty_collision = -20 if (d_agent_divider_stand < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Mosque. Goal: It must avoid colliding with ablution fountain. Assumptions: The states of the drone and the ablution fountain's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_ablution_fountain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ablution_fountain = calculate_distance_min(s_agent, s_ablution_fountain) \n    penalty_collision = -20 if (d_agent_ablution_fountain < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Mosque. Goal: It must avoid colliding with Mosque's book shelves, storage room and divider stands. Assumptions: The Agent's state, target's state and book shelves, storage room and divider stands's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Mosque. Goal: Detect wooden bench, shoe rack and Quran stand and do not collide. Assumptions: The states of the drone, target  wooden bench, shoe rack and Quran stand state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Mosque. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Music Hall. \n Objectives: It must avoid colliding with concrete columns or audience rows. \n Conditions: The states of the agent, the stage, and the audience rows are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Music Hall. \n Objectives: Detect coat rack and aviod it. \n Conditions: The states of the agent, the coat rack are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_coat_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_coat_rack = calculate_distance_min(s_agent, s_coat_rack) \n    penalty_collision = -20 if (d_agent_coat_rack < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Music Hall. Objectives: It must avoid colliding with spotlights. Conditions: States of the drone and the spotlights are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_spotlight, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_spotlight = calculate_distance_min(s_agent, s_spotlight) \n    penalty_collision = -20 if (d_agent_spotlight < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a concert hall, the drone must maneuver around audience row. All entity states (agent, audience row) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_audience_row, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_audience_row = calculate_distance_min(s_agent, s_audience_row) \n    penalty_collision = -20 if (d_agent_audience_row < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a concert hall. Requirement: Detect the three podiums and do not collide with it. Given: The states for the drone and many podiums are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a ablution Music Hall during experiments. \n Objectives: So not collide with shelf. \n Conditions: The states of the agent, the shelf are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_music_shelf, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_music_shelf = calculate_distance_min(s_agent, s_music_shelf) \n    penalty_collision = -20 if (d_agent_music_shelf < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Music Hall. Objectives: Do not collide hit with podium. Conditions: States of the agent, podium are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_podium, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_podium = calculate_distance_min(s_agent, s_podium) \n    penalty_collision = -20 if (d_agent_podium < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a concert hall, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -20 if (d_agent_wall < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: ticket booth. Avoid: Detect speaker cabinet and do not hitting with speaker cabinet. Known: Agent, speaker cabinet states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_speaker_cabinet, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_speaker_cabinet = calculate_distance_min(s_agent, s_speaker_cabinet) \n    penalty_collision = -20 if (d_agent_speaker_cabinet < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through Music Hall. Objectives: It must avoid colliding with sound system. Conditions: The states of the drone and sound system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_sound_system, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sound_system = calculate_distance_min(s_agent, s_sound_system) \n    penalty_collision = -20 if (d_agent_sound_system < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Music Hall. Objectives: Avoid collision with coat stands, people and sound system. Conditions: The states of the agent, coat stands, people and sound system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through Music Hall. Objectives: It must avoid colliding with any speaker cabinet, units and computers. Conditions: Agent's, speaker cabinet, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Music Hall. Objectives: Do not collide with any cloack rack. Conditions: The states of the drone and the cloack rack are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_coat_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cloak_rack = calculate_distance_min(s_agent, s_coat_stand) \n    penalty = -20 if (d_agent_cloak_rack < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Music Hall and must avoid contact with any acoustic panels. The states of the drone and the acoustic panels are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_acoustic_panel, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_acoustic_panel = calculate_distance_min(s_agent, s_acoustic_panel) \n    penalty = -20 if (d_agent_acoustic_panel < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Music Hall. It must avoid colliding with the speaker cabinet, music stand and play sign. The states of the agent, music shelves, coat racks and safety barrier stands are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Music Hall. It must avoid colliding with barrier stands and audience rows. The states of the drone and the water tuck and speaker_cabinet are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Music Hall. It must avoid colliding with guitar. Agent's state, target's state and guitar's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_guitar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_guitar = calculate_distance_min(s_agent, s_guitar) \n    penalty_collision = -20 if (d_agent_guitar < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Music Hall. Goal: It must avoid colliding with violin. Assumptions: The states of the drone and the violin's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_violin, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_violin = calculate_distance_min(s_agent, s_violin) \n    penalty_collision = -20 if (d_agent_violin < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Music Hall. Goal: It must avoid colliding with Music Hall's music shelves, backstage room and barrier stands. Assumptions: The Agent's state, target's state and music shelves, backstage room and barrier stands's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Music Hall. Goal: Detect audience bench, coat rack and music stand and do not collide. Assumptions: The states of the drone, target  audience bench, coat rack and music stand state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Music Hall. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Hair Salon. \n Objectives: It must avoid colliding with concrete columns or styling chairs. \n Conditions: The states of the agent, the stylist station, and the styling chairs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Hair Salon. \n Objectives: Detect towel rack and aviod it. \n Conditions: The states of the agent, the towel rack are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_towel_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_towel_rack = calculate_distance_min(s_agent, s_towel_rack) \n    penalty_collision = -20 if (d_agent_towel_rack < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Hair Salon. Objectives: It must avoid colliding with hair dryers. Conditions: States of the drone and the hair dryers are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_hair_dryer, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_hair_dryer = calculate_distance_min(s_agent, s_hair_dryer) \n    penalty_collision = -20 if (d_agent_hair_dryer < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a salon floor, the drone must maneuver around styling chair. All entity states (agent, chair) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -20 if (d_agent_chair < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a salon floor. Requirement: Detect the three cashier counters and do not collide with it. Given: The states for the drone and many cashier counters are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a ablution Hair Salon during experiments. \n Objectives: So not collide with shelf. \n Conditions: The states of the agent, the shelf are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_product_shelf, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_product_shelf = calculate_distance_min(s_agent, s_product_shelf) \n    penalty_collision = -20 if (d_agent_product_shelf < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Hair Salon. Objectives: Do not collide hit with cashier counter. Conditions: States of the agent, cashier counter are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_cashier_counter, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cashier_counter = calculate_distance_min(s_agent, s_cashier_counter) \n    penalty_collision = -20 if (d_agent_cashier_counter < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a salon floor, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -20 if (d_agent_wall < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: reception desk. Avoid: Detect product display and do not hitting with product display. Known: Agent, product display states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_product_display, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_product_display = calculate_distance_min(s_agent, s_product_display) \n    penalty_collision = -20 if (d_agent_product_display < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Hair Salon. Objectives: It must avoid colliding with stereo system. Conditions: The states of the drone and stereo system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_customer, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_customer = calculate_distance_min(s_agent, s_customer) \n    penalty_collision = -20 if (d_agent_customer < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Hair Salon. Objectives: Avoid collision with mirror stands, people and stereo system. Conditions: The states of the agent, mirror stands, people and stereo system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Hair Salon. Objectives: It must avoid colliding with any product display, units and computers. Conditions: Agent's, product display, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Hair Salon. Objectives: Do not collide with any cloack rack. Conditions: The states of the drone and the cloack rack are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_mirror_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_mirror_stand = calculate_distance_min(s_agent, s_mirror_stand) \n    penalty = -20 if (d_agent_mirror_stand < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Hair Salon and must avoid contact with any mirrors. The states of the drone and the customer are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_customer, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_customer = calculate_distance_min(s_agent, s_customer) \n    penalty = -20 if (d_agent_customer < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Hair Salon. It must avoid colliding with the product display, magazine stand and play sign. The states of the agent, product shelves, towel racks and doors are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Hair Salon. It must avoid colliding with partition stands and styling chairs. The states of the drone and the basket and product_display are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Hair Salon. It must avoid colliding with hair dryer stand. Agent's state, target's state and hair dryer stand's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_hair_dryer_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_hair_dryer_stand = calculate_distance_min(s_agent, s_hair_dryer_stand) \n    penalty_collision = -20 if (d_agent_hair_dryer_stand < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Hair Salon. Goal: It must avoid colliding with shampoo chair. Assumptions: The states of the drone and the shampoo chair's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_shampoo_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_shampoo_chair = calculate_distance_min(s_agent, s_shampoo_chair) \n    penalty_collision = -20 if (d_agent_shampoo_chair < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Hair Salon. Goal: It must avoid colliding with Hair Salon's product shelves, backstylist station room and partition stands. Assumptions: The Agent's state, target's state and product shelves, backstylist station room and partition stands's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Hair Salon. Goal: Detect waiting bench, towel rack and magazine stand and do not collide. Assumptions: The states of the drone, target  waiting bench, towel rack and magazine stand state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Hair Salon. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Spa Resort. \n Objectives: It must avoid colliding with concrete columns or massage tables. \n Conditions: The states of the agent, the spa station, and the massage tables are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Spa Resort. \n Objectives: Detect towel warmer and aviod it. \n Conditions: The states of the agent, the towel warmer are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_towel_warmer, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_towel_warmer = calculate_distance_min(s_agent, s_towel_warmer) \n    penalty_collision = -20 if (d_agent_towel_warmer < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through a Spa Resort. Objectives: It must avoid colliding with sauna heater. Conditions: States of the drone and the sauna heater are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_sauna_heater, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sauna_heater = calculate_distance_min(s_agent, s_sauna_heater) \n    penalty_collision = -20 if (d_agent_sauna_heater < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a spa floor, the drone must maneuver around massage table. All entity states (agent, chair) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -20 if (d_agent_chair < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a spa floor. Requirement: Detect the three spa counters and do not collide with it. Given: The states for the drone and many spa counters are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a ablution Spa Resort during experiments. \n Objectives: So not collide with aroma diffuser. \n Conditions: The states of the agent, the aroma diffuser are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_aroma_diffuser, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_aroma_diffuser = calculate_distance_min(s_agent, s_aroma_diffuser) \n    penalty_collision = -20 if (d_agent_aroma_diffuser < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Spa Resort. Objectives: Do not collide hit with spa counter. Conditions: States of the agent, spa counter are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_spa_counter, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_spa_counter = calculate_distance_min(s_agent, s_spa_counter) \n    penalty_collision = -20 if (d_agent_spa_counter < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a spa floor, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -20 if (d_agent_wall < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: front desk. Avoid: Detect flower arrangement and do not hitting with flower arrangement. Known: Agent, flower arrangement states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_flower_arrangement, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_flower_arrangement = calculate_distance_min(s_agent, s_flower_arrangement) \n    penalty_collision = -20 if (d_agent_flower_arrangement < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Spa Resort. Objectives: It must avoid colliding with spa statue. Conditions: The states of the drone and spa statue are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_spa_statue, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_spa_statue = calculate_distance_min(s_agent, s_spa_statue) \n    penalty_collision = -20 if (d_agent_spa_statue < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Spa Resort. Objectives: Avoid collision with decorative screen, people and water fountain. Conditions: The states of the agent, decorative screen, people and water fountain are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Spa Resort. Objectives: It must avoid colliding with any flower arrangement, units and computers. Conditions: Agent's, flower arrangement, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Spa Resort. Objectives: Do not collide with sauna heater. Conditions: The states of the drone and the sauna heater are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_sauna_heater, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sauna_heater = calculate_distance_min(s_agent, s_sauna_heater) \n    penalty = -20 if (d_agent_sauna_heater < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Spa Resort and must avoid contact with the oil diffuser stand. The states of the drone and the oil stand are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_oil_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_oil_stand = calculate_distance_min(s_agent, s_oil_stand) \n    penalty = -20 if (d_agent_oil_stand < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Spa Resort. It must avoid colliding with the flower arrangement, tea table and play sign. The states of the agent, insliding screen plants, towel warmers and sliding screens are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Spa Resort. It must avoid colliding with spa divider and massage tables. The states of the drone and the stone sculpture and flower_arrangement are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Spa Resort. It must avoid colliding with the tea table. Agent's state, target's state and tea table's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_tea_table, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tea_table = calculate_distance_min(s_agent, s_tea_table) \n    penalty_collision = -20 if (d_agent_tea_table < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Spa Resort. Goal: It must avoid colliding with foot bath. Assumptions: The states of the drone and the foot bath's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_foot_bath, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_foot_bath = calculate_distance_min(s_agent, s_foot_bath) \n    penalty_collision = -20 if (d_agent_foot_bath < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Spa Resort. Goal: It must avoid colliding with Spa Resort's insliding screen plants, backspa station room and spa divider. Assumptions: The Agent's state, target's state and insliding screen plants, backspa station room and spa divider's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Spa Resort. Goal: Detect lounge chair, towel warmer and tea table and do not collide. Assumptions: The states of the drone, target  lounge chair, towel warmer and tea table state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Spa Resort. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Aquarium. \n Objectives: It must avoid colliding with concrete columns or viewing benches. \n Conditions: The states of the agent, the aquarium tank, and the viewing benches are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Aquarium. \n Objectives: Detect filter pump and aviod it. \n Conditions: The states of the agent, the filter pump are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_filter_pump, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_filter_pump = calculate_distance_min(s_agent, s_filter_pump) \n    penalty_collision = -20 if (d_agent_filter_pump < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through an aquarium. Objectives: It must avoid colliding with UV sterilizer. Conditions: States of the drone and the UV sterilizer are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bench, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bench = calculate_distance_min(s_agent, s_bench) \n    penalty_collision = -20 if (d_agent_bench < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a aquarium hall, the drone must maneuver around viewing bench. All entity states (agent, chair) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -20 if (d_agent_chair < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a aquarium hall. Requirement: Detect the three touch pools and do not collide with it. Given: The states for the drone and many touch pools are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a ablution Aquarium during experiments. \n Objectives: So not collide with air bubbler. \n Conditions: The states of the agent, the air bubbler are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_air_bubbler, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_air_bubbler = calculate_distance_min(s_agent, s_air_bubbler) \n    penalty_collision = -20 if (d_agent_air_bubbler < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Aquarium. Objectives: Do not collide hit with touch pool. Conditions: States of the agent, touch pool are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_touch_pool, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_touch_pool = calculate_distance_min(s_agent, s_touch_pool) \n    penalty_collision = -20 if (d_agent_touch_pool < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a aquarium hall, the drone must steer clear of the visitor. All states (agent, visitor) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_visitor, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_visitor = calculate_distance_min(s_agent, s_visitor) \n    penalty_collision = -20 if (d_agent_visitor < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: ticket counter. Avoid: Detect coral display and do not hitting with coral display. Known: Agent, coral display states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_coral_display, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_coral_display = calculate_distance_min(s_agent, s_coral_display) \n    penalty_collision = -20 if (d_agent_coral_display < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Aquarium. Objectives: It must avoid colliding with marine sculpture. Conditions: The states of the drone and marine sculpture are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_marine_sculpture, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_marine_sculpture = calculate_distance_min(s_agent, s_marine_sculpture) \n    penalty_collision = -20 if (d_agent_marine_sculpture < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Aquarium. Objectives: Avoid collision with glass panel, people and fish tank. Conditions: The states of the agent, glass panel, people and fish tank are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Aquarium. Objectives: It must avoid colliding with any coral display, units and computers. Conditions: Agent's, coral display, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Aquarium. Objectives: Do not collide with viewing_bench. Conditions: The states of the drone and the viewing_bench are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bench, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bench = calculate_distance_min(s_agent, s_bench) \n    penalty = -20 if (d_agent_bench < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Aquarium and must avoid contact with the oxygen pump. The states of the drone and the oxygen pump are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_oxygen_pump, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_oxygen_pump = calculate_distance_min(s_agent, s_oxygen_pump) \n    penalty = -20 if (d_agent_oxygen_pump < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Aquarium. It must avoid colliding with the coral display, info stand and play sign. The states of the agent, inglass door plants, filter pumps and glass doors are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Aquarium. It must avoid colliding with aquarium barrier and viewing benches. The states of the drone and the aquarium statue and coral_display are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Aquarium. It must avoid colliding with the info stand. Agent's state, target's state and info stand's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_info_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_info_stand = calculate_distance_min(s_agent, s_info_stand) \n    penalty_collision = -20 if (d_agent_info_stand < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Aquarium. Goal: It must avoid colliding with shallow tank. Assumptions: The states of the drone and the shallow tank's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_tank, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tank = calculate_distance_min(s_agent, s_tank) \n    penalty_collision = -20 if (d_agent_tank < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Aquarium. Goal: It must avoid colliding with Aquarium's inglass door plants, backaquarium tank room and aquarium barrier. Assumptions: The Agent's state, target's state and inglass door plants, backaquarium tank room and aquarium barrier's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Aquarium. Goal: Detect visitor bench, filter pump and info stand and do not collide. Assumptions: The states of the drone, target  visitor bench, filter pump and info stand state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Aquarium. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Tennis Court. \n Objectives: It must avoid colliding with concrete columns or spectator benches. \n Conditions: The states of the agent, the net post, and the spectator benches are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Tennis Court. \n Objectives: Detect ball basket and aviod it. \n Conditions: The states of the agent, the ball basket are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ball_basket, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ball_basket = calculate_distance_min(s_agent, s_ball_basket) \n    penalty_collision = -20 if (d_agent_ball_basket < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Tennis Court open workspace. Objectives: It must avoid colliding with ball machine. Conditions: States of the drone and the ball machine are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ball_machine, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ball_machine = calculate_distance_min(s_agent, s_ball_machine) \n    penalty_collision = -20 if (d_agent_ball_machine < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a tennis court area, the drone must maneuver around spectator bench. All entity states (agent, chair) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -20 if (d_agent_chair < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a tennis court area. Requirement: Detect the three practice walls and do not collide with it. Given: The states for the drone and many practice walls are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a ablution Tennis Court during experiments. \n Objectives: So not collide with water cooler. \n Conditions: The states of the agent, the water cooler are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_water_cooler, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_water_cooler = calculate_distance_min(s_agent, s_water_cooler) \n    penalty_collision = -20 if (d_agent_water_cooler < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Tennis Court. Objectives: Do not collide hit with practice wall. Conditions: States of the agent, practice wall are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_practice_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_practice_wall = calculate_distance_min(s_agent, s_practice_wall) \n    penalty_collision = -20 if (d_agent_practice_wall < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a tennis court area, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -20 if (d_agent_wall < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: umpire chair. Avoid: Detect scoreboard and do not hitting with scoreboard. Known: Agent, scoreboard states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_scoreboard, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_scoreboard = calculate_distance_min(s_agent, s_scoreboard) \n    penalty_collision = -20 if (d_agent_scoreboard < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Tennis Court. Objectives: It must avoid colliding with tennis monument. Conditions: The states of the drone and tennis monument are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_monument, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_monument = calculate_distance_min(s_agent, s_monument) \n    penalty_collision = -20 if (d_agent_monument < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Tennis Court. Objectives: Avoid collision with fence panel, people and ball cart. Conditions: The states of the agent, fence panel, people and ball cart are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Tennis Court. Objectives: It must avoid colliding with any scoreboard, units and computers. Conditions: Agent's, scoreboard, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Tennis Court. Objectives: Do not collide with ball machine. Conditions: The states of the drone and the ball machine are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ball_machine, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ball_machine = calculate_distance_min(s_agent, s_ball_machine) \n    penalty = -20 if (d_agent_ball_machine < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Tennis Court and must avoid contact with the lighting pole. The states of the drone and the lighting pole are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_pole, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_pole = calculate_distance_min(s_agent, s_pole) \n    penalty = -20 if (d_agent_pole < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Tennis Court. It must avoid colliding with the scoreboard, schedule board and play sign. The states of the agent, incourt gate plants, ball baskets and court gates are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Tennis Court. It must avoid colliding with boundary fence and spectator benches. The states of the drone and the court statue and scoreboard are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Tennis Court. It must avoid colliding with the schedule board. Agent's state, target's state and schedule board's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_schedule_board, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_schedule_board = calculate_distance_min(s_agent, s_schedule_board) \n    penalty_collision = -20 if (d_agent_schedule_board < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Tennis Court. Goal: It must avoid colliding with clay patch. Assumptions: The states of the drone and the clay patch's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_clay_patch, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_foot_bath = calculate_distance_min(s_agent, s_clay_patch) \n    penalty_collision = -20 if (d_agent_foot_bath < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Tennis Court. Goal: It must avoid colliding with Tennis Court's incourt gate plants, backnet post room and boundary fence. Assumptions: The Agent's state, target's state and incourt gate plants, backnet post room and boundary fence's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Tennis Court. Goal: Detect player bench, ball basket and schedule board and do not collide. Assumptions: The states of the drone, target  player bench, ball basket and schedule board state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Tennis Court. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -2**d_agent_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Mosque. \n Objectives: It must avoid colliding with concrete columns or prayer rows. \n Conditions: The states of the agent, the mihrab, and the prayer rows are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Mosque. \n Objectives: Detect shoe rack and aviod it. \n Conditions: The states of the agent, the shoe rack are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_shoe_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_shoe_rack = calculate_distance_min(s_agent, s_shoe_rack) \n    penalty_collision = -20 if (d_agent_shoe_rack < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through Mosque open workspace. Objectives: It must avoid colliding with chandeliers. Conditions: States of the drone and the chandeliers are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_chandelier, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chandelier = calculate_distance_min(s_agent, s_chandelier) \n    penalty_collision = -20 if (d_agent_chandelier < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a prayer hall, the drone must maneuver around prayer row. All entity states (agent, ) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_prayer_row, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_prayer_row = calculate_distance_min(s_agent, s_prayer_row) \n    penalty_collision = -20 if (d_agent_prayer_row < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a prayer hall. Requirement: Detect the three minbars and do not collide with it. Given: The states for the drone and many minbars are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a ablution Mosque during experiments. \n Objectives: So not collide with shelf. \n Conditions: The states of the agent, the shelf are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_bookshelf, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bookshelf = calculate_distance_min(s_agent, s_bookshelf) \n    penalty_collision = -20 if (d_agent_bookshelf < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Mosque. Objectives: Do not collide hit with minbar. Conditions: States of the agent, minbar are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_minbar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_target_minbart = calculate_distance_min(s_agent, s_minbar) \n    penalty_collision = -20 if (d_agent_target_minbart < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a prayer hall, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -20 if (d_agent_wall < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: shoe area. Avoid: Detect donation box and do  not hitting with donation box. Known: Agent, donation box states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_donation_box, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_donation_box = calculate_distance_min(s_agent, s_donation_box) \n    penalty_collision = -20 if (d_agent_donation_box < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through Mosque. Objectives: It must avoid colliding with loudspeaker. Conditions: The states of the drone and loudspeaker are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_loudspeaker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_loudspeaker = calculate_distance_min(s_agent, s_loudspeaker) \n    penalty_collision = -20 if (d_agent_loudspeaker < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Mosque. Objectives: Avoid collision with cloak racks, people and loudspeaker system. Conditions: The states of the agent, cloak racks, people and loudspeaker system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through Mosque. Objectives: It must avoid colliding with any donation box, units and computers. Conditions: Agent's, donation box, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Mosque. Objectives: Do not collide with any cloack rack. Conditions: The states of the drone and the cloack rack are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_cloak_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cloak_rack = calculate_distance_min(s_agent, s_cloak_rack) \n    penalty = -20 if (d_agent_cloak_rack < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Mosque and must avoid contact with any calligraphy panels. The states of the drone and the calligraphy panels are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_calligraphy_panel, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_calligraphy_panel = calculate_distance_min(s_agent, s_calligraphy_panel) \n    penalty = -20 if (d_agent_calligraphy_panel < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Mosque. It must avoid colliding with the donation box, Quran stand and play sign. The states of the agent, book shelves, shoe racks and safety divider stands are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Mosque. It must avoid colliding with divider stands and prayer rows. The states of the drone and the water tuck and donation_box are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Mosque. It must avoid colliding with Mosque aisle divider stand. Agent's state, target's state and divider stand's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_divider_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_divider_stand = calculate_distance_min(s_agent, s_divider_stand) \n    penalty_collision = -20 if (d_agent_divider_stand < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Mosque. Goal: It must avoid colliding with ablution fountain. Assumptions: The states of the drone and the ablution fountain's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_ablution_fountain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ablution_fountain = calculate_distance_min(s_agent, s_ablution_fountain) \n    penalty_collision = -20 if (d_agent_ablution_fountain < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Mosque. Goal: It must avoid colliding with Mosque's book shelves, storage room and divider stands. Assumptions: The Agent's state, target's state and book shelves, storage room and divider stands's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Mosque. Goal: Detect wooden bench, shoe rack and Quran stand and do not collide. Assumptions: The states of the drone, target  wooden bench, shoe rack and Quran stand state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Mosque. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Music Hall. \n Objectives: It must avoid colliding with concrete columns or audience rows. \n Conditions: The states of the agent, the stage, and the audience rows are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Music Hall. \n Objectives: Detect coat rack and aviod it. \n Conditions: The states of the agent, the coat rack are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_coat_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_coat_rack = calculate_distance_min(s_agent, s_coat_rack) \n    penalty_collision = -20 if (d_agent_coat_rack < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Music Hall. Objectives: It must avoid colliding with spotlights. Conditions: States of the drone and the spotlights are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_spotlight, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_spotlight = calculate_distance_min(s_agent, s_spotlight) \n    penalty_collision = -20 if (d_agent_spotlight < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a concert hall, the drone must maneuver around audience row. All entity states (agent, audience row) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_audience_row, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_audience_row = calculate_distance_min(s_agent, s_audience_row) \n    penalty_collision = -20 if (d_agent_audience_row < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a concert hall. Requirement: Detect the three podiums and do not collide with it. Given: The states for the drone and many podiums are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a ablution Music Hall during experiments. \n Objectives: So not collide with shelf. \n Conditions: The states of the agent, the shelf are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_music_shelf, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_music_shelf = calculate_distance_min(s_agent, s_music_shelf) \n    penalty_collision = -20 if (d_agent_music_shelf < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Music Hall. Objectives: Do not collide hit with podium. Conditions: States of the agent, podium are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_podium, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_podium = calculate_distance_min(s_agent, s_podium) \n    penalty_collision = -20 if (d_agent_podium < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a concert hall, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -20 if (d_agent_wall < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: ticket booth. Avoid: Detect speaker cabinet and do not hitting with speaker cabinet. Known: Agent, speaker cabinet states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_speaker_cabinet, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_speaker_cabinet = calculate_distance_min(s_agent, s_speaker_cabinet) \n    penalty_collision = -20 if (d_agent_speaker_cabinet < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through Music Hall. Objectives: It must avoid colliding with sound system. Conditions: The states of the drone and sound system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_sound_system, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sound_system = calculate_distance_min(s_agent, s_sound_system) \n    penalty_collision = -20 if (d_agent_sound_system < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Music Hall. Objectives: Avoid collision with coat stands, people and sound system. Conditions: The states of the agent, coat stands, people and sound system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through Music Hall. Objectives: It must avoid colliding with any speaker cabinet, units and computers. Conditions: Agent's, speaker cabinet, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Music Hall. Objectives: Do not collide with any cloack rack. Conditions: The states of the drone and the cloack rack are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_coat_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cloak_rack = calculate_distance_min(s_agent, s_coat_stand) \n    penalty = -20 if (d_agent_cloak_rack < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Music Hall and must avoid contact with any acoustic panels. The states of the drone and the acoustic panels are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_acoustic_panel, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_acoustic_panel = calculate_distance_min(s_agent, s_acoustic_panel) \n    penalty = -20 if (d_agent_acoustic_panel < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Music Hall. It must avoid colliding with the speaker cabinet, music stand and play sign. The states of the agent, music shelves, coat racks and safety barrier stands are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Music Hall. It must avoid colliding with barrier stands and audience rows. The states of the drone and the water tuck and speaker_cabinet are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Music Hall. It must avoid colliding with guitar. Agent's state, target's state and guitar's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_guitar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_guitar = calculate_distance_min(s_agent, s_guitar) \n    penalty_collision = -20 if (d_agent_guitar < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Music Hall. Goal: It must avoid colliding with violin. Assumptions: The states of the drone and the violin's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_violin, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_violin = calculate_distance_min(s_agent, s_violin) \n    penalty_collision = -20 if (d_agent_violin < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Music Hall. Goal: It must avoid colliding with Music Hall's music shelves, backstage room and barrier stands. Assumptions: The Agent's state, target's state and music shelves, backstage room and barrier stands's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Music Hall. Goal: Detect audience bench, coat rack and music stand and do not collide. Assumptions: The states of the drone, target  audience bench, coat rack and music stand state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Music Hall. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Hair Salon. \n Objectives: It must avoid colliding with concrete columns or styling chairs. \n Conditions: The states of the agent, the stylist station, and the styling chairs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Hair Salon. \n Objectives: Detect towel rack and aviod it. \n Conditions: The states of the agent, the towel rack are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_towel_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_towel_rack = calculate_distance_min(s_agent, s_towel_rack) \n    penalty_collision = -20 if (d_agent_towel_rack < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Hair Salon. Objectives: It must avoid colliding with hair dryers. Conditions: States of the drone and the hair dryers are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_hair_dryer, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_hair_dryer = calculate_distance_min(s_agent, s_hair_dryer) \n    penalty_collision = -20 if (d_agent_hair_dryer < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a salon floor, the drone must maneuver around styling chair. All entity states (agent, chair) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -20 if (d_agent_chair < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a salon floor. Requirement: Detect the three cashier counters and do not collide with it. Given: The states for the drone and many cashier counters are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a ablution Hair Salon during experiments. \n Objectives: So not collide with shelf. \n Conditions: The states of the agent, the shelf are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_product_shelf, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_product_shelf = calculate_distance_min(s_agent, s_product_shelf) \n    penalty_collision = -20 if (d_agent_product_shelf < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Hair Salon. Objectives: Do not collide hit with cashier counter. Conditions: States of the agent, cashier counter are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_cashier_counter, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cashier_counter = calculate_distance_min(s_agent, s_cashier_counter) \n    penalty_collision = -20 if (d_agent_cashier_counter < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a salon floor, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -20 if (d_agent_wall < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: reception desk. Avoid: Detect product display and do not hitting with product display. Known: Agent, product display states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_product_display, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_product_display = calculate_distance_min(s_agent, s_product_display) \n    penalty_collision = -20 if (d_agent_product_display < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Hair Salon. Objectives: It must avoid colliding with stereo system. Conditions: The states of the drone and stereo system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_customer, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_customer = calculate_distance_min(s_agent, s_customer) \n    penalty_collision = -20 if (d_agent_customer < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Hair Salon. Objectives: Avoid collision with mirror stands, people and stereo system. Conditions: The states of the agent, mirror stands, people and stereo system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Hair Salon. Objectives: It must avoid colliding with any product display, units and computers. Conditions: Agent's, product display, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Hair Salon. Objectives: Do not collide with any cloack rack. Conditions: The states of the drone and the cloack rack are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_mirror_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_mirror_stand = calculate_distance_min(s_agent, s_mirror_stand) \n    penalty = -20 if (d_agent_mirror_stand < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Hair Salon and must avoid contact with any mirrors. The states of the drone and the customer are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_customer, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_customer = calculate_distance_min(s_agent, s_customer) \n    penalty = -20 if (d_agent_customer < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Hair Salon. It must avoid colliding with the product display, magazine stand and play sign. The states of the agent, product shelves, towel racks and doors are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Hair Salon. It must avoid colliding with partition stands and styling chairs. The states of the drone and the basket and product_display are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Hair Salon. It must avoid colliding with hair dryer stand. Agent's state, target's state and hair dryer stand's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_hair_dryer_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_hair_dryer_stand = calculate_distance_min(s_agent, s_hair_dryer_stand) \n    penalty_collision = -20 if (d_agent_hair_dryer_stand < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Hair Salon. Goal: It must avoid colliding with shampoo chair. Assumptions: The states of the drone and the shampoo chair's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_shampoo_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_shampoo_chair = calculate_distance_min(s_agent, s_shampoo_chair) \n    penalty_collision = -20 if (d_agent_shampoo_chair < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Hair Salon. Goal: It must avoid colliding with Hair Salon's product shelves, backstylist station room and partition stands. Assumptions: The Agent's state, target's state and product shelves, backstylist station room and partition stands's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Hair Salon. Goal: Detect waiting bench, towel rack and magazine stand and do not collide. Assumptions: The states of the drone, target  waiting bench, towel rack and magazine stand state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Hair Salon. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Spa Resort. \n Objectives: It must avoid colliding with concrete columns or massage tables. \n Conditions: The states of the agent, the spa station, and the massage tables are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Spa Resort. \n Objectives: Detect towel warmer and aviod it. \n Conditions: The states of the agent, the towel warmer are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_towel_warmer, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_towel_warmer = calculate_distance_min(s_agent, s_towel_warmer) \n    penalty_collision = -20 if (d_agent_towel_warmer < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through a Spa Resort. Objectives: It must avoid colliding with sauna heater. Conditions: States of the drone and the sauna heater are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_sauna_heater, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sauna_heater = calculate_distance_min(s_agent, s_sauna_heater) \n    penalty_collision = -20 if (d_agent_sauna_heater < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a spa floor, the drone must maneuver around massage table. All entity states (agent, chair) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -20 if (d_agent_chair < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a spa floor. Requirement: Detect the three spa counters and do not collide with it. Given: The states for the drone and many spa counters are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a ablution Spa Resort during experiments. \n Objectives: So not collide with aroma diffuser. \n Conditions: The states of the agent, the aroma diffuser are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_aroma_diffuser, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_aroma_diffuser = calculate_distance_min(s_agent, s_aroma_diffuser) \n    penalty_collision = -20 if (d_agent_aroma_diffuser < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Spa Resort. Objectives: Do not collide hit with spa counter. Conditions: States of the agent, spa counter are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_spa_counter, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_spa_counter = calculate_distance_min(s_agent, s_spa_counter) \n    penalty_collision = -20 if (d_agent_spa_counter < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a spa floor, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -20 if (d_agent_wall < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: front desk. Avoid: Detect flower arrangement and do not hitting with flower arrangement. Known: Agent, flower arrangement states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_flower_arrangement, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_flower_arrangement = calculate_distance_min(s_agent, s_flower_arrangement) \n    penalty_collision = -20 if (d_agent_flower_arrangement < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Spa Resort. Objectives: It must avoid colliding with spa statue. Conditions: The states of the drone and spa statue are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_spa_statue, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_spa_statue = calculate_distance_min(s_agent, s_spa_statue) \n    penalty_collision = -20 if (d_agent_spa_statue < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Spa Resort. Objectives: Avoid collision with decorative screen, people and water fountain. Conditions: The states of the agent, decorative screen, people and water fountain are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Spa Resort. Objectives: It must avoid colliding with any flower arrangement, units and computers. Conditions: Agent's, flower arrangement, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Spa Resort. Objectives: Do not collide with sauna heater. Conditions: The states of the drone and the sauna heater are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_sauna_heater, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sauna_heater = calculate_distance_min(s_agent, s_sauna_heater) \n    penalty = -20 if (d_agent_sauna_heater < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Spa Resort and must avoid contact with the oil diffuser stand. The states of the drone and the oil stand are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_oil_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_oil_stand = calculate_distance_min(s_agent, s_oil_stand) \n    penalty = -20 if (d_agent_oil_stand < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Spa Resort. It must avoid colliding with the flower arrangement, tea table and play sign. The states of the agent, insliding screen plants, towel warmers and sliding screens are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Spa Resort. It must avoid colliding with spa divider and massage tables. The states of the drone and the stone sculpture and flower_arrangement are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Spa Resort. It must avoid colliding with the tea table. Agent's state, target's state and tea table's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_tea_table, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tea_table = calculate_distance_min(s_agent, s_tea_table) \n    penalty_collision = -20 if (d_agent_tea_table < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Spa Resort. Goal: It must avoid colliding with foot bath. Assumptions: The states of the drone and the foot bath's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_foot_bath, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_foot_bath = calculate_distance_min(s_agent, s_foot_bath) \n    penalty_collision = -20 if (d_agent_foot_bath < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Spa Resort. Goal: It must avoid colliding with Spa Resort's insliding screen plants, backspa station room and spa divider. Assumptions: The Agent's state, target's state and insliding screen plants, backspa station room and spa divider's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Spa Resort. Goal: Detect lounge chair, towel warmer and tea table and do not collide. Assumptions: The states of the drone, target  lounge chair, towel warmer and tea table state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Spa Resort. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Aquarium. \n Objectives: It must avoid colliding with concrete columns or viewing benches. \n Conditions: The states of the agent, the aquarium tank, and the viewing benches are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Aquarium. \n Objectives: Detect filter pump and aviod it. \n Conditions: The states of the agent, the filter pump are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_filter_pump, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_filter_pump = calculate_distance_min(s_agent, s_filter_pump) \n    penalty_collision = -20 if (d_agent_filter_pump < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through an aquarium. Objectives: It must avoid colliding with UV sterilizer. Conditions: States of the drone and the UV sterilizer are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bench, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bench = calculate_distance_min(s_agent, s_bench) \n    penalty_collision = -20 if (d_agent_bench < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a aquarium hall, the drone must maneuver around viewing bench. All entity states (agent, chair) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -20 if (d_agent_chair < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a aquarium hall. Requirement: Detect the three touch pools and do not collide with it. Given: The states for the drone and many touch pools are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a ablution Aquarium during experiments. \n Objectives: So not collide with air bubbler. \n Conditions: The states of the agent, the air bubbler are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_air_bubbler, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_air_bubbler = calculate_distance_min(s_agent, s_air_bubbler) \n    penalty_collision = -20 if (d_agent_air_bubbler < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Aquarium. Objectives: Do not collide hit with touch pool. Conditions: States of the agent, touch pool are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_touch_pool, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_touch_pool = calculate_distance_min(s_agent, s_touch_pool) \n    penalty_collision = -20 if (d_agent_touch_pool < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a aquarium hall, the drone must steer clear of the visitor. All states (agent, visitor) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_visitor, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_visitor = calculate_distance_min(s_agent, s_visitor) \n    penalty_collision = -20 if (d_agent_visitor < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: ticket counter. Avoid: Detect coral display and do not hitting with coral display. Known: Agent, coral display states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_coral_display, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_coral_display = calculate_distance_min(s_agent, s_coral_display) \n    penalty_collision = -20 if (d_agent_coral_display < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Aquarium. Objectives: It must avoid colliding with marine sculpture. Conditions: The states of the drone and marine sculpture are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_marine_sculpture, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_marine_sculpture = calculate_distance_min(s_agent, s_marine_sculpture) \n    penalty_collision = -20 if (d_agent_marine_sculpture < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Aquarium. Objectives: Avoid collision with glass panel, people and fish tank. Conditions: The states of the agent, glass panel, people and fish tank are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Aquarium. Objectives: It must avoid colliding with any coral display, units and computers. Conditions: Agent's, coral display, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Aquarium. Objectives: Do not collide with viewing_bench. Conditions: The states of the drone and the viewing_bench are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bench, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bench = calculate_distance_min(s_agent, s_bench) \n    penalty = -20 if (d_agent_bench < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Aquarium and must avoid contact with the oxygen pump. The states of the drone and the oxygen pump are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_oxygen_pump, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_oxygen_pump = calculate_distance_min(s_agent, s_oxygen_pump) \n    penalty = -20 if (d_agent_oxygen_pump < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Aquarium. It must avoid colliding with the coral display, info stand and play sign. The states of the agent, inglass door plants, filter pumps and glass doors are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Aquarium. It must avoid colliding with aquarium barrier and viewing benches. The states of the drone and the aquarium statue and coral_display are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Aquarium. It must avoid colliding with the info stand. Agent's state, target's state and info stand's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_info_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_info_stand = calculate_distance_min(s_agent, s_info_stand) \n    penalty_collision = -20 if (d_agent_info_stand < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Aquarium. Goal: It must avoid colliding with shallow tank. Assumptions: The states of the drone and the shallow tank's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_tank, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tank = calculate_distance_min(s_agent, s_tank) \n    penalty_collision = -20 if (d_agent_tank < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Aquarium. Goal: It must avoid colliding with Aquarium's inglass door plants, backaquarium tank room and aquarium barrier. Assumptions: The Agent's state, target's state and inglass door plants, backaquarium tank room and aquarium barrier's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Aquarium. Goal: Detect visitor bench, filter pump and info stand and do not collide. Assumptions: The states of the drone, target  visitor bench, filter pump and info stand state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Aquarium. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Tennis Court. \n Objectives: It must avoid colliding with concrete columns or spectator benches. \n Conditions: The states of the agent, the net post, and the spectator benches are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Tennis Court. \n Objectives: Detect ball basket and aviod it. \n Conditions: The states of the agent, the ball basket are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ball_basket, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ball_basket = calculate_distance_min(s_agent, s_ball_basket) \n    penalty_collision = -20 if (d_agent_ball_basket < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Tennis Court open workspace. Objectives: It must avoid colliding with ball machine. Conditions: States of the drone and the ball machine are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ball_machine, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ball_machine = calculate_distance_min(s_agent, s_ball_machine) \n    penalty_collision = -20 if (d_agent_ball_machine < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a tennis court area, the drone must maneuver around spectator bench. All entity states (agent, chair) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -20 if (d_agent_chair < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a tennis court area. Requirement: Detect the three practice walls and do not collide with it. Given: The states for the drone and many practice walls are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a ablution Tennis Court during experiments. \n Objectives: So not collide with water cooler. \n Conditions: The states of the agent, the water cooler are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_water_cooler, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_water_cooler = calculate_distance_min(s_agent, s_water_cooler) \n    penalty_collision = -20 if (d_agent_water_cooler < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Tennis Court. Objectives: Do not collide hit with practice wall. Conditions: States of the agent, practice wall are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_practice_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_practice_wall = calculate_distance_min(s_agent, s_practice_wall) \n    penalty_collision = -20 if (d_agent_practice_wall < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a tennis court area, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -20 if (d_agent_wall < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: umpire chair. Avoid: Detect scoreboard and do not hitting with scoreboard. Known: Agent, scoreboard states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_scoreboard, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_scoreboard = calculate_distance_min(s_agent, s_scoreboard) \n    penalty_collision = -20 if (d_agent_scoreboard < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Tennis Court. Objectives: It must avoid colliding with tennis monument. Conditions: The states of the drone and tennis monument are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_monument, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_monument = calculate_distance_min(s_agent, s_monument) \n    penalty_collision = -20 if (d_agent_monument < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Tennis Court. Objectives: Avoid collision with fence panel, people and ball cart. Conditions: The states of the agent, fence panel, people and ball cart are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Tennis Court. Objectives: It must avoid colliding with any scoreboard, units and computers. Conditions: Agent's, scoreboard, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Tennis Court. Objectives: Do not collide with ball machine. Conditions: The states of the drone and the ball machine are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ball_machine, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ball_machine = calculate_distance_min(s_agent, s_ball_machine) \n    penalty = -20 if (d_agent_ball_machine < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Tennis Court and must avoid contact with the lighting pole. The states of the drone and the lighting pole are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_pole, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_pole = calculate_distance_min(s_agent, s_pole) \n    penalty = -20 if (d_agent_pole < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Tennis Court. It must avoid colliding with the scoreboard, schedule board and play sign. The states of the agent, incourt gate plants, ball baskets and court gates are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Tennis Court. It must avoid colliding with boundary fence and spectator benches. The states of the drone and the court statue and scoreboard are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Tennis Court. It must avoid colliding with the schedule board. Agent's state, target's state and schedule board's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_schedule_board, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_schedule_board = calculate_distance_min(s_agent, s_schedule_board) \n    penalty_collision = -20 if (d_agent_schedule_board < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Tennis Court. Goal: It must avoid colliding with clay patch. Assumptions: The states of the drone and the clay patch's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_clay_patch, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_foot_bath = calculate_distance_min(s_agent, s_clay_patch) \n    penalty_collision = -20 if (d_agent_foot_bath < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Tennis Court. Goal: It must avoid colliding with Tennis Court's incourt gate plants, backnet post room and boundary fence. Assumptions: The Agent's state, target's state and incourt gate plants, backnet post room and boundary fence's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Tennis Court. Goal: Detect player bench, ball basket and schedule board and do not collide. Assumptions: The states of the drone, target  player bench, ball basket and schedule board state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Tennis Court. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -math.exp(d_agent_target) \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Mosque. \n Objectives: It must avoid colliding with concrete columns or prayer rows. \n Conditions: The states of the agent, the mihrab, and the prayer rows are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Mosque. \n Objectives: Detect shoe rack and aviod it. \n Conditions: The states of the agent, the shoe rack are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_shoe_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_shoe_rack = calculate_distance_min(s_agent, s_shoe_rack) \n    penalty_collision = -20 if (d_agent_shoe_rack < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through Mosque open workspace. Objectives: It must avoid colliding with chandeliers. Conditions: States of the drone and the chandeliers are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_chandelier, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chandelier = calculate_distance_min(s_agent, s_chandelier) \n    penalty_collision = -20 if (d_agent_chandelier < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a prayer hall, the drone must maneuver around prayer row. All entity states (agent, ) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_prayer_row, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_prayer_row = calculate_distance_min(s_agent, s_prayer_row) \n    penalty_collision = -20 if (d_agent_prayer_row < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a prayer hall. Requirement: Detect the three minbars and do not collide with it. Given: The states for the drone and many minbars are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a ablution Mosque during experiments. \n Objectives: So not collide with shelf. \n Conditions: The states of the agent, the shelf are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_bookshelf, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bookshelf = calculate_distance_min(s_agent, s_bookshelf) \n    penalty_collision = -20 if (d_agent_bookshelf < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Mosque. Objectives: Do not collide hit with minbar. Conditions: States of the agent, minbar are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_minbar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_target_minbart = calculate_distance_min(s_agent, s_minbar) \n    penalty_collision = -20 if (d_agent_target_minbart < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a prayer hall, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -20 if (d_agent_wall < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: shoe area. Avoid: Detect donation box and do  not hitting with donation box. Known: Agent, donation box states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_donation_box, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_donation_box = calculate_distance_min(s_agent, s_donation_box) \n    penalty_collision = -20 if (d_agent_donation_box < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through Mosque. Objectives: It must avoid colliding with loudspeaker. Conditions: The states of the drone and loudspeaker are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_loudspeaker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_loudspeaker = calculate_distance_min(s_agent, s_loudspeaker) \n    penalty_collision = -20 if (d_agent_loudspeaker < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Mosque. Objectives: Avoid collision with cloak racks, people and loudspeaker system. Conditions: The states of the agent, cloak racks, people and loudspeaker system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through Mosque. Objectives: It must avoid colliding with any donation box, units and computers. Conditions: Agent's, donation box, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Mosque. Objectives: Do not collide with any cloack rack. Conditions: The states of the drone and the cloack rack are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_cloak_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cloak_rack = calculate_distance_min(s_agent, s_cloak_rack) \n    penalty = -20 if (d_agent_cloak_rack < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Mosque and must avoid contact with any calligraphy panels. The states of the drone and the calligraphy panels are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_calligraphy_panel, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_calligraphy_panel = calculate_distance_min(s_agent, s_calligraphy_panel) \n    penalty = -20 if (d_agent_calligraphy_panel < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Mosque. It must avoid colliding with the donation box, Quran stand and play sign. The states of the agent, book shelves, shoe racks and safety divider stands are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Mosque. It must avoid colliding with divider stands and prayer rows. The states of the drone and the water tuck and donation_box are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Mosque. It must avoid colliding with Mosque aisle divider stand. Agent's state, target's state and divider stand's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_divider_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_divider_stand = calculate_distance_min(s_agent, s_divider_stand) \n    penalty_collision = -20 if (d_agent_divider_stand < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Mosque. Goal: It must avoid colliding with ablution fountain. Assumptions: The states of the drone and the ablution fountain's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_ablution_fountain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ablution_fountain = calculate_distance_min(s_agent, s_ablution_fountain) \n    penalty_collision = -20 if (d_agent_ablution_fountain < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Mosque. Goal: It must avoid colliding with Mosque's book shelves, storage room and divider stands. Assumptions: The Agent's state, target's state and book shelves, storage room and divider stands's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Mosque. Goal: Detect wooden bench, shoe rack and Quran stand and do not collide. Assumptions: The states of the drone, target  wooden bench, shoe rack and Quran stand state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Mosque. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Music Hall. \n Objectives: It must avoid colliding with concrete columns or audience rows. \n Conditions: The states of the agent, the stage, and the audience rows are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Music Hall. \n Objectives: Detect coat rack and aviod it. \n Conditions: The states of the agent, the coat rack are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_coat_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_coat_rack = calculate_distance_min(s_agent, s_coat_rack) \n    penalty_collision = -20 if (d_agent_coat_rack < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Music Hall. Objectives: It must avoid colliding with spotlights. Conditions: States of the drone and the spotlights are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_spotlight, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_spotlight = calculate_distance_min(s_agent, s_spotlight) \n    penalty_collision = -20 if (d_agent_spotlight < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a concert hall, the drone must maneuver around audience row. All entity states (agent, audience row) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_audience_row, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_audience_row = calculate_distance_min(s_agent, s_audience_row) \n    penalty_collision = -20 if (d_agent_audience_row < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a concert hall. Requirement: Detect the three podiums and do not collide with it. Given: The states for the drone and many podiums are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a ablution Music Hall during experiments. \n Objectives: So not collide with shelf. \n Conditions: The states of the agent, the shelf are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_music_shelf, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_music_shelf = calculate_distance_min(s_agent, s_music_shelf) \n    penalty_collision = -20 if (d_agent_music_shelf < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Music Hall. Objectives: Do not collide hit with podium. Conditions: States of the agent, podium are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_podium, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_podium = calculate_distance_min(s_agent, s_podium) \n    penalty_collision = -20 if (d_agent_podium < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a concert hall, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -20 if (d_agent_wall < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: ticket booth. Avoid: Detect speaker cabinet and do not hitting with speaker cabinet. Known: Agent, speaker cabinet states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_speaker_cabinet, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_speaker_cabinet = calculate_distance_min(s_agent, s_speaker_cabinet) \n    penalty_collision = -20 if (d_agent_speaker_cabinet < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through Music Hall. Objectives: It must avoid colliding with sound system. Conditions: The states of the drone and sound system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_sound_system, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sound_system = calculate_distance_min(s_agent, s_sound_system) \n    penalty_collision = -20 if (d_agent_sound_system < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Music Hall. Objectives: Avoid collision with coat stands, people and sound system. Conditions: The states of the agent, coat stands, people and sound system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through Music Hall. Objectives: It must avoid colliding with any speaker cabinet, units and computers. Conditions: Agent's, speaker cabinet, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Music Hall. Objectives: Do not collide with any cloack rack. Conditions: The states of the drone and the cloack rack are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_coat_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cloak_rack = calculate_distance_min(s_agent, s_coat_stand) \n    penalty = -20 if (d_agent_cloak_rack < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Music Hall and must avoid contact with any acoustic panels. The states of the drone and the acoustic panels are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_acoustic_panel, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_acoustic_panel = calculate_distance_min(s_agent, s_acoustic_panel) \n    penalty = -20 if (d_agent_acoustic_panel < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Music Hall. It must avoid colliding with the speaker cabinet, music stand and play sign. The states of the agent, music shelves, coat racks and safety barrier stands are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Music Hall. It must avoid colliding with barrier stands and audience rows. The states of the drone and the water tuck and speaker_cabinet are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Music Hall. It must avoid colliding with guitar. Agent's state, target's state and guitar's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_guitar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_guitar = calculate_distance_min(s_agent, s_guitar) \n    penalty_collision = -20 if (d_agent_guitar < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Music Hall. Goal: It must avoid colliding with violin. Assumptions: The states of the drone and the violin's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_violin, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_violin = calculate_distance_min(s_agent, s_violin) \n    penalty_collision = -20 if (d_agent_violin < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Music Hall. Goal: It must avoid colliding with Music Hall's music shelves, backstage room and barrier stands. Assumptions: The Agent's state, target's state and music shelves, backstage room and barrier stands's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Music Hall. Goal: Detect audience bench, coat rack and music stand and do not collide. Assumptions: The states of the drone, target  audience bench, coat rack and music stand state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Music Hall. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Hair Salon. \n Objectives: It must avoid colliding with concrete columns or styling chairs. \n Conditions: The states of the agent, the stylist station, and the styling chairs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Hair Salon. \n Objectives: Detect towel rack and aviod it. \n Conditions: The states of the agent, the towel rack are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_towel_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_towel_rack = calculate_distance_min(s_agent, s_towel_rack) \n    penalty_collision = -20 if (d_agent_towel_rack < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Hair Salon. Objectives: It must avoid colliding with hair dryers. Conditions: States of the drone and the hair dryers are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_hair_dryer, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_hair_dryer = calculate_distance_min(s_agent, s_hair_dryer) \n    penalty_collision = -20 if (d_agent_hair_dryer < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a salon floor, the drone must maneuver around styling chair. All entity states (agent, chair) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -20 if (d_agent_chair < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a salon floor. Requirement: Detect the three cashier counters and do not collide with it. Given: The states for the drone and many cashier counters are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a ablution Hair Salon during experiments. \n Objectives: So not collide with shelf. \n Conditions: The states of the agent, the shelf are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_product_shelf, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_product_shelf = calculate_distance_min(s_agent, s_product_shelf) \n    penalty_collision = -20 if (d_agent_product_shelf < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Hair Salon. Objectives: Do not collide hit with cashier counter. Conditions: States of the agent, cashier counter are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_cashier_counter, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cashier_counter = calculate_distance_min(s_agent, s_cashier_counter) \n    penalty_collision = -20 if (d_agent_cashier_counter < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a salon floor, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -20 if (d_agent_wall < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: reception desk. Avoid: Detect product display and do not hitting with product display. Known: Agent, product display states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_product_display, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_product_display = calculate_distance_min(s_agent, s_product_display) \n    penalty_collision = -20 if (d_agent_product_display < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Hair Salon. Objectives: It must avoid colliding with stereo system. Conditions: The states of the drone and stereo system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_customer, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_customer = calculate_distance_min(s_agent, s_customer) \n    penalty_collision = -20 if (d_agent_customer < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Hair Salon. Objectives: Avoid collision with mirror stands, people and stereo system. Conditions: The states of the agent, mirror stands, people and stereo system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Hair Salon. Objectives: It must avoid colliding with any product display, units and computers. Conditions: Agent's, product display, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Hair Salon. Objectives: Do not collide with any cloack rack. Conditions: The states of the drone and the cloack rack are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_mirror_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_mirror_stand = calculate_distance_min(s_agent, s_mirror_stand) \n    penalty = -20 if (d_agent_mirror_stand < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Hair Salon and must avoid contact with any mirrors. The states of the drone and the customer are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_customer, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_customer = calculate_distance_min(s_agent, s_customer) \n    penalty = -20 if (d_agent_customer < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Hair Salon. It must avoid colliding with the product display, magazine stand and play sign. The states of the agent, product shelves, towel racks and doors are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Hair Salon. It must avoid colliding with partition stands and styling chairs. The states of the drone and the basket and product_display are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Hair Salon. It must avoid colliding with hair dryer stand. Agent's state, target's state and hair dryer stand's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_hair_dryer_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_hair_dryer_stand = calculate_distance_min(s_agent, s_hair_dryer_stand) \n    penalty_collision = -20 if (d_agent_hair_dryer_stand < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Hair Salon. Goal: It must avoid colliding with shampoo chair. Assumptions: The states of the drone and the shampoo chair's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_shampoo_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_shampoo_chair = calculate_distance_min(s_agent, s_shampoo_chair) \n    penalty_collision = -20 if (d_agent_shampoo_chair < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Hair Salon. Goal: It must avoid colliding with Hair Salon's product shelves, backstylist station room and partition stands. Assumptions: The Agent's state, target's state and product shelves, backstylist station room and partition stands's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Hair Salon. Goal: Detect waiting bench, towel rack and magazine stand and do not collide. Assumptions: The states of the drone, target  waiting bench, towel rack and magazine stand state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Hair Salon. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Spa Resort. \n Objectives: It must avoid colliding with concrete columns or massage tables. \n Conditions: The states of the agent, the spa station, and the massage tables are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Spa Resort. \n Objectives: Detect towel warmer and aviod it. \n Conditions: The states of the agent, the towel warmer are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_towel_warmer, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_towel_warmer = calculate_distance_min(s_agent, s_towel_warmer) \n    penalty_collision = -20 if (d_agent_towel_warmer < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through a Spa Resort. Objectives: It must avoid colliding with sauna heater. Conditions: States of the drone and the sauna heater are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_sauna_heater, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sauna_heater = calculate_distance_min(s_agent, s_sauna_heater) \n    penalty_collision = -20 if (d_agent_sauna_heater < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a spa floor, the drone must maneuver around massage table. All entity states (agent, chair) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -20 if (d_agent_chair < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a spa floor. Requirement: Detect the three spa counters and do not collide with it. Given: The states for the drone and many spa counters are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a ablution Spa Resort during experiments. \n Objectives: So not collide with aroma diffuser. \n Conditions: The states of the agent, the aroma diffuser are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_aroma_diffuser, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_aroma_diffuser = calculate_distance_min(s_agent, s_aroma_diffuser) \n    penalty_collision = -20 if (d_agent_aroma_diffuser < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Spa Resort. Objectives: Do not collide hit with spa counter. Conditions: States of the agent, spa counter are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_spa_counter, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_spa_counter = calculate_distance_min(s_agent, s_spa_counter) \n    penalty_collision = -20 if (d_agent_spa_counter < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a spa floor, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -20 if (d_agent_wall < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: front desk. Avoid: Detect flower arrangement and do not hitting with flower arrangement. Known: Agent, flower arrangement states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_flower_arrangement, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_flower_arrangement = calculate_distance_min(s_agent, s_flower_arrangement) \n    penalty_collision = -20 if (d_agent_flower_arrangement < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Spa Resort. Objectives: It must avoid colliding with spa statue. Conditions: The states of the drone and spa statue are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_spa_statue, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_spa_statue = calculate_distance_min(s_agent, s_spa_statue) \n    penalty_collision = -20 if (d_agent_spa_statue < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Spa Resort. Objectives: Avoid collision with decorative screen, people and water fountain. Conditions: The states of the agent, decorative screen, people and water fountain are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Spa Resort. Objectives: It must avoid colliding with any flower arrangement, units and computers. Conditions: Agent's, flower arrangement, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Spa Resort. Objectives: Do not collide with sauna heater. Conditions: The states of the drone and the sauna heater are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_sauna_heater, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sauna_heater = calculate_distance_min(s_agent, s_sauna_heater) \n    penalty = -20 if (d_agent_sauna_heater < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Spa Resort and must avoid contact with the oil diffuser stand. The states of the drone and the oil stand are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_oil_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_oil_stand = calculate_distance_min(s_agent, s_oil_stand) \n    penalty = -20 if (d_agent_oil_stand < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Spa Resort. It must avoid colliding with the flower arrangement, tea table and play sign. The states of the agent, insliding screen plants, towel warmers and sliding screens are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Spa Resort. It must avoid colliding with spa divider and massage tables. The states of the drone and the stone sculpture and flower_arrangement are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Spa Resort. It must avoid colliding with the tea table. Agent's state, target's state and tea table's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_tea_table, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tea_table = calculate_distance_min(s_agent, s_tea_table) \n    penalty_collision = -20 if (d_agent_tea_table < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Spa Resort. Goal: It must avoid colliding with foot bath. Assumptions: The states of the drone and the foot bath's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_foot_bath, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_foot_bath = calculate_distance_min(s_agent, s_foot_bath) \n    penalty_collision = -20 if (d_agent_foot_bath < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Spa Resort. Goal: It must avoid colliding with Spa Resort's insliding screen plants, backspa station room and spa divider. Assumptions: The Agent's state, target's state and insliding screen plants, backspa station room and spa divider's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Spa Resort. Goal: Detect lounge chair, towel warmer and tea table and do not collide. Assumptions: The states of the drone, target  lounge chair, towel warmer and tea table state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Spa Resort. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Aquarium. \n Objectives: It must avoid colliding with concrete columns or viewing benches. \n Conditions: The states of the agent, the aquarium tank, and the viewing benches are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Aquarium. \n Objectives: Detect filter pump and aviod it. \n Conditions: The states of the agent, the filter pump are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_filter_pump, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_filter_pump = calculate_distance_min(s_agent, s_filter_pump) \n    penalty_collision = -20 if (d_agent_filter_pump < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through an aquarium. Objectives: It must avoid colliding with UV sterilizer. Conditions: States of the drone and the UV sterilizer are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bench, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bench = calculate_distance_min(s_agent, s_bench) \n    penalty_collision = -20 if (d_agent_bench < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a aquarium hall, the drone must maneuver around viewing bench. All entity states (agent, chair) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -20 if (d_agent_chair < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a aquarium hall. Requirement: Detect the three touch pools and do not collide with it. Given: The states for the drone and many touch pools are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a ablution Aquarium during experiments. \n Objectives: So not collide with air bubbler. \n Conditions: The states of the agent, the air bubbler are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_air_bubbler, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_air_bubbler = calculate_distance_min(s_agent, s_air_bubbler) \n    penalty_collision = -20 if (d_agent_air_bubbler < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Aquarium. Objectives: Do not collide hit with touch pool. Conditions: States of the agent, touch pool are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_touch_pool, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_touch_pool = calculate_distance_min(s_agent, s_touch_pool) \n    penalty_collision = -20 if (d_agent_touch_pool < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a aquarium hall, the drone must steer clear of the visitor. All states (agent, visitor) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_visitor, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_visitor = calculate_distance_min(s_agent, s_visitor) \n    penalty_collision = -20 if (d_agent_visitor < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: ticket counter. Avoid: Detect coral display and do not hitting with coral display. Known: Agent, coral display states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_coral_display, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_coral_display = calculate_distance_min(s_agent, s_coral_display) \n    penalty_collision = -20 if (d_agent_coral_display < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Aquarium. Objectives: It must avoid colliding with marine sculpture. Conditions: The states of the drone and marine sculpture are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_marine_sculpture, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_marine_sculpture = calculate_distance_min(s_agent, s_marine_sculpture) \n    penalty_collision = -20 if (d_agent_marine_sculpture < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Aquarium. Objectives: Avoid collision with glass panel, people and fish tank. Conditions: The states of the agent, glass panel, people and fish tank are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Aquarium. Objectives: It must avoid colliding with any coral display, units and computers. Conditions: Agent's, coral display, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Aquarium. Objectives: Do not collide with viewing_bench. Conditions: The states of the drone and the viewing_bench are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bench, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bench = calculate_distance_min(s_agent, s_bench) \n    penalty = -20 if (d_agent_bench < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Aquarium and must avoid contact with the oxygen pump. The states of the drone and the oxygen pump are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_oxygen_pump, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_oxygen_pump = calculate_distance_min(s_agent, s_oxygen_pump) \n    penalty = -20 if (d_agent_oxygen_pump < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Aquarium. It must avoid colliding with the coral display, info stand and play sign. The states of the agent, inglass door plants, filter pumps and glass doors are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Aquarium. It must avoid colliding with aquarium barrier and viewing benches. The states of the drone and the aquarium statue and coral_display are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Aquarium. It must avoid colliding with the info stand. Agent's state, target's state and info stand's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_info_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_info_stand = calculate_distance_min(s_agent, s_info_stand) \n    penalty_collision = -20 if (d_agent_info_stand < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Aquarium. Goal: It must avoid colliding with shallow tank. Assumptions: The states of the drone and the shallow tank's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_tank, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tank = calculate_distance_min(s_agent, s_tank) \n    penalty_collision = -20 if (d_agent_tank < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Aquarium. Goal: It must avoid colliding with Aquarium's inglass door plants, backaquarium tank room and aquarium barrier. Assumptions: The Agent's state, target's state and inglass door plants, backaquarium tank room and aquarium barrier's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Aquarium. Goal: Detect visitor bench, filter pump and info stand and do not collide. Assumptions: The states of the drone, target  visitor bench, filter pump and info stand state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Aquarium. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Tennis Court. \n Objectives: It must avoid colliding with concrete columns or spectator benches. \n Conditions: The states of the agent, the net post, and the spectator benches are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Tennis Court. \n Objectives: Detect ball basket and aviod it. \n Conditions: The states of the agent, the ball basket are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ball_basket, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ball_basket = calculate_distance_min(s_agent, s_ball_basket) \n    penalty_collision = -20 if (d_agent_ball_basket < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Tennis Court open workspace. Objectives: It must avoid colliding with ball machine. Conditions: States of the drone and the ball machine are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ball_machine, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ball_machine = calculate_distance_min(s_agent, s_ball_machine) \n    penalty_collision = -20 if (d_agent_ball_machine < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a tennis court area, the drone must maneuver around spectator bench. All entity states (agent, chair) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -20 if (d_agent_chair < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a tennis court area. Requirement: Detect the three practice walls and do not collide with it. Given: The states for the drone and many practice walls are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a ablution Tennis Court during experiments. \n Objectives: So not collide with water cooler. \n Conditions: The states of the agent, the water cooler are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_water_cooler, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_water_cooler = calculate_distance_min(s_agent, s_water_cooler) \n    penalty_collision = -20 if (d_agent_water_cooler < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Tennis Court. Objectives: Do not collide hit with practice wall. Conditions: States of the agent, practice wall are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_practice_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_practice_wall = calculate_distance_min(s_agent, s_practice_wall) \n    penalty_collision = -20 if (d_agent_practice_wall < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a tennis court area, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -20 if (d_agent_wall < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: umpire chair. Avoid: Detect scoreboard and do not hitting with scoreboard. Known: Agent, scoreboard states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_scoreboard, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_scoreboard = calculate_distance_min(s_agent, s_scoreboard) \n    penalty_collision = -20 if (d_agent_scoreboard < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Tennis Court. Objectives: It must avoid colliding with tennis monument. Conditions: The states of the drone and tennis monument are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_monument, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_monument = calculate_distance_min(s_agent, s_monument) \n    penalty_collision = -20 if (d_agent_monument < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Tennis Court. Objectives: Avoid collision with fence panel, people and ball cart. Conditions: The states of the agent, fence panel, people and ball cart are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Tennis Court. Objectives: It must avoid colliding with any scoreboard, units and computers. Conditions: Agent's, scoreboard, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Tennis Court. Objectives: Do not collide with ball machine. Conditions: The states of the drone and the ball machine are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ball_machine, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ball_machine = calculate_distance_min(s_agent, s_ball_machine) \n    penalty = -20 if (d_agent_ball_machine < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Tennis Court and must avoid contact with the lighting pole. The states of the drone and the lighting pole are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_pole, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_pole = calculate_distance_min(s_agent, s_pole) \n    penalty = -20 if (d_agent_pole < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Tennis Court. It must avoid colliding with the scoreboard, schedule board and play sign. The states of the agent, incourt gate plants, ball baskets and court gates are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Tennis Court. It must avoid colliding with boundary fence and spectator benches. The states of the drone and the court statue and scoreboard are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Tennis Court. It must avoid colliding with the schedule board. Agent's state, target's state and schedule board's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_schedule_board, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_schedule_board = calculate_distance_min(s_agent, s_schedule_board) \n    penalty_collision = -20 if (d_agent_schedule_board < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Tennis Court. Goal: It must avoid colliding with clay patch. Assumptions: The states of the drone and the clay patch's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_clay_patch, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_foot_bath = calculate_distance_min(s_agent, s_clay_patch) \n    penalty_collision = -20 if (d_agent_foot_bath < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Tennis Court. Goal: It must avoid colliding with Tennis Court's incourt gate plants, backnet post room and boundary fence. Assumptions: The Agent's state, target's state and incourt gate plants, backnet post room and boundary fence's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Tennis Court. Goal: Detect player bench, ball basket and schedule board and do not collide. Assumptions: The states of the drone, target  player bench, ball basket and schedule board state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Tennis Court. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -d_agent_target \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Mosque. \n Objectives: It must avoid colliding with concrete columns or prayer rows. \n Conditions: The states of the agent, the mihrab, and the prayer rows are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Mosque. \n Objectives: Detect shoe rack and aviod it. \n Conditions: The states of the agent, the shoe rack are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_shoe_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_shoe_rack = calculate_distance_min(s_agent, s_shoe_rack) \n    penalty_collision = -20 if (d_agent_shoe_rack < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through Mosque open workspace. Objectives: It must avoid colliding with chandeliers. Conditions: States of the drone and the chandeliers are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_chandelier, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chandelier = calculate_distance_min(s_agent, s_chandelier) \n    penalty_collision = -20 if (d_agent_chandelier < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a prayer hall, the drone must maneuver around prayer row. All entity states (agent, ) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_prayer_row, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_prayer_row = calculate_distance_min(s_agent, s_prayer_row) \n    penalty_collision = -20 if (d_agent_prayer_row < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a prayer hall. Requirement: Detect the three minbars and do not collide with it. Given: The states for the drone and many minbars are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a ablution Mosque during experiments. \n Objectives: So not collide with shelf. \n Conditions: The states of the agent, the shelf are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_bookshelf, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bookshelf = calculate_distance_min(s_agent, s_bookshelf) \n    penalty_collision = -20 if (d_agent_bookshelf < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Mosque. Objectives: Do not collide hit with minbar. Conditions: States of the agent, minbar are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_minbar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_target_minbart = calculate_distance_min(s_agent, s_minbar) \n    penalty_collision = -20 if (d_agent_target_minbart < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a prayer hall, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -20 if (d_agent_wall < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: shoe area. Avoid: Detect donation box and do  not hitting with donation box. Known: Agent, donation box states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_donation_box, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_donation_box = calculate_distance_min(s_agent, s_donation_box) \n    penalty_collision = -20 if (d_agent_donation_box < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through Mosque. Objectives: It must avoid colliding with loudspeaker. Conditions: The states of the drone and loudspeaker are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_loudspeaker, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_loudspeaker = calculate_distance_min(s_agent, s_loudspeaker) \n    penalty_collision = -20 if (d_agent_loudspeaker < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Mosque. Objectives: Avoid collision with cloak racks, people and loudspeaker system. Conditions: The states of the agent, cloak racks, people and loudspeaker system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through Mosque. Objectives: It must avoid colliding with any donation box, units and computers. Conditions: Agent's, donation box, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Mosque. Objectives: Do not collide with any cloack rack. Conditions: The states of the drone and the cloack rack are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_cloak_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cloak_rack = calculate_distance_min(s_agent, s_cloak_rack) \n    penalty = -20 if (d_agent_cloak_rack < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Mosque and must avoid contact with any calligraphy panels. The states of the drone and the calligraphy panels are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_calligraphy_panel, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_calligraphy_panel = calculate_distance_min(s_agent, s_calligraphy_panel) \n    penalty = -20 if (d_agent_calligraphy_panel < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Mosque. It must avoid colliding with the donation box, Quran stand and play sign. The states of the agent, book shelves, shoe racks and safety divider stands are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Mosque. It must avoid colliding with divider stands and prayer rows. The states of the drone and the water tuck and donation_box are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Mosque. It must avoid colliding with Mosque aisle divider stand. Agent's state, target's state and divider stand's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_divider_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_divider_stand = calculate_distance_min(s_agent, s_divider_stand) \n    penalty_collision = -20 if (d_agent_divider_stand < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Mosque. Goal: It must avoid colliding with ablution fountain. Assumptions: The states of the drone and the ablution fountain's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_ablution_fountain, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ablution_fountain = calculate_distance_min(s_agent, s_ablution_fountain) \n    penalty_collision = -20 if (d_agent_ablution_fountain < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Mosque. Goal: It must avoid colliding with Mosque's book shelves, storage room and divider stands. Assumptions: The Agent's state, target's state and book shelves, storage room and divider stands's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Mosque. Goal: Detect wooden bench, shoe rack and Quran stand and do not collide. Assumptions: The states of the drone, target  wooden bench, shoe rack and Quran stand state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Mosque. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Music Hall. \n Objectives: It must avoid colliding with concrete columns or audience rows. \n Conditions: The states of the agent, the stage, and the audience rows are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Music Hall. \n Objectives: Detect coat rack and aviod it. \n Conditions: The states of the agent, the coat rack are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_coat_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_coat_rack = calculate_distance_min(s_agent, s_coat_rack) \n    penalty_collision = -20 if (d_agent_coat_rack < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Music Hall. Objectives: It must avoid colliding with spotlights. Conditions: States of the drone and the spotlights are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_spotlight, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_spotlight = calculate_distance_min(s_agent, s_spotlight) \n    penalty_collision = -20 if (d_agent_spotlight < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a concert hall, the drone must maneuver around audience row. All entity states (agent, audience row) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_audience_row, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_audience_row = calculate_distance_min(s_agent, s_audience_row) \n    penalty_collision = -20 if (d_agent_audience_row < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a concert hall. Requirement: Detect the three podiums and do not collide with it. Given: The states for the drone and many podiums are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a ablution Music Hall during experiments. \n Objectives: So not collide with shelf. \n Conditions: The states of the agent, the shelf are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_music_shelf, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_music_shelf = calculate_distance_min(s_agent, s_music_shelf) \n    penalty_collision = -20 if (d_agent_music_shelf < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Music Hall. Objectives: Do not collide hit with podium. Conditions: States of the agent, podium are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_podium, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_podium = calculate_distance_min(s_agent, s_podium) \n    penalty_collision = -20 if (d_agent_podium < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a concert hall, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -20 if (d_agent_wall < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: ticket booth. Avoid: Detect speaker cabinet and do not hitting with speaker cabinet. Known: Agent, speaker cabinet states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_speaker_cabinet, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_speaker_cabinet = calculate_distance_min(s_agent, s_speaker_cabinet) \n    penalty_collision = -20 if (d_agent_speaker_cabinet < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through Music Hall. Objectives: It must avoid colliding with sound system. Conditions: The states of the drone and sound system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_sound_system, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sound_system = calculate_distance_min(s_agent, s_sound_system) \n    penalty_collision = -20 if (d_agent_sound_system < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Music Hall. Objectives: Avoid collision with coat stands, people and sound system. Conditions: The states of the agent, coat stands, people and sound system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through Music Hall. Objectives: It must avoid colliding with any speaker cabinet, units and computers. Conditions: Agent's, speaker cabinet, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Music Hall. Objectives: Do not collide with any cloack rack. Conditions: The states of the drone and the cloack rack are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_coat_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cloak_rack = calculate_distance_min(s_agent, s_coat_stand) \n    penalty = -20 if (d_agent_cloak_rack < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Music Hall and must avoid contact with any acoustic panels. The states of the drone and the acoustic panels are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_acoustic_panel, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_acoustic_panel = calculate_distance_min(s_agent, s_acoustic_panel) \n    penalty = -20 if (d_agent_acoustic_panel < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Music Hall. It must avoid colliding with the speaker cabinet, music stand and play sign. The states of the agent, music shelves, coat racks and safety barrier stands are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Music Hall. It must avoid colliding with barrier stands and audience rows. The states of the drone and the water tuck and speaker_cabinet are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Music Hall. It must avoid colliding with guitar. Agent's state, target's state and guitar's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_guitar, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_guitar = calculate_distance_min(s_agent, s_guitar) \n    penalty_collision = -20 if (d_agent_guitar < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Music Hall. Goal: It must avoid colliding with violin. Assumptions: The states of the drone and the violin's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_violin, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_violin = calculate_distance_min(s_agent, s_violin) \n    penalty_collision = -20 if (d_agent_violin < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Music Hall. Goal: It must avoid colliding with Music Hall's music shelves, backstage room and barrier stands. Assumptions: The Agent's state, target's state and music shelves, backstage room and barrier stands's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Music Hall. Goal: Detect audience bench, coat rack and music stand and do not collide. Assumptions: The states of the drone, target  audience bench, coat rack and music stand state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating through Music Hall. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Hair Salon. \n Objectives: It must avoid colliding with concrete columns or styling chairs. \n Conditions: The states of the agent, the stylist station, and the styling chairs are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Hair Salon. \n Objectives: Detect towel rack and aviod it. \n Conditions: The states of the agent, the towel rack are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_towel_rack, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_towel_rack = calculate_distance_min(s_agent, s_towel_rack) \n    penalty_collision = -20 if (d_agent_towel_rack < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Hair Salon. Objectives: It must avoid colliding with hair dryers. Conditions: States of the drone and the hair dryers are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_hair_dryer, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_hair_dryer = calculate_distance_min(s_agent, s_hair_dryer) \n    penalty_collision = -20 if (d_agent_hair_dryer < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a salon floor, the drone must maneuver around styling chair. All entity states (agent, chair) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -20 if (d_agent_chair < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a salon floor. Requirement: Detect the three cashier counters and do not collide with it. Given: The states for the drone and many cashier counters are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a ablution Hair Salon during experiments. \n Objectives: So not collide with shelf. \n Conditions: The states of the agent, the shelf are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_product_shelf, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_product_shelf = calculate_distance_min(s_agent, s_product_shelf) \n    penalty_collision = -20 if (d_agent_product_shelf < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Hair Salon. Objectives: Do not collide hit with cashier counter. Conditions: States of the agent, cashier counter are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_cashier_counter, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_cashier_counter = calculate_distance_min(s_agent, s_cashier_counter) \n    penalty_collision = -20 if (d_agent_cashier_counter < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a salon floor, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -20 if (d_agent_wall < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: reception desk. Avoid: Detect product display and do not hitting with product display. Known: Agent, product display states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_product_display, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_product_display = calculate_distance_min(s_agent, s_product_display) \n    penalty_collision = -20 if (d_agent_product_display < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Hair Salon. Objectives: It must avoid colliding with stereo system. Conditions: The states of the drone and stereo system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_customer, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_customer = calculate_distance_min(s_agent, s_customer) \n    penalty_collision = -20 if (d_agent_customer < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Hair Salon. Objectives: Avoid collision with mirror stands, people and stereo system. Conditions: The states of the agent, mirror stands, people and stereo system are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Hair Salon. Objectives: It must avoid colliding with any product display, units and computers. Conditions: Agent's, product display, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Hair Salon. Objectives: Do not collide with any cloack rack. Conditions: The states of the drone and the cloack rack are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_mirror_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_mirror_stand = calculate_distance_min(s_agent, s_mirror_stand) \n    penalty = -20 if (d_agent_mirror_stand < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Hair Salon and must avoid contact with any mirrors. The states of the drone and the customer are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_customer, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_customer = calculate_distance_min(s_agent, s_customer) \n    penalty = -20 if (d_agent_customer < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Hair Salon. It must avoid colliding with the product display, magazine stand and play sign. The states of the agent, product shelves, towel racks and doors are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Hair Salon. It must avoid colliding with partition stands and styling chairs. The states of the drone and the basket and product_display are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Hair Salon. It must avoid colliding with hair dryer stand. Agent's state, target's state and hair dryer stand's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_hair_dryer_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_hair_dryer_stand = calculate_distance_min(s_agent, s_hair_dryer_stand) \n    penalty_collision = -20 if (d_agent_hair_dryer_stand < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Hair Salon. Goal: It must avoid colliding with shampoo chair. Assumptions: The states of the drone and the shampoo chair's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_shampoo_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_shampoo_chair = calculate_distance_min(s_agent, s_shampoo_chair) \n    penalty_collision = -20 if (d_agent_shampoo_chair < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Hair Salon. Goal: It must avoid colliding with Hair Salon's product shelves, backstylist station room and partition stands. Assumptions: The Agent's state, target's state and product shelves, backstylist station room and partition stands's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Hair Salon. Goal: Detect waiting bench, towel rack and magazine stand and do not collide. Assumptions: The states of the drone, target  waiting bench, towel rack and magazine stand state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Hair Salon. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Spa Resort. \n Objectives: It must avoid colliding with concrete columns or massage tables. \n Conditions: The states of the agent, the spa station, and the massage tables are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Spa Resort. \n Objectives: Detect towel warmer and aviod it. \n Conditions: The states of the agent, the towel warmer are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_towel_warmer, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_towel_warmer = calculate_distance_min(s_agent, s_towel_warmer) \n    penalty_collision = -20 if (d_agent_towel_warmer < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through a Spa Resort. Objectives: It must avoid colliding with sauna heater. Conditions: States of the drone and the sauna heater are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_sauna_heater, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sauna_heater = calculate_distance_min(s_agent, s_sauna_heater) \n    penalty_collision = -20 if (d_agent_sauna_heater < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a spa floor, the drone must maneuver around massage table. All entity states (agent, chair) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -20 if (d_agent_chair < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a spa floor. Requirement: Detect the three spa counters and do not collide with it. Given: The states for the drone and many spa counters are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a ablution Spa Resort during experiments. \n Objectives: So not collide with aroma diffuser. \n Conditions: The states of the agent, the aroma diffuser are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_aroma_diffuser, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_aroma_diffuser = calculate_distance_min(s_agent, s_aroma_diffuser) \n    penalty_collision = -20 if (d_agent_aroma_diffuser < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Spa Resort. Objectives: Do not collide hit with spa counter. Conditions: States of the agent, spa counter are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_spa_counter, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_spa_counter = calculate_distance_min(s_agent, s_spa_counter) \n    penalty_collision = -20 if (d_agent_spa_counter < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a spa floor, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -20 if (d_agent_wall < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: front desk. Avoid: Detect flower arrangement and do not hitting with flower arrangement. Known: Agent, flower arrangement states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_flower_arrangement, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_flower_arrangement = calculate_distance_min(s_agent, s_flower_arrangement) \n    penalty_collision = -20 if (d_agent_flower_arrangement < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Spa Resort. Objectives: It must avoid colliding with spa statue. Conditions: The states of the drone and spa statue are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_spa_statue, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_spa_statue = calculate_distance_min(s_agent, s_spa_statue) \n    penalty_collision = -20 if (d_agent_spa_statue < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Spa Resort. Objectives: Avoid collision with decorative screen, people and water fountain. Conditions: The states of the agent, decorative screen, people and water fountain are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Spa Resort. Objectives: It must avoid colliding with any flower arrangement, units and computers. Conditions: Agent's, flower arrangement, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Spa Resort. Objectives: Do not collide with sauna heater. Conditions: The states of the drone and the sauna heater are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_sauna_heater, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_sauna_heater = calculate_distance_min(s_agent, s_sauna_heater) \n    penalty = -20 if (d_agent_sauna_heater < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Spa Resort and must avoid contact with the oil diffuser stand. The states of the drone and the oil stand are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_oil_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_oil_stand = calculate_distance_min(s_agent, s_oil_stand) \n    penalty = -20 if (d_agent_oil_stand < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Spa Resort. It must avoid colliding with the flower arrangement, tea table and play sign. The states of the agent, insliding screen plants, towel warmers and sliding screens are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Spa Resort. It must avoid colliding with spa divider and massage tables. The states of the drone and the stone sculpture and flower_arrangement are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Spa Resort. It must avoid colliding with the tea table. Agent's state, target's state and tea table's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_tea_table, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tea_table = calculate_distance_min(s_agent, s_tea_table) \n    penalty_collision = -20 if (d_agent_tea_table < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Spa Resort. Goal: It must avoid colliding with foot bath. Assumptions: The states of the drone and the foot bath's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_foot_bath, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_foot_bath = calculate_distance_min(s_agent, s_foot_bath) \n    penalty_collision = -20 if (d_agent_foot_bath < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Spa Resort. Goal: It must avoid colliding with Spa Resort's insliding screen plants, backspa station room and spa divider. Assumptions: The Agent's state, target's state and insliding screen plants, backspa station room and spa divider's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Spa Resort. Goal: Detect lounge chair, towel warmer and tea table and do not collide. Assumptions: The states of the drone, target  lounge chair, towel warmer and tea table state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Spa Resort. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Aquarium. \n Objectives: It must avoid colliding with concrete columns or viewing benches. \n Conditions: The states of the agent, the aquarium tank, and the viewing benches are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Aquarium. \n Objectives: Detect filter pump and aviod it. \n Conditions: The states of the agent, the filter pump are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_filter_pump, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_filter_pump = calculate_distance_min(s_agent, s_filter_pump) \n    penalty_collision = -20 if (d_agent_filter_pump < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating through an aquarium. Objectives: It must avoid colliding with UV sterilizer. Conditions: States of the drone and the UV sterilizer are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bench, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bench = calculate_distance_min(s_agent, s_bench) \n    penalty_collision = -20 if (d_agent_bench < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a aquarium hall, the drone must maneuver around viewing bench. All entity states (agent, chair) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -20 if (d_agent_chair < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a aquarium hall. Requirement: Detect the three touch pools and do not collide with it. Given: The states for the drone and many touch pools are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a ablution Aquarium during experiments. \n Objectives: So not collide with air bubbler. \n Conditions: The states of the agent, the air bubbler are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_air_bubbler, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_air_bubbler = calculate_distance_min(s_agent, s_air_bubbler) \n    penalty_collision = -20 if (d_agent_air_bubbler < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Aquarium. Objectives: Do not collide hit with touch pool. Conditions: States of the agent, touch pool are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_touch_pool, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_touch_pool = calculate_distance_min(s_agent, s_touch_pool) \n    penalty_collision = -20 if (d_agent_touch_pool < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a aquarium hall, the drone must steer clear of the visitor. All states (agent, visitor) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_visitor, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_visitor = calculate_distance_min(s_agent, s_visitor) \n    penalty_collision = -20 if (d_agent_visitor < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: ticket counter. Avoid: Detect coral display and do not hitting with coral display. Known: Agent, coral display states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_coral_display, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_coral_display = calculate_distance_min(s_agent, s_coral_display) \n    penalty_collision = -20 if (d_agent_coral_display < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Aquarium. Objectives: It must avoid colliding with marine sculpture. Conditions: The states of the drone and marine sculpture are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_marine_sculpture, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_marine_sculpture = calculate_distance_min(s_agent, s_marine_sculpture) \n    penalty_collision = -20 if (d_agent_marine_sculpture < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Aquarium. Objectives: Avoid collision with glass panel, people and fish tank. Conditions: The states of the agent, glass panel, people and fish tank are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Aquarium. Objectives: It must avoid colliding with any coral display, units and computers. Conditions: Agent's, coral display, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Aquarium. Objectives: Do not collide with viewing_bench. Conditions: The states of the drone and the viewing_bench are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_bench, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_bench = calculate_distance_min(s_agent, s_bench) \n    penalty = -20 if (d_agent_bench < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Aquarium and must avoid contact with the oxygen pump. The states of the drone and the oxygen pump are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_oxygen_pump, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_oxygen_pump = calculate_distance_min(s_agent, s_oxygen_pump) \n    penalty = -20 if (d_agent_oxygen_pump < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Aquarium. It must avoid colliding with the coral display, info stand and play sign. The states of the agent, inglass door plants, filter pumps and glass doors are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Aquarium. It must avoid colliding with aquarium barrier and viewing benches. The states of the drone and the aquarium statue and coral_display are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Aquarium. It must avoid colliding with the info stand. Agent's state, target's state and info stand's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_info_stand, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_info_stand = calculate_distance_min(s_agent, s_info_stand) \n    penalty_collision = -20 if (d_agent_info_stand < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Aquarium. Goal: It must avoid colliding with shallow tank. Assumptions: The states of the drone and the shallow tank's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_tank, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_tank = calculate_distance_min(s_agent, s_tank) \n    penalty_collision = -20 if (d_agent_tank < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Aquarium. Goal: It must avoid colliding with Aquarium's inglass door plants, backaquarium tank room and aquarium barrier. Assumptions: The Agent's state, target's state and inglass door plants, backaquarium tank room and aquarium barrier's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Aquarium. Goal: Detect visitor bench, filter pump and info stand and do not collide. Assumptions: The states of the drone, target  visitor bench, filter pump and info stand state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Aquarium. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating along Tennis Court. \n Objectives: It must avoid colliding with concrete columns or spectator benches. \n Conditions: The states of the agent, the net post, and the spectator benches are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone is operating inside a busy Tennis Court. \n Objectives: Detect ball basket and aviod it. \n Conditions: The states of the agent, the ball basket are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ball_basket, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ball_basket = calculate_distance_min(s_agent, s_ball_basket) \n    penalty_collision = -20 if (d_agent_ball_basket < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Tennis Court open workspace. Objectives: It must avoid colliding with ball machine. Conditions: States of the drone and the ball machine are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ball_machine, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ball_machine = calculate_distance_min(s_agent, s_ball_machine) \n    penalty_collision = -20 if (d_agent_ball_machine < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: Inside a tennis court area, the drone must maneuver around spectator bench. All entity states (agent, chair) are known. Provide a Python reward function.",
        "chosen": "def reward(s_agent, s_chair, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_chair = calculate_distance_min(s_agent, s_chair) \n    penalty_collision = -20 if (d_agent_chair < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task: A drone is flying through a tennis court area. Requirement: Detect the three practice walls and do not collide with it. Given: The states for the drone and many practice walls are known. Write a Python reward function.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacle_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacle_min < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "\n Mission: A drone operates near a ablution Tennis Court during experiments. \n Objectives: So not collide with water cooler. \n Conditions: The states of the agent, the water cooler are known. Write the Python reward function.",
        "chosen": "def reward(s_agent, s_water_cooler, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_water_cooler = calculate_distance_min(s_agent, s_water_cooler) \n    penalty_collision = -20 if (d_agent_water_cooler < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: Navigate in Tennis Court. Objectives: Do not collide hit with practice wall. Conditions: States of the agent, practice wall are obtainable. Deliver: Python reward function.",
        "chosen": "def reward(s_agent, s_practice_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_practice_wall = calculate_distance_min(s_agent, s_practice_wall) \n    penalty_collision = -20 if (d_agent_practice_wall < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Context: Within a tennis court area, the drone must steer clear of wall. All states (agent, wall) are known. Provide a Python reward function for training.",
        "chosen": "def reward(s_agent, s_wall, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_wall = calculate_distance_min(s_agent, s_wall) \n    penalty_collision = -20 if (d_agent_wall < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Task Briefing: Environment: umpire chair. Avoid: Detect scoreboard and do not hitting with scoreboard. Known: Agent, scoreboard states. Create the Python reward function.",
        "chosen": "def reward(s_agent, s_scoreboard, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_scoreboard = calculate_distance_min(s_agent, s_scoreboard) \n    penalty_collision = -20 if (d_agent_scoreboard < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Tennis Court. Objectives: It must avoid colliding with tennis monument. Conditions: The states of the drone and tennis monument are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_monument, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_monument = calculate_distance_min(s_agent, s_monument) \n    penalty_collision = -20 if (d_agent_monument < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Tennis Court. Objectives: Avoid collision with fence panel, people and ball cart. Conditions: The states of the agent, fence panel, people and ball cart are known. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is navigating in a Tennis Court. Objectives: It must avoid colliding with any scoreboard, units and computers. Conditions: Agent's, scoreboard, units and computers states are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Mission: A drone is flying through Tennis Court. Objectives: Do not collide with ball machine. Conditions: The states of the drone and the ball machine are obtainable. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_ball_machine, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_ball_machine = calculate_distance_min(s_agent, s_ball_machine) \n    penalty = -20 if (d_agent_ball_machine < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating along Tennis Court and must avoid contact with the lighting pole. The states of the drone and the lighting pole are known. Write a Python reward function for this case.",
        "chosen": "def reward(s_agent, s_pole, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_pole = calculate_distance_min(s_agent, s_pole) \n    penalty = -20 if (d_agent_pole < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Tennis Court. It must avoid colliding with the scoreboard, schedule board and play sign. The states of the agent, incourt gate plants, ball baskets and court gates are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is operating in Tennis Court. It must avoid colliding with boundary fence and spectator benches. The states of the drone and the court statue and scoreboard are known. Write a reward function in Python",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "A drone is inspecting in Tennis Court. It must avoid colliding with the schedule board. Agent's state, target's state and schedule board's state are accessible. Write a reward function in Python.",
        "chosen": "def reward(s_agent, s_schedule_board, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_schedule_board = calculate_distance_min(s_agent, s_schedule_board) \n    penalty_collision = -20 if (d_agent_schedule_board < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Tennis Court. Goal: It must avoid colliding with clay patch. Assumptions: The states of the drone and the clay patch's state are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_clay_patch, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_foot_bath = calculate_distance_min(s_agent, s_clay_patch) \n    penalty_collision = -20 if (d_agent_foot_bath < ob_threshold) else 10 \n    reward_reaching_target = 19 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Tennis Court. Goal: It must avoid colliding with Tennis Court's incourt gate plants, backnet post room and boundary fence. Assumptions: The Agent's state, target's state and incourt gate plants, backnet post room and boundary fence's states are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Tennis Court. Goal: Detect player bench, ball basket and schedule board and do not collide. Assumptions: The states of the drone, target  player bench, ball basket and schedule board state are availalbe. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_multiple_obstacles, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    d_agent_multiple_obstacles_min = calculate_distance_min(s_agent, s_multiple_obstacles) \n    penalty_collision = -20 if (d_agent_multiple_obstacles_min < ob_threshold) else 10 \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return penalty_collision + reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    },
    {
        "prompt": "Scenario: A drone is navigating in a Tennis Court. Goal: Drone target is to reach the target. Assumptions: The states of the drone and the target are known. Instruction: Implement a reward function in Python.",
        "chosen": "def reward(s_agent, s_target): \n    d_agent_target = calculate_distance(s_agent, s_target) \n    reward_reaching_target = 25 if (d_agent_target < target_threshold) else -30 \n    return reward_reaching_target",
        "rejected": "def calculate(): \n    return 1"
    }
]