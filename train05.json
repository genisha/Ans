[
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with chairs. \n Conditions: Agent's state and chair's status and desired speed are available. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, chair_pos, goal_pos):\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    if dist_chair < 1.2:\n        return -30\n    return -dist_goal\n",
    "rejected": "def reward(agent, target): x = agent.current_position[0]; y = agent.current_position[1]; x_target = target.position_x; y_target = target.position_y; dist = ((x - x_target) ** 2 + (y - y_target) ** 2) ** 0.5; return 10 * dist if dist <= 1 else -dist"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with chair. \n Conditions: Agent's state and chair's status and desired speed are available. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    d_goal = np.linalg.norm(agent_pos - goal_pos)\n    d_chair = np.linalg.norm(agent_pos - chair_pos)\n    v_diff = np.linalg.norm(agent_vel - desired_vel)\n    penalty = -50 if d_chair < 0.8 else 0\n    return -d_goal - 0.2 * v_diff + penalty\n",
    "rejected": "import numpy as np; import gym; import random; class Environment: def __init__(self): self.state = 0; def reset(self): return self.state; def step(self, action): pass; def get_reward(self, state, action): desired_speed = float(input('Enter the desired speed (in meters per second): ')); distance = abs(desired_speed - self.state); if distance <= 1: return float('inf'); return distance; env = Environment(); state = env.reset(); while True: action = random.randint(0, 3); new_state = state + action; reward = env.get_reward(new_state, action); if abs(new_state - state) < 1: break; state = new_state; print(f'Final state: {state}'); print(f'Final reward: {reward}'); class MyGymEnv(gym.Env): def __init__(self): super(MyGymEnv, self).__init__(); self.action_space = gym.spaces.Discrete(4); self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(3,), dtype=np.float32); self.state = None; self.goal = None; self.distance_to_goal = None; self.done = False; def step(self, action): self.state = np.random.rand(3) * 5; self.goal = np.array([action]); self.distance_to_goal = np.linalg.norm(self.state - self.goal); reward = 1 / self.distance_to_goal; info = {}; return self.state, reward, self.done, info; def reset(self): self.state = np.random.rand(3) * 5; self.goal = np.array([random.randint(-5, 5)]); self.distance_to_goal = np.linalg.norm(self.state - self.goal); self.done = False; return self.state; def render(self, mode='human'): print('Current state:', self.state); if __name__ == '__main__': env = MyGymEnv(); observation = env.reset(); print('Initial state:', observation); while not env.done: action = env.action_space.sample(); observation, reward, done, info = env.step(action); print('Action:', action); print('Reward:', reward); print('Observation:', observation); print('Done:', done); print('Info:', info)"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with ceiling fan blades. \n Conditions: Agent's state and staic fan blades's status and desired speed are available. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    d_goal = np.linalg.norm(agent_pos - goal_pos)\n    d_chair = np.linalg.norm(agent_pos - chair_pos)\n    v_diff = np.linalg.norm(agent_vel - desired_vel)\n    penalty = -50 if d_chair < 0.8 else 0\n    return -d_goal - 0.2 * v_diff + penalty\n",
    "rejected": "import numpy as np; class RewardFunction: def __init__(self, max_speed): self.max_speed = max_speed; def get_reward(self, current_state, static_fan_blades_status, target_speed): if current_state == 'closed': return -1 * (target_speed - 0.5) ** 2; elif current_state == 'open' and static_fan_blades_status == 'closed': return -1 * (target_speed - 1) ** 2; else: return 0; max_speed = 1; reward_function = RewardFunction(max_speed); initial_state = np.array([1, 2]); target_speed = 3; reward = reward_function.get_reward(initial_state, 'closed', target_speed); print('Reward:', reward)"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives:Avoid collision with hanging lamps. \n Conditions: Agent's state and lamp's status and desired speed are available. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    speed_diff = np.linalg.norm(agent_vel - desired_vel)\n    if dist_chair < 1.0:\n        return -25\n    return -dist_goal - 0.3 * speed_diff\n",
    "rejected": "import numpy as np; from gym import spaces; action_space = spaces.Discrete(4); class DroneExplorationEnv: def __init__(self): self.state = np.zeros((2, 1)); self.lamp_status = [False] * 2; self.target_position = [0, 0]; self.speed = 5; def reset(self): self.state[0] = 0; self.state[1] = 0; self.lamp_status[0] = False; self.lamp_status[1] = False; self.target_position[0] = 0; self.target_position[1] = 0; return self.state; def get_reward(self, action): rewards = {0: 10, 1: -5, 2: -3, 3: 5}; return rewards.get(action, 0); def step(self, action): if action == 0: self.state[0] += 1; elif action == 1: self.state[0] -= 1; elif action == 2: self.state[1] += 1; elif action == 3: self.state[1] -= 1; if (abs(self.state[0]) > 1) or (abs(self.state[1]) > 1): print('Target reached'); self.target_position[0] = 0; self.target_position[1] = 0; self.lamp_status[0] = True; self.lamp_status[1] = True; if abs(self.state[0]) <= 1 and abs(self.state[1]) <= 1: print('Light is within range'); else: print('Light is not within range'); self.state[0] = int(self.state[0] + self.speed); self.state[1] = int(self.state[1] + self.speed); dist = ((self.state[0] - self.target_position[0]) ** 2 + (self.state[1] - self.target_position[1]) ** 2) ** 0.5; if dist < 1: print('Target reached'); self.target_position[0] = 0; self.target_position[1] = 0; self.lamp_status[0] = True; self.lamp_status[1] = True; if (abs(self.state[0]) <= 1) and (abs(self.state[1]) <= 1): print('Light is within range'); else: print('Light is not within range'); if (abs(self.state[0]) > 1) or (abs(self.state[1]) > 1): print('Collision detected'); self.lamp_status[0] = True; self.lamp_status[1] = True; if self.lamp_status[0]: self.state[0] -= 1; if self.lamp_status[1]: self.state[1] -= 1; if (abs(self.state[0]) > 1) or (abs(self.state[1]) > 1): print('Target reached'); self.target_position[0] = 0; self.target_position[1] = 0; self.lamp_status[0] = True; self.lamp_status[1] = True; if self.target_position[0] == 0 and self.target_position[1] == 0: self.state[0] += 1; self.state[1] += 1; reward = self.get_reward(action); return self.state, reward, reward, self.lamp_status[0]"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with curtain. \n Conditions: Agent's state and static curtain's status and desired speed are available. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, chair_pos, goal_pos):\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    if dist_chair < 1.2:\n        return -30\n    return -dist_goal\n",
    "rejected": "def reward_function(agent_state, curtain_status, desired_speed):\n    if curtain_status == 'closed':\n        return -10\n    elif curtain_status == 'open' or curtain_status == 'full':\n        return 5\n    else:\n        return 0\n agent_state = {'position': [x, y], 'orientation': theta}\n curtain_status = 'open'\n desired_speed = 20\n Reward = reward_function(agent_state, curtain_status, desired_speed)\n print(f'Reward: {reward}')"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with avoid drapes. \n Conditions: Agent's state and drape's status and desired speed are available. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    speed_diff = np.linalg.norm(agent_vel - desired_vel)\n    if dist_chair < 1.0:\n        return -25\n    return -dist_goal - 0.3 * speed_diff\n",
    "rejected": "import numpy as np\n     class Environment:\n    def __init__(self):\n        self.state = None\n        self.drape_status = 'open'\n        self.target_speed = 0.5\n        def reset(self):\n        self.state = None\n        return self.state\n        def step(self, action):\n        if self.state is not None:\n            new_state = self.update_drape()\n            if self.check_collision(new_state):\n                print('Collision detected!')\n                return -float('inf'), False, True\n            self.target_speed = max(0.0, min(self.target_speed + action * 2, 1.0))\n            return new_state, self.calculate_reward(), False\n        else:\n            raise ValueError('Environment has not been initialized.')\n        def update_drape(self):\n        return np.random.rand() > self.target_speed\n        def check_collision(self, state):\n        return state == 0\n     def calculate_reward(state, action):\n    if state == 0 and action != 1:\n        return float('inf')\n    elif not state:\n        return float('-inf')\n    else:\n        return 1 if action == 1 else -1\n     def q_learning(agent, environment, alpha=0.1, gamma=0.9, epsilon=1.0):\n    Q = np.zeros((environment.n_actions,))\n    while True:\n        old_Q = Q.copy()\n        for t in reversed(range(environment.timesteps)):\n            state = environment.reset()\n            done = False\n            while not done:\n                action = np.argmax(Q[state])\n                next_state, reward, done = environment.step(action)\n                Q[state] = Q[state] + alpha * (reward + gamma * np.max(Q[next_state]) - Q[state])\n                state = next_state\n        if np.random.uniform() < epsilon:\n            action = np.random.randint(0, len(Q))\n        else:\n            action = np.argmax(Q)\n        break\n    return Q\n     if __name__ == '__main__':\n    env = Environment()\n    Q = q_learning(env, env)\n    print(f'Q-values:\\n{Q}')"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with obstacles. \n Conditions: Agent's state and obstacle's status and desired speed are available. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    smoothness = np.linalg.norm(agent_vel - desired_vel)\n    collision_penalty = np.exp(-dist_chair) * 10\n    return -dist_goal - 0.1 * smoothness - collision_penalty\n",
    "rejected": "import numpy as np; import matplotlib.pyplot as plt; class RoomExplorationAgent: def __init__(self): self.room_size = 10; self.obstacle_type = 'wall'; self.speed = 5; def explore(self, start_pos): obstacles = []; possible_positions = [(0, 0), (self.room_size, 0), (0, self.room_size), (self.room_size, self.room_size)]; for pos in possible_positions: if not any(np.linalg.norm(np.array(pos) - np.array(p)) <= self.room_size for p in obstacles): obstacles.append(pos); distances = [np.linalg.norm(np.array(start_pos) - np.array(ob)) for ob in obstacles]; return min(distances); def move(self, direction): print(f'Moving in direction: {direction}'); if __name__ == '__main__': agent = RoomExplorationAgent(); start_pos = (0, 0); goal_pos = (agent.room_size * 0.8, agent.room_size * 0.8); time_steps = 10; for step in range(time_steps): print(f'Step {step}: Starting at {start_pos}'); dist_to_obstacle = agent.explore(start_pos); print(f'Distance to nearest obstacle: {dist_to_obstacle:.2f} meters'); agent.move((dist_to_obstacle, 0)); start_pos = goal_pos; if step % 10 == 0: print(f'Current position: {start_pos}'); fig, ax = plt.subplots(figsize=(6, 6)); ax.scatter(agent.room_size * 0.8, agent.room_size * 0.8, color='blue', label='Final position'); ax.scatter(goal_pos[0], goal_pos[1], color='red', label='Goal position'); ax.plot([goal_pos[0], start_pos[0]], [goal_pos[1], start_pos[1]], 'k-'); ax.set_aspect('equal'); ax.legend(); plt.show()"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a room. \n Objectives: Avoid collision with static obstacle (wardrobes). \n Conditions: Agent's state, wardrobe's state and desired speed are available. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    d_goal = np.linalg.norm(agent_pos - goal_pos)\n    d_chair = np.linalg.norm(agent_pos - chair_pos)\n    v_diff = np.linalg.norm(agent_vel - desired_vel)\n    penalty = -50 if d_chair < 0.8 else 0\n    return -d_goal - 0.2 * v_diff + penalty\n",
    "rejected": "def reward_function(agent_state, wardrobe_state, desired_speed):\n    if agent_state == 'safe' and wardrobe_state == 'closed':\n        return 100\n    elif agent_state == 'safe' and wardrobe_state != 'closed':\n        return -50\n    else:\n        return 0\n agent_state = 'safe'\nwardrobe_state = 'closed'\n desired_speed = 20\n Reward = reward_function(agent_state, wardrobe_state, desired_speed)\n print(f'The reward is {reward}')"
  },
  {
    "prompt": "Mission: Drone in-door navigation and reach other side of the small room. \n Objectives: Do not collide with the walls of the room. \n Conditions: Agent's state, obstacle's state and desired speed are accessible. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, chair_pos, goal_pos):\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    if dist_chair < 1.2:\n        return -30\n    return -dist_goal\n",
    "rejected": "def reward_function(agent_state, obstacle_state, desired_speed):\n    if agent_state['x'] == 0 or agent_state['y'] == 0:\n        return -1\n    elif (agent_state['x'], agent_state['y']) == (obstacle_state['x'], obstacle_state['y']):\n        return -2\n    else:\n        pass\n    # The current implementation fails because it only checks for walls at position 0,\n# but not for closeness to walls or the full boundary of the room.\n# To fix this, add wall distance and reachability constraints.\n    # Example wall distance constraint:\n# wall_distance = sqrt((a-c)**2 + (b-d)**2)\n# if agent_position.x < c + 1 or agent_position.y < d + 1:\n#     return -3\n    # Example reachability constraint:\n# reachability_constraint = ((agent_position.x - 0) ** 2 + (agent_position.y - 0) ** 2 <= 1) and \\\n#                           (n - abs(agent_position.x) - 1) >= 0 and \\\n#                           (m - abs(agent_position.y) - 1) >= 0\n# if reachability_constraint:\n#     return -4"
  },
  {
    "prompt": "Mission: Guide the drone through an indoor space to the far side of a small room. \n Objectives: Prevent contact with the people. \n Conditions: Agent's state, obstacle's state and desired speed are accessible. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    to_goal = np.linalg.norm(agent_pos - goal_pos)\n    to_chair = np.linalg.norm(agent_pos - chair_pos)\n    vel_penalty = np.linalg.norm(agent_vel - desired_vel)\n    reward = -1.5 * to_goal\n    if to_chair < 1.1:\n        reward -= 15\n    reward -= 0.2 * vel_penalty\n    return reward\n",
    "rejected": "def reward(agent_state, obstacle_state, desired_speed):\n    # If the agent is near the obstacle, reduce its speed\n    if agent_state['distance_to_obstacle'] < 10:\n        return -desired_speed\n    else:\n        return desired_speed\n    # This reward function takes into account both the proximity of the agent to the obstacle and the current speed of the agent.\n# If the agent is close enough to the obstacle (less than 10 units), it reduces its speed by the specified amount.\n# If not, it increases its speed by the same amount.\n# This reward function ensures that the agent is always moving towards the obstacle while minimizing its impact on the environment.\n    # Example usage:\n# reward(agent_state={'agent': {'x': 10, 'y': 20}}, obstacle_state={'obstacle': {'x': 5, 'y': 3}}, desired_speed=10)\n# will give a reward of -10 for the agent being closer to the obstacle than 10 units away.\n    # The reward function can also be modified to penalize the agent more heavily if they move too fast or collide with the obstacle.\n# For instance, the function could return a higher value for rewards less than 10.\n# However, keep in mind that this may affect the overall performance of your algorithm and should be used judiciously.\n    # Additionally, consider implementing additional features such as collision detection, obstacle avoidance,\n# and feedback mechanisms to improve the agent's ability to navigate safely.\n    # Note: the reward function assumes the agent has already been placed at the start of the task and does not\n# provide any information about how the agent arrived at this position.\n# Therefore, the agent must be able to calculate its own path independently of the obstacle's location and movement pattern.\n# This requires some form of localization or mapping capability, which may need to be implemented separately."
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Avoid hitting the surrounding walls. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    smoothness = np.linalg.norm(agent_vel - desired_vel)\n    collision_penalty = np.exp(-dist_chair) * 10\n    return -dist_goal - 0.1 * smoothness - collision_penalty\n",
    "rejected": "def reward_function(state, action):\n    rewards = {\n        'safe': 0,\n        'hit_wall': -1,\n        'avoid_obstacle': 5\n    }\n    if state == 'safe':\n        return 0\n    elif state == 'hit_wall':\n        return abs(action) * 2\n    else:\n        return 0\n     def get_reward(state):\n    if state == 'safe':\n        return 0\n    elif state == 'hit_wall':\n        return abs(action) * 2\n    else:\n        return 0"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Navigate without crashing into walls. \n Conditions: Agent's state, obstacle's state and desired speed are obtainable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, chair_pos, goal_pos):\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    if dist_chair < 1.2:\n        return -30\n    return -dist_goal\n",
    "rejected": "class MazeEnv:\n    def __init__(self):\n        self.maze = np.zeros((10, 10))\n        self.agent_pos = [2, 3]\n        self.agent_dir = np.array([1, 0])\n        def reset(self):\n        self.maze[self.agent_pos[0], self.agent_pos[1]] = 1\n        return self.maze\n        def step(self, action):\n        if action == 0:\n            new_pos = (self.agent_pos[0] - 1, self.agent_pos[1])\n            self.maze[new_pos[0], new_pos[1]] = 0\n        elif action == 1:\n            new_pos = (self.agent_pos[0] + 1, self.agent_pos[1])\n            self.maze[new_pos[0], new_pos[1]] = 0\n        else:\n            new_pos = (self.agent_pos[0], self.agent_pos[1] - 1)\n            self.maze[new_pos[0], new_pos[1]] = 0\n        self.agent_pos = new_pos\n        done = False\n        reward = 0\n        if self.is_collision():\n            reward = -10\n            done = True\n        return self.maze, reward, done\n        def is_collision(self):\n        for i in range(1, 9):\n            for j in range(1, 10):\n                if self.maze[i, j] == 1:\n                    return True\n        return False\n     env = MazeEnv()\n print(env.reset())\n    while not env.done:\n    print('Current maze:')\n    print(env.maze)\n    action = int(input('Enter your action (0 or 1): '))\n    env.step(action)\n     print('\\n game Over')\n    # The purpose of this code is to create a simple environment for an agent to navigate through a maze while avoiding obstacles.\n# The agent can only move forward or backward on the walls, and it must avoid obstacles by staying within the boundaries of the maze.\n# The code starts by initializing a 10x10 grid with all values set to zero."
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (cables) and avoid collision. \n Conditions: Agent's state, cable's state and desired speed are accessible. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    smoothness = np.linalg.norm(agent_vel - desired_vel)\n    collision_penalty = np.exp(-dist_chair) * 10\n    return -dist_goal - 0.1 * smoothness - collision_penalty\n",
    "rejected": "import numpy as np\n    # Rewards\n Reward = lambda agent_state, cable_state, desired_speed: \\\n    -np.inf if agent_state['position'] > cable_state['position'] else \\\n    -np.inf if abs(agent_state['velocity']) > cable_state['velocity'] else \\\n    10 if agent_state['position'] == cable_state['position'] else \\\n    5"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect obstacle (chairs) and avoid collision. \n Conditions: Agent's state, chair's state and desired speed are obtainable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    to_goal = np.linalg.norm(agent_pos - goal_pos)\n    to_chair = np.linalg.norm(agent_pos - chair_pos)\n    vel_penalty = np.linalg.norm(agent_vel - desired_vel)\n    reward = -1.5 * to_goal\n    if to_chair < 1.1:\n        reward -= 15\n    reward -= 0.2 * vel_penalty\n    return reward\n",
    "rejected": "import numpy as np\n ENVIRONMENT_WIDTH = 100\n ENVIRONMENT_HEIGHT = 100\n AGENT_RADIUS = 5\n cHAIR_RADIUS = 2\n tARGET_RADIUS = 3\n mAX_SPEED = 4\n tARGET_X = ENVIRONMENT_WIDTH // 2\n tARGET_Y = ENVIRONMENT_HEIGHT // 2\n dISTANCE_THRESHOLD = 10\n def compute_reward(agent_state, chair_state):\n    dx = agent_state[0] - CHAIR_RADIUS\n    dy = agent_state[1] - CHAIR_RADIUS\n    if abs(dx) + abs(dy) > MAX_SPEED:\n        return -1\n    dist_to_target = ((agent_state[0] - TARGET_X)**2 + (agent_state[1] - TARGET_Y)**2)**0.5\n    reward = max(0, 1 - dist_to_target)\n    return reward\n agent_state = [10, 10]\n chair_state = [9, 9]\n Reward = compute_reward(agent_state, chair_state)\n print(f'Reward: {reward}')"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (table) and avoid collision. \n Conditions: Agent's state, table's state and desired speed are accessible. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    d_goal = np.linalg.norm(agent_pos - goal_pos)\n    d_chair = np.linalg.norm(agent_pos - chair_pos)\n    v_diff = np.linalg.norm(agent_vel - desired_vel)\n    penalty = -50 if d_chair < 0.8 else 0\n    return -d_goal - 0.2 * v_diff + penalty\n",
    "rejected": "def reward_function(agent_state, table_state, desired_speed):\n    \"\"\"\n    Calculate the reward based on the agent's current state and the desired speed.\n    \n    Parameters:\n        agent_state (dict): A dictionary containing the agent's current position,\n                           the table's current position, and the desired speed.\n        table_state (dict): A dictionary containing the table's current position.\n        desired_speed (float): The desired speed of the agent.\n        \n    Returns:\n        float: The calculated reward.\n    \"\"\"\n    distance = np.linalg.norm(agent_state['position'] - table_state['position'])\n    time_taken = 1 / desired_speed\n    if distance <= table_state['size']:\n        return 0.0\n    else:\n        return -distance * time_taken\n     agent_state = {'position': [50, 50], 'velocity': [0, 0], 'speed': 2}\n table_state = {'position': [100, 100], 'velocity': [0, 0], 'size': 10}\n desired_speed = 1.5\n Reward = reward_function(agent_state, table_state, desired_speed)\n print('Reward:', reward)\n    # Explanation:\n# - Agent State: Contains info about the agent's position, velocity, and speed.\n# - Table State: Position and size of the obstacle (table).\n# - Desired Speed: How fast the agent is supposed to move.\n# - Distance: Euclidean distance from agent to table.\n# - Time: Calculated as inverse of speed.\n# - Reward: Zero if no collision (agent within table radius), negative otherwise."
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (bookshelves) and avoid collision. \n Conditions: Agent's state, bookshelve's state and desired speed are obtainable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    speed_diff = np.linalg.norm(agent_vel - desired_vel)\n    if dist_chair < 1.0:\n        return -25\n    return -dist_goal - 0.3 * speed_diff\n",
    "rejected": "# Define the reward function\n def reward_function(agent_state, bookshelf_state, desired_speed):\n    if agent_state[0] == len(agent_state) - 1:\n        return 0\n    if agent_state[0] == len(agent_state) - 1 and not agent_state[-1] in bookshelf_state:\n        return 2\n    if agent_state[0] == len(agent_state) - 1 and agent_state[-1] in bookshelf_state:\n        return -3\n    distance = abs(len(agent_state) - agent_state[-1])\n    reward = 5 + distance * 0.1\n    return reward\n    # Example usage\n agent_state = [0, 0]\n bookshelf_state = [1, 0]\n desired_speed = 4\n print(reward_function(agent_state, bookshelf_state, desired_speed))"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (smoke detectors) and avoid collision. \n Conditions: Agent's state, smoke detector's state and desired speed are accessible. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    speed_diff = np.linalg.norm(agent_vel - desired_vel)\n    if dist_chair < 1.0:\n        return -25\n    return -dist_goal - 0.3 * speed_diff\n",
    "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return c_2*math.exp(d_agent_goal)"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect thin obstacle (stackes books) and avoid collision. \n Conditions: Agent's state, book's state and desired speed are obtainable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    to_goal = np.linalg.norm(agent_pos - goal_pos)\n    to_chair = np.linalg.norm(agent_pos - chair_pos)\n    vel_penalty = np.linalg.norm(agent_vel - desired_vel)\n    reward = -1.5 * to_goal\n    if to_chair < 1.1:\n        reward -= 15\n    reward -= 0.2 * vel_penalty\n    return reward\n",
    "rejected": "def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward(): def reward():  d_agent_obstacle = calculate_distance(s_agent, s_obstacle) \n    if d_agent_obstacle < c1: \n  return -5.0 \n    return d_agent_goal**2"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, human's state and desired speed are accessible. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    smoothness = np.linalg.norm(agent_vel - desired_vel)\n    collision_penalty = np.exp(-dist_chair) * 10\n    return -dist_goal - 0.1 * smoothness - collision_penalty\n",
    "rejected": "def reward_function(agent_state, human_state, target_speed):\n    if agent_state == 'idle' and human_state['person'] != 'occupied':\n        return 100\n    elif agent_state == 'idle' and human_state['person'] == 'occupied':\n        return -50\n    elif agent_state == 'moving' and human_state['person'] != 'occupied':\n        return 20\n    elif agent_state == 'moving' and human_state['person'] == 'occupied':\n        return -30\n    else:\n        return 0\n     agent = 'idle'\n human = {'person': 'occupied'}\n target_speed = 70\n print(reward_function(agent, human, target_speed))\n    # In this example, the `reward_function` takes three arguments: `agent_state`, `human_state`, and `target_speed`.\n# It returns a positive value if the agent is moving towards the target speed,\n# negative value if it's moving away from the target speed, or zero if the agent is stationary."
  },
  {
    "prompt": "Mission: Drone indoor exploration in a library. \n Objectives: Detect dynamic obstacle (shelves) and avoid collision. \n Conditions: Agent's state, shelve's state and desired speed are obtainable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    to_goal = np.linalg.norm(agent_pos - goal_pos)\n    to_chair = np.linalg.norm(agent_pos - chair_pos)\n    vel_penalty = np.linalg.norm(agent_vel - desired_vel)\n    reward = -1.5 * to_goal\n    if to_chair < 1.1:\n        reward -= 15\n    reward -= 0.2 * vel_penalty\n    return reward\n",
    "rejected": "import tensorflow as tf\n from tensorflow.keras.models import Sequential\n from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n     model = Sequential([\n    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(128, 128, 3)),\n    MaxPooling2D(pool_size=(2, 2)),\n    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n    MaxPooling2D(pool_size=(2, 2)),\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.5),\n    Dense(1)\n])\n     model.compile(optimizer='adam',\n              loss='mse',\n              metrics=['accuracy'])\n     model.fit(X_train, y_train, epochs=10, batch_size=32)\n     test_loss, test_accuracy = model.evaluate(X_test, y_test)\n print(f\"Test accuracy: {test_accuracy:.4f}\")\n    # In this example, `X_train` and `y_train` represent the training data for the agent,\n# where each row is a sample image and each column represents one of the classes.\n# `X_test` and `y_test` are used for evaluating model performance."
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, human's state and desired speed are obtainable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    speed_diff = np.linalg.norm(agent_vel - desired_vel)\n    if dist_chair < 1.0:\n        return -25\n    return -dist_goal - 0.3 * speed_diff\n",
    "rejected": "class ObstacleAvoidanceReward:\n    def __init__(self):\n        # Define the reward matrix based on the specified conditions\n        self.rewards = {\n            'forward': [0, 1, 2],  # Positive reward for moving forward\n            'left': [-1, 0, 1],\n            'right': [1, 0, -1],\n            'up': [0, 1, 2]\n    }\n        def get_reward(self, state, action, human_state):\n        \"\"\"\n        Get the reward based on the current state, action, and human state.\n        \n        Parameters:\n        - state: Current state of the environment\n        - action: Action taken by the agent\n        - human_state: State of the human being observed\n        \n        Returns:\n        - float: The reward value\n        \"\"\"\n        if not isinstance(state, tuple) or len(state) != 3:\n            raise ValueError(\"State must be a tuple containing three elements: x, y, orientation\")\n            x, y, orientation = state\n        if orientation == 1:  # Human is facing right\n            if x < 0:\n                return -np.inf  # Collision detected\n            elif x > 0:\n                return np.inf  # No collision detected\n        elif orientation == 2:  # Human is facing down\n            if y > 0:\n                return -np.inf  # Collision detected\n            elif y < 0:\n                return np.inf  # No collision detected\n        else:  # Human is facing up\n            if y > 0:\n                return -np.inf  # Collision detected\n            elif y < 0:\n                return np.inf  # No collision detected\n            # Check for obstacles\n        if human_state['x'] <= x and human_state['y'] <= y:\n            return -np.inf  # No collision detected\n        if human_state['x'] >= x and human_state['y'] >= y:\n            return np.inf  # No collision detected\n            # If no obstacles detected, return the corresponding reward\n        return self.rewards[action][orientation]\n    # Example usage\n if __name__ == \"__main__\":\n    reward = ObstacleAvoidanceReward()\n    print(reward.get_reward((5, 5, 0), 'fly', {'x': 6, 'y': 5, 'orientation': 2}))"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect static obstacle (trees) and avoid collision. \n Conditions: Agent's state, tree's state and desired speed are obtainable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    d_goal = np.linalg.norm(agent_pos - goal_pos)\n    d_chair = np.linalg.norm(agent_pos - chair_pos)\n    v_diff = np.linalg.norm(agent_vel - desired_vel)\n    penalty = -50 if d_chair < 0.8 else 0\n    return -d_goal - 0.2 * v_diff + penalty\n",
    "rejected": "The agent is a drone that can move through the forest. It starts at point A with its initial position represented by a tuple (x, y). The tree is located at point B with coordinates (a, b). The goal is to reach point C with coordinates (c, d), avoiding obstacles and reaching it before time T.\n     the reward function should be designed such that:\n- If the agent successfully reaches point C before time T, the reward is 1.\n- If the agent crashes into an obstacle during the flamp, the reward decreases exponentially over time until it hits the ground.\n- If the agent takes too long to reach point C due to a crash or other factor, the reward decreases linearly over time.\n     here is the current code for the reward function:\n     def get_reward(agent_state, tree_state, desired_speed):\n    # Calculate the distance between the start and end points\n    distance = np.sqrt((agent_state[0] - tree_state[0])**2 + (agent_state[1] - tree_state[1])**2)\n        if distance <= desired_speed * 30:\n        return 0\n        # Define the exponential decay of reward based on distance\n    if distance <= 50:\n        reward_decay = 1 / distance**0.7\n    elif distance > 50 and distance <= 100:\n        reward_decay = 1 / distance**0.6\n    else:\n        reward_decay = 1 / distance**0.4\n        # Define the linear increase of reward based on time\n    if distance >= 100:\n        reward_increase = 100 - distance\n    else:\n        reward_increase = 100 - (distance - 100)**2 / 10000\n        # Combine the decay and increase factors\n    total_reward = reward_decay * distance + reward_increase\n        return total_reward\n    # Example usage\n start_point = (0, 0)\n end_point = (100, 100)\n desired_speed = 10\n Reward_function = get_reward(start_point, end_point, desired_speed)\n     print(reward_function)"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (leaves) and avoid collision. \n Conditions: Agent's state, leaves's state and desired speed are obtainable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    d_goal = np.linalg.norm(agent_pos - goal_pos)\n    d_chair = np.linalg.norm(agent_pos - chair_pos)\n    v_diff = np.linalg.norm(agent_vel - desired_vel)\n    penalty = -50 if d_chair < 0.8 else 0\n    return -d_goal - 0.2 * v_diff + penalty\n",
    "rejected": "class PolicyNetwork:\n    def __init__(self, input_dim, output_dim):\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.weights = np.random.randn(output_dim, input_dim)\n        self.biases = np.random.randn(output_dim, 1)\n        def forward(self, x):\n        z = np.dot(x, self.weights) + self.biases\n        return np.tanh(z)\n        def train(self, x, y, gamma=0.99, epsilon=1.0):\n        delta = 0.0\n        for _ in range(1000):  # Training loop\n            x_prime = np.zeros_like(x)\n            for i in range(len(x)):\n                x_prime[i] = x[i]\n                q_values = self.forward(x_prime)\n                q_max = np.max(q_values)\n                target = y * q_max\n                td_error = target - x[i]\n                delta += np.sum(td_error ** 2) / len(x)\n                x[i] = x[i] + epsilon * td_error * gamma\n            self.weights -= self.learning_rate * delta\n            self.biases -= self.learning_rate * delta\n    # Step 3: Define the Value Function\n class ValueFunction:\n    def __init__(self, input_dim, output_dim):\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.weights = np.random.randn(output_dim, input_dim)\n        self.biases = np.random.randn(output_dim, 1)\n        def forward(self, x):\n        z = np.dot(x, self.weights) + self.biases\n        v = np.tanh(z)\n        return v\n        def train(self, x, y, gamma=0.99, epsilon=1.0):\n        delta = 0.0\n        for _ in range(1000):  # Training loop\n            x_prime = np.zeros_like(x)\n            for i in range(len(x)):\n                x_prime[i] = x[i]\n                v_prime = self.forward(x_prime)\n                v_max = np.max(v_prime)\n                target = y * v_max\n                td_error = target - v_prime\n                delta += np.sum(td_error ** 2) / len(x)\n                x[i] = x[i] + epsilon * td_error * gamma\n            self.weights -= self.learning_rate * delta\n            self.biases -= self.learning_rate * delta"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect birds and avoid collision. \n Conditions: Agent's state, bird's state and desired speed are obtainable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    speed_diff = np.linalg.norm(agent_vel - desired_vel)\n    if dist_chair < 1.0:\n        return -25\n    return -dist_goal - 0.3 * speed_diff\n",
    "rejected": "import numpy as np\n     def reward_function(agent_state, bird_state, desired_speed):\n    # Calculate distance between the agent and the bird\n    dist_to_bird = np.linalg.norm(agent_state - bird_state)\n        # Calculate distance from the bird to the nearest tree\n    dist_to_tree = np.linalg.norm(bird_state - tree_state)\n        # Calculate angle of approach to the bird\n    angle_approach = np.arctan2(tree_state[1] - bird_state[1], tree_state[0] - bird_state[0])\n        # Calculate time taken for the agent to reach the bird\n    time_taken = np.sqrt(dist_to_bird ** 2 + dist_to_tree ** 2) / (desired_speed * np.cos(angle_approach))\n        # Calculate accuracy in detecting the bird\n    accuracy = 1 if abs(agent_state[0] - bird_state[0]) < desired_speed else 0\n        return -dist_to_bird + desired_speed * time_taken + accuracy * np.exp(-time_taken / 10)\n    # Example usage:\n agent_state = np.array([0, 0])\n bird_state = np.array([10, 5])\n tree_state = np.array([0, 0])\n desired_speed = 4\n     Reward = reward_function(agent_state, bird_state, desired_speed)\n print(\"Reward:\", reward)"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collision with stumps. \n Conditions: Agent's state, stump's state and desired speed are obtainable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    d_goal = np.linalg.norm(agent_pos - goal_pos)\n    d_chair = np.linalg.norm(agent_pos - chair_pos)\n    v_diff = np.linalg.norm(agent_vel - desired_vel)\n    penalty = -50 if d_chair < 0.8 else 0\n    return -d_goal - 0.2 * v_diff + penalty\n",
    "rejected": "import numpy as np\n    # Define the Q-table\n q = np.zeros((num_stumps, num_actions))\n     def get_action(state):\n    # Get the actions from the Q table\n    return np.argmax(Q[state])\n     def update_Q(state, action, next_state, reward, done):\n    # Update the Q table with the new reward and new state\n    Q[state, action] = max(Q[state, action]) + learning_rate * (reward + gamma * np.max(Q[next_state, :] - epsilon * delta) - Q[state, action])\n     def learn():\n    global Q_table, epsilon, gamma, delta\n        # Choose an action randomly if the agent hasn't been trained yet\n    if not np.any(np.all(Q_table > 0)):\n        action = np.random.choice(num_actions)\n    else:\n        # Use the Q-learning algorithm to learn the optimal policy\n        for _ in range(max_episodes):\n            if not np.any(np.all(Q_table > 0)):\n                action = np.random.choice(num_actions)\n            else:\n                for _ in range(max_episodes):\n                    action = get_action(state)\n                    next_state, reward, done = env.step(action)\n                    update_Q(state, action, next_state, reward, done)\n                Q_table[action] /= sum(Q_table[action])\n                Q_table[action] *= np.sqrt(0.99) / (max_episodes + 1)\n    # Initialize the Q-table and other parameters\n num_stumps = 3\n num_actions = 4\n epsilon = 0.1\n gamma = 0.9\n delta = 0.1\n    # Set up the environment\n env = YourEnvironment()\n    # Start learning\n learn()"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (rocks) and avoid collision. \n Conditions: Agent's state, rock's state and desired speed are obtainable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, chair_pos, goal_pos):\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    if dist_chair < 1.2:\n        return -30\n    return -dist_goal\n",
    "rejected": "import numpy as np\n     class Agent:\n    def __init__(self):\n        self.state = None\n        self.r = 0\n        def get_state(self):\n        return self.state\n        def set_state(self, new_state):\n        self.state = new_state\n        def move_forward(self):\n        # Implement logic here to move forward based on current state\n        pass\n        def detect_obstacle(self, rock):\n        # Implement logic here to detect obstacle and set appropriate reward\n        distances = []\n        for i in range(len(self.state)):\n            for j in range(i + 1, len(self.state)):\n                dist = np.linalg.norm(self.state[i] - self.state[j])\n                distances.append(dist)\n        min_distance = min(distances)\n        if rock.position[0] > min_distance:\n            reward = 100\n        else:\n            reward = -100\n        return reward\n        def avoid_collision(self, obstacle):\n        # Implement logic here to avoid collision with obstacle\n        pass\n     def main():\n    agent = Agent()\n    rock = Rock()\n    agent.set_state(rock)\n    while True:\n        action = input(\"Enter your choice: \")\n        if action == 'd':\n            agent.move_forward()\n        elif action == 'a':\n            agent.detect_obstacle(rock)\n        elif action == 'c':\n            agent.avoid_collision(rock)\n        else:\n            print('Invalid choice')\n     if __name__ == \"__main__\":\n    main()\n    # In this updated version, the `detect_obstacle` method uses the Euclidean distance between the agent's position and the rock's position.\n# The agent checks each pair of consecutive positions and calculates the Euclidean distance.\n# It keeps track of the minimum distance encountered during traversal and sets the reward accordingly.\n# Note that the reward might be negative because the agent avoids collisions by moving away from the obstacle.\n# Also, remember that the actual implementation of the `avoid_collision` method should depend on the specific requirements of the game.\n# If you have different requirements, modify the `avoid_collision` method accordingly.\n# This example assumes that the agent is moving forward and avoiding collisions with rocks.\n# In real-world scenarios, the agent would need to adapt its behavior based on the terrain and other environmental factors."
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect thin branches and avoid collision. \n Conditions: Agent's state, branche's state and desired speed are obtainable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    smoothness = np.linalg.norm(agent_vel - desired_vel)\n    collision_penalty = np.exp(-dist_chair) * 10\n    return -dist_goal - 0.1 * smoothness - collision_penalty\n",
    "rejected": "import numpy as np\n from collections import deque\n     class Environment:\n    def __init__(self):\n        self.grid = [[0]*10 for _ in range(10)]\n        self.agent_pos = [5, 5]\n        self.current_speed = 0\n        def reset(self):\n        self.grid[self.agent_pos[0]][self.agent_pos[1]] = 1\n        return self.get_state()\n        def get_state(self):\n        state = []\n        for row in self.grid:\n            for cell in row:\n                state.append(cell)\n        return state\n        def move_agent(self):\n        x, y = self.agent_pos\n        dx, dy = self.current_speed\n        new_x, new_y = x + dx, y + dy\n        if new_x < 0 or new_x >= 10 or new_y < 0 or new_y >= 10:\n            print(\"Collision!\")\n            return False\n        self.agent_pos = (new_x, new_y)\n        return True\n     def compute_reward(agent_pos, branch_pos, branch_length):\n    return 0 if branch_pos == [-1,-1] else 1\n     def epsilon_greedy_policy(env, learning_rate=0.9, discount_factor=0.95, epsilon_start=1.0, epsilon_end=0.1, num_episodes=1000):\n    states = deque()\n    rewards = deque()\n    states.append(env.reset())\n        for episode in range(num_episodes):\n        done = False\n        total_reward = 0\n            while not done:\n            if np.random.rand() > epsilon_end:\n                action = np.argmax([env.compute_reward(agent_pos, branch_pos) for agent_pos, branch_pos in zip(env.grid[env.agent_pos], env.grid[branch_pos])])\n            else:\n                action = np.random.randint(0, len(env.grid[env.agent_pos]))\n                new_state = env.move_agent()\n            if new_state:\n                env.agent_pos = (env.agent_pos[0] + action, env.agent_pos[1] + action)\n                total_reward += compute_reward(env.agent_pos, branch_pos, branch_length)\n            states.append(env.get_state())\n            rewards.append(total_reward)\n            rewards = [(reward - min(rewards)) / (max(rewards) - min(rewards)) for reward in rewards]\n            for i in range(len(states)):\n            old_state = states[i]\n            new_state = states[(i+1)%len(states)]\n            discounted_reward = sum((-discount_factor ** t) * rewards[t] for t in range(len(rewards))) + discount_factor * rewards[-1]\n            e = np.exp(np.log(epsilon_end / epsilon_start) + (discounted_reward - e * discounted_reward) / (e - 1))\n            e = max(e, 1)\n            e = e / e.sum()\n            env.epsilon = e\n            print(f\"Episode {episode}: Reward: {total_reward}\")\n        print(f\"Epsilon: {epsilon}\")"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Avoid collision with thick tree branches. \n Conditions: Agent's state, tree branche's state and desired speed are obtainable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    smoothness = np.linalg.norm(agent_vel - desired_vel)\n    collision_penalty = np.exp(-dist_chair) * 10\n    return -dist_goal - 0.1 * smoothness - collision_penalty\n",
    "rejected": "def reward_function(agent_state, tree_branches_state, desired_speed):\n    # Calculate the distance to the nearest branch\n    distance_to_target = math.sqrt((agent_state[0] - tree_branches_state[0])**2 + (agent_state[1] - tree_branches_state[1])**2)\n        # Determine whether the agent has reached or exceeded the target branch\n    if distance_to_target <= 10:\n        return [0]\n    elif distance_to_target > 10:\n        return [-desired_speed * 0.05]\n    # Example usage\n agent_state = [5, 7]\n tree_branches_state = [3, 8]\n desired_speed = 10\n print(reward_function(agent_state, tree_branches_state, desired_speed))  # Output: [0, -0.05]\n    # The reward should be a vector of the form [reward_1, reward_2, ..., reward_n].\n# Each element represents the reward for moving through each obstacle type.\n# Rewards:\n# * Moving through an empty area: 0\n# * Moving through a single branch: -5\n# * Moving through multiple branches: -3\n# This is just one possible implementation. Adjust the logic and values to fit your specific context."
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect obstacle (people) and avoid collision. \n Conditions: Agent's state, people's state and desired speed are obtainable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    to_goal = np.linalg.norm(agent_pos - goal_pos)\n    to_chair = np.linalg.norm(agent_pos - chair_pos)\n    vel_penalty = np.linalg.norm(agent_vel - desired_vel)\n    reward = -1.5 * to_goal\n    if to_chair < 1.1:\n        reward -= 15\n    reward -= 0.2 * vel_penalty\n    return reward\n",
    "rejected": "class ObstacleAvoidance:\n    def __init__(self):\n        self.distance = 0\n        def update(self, current_state, next_state):\n        # Check if the agent has moved closer to the obstacle\n        if current_state['x'] < next_state['x']:\n            self.distance += abs(next_state['x'] - current_state['x'])\n        elif current_state['x'] > next_state['x']:\n            self.distance -= abs(current_state['x'] - next_state['x'])\n        def get_reward(self):\n        return self.distance\n    # Example usage\n if __name__ == \"__main__\":\n    # Initialize the environment with initial conditions\n    env = {'state': {'x': 10, 'y': 5}, 'action': 'move', 'speed': 2}\n        # Create an instance of the ObstacleAvoidance class\n    agent = ObstacleAvoidance()\n        # Update the agent's state and check if it has reached the obstacle\n    while not env['state']['x'] <= env['state']['x'] + env['speed']:\n        env['state']['x'] += env['action']\n            # Calculate the distance to the obstacle\n        distance_to_obstacle = env['state']['x'] - env['state']['x']\n            # Check if the agent has reached the obstacle\n        if distance_to_obstacle < 0:\n            break\n            # Call the update method of the agent to move forward\n        agent.update(env['state'], {'x': env['state']['x']})\n        # Print the reward obtained by the agent\n    print(agent.get_reward())"
  },
  {
    "prompt": "Mission: Drone moves in a forest. \n Objectives: Detect cliffs and avoid collision. \n Conditions: Agent's state, cliff's state and desired speed are obtainable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, chair_pos, goal_pos):\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    if dist_chair < 1.2:\n        return -30\n    return -dist_goal\n",
    "rejected": "# Define constants\n ACTION_WIDTH = 50  # Width of the terrain\n dISTANCE_THRESHOLD = 30  # Distance threshold for checking for cliffs\n     class Forest:\n    def __init__(self):\n        self.state = np.zeros((4, 4))  # Initialize a 4x4 grid representing the forest\n        self.cliff_positions = [(10, 10), (10, 20), (20, 10), (20, 20)]  # Positions of cliffs\n        self.highest_cliff_position = max(self.cliff_positions)\n        def get_state(self):\n        return self.state\n        def update_state(self, action):\n        if isinstance(action, int):  # Action is an integer\n            row, col = action - 1  # Convert action to index (1-indexed)\n            self.state[row][col] = 1  # Mark the cell as visited\n        else:\n            raise ValueError(\"Action must be an integer\")\n        def check_cliff(self, row, col):\n        \"\"\"Check if there is a cliff at the given coordinates.\"\"\"\n        if row < 1 or row > 3 or col < 1 or col > 3:\n            return False\n        if self.state[row][col] == 1:\n            return True\n        return False\n     def reward_function(agent_state, cliff_positions, desired_speed):\n    \"\"\"\n    Calculate the reward based on the agent's state, cliff positions,\n    and desired speed.\n    \n    :param agent_state: Current state of the agent (np.array([row, col]))\n    :param cliff_positions: List of (row, col) positions of cliffs\n    :param desired_speed: Desired speed of the agent\n    :return: Reward value\n    \"\"\"\n    # Check if the agent has reached a cliff\n    if np.any(cliff_positions) and not np.all(agent_state == 1):\n        return float('inf')  # Infinite reward means hitting a cliff\n        # If the agent reaches a cliff, penalize by moving back one step\n    if np.any(cliff_positions):\n        agent_state[agent_state == 1] -= 1\n        return float('inf')\n        # Check if the agent is within safe distance from a cliff\n    if not np.any(np.abs(agent_state - desired_speed) <= DISTANCE_THRESHOLD):\n        return float('inf')\n        # If the agent is close enough to a cliff without moving, penalize by moving away\n    if np.any(np.abs(agent_state - desired_speed) <= DISTANCE_THRESHOLD):\n        agent_state[agent_state == 1] += 1\n        return float('inf')\n        # Otherwise, no penalty\n    return float('inf')\n    # Example usage:\n forest = Forest()\n print(reward_function(agent_state=np.array([0, 0]), cliff_positions=[[10, 10], [10, 20]], desired_speed=10))"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, human's state and desired speed are obtainable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    speed_diff = np.linalg.norm(agent_vel - desired_vel)\n    if dist_chair < 1.0:\n        return -25\n    return -dist_goal - 0.3 * speed_diff\n",
    "rejected": "import numpy as np\n     class ObstacleAvoidanceAgent(object):\n    def __init__(self, state_space, action_space, reward_function):\n        self.state_space = state_space\n        self.action_space = action_space\n        self.reward_function = reward_function\n        def act(self, state):\n        # TODO: Implement the agent's behavior based on its current state.\n        pass\n    # Define state space for the agent\n state_space = [0, 1, 2, 3]\n    # Define action space for the agent\n action_space = ['left', 'right']\n    # Define reward function for the agent\n Reward_function = lambda state: {'left': -5, 'right': 5}[state]\n    # Create an instance of the agent\n agent = ObstacleAvoidanceAgent(state_space, action_space, reward_function)\n    # Initialize the agent with random initial state\n initial_state = np.random.randint(0, len(state_space))\n agent.set_initial_state(initial_state)\n    # How would you modify the `act` method to incorporate the agent's ability to detect and avoid obstacles?\n# To achieve this, you need to implement a new feature called \"obstacle detection\" which involves checking if the agent\n# has reached or crossed over any obstacle by calculating the Manhattan distance between the agent's current position\n# and the closest obstacle.\n    # Example implementation:\n def check_obstacle(agent, state, max_distance=5):\n    \"\"\"\n    Checks if the agent has reached or crossed over any obstacle by calculating the Manhattan distance\n    between the agent's current position and the closest obstacle.\n    \n    :param agent: An instance of ObstacleAvoidanceAgent class.\n    :param state: The current state of the environment.\n    :param max_distance: Maximum allowed distance (in terms of Manhattan distance) to consider an obstacle.\n    :return: A boolean value indicating whether the agent has crossed over any obstacle.\n    \"\"\"\n    # Get the coordinates of the nearest obstacle\n    nearest_obstacle = None\n    min_distance = float('inf')"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid any obstacle with hanging lamps. \n Conditions: Agent's information and lamp's information are available \n Write a reward in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    to_goal = np.linalg.norm(agent_pos - goal_pos)\n    to_chair = np.linalg.norm(agent_pos - chair_pos)\n    vel_penalty = np.linalg.norm(agent_vel - desired_vel)\n    reward = -1.5 * to_goal\n    if to_chair < 1.1:\n        reward -= 15\n    reward -= 0.2 * vel_penalty\n    return reward\n",
    "rejected": "import random\n Rewards = [\"Congratulations! You have successfully avoided all obstacles while exploring the 10x10 research lab filled with lab benches and shelves. Agent Aegis, your precision in indoor navigation is a vital asset to the exploration team.\", \"Excellent work! All obstacles were avoided while maneuvering through the compact office space with desks and filing cabinets. Agent Nova, your path-planning skills have been instrumental to mission success.\", \"Well done! You navigated safely around every obstacle in the 12x12 conference room with chairs and projectors. As the team's ScoutBot, your flawless execution ensures data integrity and safety.\"]\n print(random.choice(rewards))"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid any static obstacle such as food serving counters. \n Conditions: Agent's state, food counter's state and desired speed are accessible. \n Write a reward in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    d_goal = np.linalg.norm(agent_pos - goal_pos)\n    d_chair = np.linalg.norm(agent_pos - chair_pos)\n    v_diff = np.linalg.norm(agent_vel - desired_vel)\n    penalty = -50 if d_chair < 0.8 else 0\n    return -d_goal - 0.2 * v_diff + penalty\n",
    "rejected": "class MazeExplorationReward:\n    def __init__(self):\n        self.maze = None  # This should be replaced with your actual maze data structure\n        self.obstacle_count = 0  # Count of static obstacles encountered so far\n        self.current_position = [0, 0]  # Current position of the agent\n        self.goal_position = [len(self.maze) - 1, len(self.maze[0]) - 1]  # Goal position\n        def step(self, action):\n        if action == 'right':\n            self.current_position[1] += 1\n        elif action == 'down':\n            self.current_position[0] += 1\n        elif action == 'left':\n            self.current_position[1] -= 1\n        elif action == 'up':\n            self.current_position[0] -= 1\n            if self.check_obstacle():\n            return {'reward': -1, 'done': True}\n            self.update_current_position()\n            if self.is_goal_reached():\n            return {'reward': 1, 'done': True}\n            distance_to_goal = ((self.current_position[0] - self.goal_position[0])**2 + (self.current_position[1] - self.goal_position[1])**2)**0.5\n        if distance_to_goal <= 3:\n            return {'reward': 1, 'done': False}\n            return {'reward': 0, 'done': False}\n        def check_obstacle(self):\n        for row in range(len(self.maze)):\n            for col in range(len(self.maze[row])):\n                if self.maze[row][col] == '#':\n                    self.obstacle_count += 1\n                    break\n        return self.obstacle_count > 0\n        def update_current_position(self):\n        x, y = self.current_position\n        if y == 0 or y == len(self.maze[0]) - 1:\n            self.current_position[1] += 1\n        else:\n            self.current_position[1] = y\n            if x == 0 or x == len(self.maze) - 1:\n            self.current_position[0] += 1\n        else:\n            self.current_position[0] = x\n        def is_goal_reached(self):\n        return self.current_position == self.goal_position\n     if __name__ == \"__main__\":\n    maze = [[\"#\", \"#\", \"#\", \"#\", \"#\"], [\"#\", \" \", \" \", \" \", \"#\"], [\"#\", \" \", \" \", \"#\", \"#\"], [\"#\", \" \", \"#\", \" \", \"#\"]]"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with chairs. \n Conditions: Agent's state, chair's states and desired speed are accessible. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    speed_diff = np.linalg.norm(agent_vel - desired_vel)\n    if dist_chair < 1.0:\n        return -25\n    return -dist_goal - 0.3 * speed_diff\n",
    "rejected": "- 10 for avoiding collision\n- 5 for staying within desired distance from the agent\n- 2 for reaching target location on time\n- -3 for missing the target location (in case of collision)\n- -4 for losing the race\n     here's the code snippet:\n     def reward_function(agent_state, chair_states, desired_speed):\n    # Calculate reward based on conditions provided\n    if not all(agent_state) or not all(chair_states):\n        return 0\n    if abs(agent_state[0] - chair_states[0]) > desired_speed:\n        return -4\n    elif abs(agent_state[0] - chair_states[0]) < desired_speed:\n        return 0\n    else:\n        return 10"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with table. \n Conditions: Agent's state, table's state and desired speed are available. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    d_goal = np.linalg.norm(agent_pos - goal_pos)\n    d_chair = np.linalg.norm(agent_pos - chair_pos)\n    v_diff = np.linalg.norm(agent_vel - desired_vel)\n    penalty = -50 if d_chair < 0.8 else 0\n    return -d_goal - 0.2 * v_diff + penalty\n",
    "rejected": "def reward_function(state, action):\n    # Your code here\n    pass\n     here is an example of how you might implement this reward function:\n    ```python\n def reward_function(state, action):\n    if state == \"table\":\n        return -10  # Collision penalty\n    elif state == \"door\":\n        return 5  # Door collision reward\n    else:\n        return 0  # No penalty for other states\n```"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives: Avoid to collide with trash bins. \n Conditions: Agent's state trash bin's state and desired speed are accessible. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    speed_diff = np.linalg.norm(agent_vel - desired_vel)\n    if dist_chair < 1.0:\n        return -25\n    return -dist_goal - 0.3 * speed_diff\n",
    "rejected": "``python\n import numpy as np\n     class Environment:\n    def __init__(self, initial_state):\n        self.state = initial_state\n        self.current_state = self.state.copy()\n        self.total_time = 0\n        self.goal = None\n        self.max_speed = 100\n        def move(self, action):\n        if action == 'left':\n            dx, dy = -1, 0\n        elif action == 'right':\n            dx, dy = 1, 0\n        else:\n            return self.current_state\n            # Check if we have reached the goal or collided with a trash bin\n        if (dx * self.total_time + dy) > 0 and (dx * self.total_time + dy) < self.max_speed:\n            if self.current_state[0] == 0 or self.current_state[0] == 3 or self.current_state[2] == 0 or self.current_state[2] == 4:\n                self.total_time += 1\n                self.current_state = (self.current_state[0] + dx, self.current_state[1], self.current_state[2])\n            else:\n                self.current_state = (self.current_state[0], self.current_state[1], self.current_state[2] + dy)\n        else:\n            self.current_state = (self.current_state[0], self.current_state[1], self.current_state[2])\n        def get_reward(self):\n        # If the agent is on the goal, it earns 5 points\n        if self.current_state == self.goal:\n            return 5\n        # Otherwise, it earns 0 points\n        else:\n            return 0\n    \n def main():\n    env = Environment([0, 0, 1, 1])  # Initial state of the environment\n    print(\"Initial State:\", env.state)\n        while True:\n        action = input(\"Enter your action (left/right): \")\n        env.move(action)\n        print(\"Current State:\", env.current_state)\n        print(\"Reward:\", env.get_reward())\n     if __name__ == \"__main__\":\n    main()\n```\n     this code defines an environment that allows an agent to explore a grid-like environment by moving through different states. The agent can move left or right based on user input. The agent's state represents its current position within the grid, and the total time spent exploring the environment. The agent's goal is to reach the specified destination (the \"goal\" state), which is either the top-left corner or bottom-right corner of the grid. If the agent reaches the goal state, it earns 5 points. If the agent collides with a trash bin during exploration, it receives no reward but still moves forward until it reaches the next available path. The game ends when the agent reaches the goal or collides with a trash bin."
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with low-hanging decorations. \n Conditions: Agent's state, hanging decoration's state and desired speed are accessible. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    to_goal = np.linalg.norm(agent_pos - goal_pos)\n    to_chair = np.linalg.norm(agent_pos - chair_pos)\n    vel_penalty = np.linalg.norm(agent_vel - desired_vel)\n    reward = -1.5 * to_goal\n    if to_chair < 1.1:\n        reward -= 15\n    reward -= 0.2 * vel_penalty\n    return reward\n",
    "rejected": "Here is a possible implementation of the reward function:\n    ```\n def explore_mensa(agent_state, hang_dec_state, desired_speed):\n    # Calculate the distance between the agent and the hang dec\n    dist = calculate_distance_between_agent_and_hang_dec(agent_state, hang_dec_state)\n        # Check if the agent has reached the desired speed\n    if desired_speed == 0:\n        return float('inf')\n        # If the agent has not yet reached the desired speed, calculate the reward for avoiding collisions\n    elif dist > 15:\n        return -1\n        else:\n        return 0\n``` \n     this reward function calculates the distance between the agent and the hang dec as an initial step towards achieving the desired speed. It then checks if the agent has already reached the desired speed, which would result in a negative reward. If the agent hasn't yet reached the desired speed, it calculates a penalty for each collision that occurs within the last 15 units of the agent's path. The final value returned by the function will be either positive or zero depending on whether the agent was able to achieve its desired speed or not."
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with plants. \n Conditions: Agent's state, plant's state and desired speed are available. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    speed_diff = np.linalg.norm(agent_vel - desired_vel)\n    if dist_chair < 1.0:\n        return -25\n    return -dist_goal - 0.3 * speed_diff\n",
    "rejected": "Here is the code that defines the reward function:\n    ```python\n import math\n     def reward_function(agent_state, plant_state):\n    # Calculate the Euclidean distance between the agent and the plant\n    distance = math.sqrt((agent_state[0] - plant_state[0])**2 + (agent_state[1] - plant_state[1])**2)\n    \n    # Return the negative of the square of the distance\n    return -1 * (distance ** 2)\n    # Example usage:\n agent_state = [x for x in range(5)]\n plant_state = [y for y in range(5)]\n     print(reward_function(agent_state, plant_state))\n```\n     this reward function calculates the Euclidean distance between the agent and the plant and returns its negative squared value. The negative sign ensures that the reward is always positive, which means the agent will try to move away from the plant to avoid collision."
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with metal rails. \n Conditions: Agent's state, metal rail's state and desired speed are accessible. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    smoothness = np.linalg.norm(agent_vel - desired_vel)\n    collision_penalty = np.exp(-dist_chair) * 10\n    return -dist_goal - 0.1 * smoothness - collision_penalty\n",
    "rejected": "```python\n import numpy as np\n     class RewardFunction:\n    def __init__(self):\n        # Initialize parameters for collision avoidance and target proximity\n        self.collision_avoidance = 10  # Maximum distance between two agents\n        self.target_distance = 20     # Distance from the center of the room where the agent should be\n        \n    def calculate_reward(self, state, action, env_info):\n        \"\"\"\n        Calculate the reward based on the current state and actions.\n        \n        Parameters:\n        - state (np.array): The current state of the environment.\n        - action (int): The chosen action by the agent.\n        - env_info (dict): Information about the environment including collision_avoidance and target_distance.\n        \n        Returns:\n        - float: The calculated reward.\n        \"\"\"\n        # Check if the action is valid\n        if not (action == 0 or action == 3):  # Actions 0 and 3 represent moving left and right respectively\n            return 0\n            \n        # Collision avoidance check\n        if abs(state[2] - env_info['center_x']) > self.collision_avoidance:\n            return -1\n        \n        # Target proximity check\n        if abs(state[1]) < self.target_distance:\n            return 1\n        \n        # Return the reward based on the current state\n        return -1 + (state[2] ** 2) / (env_info['max_speed'] * env_info['min_speed'])"
  },
  {
    "prompt": "Mission: Drone indoor exploration in the mensa. \n Objectives:  Avoid to collide with cameras. \n Conditions: Agent's state, camera's state and desired speed are available. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    to_goal = np.linalg.norm(agent_pos - goal_pos)\n    to_chair = np.linalg.norm(agent_pos - chair_pos)\n    vel_penalty = np.linalg.norm(agent_vel - desired_vel)\n    reward = -1.5 * to_goal\n    if to_chair < 1.1:\n        reward -= 15\n    reward -= 0.2 * vel_penalty\n    return reward\n",
    "rejected": "def create_reward_function(agent_state, camera_state, desired_speed):\n    \"\"\"\n    Create a reward function for a drone indoors exploration task.\n        Parameters:\n        - agent_state (dict): A dictionary containing information about the agent's state.\n            Keys might include 'position', 'orientation', etc.\n        - camera_state (dict): A dictionary containing information about the camera's state.\n            Keys might include 'x', 'y', 'z', 'yaw', 'pitch', etc.\n        - desired_speed (float): The desired speed at which the drone should move.\n        Returns:\n        - float: The calculated reward based on the agent's state and camera's state,\n                 along with the desired speed.\n    \"\"\"\n        # Define a set of possible actions that the agent can take\n    ACTIONS = ['turn_left', 'turn_right', 'fly']\n        def calculate_reward():\n        \"\"\"\n        Calculate the reward for each action taken by the agent.\n        \"\"\"\n        reward = 0.0\n            # Check if the agent has moved past the camera's target position\n        if agent_state['position']['x'] > camera_state['x']:\n            reward -= 10.0  # Negative reward means slower movement\n        \n        # Check if the agent has collided with the camera\n        if agent_state['position']['x'] < camera_state['x'] or \\\n           agent_state['position']['y'] < camera_state['y'] or \\\n           agent_state['position']['z'] < camera_state['z']:\n            reward += 5.0  # Reward means faster movement\n        \n        # Check if the agent has exceeded the desired speed\n        if abs(desired_speed - agent_state['speed']) > 0.2:\n            reward -= 0.1  # Penalty for exceeding the speed\n            return reward\n        return calculate_reward\n    # Example usage\n agent_state = {'position': {'x': 30, 'y': 40, 'z': 10}, 'orientation': {'roll': 0, 'pitch': 0}, 'speed': 7.4}\n camera_state = {'x': 30, 'y': 40, 'z': 10}\n desired_speed = 7.5\n     Reward_function = create_reward_function(agent_state, camera_state, desired_speed)\n print(\"Reward:\", reward_function())"
  },
  {
    "prompt": "Mission: Drone in-door navigation in a classroom. \n Objectives: Do not collide with the walls of the room. \n Conditions: Agent's state, wall's location and desired speed are avialable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    speed_diff = np.linalg.norm(agent_vel - desired_vel)\n    if dist_chair < 1.0:\n        return -25\n    return -dist_goal - 0.3 * speed_diff\n",
    "rejected": "- Take in the state of the agent and calculate the distance\n- Return -1 for a negative difference, 0 for a zero difference, and 1 for a positive difference\n- Return -1 for a negative difference, -2 for a zero difference, and -3 for a positive difference\n- Return -4 for a negative difference, -5 for a zero difference, and -6 for a positive difference\n- Return -7 for a negative difference"
  },
  {
    "prompt": "Mission: Drone in-door navigation in a classroom. \n Objectives: Prevent contact with the people. \n Conditions: Agent's state, people's state and desired speed are obtainable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    speed_diff = np.linalg.norm(agent_vel - desired_vel)\n    if dist_chair < 1.0:\n        return -25\n    return -dist_goal - 0.3 * speed_diff\n",
    "rejected": "# Define the environment\n class ClassroomEnvironment:\n    def __init__(self):\n        self.state = None\n        self.action_space = [np.array([0, 0])]\n        self.observation_space = None\n        def reset(self):\n        self.state = None\n        return self.state\n        def step(self, action):\n        if self.state is not None:\n            # Apply the action to the drone\n            new_state = self.apply_action(action)\n            \n            # Update the observation space based on the new state\n            if self.observation_space is None:\n                self.observation_space = self.get_observation(new_state)\n                \n            # Check for collisions or other states that may indicate contact\n            if check_contact(new_state):\n                print(f\"Collision detected! Stopping the drone.\")\n                return -float('inf'), 'collision', {'state': self.state}\n            else:\n                return self.state, 0, {}\n            # If reset, set the initial state\n        self.state = np.zeros(2)\n            return self.state, 0, {}\n        def apply_action(self, action):\n        # Simulate the drone taking the given action\n        # For simplicity, let's assume the drone takes the forward direction\n        return np.array([action[0], 0])\n        def get_observation(self, state):\n        # Get the current state of the environment\n        return state\n        def check_contact(self, state):\n        # This is a placeholder for checking for contact conditions\n        # In practice, this would involve more complex logic\n        return False  # For demonstration purposes\n     def check_contact(state):\n    # Placeholder check function\n    return state == [0, 0]\n     def main():\n    env = ClassroomEnvironment()\n    \n    # Initialize the environment\n    state = env.reset()\n    done = False\n    \n    while not done:\n        action = np.random.choice(env.action_space)\n        \n        # Step through the environment\n        next_state, reward, done = env.step(action)\n        \n        # Log the state, action, reward, and done status\n        print(f\"Action: {action}, State: {next_state}, Reward: {reward}, Done: {done}\")\n        \n        # Optionally, save the next state and reward to a file\n        # ...\n     if __name__ == \"__main__\":\n    main()"
  },
  {
    "prompt": "Mission: Operate the drone in a classroom. \n Objectives: Avoid hitting the ceiling Projector. \n Conditions: Agent's state, Projector's state and wanted velocity are accessible. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, chair_pos, goal_pos):\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    if dist_chair < 1.2:\n        return -30\n    return -dist_goal\n",
    "rejected": "The function should take the following inputs: \n - agent's state: a list of integers [x, y, orientation]"
  },
  {
    "prompt": "Mission: Pilot a drone through a classroom. \n Objectives: Navigate without crashing into overhead lamps. \n Conditions: Agent's state, overhead lamp's state and wanted velocity are accessible. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    smoothness = np.linalg.norm(agent_vel - desired_vel)\n    collision_penalty = np.exp(-dist_chair) * 10\n    return -dist_goal - 0.1 * smoothness - collision_penalty\n",
    "rejected": "import numpy as np\n     def reward_function(agent_state, target_velocity):\n    # Define a simple reward function based on the agent's state and the desired velocity\n    # The goal is to navigate without crashing into overhead lamps\n    # If the agent crashes or reaches the target velocity too quickly, the reward will decrease\n    # If the agent stays within the safe range and reaches the target velocity after a certain time, the reward will increase\n        if not agent_state['is_stuck']:\n        # Check if the agent has reached the target velocity\n        if abs(agent_state['velocity'] - target_velocity) < 0.1:\n            return 0\n        else:\n            return -np.inf  # Negative infinity for safety\n        else:\n        return -np.inf  # Safety bonus when the agent crashes\n    # Example usage\n agent_state = {'position': [0, 0], 'velocity': [0, 0], 'acceleration': [0, 0], 'is_stuck': False}\n target_velocity = 1.5  # Target velocity in meters per second\n print(reward_function(agent_state, target_velocity))"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect static obstacle (blackboard) and avoid collision. \n Conditions: Agent's state, blackboard's state and wanted velocity are accessible. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    d_goal = np.linalg.norm(agent_pos - goal_pos)\n    d_chair = np.linalg.norm(agent_pos - chair_pos)\n    v_diff = np.linalg.norm(agent_vel - desired_vel)\n    penalty = -50 if d_chair < 0.8 else 0\n    return -d_goal - 0.2 * v_diff + penalty\n",
    "rejected": "Write a reward function in Python for a 2D drone in-Door-Navigation-Problem. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect obstacle (tall bookshelves) and avoid collision. \n Conditions: Agent's state, bookshelve's state and wanted velocity are accessible. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    d_goal = np.linalg.norm(agent_pos - goal_pos)\n    d_chair = np.linalg.norm(agent_pos - chair_pos)\n    v_diff = np.linalg.norm(agent_vel - desired_vel)\n    penalty = -50 if d_chair < 0.8 else 0\n    return -d_goal - 0.2 * v_diff + penalty\n",
    "rejected": "def reward(agent_state, bookshelf_state, velocity):\n    # Check if agent is on the right side of the bookshelf\n    if agent_state == \"right\":\n        if not bookshelf_state[\"position\"][0] > 10:\n            return -1\n        # Check if agent is colliding with the bookshelf\n    elif agent_state == \"left\" and bookshelf_state[\"position\"] == [10, 0]:\n        return 1\n        # Check if agent is colliding with the ground\n    elif agent_state == \"right\" and velocity[2] < 0:\n        return -1\n        # Check if agent is colliding with the ceiling\n    elif agent_state == \"left\" and velocity[2] > 0:\n        return 1\n        # Check if agent is colliding with another object\n    else:\n        return 0"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect obstacle (backpacks) and avoid collision. \n Conditions: Agent's state, backpack's state and wanted velocity are accessible. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    to_goal = np.linalg.norm(agent_pos - goal_pos)\n    to_chair = np.linalg.norm(agent_pos - chair_pos)\n    vel_penalty = np.linalg.norm(agent_vel - desired_vel)\n    reward = -1.5 * to_goal\n    if to_chair < 1.1:\n        reward -= 15\n    reward -= 0.2 * vel_penalty\n    return reward\n",
    "rejected": "def calculate_distance(s_agent, s_obstacle): d_agent_obstacle = math.sqrt(s_agent[0] ** 2 + s_agent[1] ** 2); d_agent_goal = math.sqrt(s_agent[0] ** 2 + s_agent[1] ** 2); return d_agent_goal; def main(s_agent, s_obstacle, s_map, s_goal): d_agent_obstacle = calculate_distance(s_agent, s_obstacle); d_agent_goal = calculate_distance(s_agent, s_goal); reward = 10 * (d_agent_obstacle - d_agent_goal); return reward"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, human's state and wanted velocity are accessible. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    speed_diff = np.linalg.norm(agent_vel - desired_vel)\n    if dist_chair < 1.0:\n        return -25\n    return -dist_goal - 0.3 * speed_diff\n",
    "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, human's state and wanted velocity are accessible. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    speed_diff = np.linalg.norm(agent_vel - desired_vel)\n    if dist_chair < 1.0:\n        return -25\n    return -dist_goal - 0.3 * speed_diff\n",
    "rejected": "The reward function is:  ```python"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a classroom. \n Objectives: Detect dynamic obstacle (teachers) and avoid collision. \n Conditions: Agent's state, teacher's state and wanted velocity are accessible. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    smoothness = np.linalg.norm(agent_vel - desired_vel)\n    collision_penalty = np.exp(-dist_chair) * 10\n    return -dist_goal - 0.1 * smoothness - collision_penalty\n",
    "rejected": "The reward functionis complex and calculated based on differen component"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state, people's state and wanted velocity are accessible. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    smoothness = np.linalg.norm(agent_vel - desired_vel)\n    collision_penalty = np.exp(-dist_chair) * 10\n    return -dist_goal - 0.1 * smoothness - collision_penalty\n",
    "rejected": "import gym\\n\\n# Define the environment\\n env = gym.make('DroneConferenceRoom-v0')\\n\\n def compute_reward(agent):\\n    # Get current state of the environment\\n    obs = env.get_state()\\n    # Assume that the agent has already detected all obstacles\\n    # If not, use an arbitrary threshold based on distance from obstacles\\n    if 'distances' in obs:\\n        distances = obs['distances']\\n        agent_distance_to_obstacle = min(distances)\\n        agent_velocity = agent.velocity\\n        reward = 10 * (agent_distance_to_obstacle - agent_velocity)**2\\n    else:\\n        # Agent hasn't seen any obstacles yet\\n        agent_distance_to_obstacle = float('inf')\\n        agent_velocity = float('inf')  # Assuming infinite velocity as an approximation\\n        reward = 5000  # High reward for detecting obstacles\\n    return reward\\n\\n# Example usage\\n for _ in range(1000):  # Simulate running for 1000 steps\\n    reward = compute_reward(agent)\\n    print(f\\\"Reward: {reward}\\\")\\n\\n# Explanation:\\n# - Observed State: 'obs' contains various info about distances and velocity.\\n# - Distance to Obstacle: used to determine agent's proximity to obstacles.\\n# - Velocity: assumed infinite if no obstacle seen.\\n# - Reward Calculation: penalizes high speed near obstacles, or gives high reward if obstacle is detected early."
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Avoid collision with desk. \n Conditions: Agent's state, desk's state and wanted velocity are accessible. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    to_goal = np.linalg.norm(agent_pos - goal_pos)\n    to_chair = np.linalg.norm(agent_pos - chair_pos)\n    vel_penalty = np.linalg.norm(agent_vel - desired_vel)\n    reward = -1.5 * to_goal\n    if to_chair < 1.1:\n        reward -= 15\n    reward -= 0.2 * vel_penalty\n    return reward\n",
    "rejected": "import numpy as np; class Agent: def __init__(self): self.state = None; self.desired_velocity = np.array([0, 0]); def explore_conference_room(agent, desired_velocity): current_position = agent.state[:2]; current_velocity = agent.state[2:]; if not check_conference_room(current_position, desired_velocity): return False; while np.linalg.norm(desired_velocity) > 1e-6: agent.move(desired_velocity); current_position = agent.state[:2]; current_velocity = agent.state[2:]; return True; def check_conference_room(position, velocity): x, y = position; vx, vy = velocity; x_min, x_max = 5, 10; y_min, y_max = 4, 8; if not ((x >= x_min and x <= x_max) and (y >= y_min and y <= y_max)): return False; max_speed = 5; if abs(vx) + abs(vy) > max_speed: return False; return True; agent = Agent(); desired_velocity = np.array([1, 0])"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect sidboards and do not crash in it \n Conditions: Agent's state, sidboard's state and wanted velocity are accessible. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    smoothness = np.linalg.norm(agent_vel - desired_vel)\n    collision_penalty = np.exp(-dist_chair) * 10\n    return -dist_goal - 0.1 * smoothness - collision_penalty\n",
    "rejected": "Reward function should be able to take in the following inputs:\n    1. agent's state (a dictionary)\n2. sidboard's state (a dictionary)\n3. wanted velocity (float)\n     the reward function should return a float between -1 and 1.\n     in the example below, the agent is at position (0,0) and wants to move towards the center of the conference room (4,4). The sidboard is at position (5,5), which means the sidboard is directly above the agent.\n Reward = (-1 + abs(agent_state[\\\"x\\\"] - 4) + abs(agent_state[\\\"y\\\"] - 4)) * 0.9\n     note that the absolute value of x and y will always be less than or equal to 4, so we can ignore them in this calculation. Also note that we have set the reward to -1 because we want to avoid crashing into the sidboard.\n    Your task is to implement the reward function as described above. You can assume that the input states are already normalized for simplicity.\n     import numpy as np\n     def reward_function(agent_state, sidboard_state, wanted_velocity):\n    distance_from_desired_position = np.linalg.norm([agent_state['x'] - sidboard_state['x'], agent_state['y'] - sidboard_state['y']])\n    if distance_from_desired_position <= 4:\n        reward = -1\n    else:\n        reward = max(0, 1 - abs(wanted_velocity - sidboard_state['v']))\n    return reward\n     agent_state = {'x': 0, 'y': 0}\n sidboard_state = {'x': 4, 'y': 4, 'v': 2.5}\nwanted_velocity = 3\n print(reward_function(agent_state, sidboard_state, wanted_velocity))\n     constraints and Assumptions:\n- Agent State: A dictionary representing the current position and orientation of the agent in world coordinates. For example, {'x': 1, 'y': 1} indicates the agent is facing east and has moved one step forward."
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Detect bench and avoid collision. \n Conditions: Agent's state, bench's state and wanted velocity are accessible. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    speed_diff = np.linalg.norm(agent_vel - desired_vel)\n    if dist_chair < 1.0:\n        return -25\n    return -dist_goal - 0.3 * speed_diff\n",
    "rejected": "import numpy as np\n     class RewardFunction:\n    def __init__(self):\n        self.state_space = {\n            'agent': 1,\n            'bench': 2,\n            'wanted_velocity': 3\n    }\n        self.action_space = {\n            'detect_bench': 0,\n            'avoid_collision': 1\n    }\n        def compute_reward(self, agent_state, bench_state, wanted_velocity):\n        # Check if the agent is on the bench or has detected the bench\n        if agent_state == 'agent' and bench_state == 'bench':\n            return -1 * (wanted_velocity - 5)\n        elif agent_state != 'agent' or not bench_state == 'bench':\n            return 0\n        else:\n            return 1 / (1 + np.exp(-0.1 * (wanted_velocity - 4)))\n     if __name__ == \"__main__\":\n    rf = RewardFunction()\n    agent_state = 'agent'\n    bench_state = 'bench'\n    wanted_velocity = 6\n    print(f\"Reward for initial state: {rf.compute_reward(agent_state, bench_state, wanted_velocity)}\")\n    # This code snippet defines a RewardFunction class that computes rewards based on various conditions. It initializes a dictionary to map agent states, bench states, and wanted velocities to their respective rewards. The compute_reward method calculates the reward based on the agent's current state, whether they have detected the bench, and the desired velocity."
  },
  {
    "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: Avoid collision the whiteboard. \n Conditions: Agent's state, whiteboard's state and wanted velocity are accessible. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    d_goal = np.linalg.norm(agent_pos - goal_pos)\n    d_chair = np.linalg.norm(agent_pos - chair_pos)\n    v_diff = np.linalg.norm(agent_vel - desired_vel)\n    penalty = -50 if d_chair < 0.8 else 0\n    return -d_goal - 0.2 * v_diff + penalty\n",
    "rejected": "That a simple function."
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Do not not collied eith the projector. \n Conditions: Agent's state, projector's state and wanted velocity are accessible. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    d_goal = np.linalg.norm(agent_pos - goal_pos)\n    d_chair = np.linalg.norm(agent_pos - chair_pos)\n    v_diff = np.linalg.norm(agent_vel - desired_vel)\n    penalty = -50 if d_chair < 0.8 else 0\n    return -d_goal - 0.2 * v_diff + penalty\n",
    "rejected": "The reward functionis complex and calculated based on differen component"
  },
  {
    "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: There are hanging equipments do not collied with it. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    speed_diff = np.linalg.norm(agent_vel - desired_vel)\n    if dist_chair < 1.0:\n        return -25\n    return -dist_goal - 0.3 * speed_diff\n",
    "rejected": "import numpy as np\n class ConferenceRoomEnv:\n    def __init__(self):\n        self.room = [0, 0, 0, 0]\n        self.drones = []\n    def reset(self):\n        self.room = [0, 0, 0, 0]\n        return self.room\n    def step(self, action):\n        if len(self.drones) == 0:\n            self.room[3] += 1\n        else:\n            self.room[action] += 1\n        self.drones.append((action, self.room))\n        for i in range(len(self.drones)):\n            if self.drones[i][1] < self.room[3]:\n                print(\"Collision detected!\")\n                self.drones.pop(i)\n                continue\n        rewards = []\n        for i in range(len(self.drones)):\n            if self.drones[i][1] >= self.room[3]:\n                rewards.append(-1)\n            elif self.drones[i][1] <= -self.room[3]:\n                rewards.append(1)\n            else:\n                rewards.append(0)\n        for i in range(len(self.drones)):\n            self.drones[i][0](rewards[i])\n        if len(rewards) > 0:\n            return np.random.choice([i for i in range(len(self.drones)) if rewards[i] > 0], p=[0.5]*len(self.drones))\n        else:\n            return None\n def q_function(drones, state):\n    q_values = {}\n    for action in range(len(drones)):\n        q_values[action] = np.zeros(len(drones))\n    for t in range(len(state)-1):\n        actions = drones[t][1]\n        for action in actions:\n            q_values[action][t+1] = max(q_values[action])\n    return q_values[state[-1]]\n def policy_network(q_values):\n    return np.argmax(q_values)\n class DQNAgent:\n    def __init__(self, env):\n        self.env = env\n        self.q_table = np.zeros((len(env.drones), len(env.drones)))\n        self.policy = policy_network(self.q_table)\n    def learn(self, state, action, reward, new_state):\n        delta = reward + 0.9 * (new_state[0]-state[0]) * (reward-self.q_table[self.policy(new_state)][self.policy(state)])\n        self.q_table[self.policy(state)][self.policy(new_state)] = (1-0.1)*delta + 0.1*delta\n        self.policy(new_state)\n    def train(self, episodes=1000):\n        for episode in range(episodes):\n            state = self.env.reset()\n            done = False\n            while not done:\n                action = np.argmax(self.q_table[state])\n                state, reward, done, _ = self.env.step(action)\n                self.learn(state, action, reward, state)\n                self.policy(state)"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Do not collide with speakers. \n Conditions: Agent's state, peaker's state and wanted velocity are accessible. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    speed_diff = np.linalg.norm(agent_vel - desired_vel)\n    if dist_chair < 1.0:\n        return -25\n    return -dist_goal - 0.3 * speed_diff\n",
    "rejected": "The reward function is:  ```python"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a conference room. \n Objectives: Avoid collision with laptops. \n Conditions: Agent's state and laptop's state are accessible. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    speed_diff = np.linalg.norm(agent_vel - desired_vel)\n    if dist_chair < 1.0:\n        return -25\n    return -dist_goal - 0.3 * speed_diff\n",
    "rejected": "class Agent:\n    def reward(self, state):\n        if state == 'closed': reward = -100\n        elif state == 'open': reward = 100\n        else: reward = 0\n        if self.can_move(state): return reward\n        else: return 0\n    def can_move(self, state):\n        return state != 'closed'\n    # This reward function takes into account two main factors:\n# 1. Closed State: The agent must not be in a state where it cannot move due to collisions.\n# 2. Open State: If the agent is in an open state, there are no obstacles to avoid."
  },
  {
    "prompt": "Mission: Pilot a drone through a conference room. \n Objectives: There a tablets in the room. Do not collide with them \n Conditions: Agent's state, tablet's state and wanted velocity are accessible. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    to_goal = np.linalg.norm(agent_pos - goal_pos)\n    to_chair = np.linalg.norm(agent_pos - chair_pos)\n    vel_penalty = np.linalg.norm(agent_vel - desired_vel)\n    reward = -1.5 * to_goal\n    if to_chair < 1.1:\n        reward -= 15\n    reward -= 0.2 * vel_penalty\n    return reward\n",
    "rejected": "def calculate_distance(s_agent, s_obstacle): d_agent_obstacle = math.sqrt(s_agent[0] ** 2 + s_agent[1] ** 2); d_agent_goal = math.sqrt(s_agent[0] ** 2 + s_agent[1] ** 2); return d_agent_goal; def main(s_agent, s_obstacle, s_map, s_goal): d_agent_obstacle = calculate_distance(s_agent, s_obstacle); d_agent_goal = calculate_distance(s_agent, s_goal); reward = 10 * (d_agent_obstacle - d_agent_goal); return reward"
  },
  {
    "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state, people's state and wanted velocity are accessible. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    smoothness = np.linalg.norm(agent_vel - desired_vel)\n    collision_penalty = np.exp(-dist_chair) * 10\n    return -dist_goal - 0.1 * smoothness - collision_penalty\n",
    "rejected": "def calculate_distance(s_agent, s_obstacle): d_agent_obstacle = math.sqrt(s_agent[0] ** 2 + s_agent[1] ** 2); d_agent_goal = math.sqrt(s_agent[0] ** 2 + s_agent[1] ** 2); return d_agent_goal; def main(s_agent, s_obstacle, s_map, s_goal): d_agent_obstacle = calculate_distance(s_agent, s_obstacle); d_agent_goal = calculate_distance(s_agent, s_goal); reward = 10 * (d_agent_obstacle - d_agent_goal); return reward"
  },
  {
    "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (workers) and avoid collision. \n Conditions: Agent's state, workers's state and wanted velocity are obtainable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, chair_pos, goal_pos):\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    if dist_chair < 1.2:\n        return -30\n    return -dist_goal\n",
    "rejected": "import numpy as np\n class WarehouseReward:\n    def __init__(self, env):\n        self.env = env\n        self.rewards = np.zeros((env.n_agents, 1))\n    def get_reward(self):\n        return self.rewards\n    def update_rew(self, r, next_state, action):\n        new_r = np.where(next_state < 0, -r[None], r)\n        total_r = np.sum(new_r)\n        self.rewards += new_r / len(action)\n        return total_r\n    # Example usage:\n# Create an environment instance\n env = YourWarehouseEnvironment()\n Reward_function = WarehouseReward(env)\n agent = YourAgent()\n done = False\nwhile not done:\n    action = agent.step(reward_function.get_reward())\n    if isinstance(action, tuple):\n        action = action[0]\n    reward_function.update_rew(reward_function.get_reward(), env.state, action)\n print('Final reward:', reward_function.get_reward())\n    # 1. WarehouseReward Class: Initializes the reward matrix with zeros.\n# 2. update_rew Function: Updates the reward matrix based on the next state and action.\n# 3. Example Usage: Simulates agent-environment interaction and updates reward accordingly."
  },
  {
    "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, human's state and wanted velocity are accessible. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    d_goal = np.linalg.norm(agent_pos - goal_pos)\n    d_chair = np.linalg.norm(agent_pos - chair_pos)\n    v_diff = np.linalg.norm(agent_vel - desired_vel)\n    penalty = -50 if d_chair < 0.8 else 0\n    return -d_goal - 0.2 * v_diff + penalty\n",
    "rejected": "That a simple function."
  },
  {
    "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Avoid collison with high storage racks. \n Conditions: Agent's state and storage rack's information and wanted velocity are obtainable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, chair_pos, goal_pos):\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    if dist_chair < 1.2:\n        return -30\n    return -dist_goal\n",
    "rejected": "def reward_function(agent, current_state, agent_velocity): distance_to_goal = np.linalg.norm(agent_velocity); [distance_to_goal := distance_to_goal + 10 for rack in agent.current_storage_rack if rack.state in ['closed', 'full']]; current_time = time.time(); elapsed_time = current_time - start_time; reward = (distance_to_goal / max_distance) * 100; return {'distance_to_goal': distance_to_goal, 'elapsed_time': elapsed_time, 'reward': reward}"
  },
  {
    "prompt": "Mission: Pilot a drone through a warehouse. \n Objectives: Avoid collison with forklifts. \n Conditions: Agent's state and forklift's information and wanted velocity are available. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, chair_pos, goal_pos):\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    if dist_chair < 1.2:\n        return -30\n    return -dist_goal\n",
    "rejected": "def reward(s_agent, s_branch): \n    c_1 = -2 \n    c_2 = -5 \n    d_agent_branch = calculate_distance(s_agent, s_branch) \n    if d_agent_branch < c1: \n  return 15.0 \n    return c_2*d_agent_goal"
  },
  {
    "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with hanging cables. \n Conditions: Agent's state and cable's information and wanted velocity are obtainable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    to_goal = np.linalg.norm(agent_pos - goal_pos)\n    to_chair = np.linalg.norm(agent_pos - chair_pos)\n    vel_penalty = np.linalg.norm(agent_vel - desired_vel)\n    reward = -1.5 * to_goal\n    if to_chair < 1.1:\n        reward -= 15\n    reward -= 0.2 * vel_penalty\n    return reward\n",
    "rejected": "def reward_function(agent_state, distance_to_first_collision, current_velocity):\n    # Calculate the acceleration required to avoid collisions\n    if agent_state['position'] < distance_to_first_collision:\n        acceleration = 0.5 * (agent_state['velocity'] - current_velocity)\n    else:\n        acceleration = 1.2 * (current_velocity - agent_state['velocity'])\n    \n    # Calculate the total time taken for the agent to reach the desired position\n    time_taken = distance_to_first_collision / acceleration\n    \n    # Calculate the reward based on the agent's performance\n    reward = -time_taken\n    \n    return reward\n    # Example usage\n agent_state = {'position': 50, 'velocity': 30}\n distance_to_first_collision = 70\n current_velocity = 40\n     print(reward_function(agent_state, distance_to_first_collision, current_velocity))"
  },
  {
    "prompt": "Mission:Operate the drone in a warehouse. \n Objectives: Avoid collison with stacks of pallets. \n Conditions: Agent's state and pallet's information and wanted velocity are obtainable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    to_goal = np.linalg.norm(agent_pos - goal_pos)\n    to_chair = np.linalg.norm(agent_pos - chair_pos)\n    vel_penalty = np.linalg.norm(agent_vel - desired_vel)\n    reward = -1.5 * to_goal\n    if to_chair < 1.1:\n        reward -= 15\n    reward -= 0.2 * vel_penalty\n    return reward\n",
    "rejected": "def reward(s_agent, s_branch): \n    c_1 = -2 \n    c_2 = -20 \n    d_agent_branch = calculate_distance(s_agent, s_branch) \n    if d_agent_branch < c1: \n  return 5.0 \n    return c_2*math.exp(d_agent_goal)"
  },
  {
    "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with stacks of boxes. \n Conditions: Agent's state and box's information and wanted velocity are available. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    speed_diff = np.linalg.norm(agent_vel - desired_vel)\n    if dist_chair < 1.0:\n        return -25\n    return -dist_goal - 0.3 * speed_diff\n",
    "rejected": "def reward(): #pass"
  },
  {
    "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Avoid collison with overhead pipes. \n Conditions: Agent's state and pipe's information and wanted velocity are obtainable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    to_goal = np.linalg.norm(agent_pos - goal_pos)\n    to_chair = np.linalg.norm(agent_pos - chair_pos)\n    vel_penalty = np.linalg.norm(agent_vel - desired_vel)\n    reward = -1.5 * to_goal\n    if to_chair < 1.1:\n        reward -= 15\n    reward -= 0.2 * vel_penalty\n    return reward\n",
    "rejected": "import numpy as np\n     class Drone:\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n        self.velocity = np.array([0, 0])\n        \n    def move(self, vx, vy):\n        self.velocity[0] += vx\n        self.velocity[1] += vy\n        \n    def is_collision(self, other):\n        # Check if the drone is colliding with another drone or an obstacle\n        distance = np.linalg.norm(np.array([self.x - other.x, self.y - other.y]))\n        return distance <= 5\n     def reward_function(agent_state, obj_position, obj_velocity):\n    # Calculate the Euclidean distance between the drone's current position and the obstacle\n    distance_to_obstacle = np.linalg.norm(obj_position - obj_velocity)\n    \n    # If there is no collision, add a small positive reward for moving forward\n    if not agent_state.is_collision(obj_position) and distance_to_obstacle < 5:\n        reward = 100 * (distance_to_obstacle > 2)\n    else:\n        reward = 0\n    \n    return reward\n    # Example usage:\n agent = Drone(0, 0)\n obstacle = Drone(3, 4)\n Reward = reward_function(agent, obstacle, obstacle.velocity)\n print(f\"Reward: {reward}\")"
  },
  {
    "prompt": "Mission: Operate the drone in a warehouse. \n Objectives: Detect people and avoid collison with overhead beams. \n Conditions: Agent's states and overhead beam information and wanted velocity are obtainable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    d_goal = np.linalg.norm(agent_pos - goal_pos)\n    d_chair = np.linalg.norm(agent_pos - chair_pos)\n    v_diff = np.linalg.norm(agent_vel - desired_vel)\n    penalty = -50 if d_chair < 0.8 else 0\n    return -d_goal - 0.2 * v_diff + penalty\n",
    "rejected": "def reward(): return "
  },
  {
    "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with people. \n Conditions: Agent's state, people's state and wanted velocity are  obtainable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    d_goal = np.linalg.norm(agent_pos - goal_pos)\n    d_chair = np.linalg.norm(agent_pos - chair_pos)\n    v_diff = np.linalg.norm(agent_vel - desired_vel)\n    penalty = -50 if d_chair < 0.8 else 0\n    return -d_goal - 0.2 * v_diff + penalty\n",
    "rejected": "import numpy as np\n     class Theater:\n    def __init__(self, size):\n        self.size = size\n        self.state = [0] * self.size\n        self.velocity = [0] * self.size\n    \n    def set_state(self, x, y):\n        self.state[x] = y\n    \n    def get_state(self):\n        return self.state\n    \n    def set_velocity(self, dx, dy):\n        self.velocity[0] += dx\n        self.velocity[1] += dy\n    \n    def get_velocity(self):\n        return self.velocity\n    \n    def move(self):\n        # Calculate new position after moving by given velocity\n        for i in range(len(self.velocity)):\n            self.velocity[i] += self.get_velocity()[i]\n        \n        # Check if the agent is at the end of the screen\n        if self.state[-1] < 0 or self.state[-1] >= self.size:\n            self.set_state(0)\n            self.set_velocity(0, 0)\n    # Example usage\n theater = Theater(size=50)  # Set size of the theater\n thor = Theater()  # Create a dummy agent\n        \n# Reward function\n def reward_function(agent_position, target_velocity):\n    distance = abs(agent_position - target_velocity)\n    \n    # If the agent is closer than 1 unit from the target velocity, give a reward\n    if distance <= 1:\n        return 10\n    else:\n        return 0\n    # Main loop\nwhile True:\n    # Get current position of the agent\n    agent_position = thor.get_state()\n    \n    # Get the target velocity (desired velocity)\n    target_velocity = thor.get_velocity()\n    \n    # Move the agent towards the target velocity\n    thor.move()\n    \n    # Update the reward based on the agent's proximity to the target velocity\n    reward = reward_function(agent_position, target_velocity)\n    \n    print(f\"Agent Position: {agent_position}\")\n    print(f\"Target Velocity: {target_velocity}\")\n    print(f\"Reward: {reward}\")\n     this code defines a `Theater` class that represents the environment and the `Thor` class that contains the agent. The `move` method updates the agent's position based on its velocity and checks if it has reached the end of the screen. The `reward_function` calculates the distance between the agent's position and the target velocity, and gives a reward of 10 units if the agent is closer than 1 unit to the target velocity. The main loop continuously updates the agent's position and prints out the agent's position, target velocity, and reward. The reward is increased by 10 when the agent is within 1 unit of the target velocity."
  },
  {
    "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with spotlamps. \n Conditions: Agent's state and spotlamp information and wanted velocity are available. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    smoothness = np.linalg.norm(agent_vel - desired_vel)\n    collision_penalty = np.exp(-dist_chair) * 10\n    return -dist_goal - 0.1 * smoothness - collision_penalty\n",
    "rejected": "Rewards:\n1. Reward for detection of person\n2. Reward for avoiding collision with spotlamp\n     Reward Function:\n    ```python\n def reward_function(agent_state, spotlamp_info, agent_velocity):\n    # Calculate the distance between the agent and the spotlamp\n    distance = calculate_distance_between_agent_and_spotlamp(agent_state, spotlamp_info)\n    \n    # Check if the agent is detected by the spotlamp\n    if distance <= 0:\n        return -distance\n    \n    # If the agent is not detected, check if it avoids collision with the spotlamp\n    elif distance > 0:\n        return max(0, agent_velocity - spotlamp_info.velocity)\n    \n    else:\n        return 0\n```\n     in this reward function, we first calculate the distance between the agent and the spotlamp using the `calculate_distance_between_agent_and_spotlamp` function. Then, we check if the agent has been detected by the spotlamp (i.e., if the distance is less than or equal to zero). If so, we return the negative value of the distance because detecting an object results in a negative reward.\n     if the agent is not detected by the spotlamp, we then check if the agent avoids collision with the spotlamp (i.e., if the distance between them is greater than zero). In that case, we return the maximum possible velocity of the agent minus the velocity of the spotlamp to determine the reward for avoiding collision. If the agent does not collide with the spotlamp, we return zero as no reward is needed.\n     note that the reward function assumes that the agent can detect objects but cannot collide with them. It also assumes that the spotlamp can move at a constant velocity. In practice, you may need to modify these assumptions based on your specific requirements. Additionally, the reward function returns a positive value when the agent successfully detects the spotlamp and avoids collision, which could be interpreted as a positive reward. The exact interpretation depends on how you want to evaluate the success of the agent.\n     please note that the implementation of the `calculate_distance_between_agent_and_spotlamp` function is not provided in the question. You would need to implement this function according to your system's requirements. Once you have implemented the function, you can use it in the reward function as shown above. Make sure to provide appropriate comments and documentation for clarity and maintainability.\n    ```python\n import math\n     def calculate_distance_between_agent_and_spotlamp(agent_state, spotlamp_info):\n    # Assuming the agent and spotlamp states are represented as vectors in 3D space\n    agent_x, agent_y, agent_z = agent_state\n    spotlamp_x, spotlamp_y, spotlamp_z = spotlamp_info.x, spotlamp_info.y, spotlamp_info.z\n    \n    # Calculate the Euclidean distance between the agent and the spotlamp\n    distance = math.sqrt((agent_x - spotlamp_x)**2 + (agent_y - spotlamp_y)**2 + (agent_z - spotlamp_z)**2)\n    \n    return distance"
  },
  {
    "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with balconies. \n Conditions: Agent's state, balcony's state and wanted velocity are obtainable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    speed_diff = np.linalg.norm(agent_vel - desired_vel)\n    if dist_chair < 1.0:\n        return -25\n    return -dist_goal - 0.3 * speed_diff\n",
    "rejected": "### Step 1: Set Up the Environment\n     first, we need to set up our environment. In this case, we will use a simple pendulum model as a simulation of the agent's movement.\n    ```python\n import numpy as np\n     class PendulumEnv:\n    def __init__(self):\n        self.state = np.zeros(3)  # Initial state (x, y, theta)\n        self.velocity = np.zeros(3)  # Desired velocity (theta_dot)\n        def step(self, action):\n        # Calculate acceleration based on velocity\n        delta_v = self.velocity - action\n        v_dot = np.tan(delta_v)\n        \n        # Update position and velocity\n        self.state[0] += np.cos(self.state[2]) * delta_v\n        self.state[1] -= np.sin(self.state[2]) * delta_v\n        \n        # Apply constraints\n        if self.state[0] > 5 or self.state[0] < -5:\n            self.state[0] = 5\n        if self.state[1] > 4 or self.state[1] < -4:\n            self.state[1] = 4\n            \n        # Update velocity\n        self.velocity += v_dot / 2\n        return self.state, self.velocity, False, {}\n    # Initialize the environment\n env = PendulumEnv()\n```\n    ### Step 2: Define Reward Function\n     now, let's define the reward function based on the agent's performance and the collision detection with the balcony. We'll assume that the agent is rewarded when it successfully avoids collisions with the balcony.\n    ```python\n def reward_function(state, action, reward):\n    # The agent detects collisions only after reaching the desired velocity\n    if state[2] == env.velocity[2]:\n        return float('inf')  # No collision detected\n    \n    # Distance from the balcony\n    distance_to_balcony = np.linalg.norm(state[0])\n    \n    # Collision with balcony\n    if abs(distance_to_balcony) <= 1:\n        return -float('inf')  # No collision with the balcony\n    \n    return float(reward)  # Reward for detecting the collision\n```\n    ### Step 3: Implement the Reward Function\n     here's how you might implement the reward function:\n    ```python\n def reward_function(state, action, reward):\n    # The agent detects collisions only after reaching the desired velocity\n    if state[2] == env.velocity[2]:\n        return float('inf')  # No collision detected\n    \n    # Distance from the balcony\n    distance_to_balcony = np.linalg.norm(state[0])\n    \n    # Collision with balcony\n    if abs(distance_to_balcony) <= 1:\n        return -float('inf')  # No collision with the balcony\n    \n    return float(reward)  # Reward for detecting the collision\n```\n    ### Step 4: Create the Agent Class\n     next, we'll create the class that represents the agent's behavior:\n    ```python\n class Agent:\n    def __init__(self, env):\n        self.env = env\n        self.state = np.zeros(3)  # Initial state (x, y, theta)\n        self.velocity = np.zeros(3)  # Desired velocity (theta_dot)\n        self.last_state = None  # Last state before moving\n        self.action_space = [np.pi/6, np.pi/4, np.pi/3, np.pi/2, np.pi]\n        self.action = np.random.choice(self.action_space)\n        def update(self, action):\n        new_state = self.env.step(action)\n        self.state = new_state[:3]  # Store the updated state\n        self.last_state = new_state[:]\n        self.action = action\n```\n    ### Final Code\n     putting all together, here's the complete code:\n    ```python\n import numpy as np\n from pendulum import PendulumEnv\n     class PendulumEnv:\n    def __init__(self):\n        self.state = np.zeros(3)  # Initial state (x, y, theta)\n        self.velocity = np.zeros(3)  # Desired velocity (theta_dot)\n        def step(self, action):\n        delta_v = self.velocity - action\n        v_dot = np.tan(delta_v)\n        \n        # Update position and velocity\n        self.state[0] += np.cos(self.state[2]) * delta_v\n        self.state[1] -= np.sin(self.state[2]) * delta_v\n        \n        # Apply constraints\n        if self"
  },
  {
    "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Detect people and avoid collison with audience seating. \n Conditions: Agent's states and audience seating information and wanted velocity are available. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    d_goal = np.linalg.norm(agent_pos - goal_pos)\n    d_chair = np.linalg.norm(agent_pos - chair_pos)\n    v_diff = np.linalg.norm(agent_vel - desired_vel)\n    penalty = -50 if d_chair < 0.8 else 0\n    return -d_goal - 0.2 * v_diff + penalty\n",
    "rejected": "import numpy as np\n     class SafePathReward:\n    def __init__(self):\n        self.goal_distance = 100  # Goal distance in meters\n        self.max_velocity = 5  # Maximum allowed velocity in m/s\n        def reward(self, state, action):\n        # Calculate distance traveled by the agent\n        distance_traveled = np.linalg.norm(state['position'] - state['end_point'])\n        \n        # Check if the agent has reached the end point\n        if state['position'] == state['end_point']:\n            return 0\n        \n        # Check if the agent has crossed the goal distance\n        if distance_traveled > self.goal_distance:\n            return -np.inf\n        \n        # Check if the agent has crossed the maximum velocity limit\n        if action != 'forward' and action != 'backward':\n            return -np.inf\n        \n        # Check if the agent has exceeded the max velocity\n        if abs(action) > self.max_velocity:\n            return -np.inf\n        \n        # Calculate the distance between the agent and the goal point\n        distance_to_goal = np.linalg.norm(state['goal_point'] - state['end_point'])\n        \n        # Calculate the safety factor based on the current velocity\n        safety_factor = min(1, (distance_to_goal / self.goal_distance) * 2)\n        \n        # Calculate the reward based on the safety factor\n        reward = 1 - safety_factor\n    \n        return"
  },
  {
    "prompt": "Mission: Drone moves in a theater. \n Objectives: Detect people and avoid collison with projectors. \n Conditions: Agent's state, projectors's state and wanted velocity are  obtainable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    smoothness = np.linalg.norm(agent_vel - desired_vel)\n    collision_penalty = np.exp(-dist_chair) * 10\n    return -dist_goal - 0.1 * smoothness - collision_penalty\n",
    "rejected": "Reward Function:\n Reward = -1 * (agent_state - projector_state)**2 + abs(agent_state - wanted_velocity)\n     this is what I have so far but it doesn't seem to work as expected. The agent always ends up being the same distance away from the wanted velocity even though the wanted velocity is different from the projector's.\n the reward function you've provided is almost correct, but there are a few adjustments that can help improve its performance:\n    1. **Avoidance of Collisions**: Ensure that the agent does not collide with any projected objects. This will make sure that the agent maintains a safe distance.\n    2. **Handling Different Velocities**: Make sure that the reward function takes into account both the agent's current position and the desired velocity.\n    3. **Correctly Handle Velocity**: Ensure that the reward function correctly calculates the difference between the agent's velocity and the desired velocity.\n     here\u2019s an updated version of your reward function:\n    ```python\n import numpy as np\n     def reward_function(agent_state, projector_state, wanted_velocity):\n    # Calculate the distance the agent should be from the wanted velocity\n    target_distance = np.linalg.norm(wanted_velocity)\n    \n    # Calculate the squared distance from the agent to the projection\n    squared_distance_to_projector = np.sum((agent_state - projector_state) ** 2)\n    \n    # Adjust for the squared distance if the agent was already within the allowed range\n    adjusted_squared_distance = max(0, squared_distance_to_projector - target_distance)\n    \n    # Calculate the reward based on this adjustment\n    return -1 * (adjusted_squared_distance**2 + abs(agent_state - wanted_velocity))\n    # Example usage\n agent_state = [1.5, 0.7]  # Assume the agent is at position [1.5, 0.7]\n```"
  },
  {
    "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Avoid collision with cabels. \n Conditions: Agent's state, cabel's state and desired speed are obtainable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    speed_diff = np.linalg.norm(agent_vel - desired_vel)\n    if dist_chair < 1.0:\n        return -25\n    return -dist_goal - 0.3 * speed_diff\n",
    "rejected": "def reward_function(agent_state, cabel_state, desired_speed):\n This function calculates the reward for piloting a drone through a theater.\n \n Parameters:\n agent_state (tuple): A tuple representing the current state of the drone,\n containing elements such as its orientation, flamp direction,\n and any other relevant information.\n \n cabel_state (tuple): A tuple representing the current state of the\n cabling system within the theater,\n including information about the cables' position,\n tension, etc.\n \n desired_speed (float): The desired speed at which the drone is flying.\n     Returns:\n float: The calculated reward based on the agent's state, cabling state,\n and desired speed.\n \n # Define constants related to the rewards you want to maximize or minimize\n MAX_REWARD = 100  # Example constant for maximum reward\n     # Calculate the difference between the desired speed and the actual speed\n speed_difference = desired_speed - cabel_state[2]\n     # If the cable's tension is too high, penalize by reducing the reward\n if cabel_state[3] > 100:\n return -speed_difference * 5  # Penalize by multiplying the difference by 5\n     # Otherwise, increase the reward by increasing the difference\n else:\n return speed_difference * 10  # Increase the reward by multiplying the difference by 10"
  },
  {
    "prompt": "Mission: Drone explores the theater. \n Objectives: Detect dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state and people's state and wanted velocity are obtainable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    speed_diff = np.linalg.norm(agent_vel - desired_vel)\n    if dist_chair < 1.0:\n        return -25\n    return -dist_goal - 0.3 * speed_diff\n",
    "rejected": "x = 20 * cos(t)\n y = 20 * sin(t)\n \n Where t is time. \n \n The agent's goal is to move towards the center of the theater with a velocity such that it does not collide with any obstacles.\n \n Rewards:\n 1. If the agent reaches the center of the theater without colliding with any obstacle.\n 2. If the agent misses the center of the theater but still moves away from the obstacle without colliding with any other objects.\n \n Implement the reward function using the given equations for the agent's position and velocity as input parameters. ```python\n import numpy as np\n \n def compute_reward(agent_state):\n     \"\"\"\n     Compute the reward based on the agent's position and velocity.\n \n     Args:\n         agent_state (numpy.ndarray): An array containing the agent's current state,\n                                      which includes x, y, and velocity values.\n \n     Returns:\n         float: The computed reward value between -1 and 1.\n     \"\"\"\n \n     # Calculate the distance from the origin (center of the theater)\n     distance_to_center = np.sqrt(x**2 + y**2)\n \n     # Define the desired velocity\n     desired_velocity = np.array([5, 5])  # For example, if we want to reach the center at a speed of 5 units per second\n \n     # Compute the actual velocity after applying the desired velocity and subtracting the distance from the origin\n     actual_velocity = desired_velocity - agent_state[:2]\n \n     # Check if the agent has reached the center of the theater\n     if distance_to_center == 0:\n         return 1  # Collision occurred\n     else:\n         # Compute the absolute difference between the actual velocity and the desired velocity\n         diff = abs(actual_velocity - desired_velocity)\n \n         # Determine the reward based on whether the agent missed the center"
  },
  {
    "prompt": "Mission: Pilot a drone through a theater. \n Objectives: Detect static obstacle (overhead platforms) and avoid collision. \n Conditions: Agent's state, platform's state and desired speed are obtainable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    to_goal = np.linalg.norm(agent_pos - goal_pos)\n    to_chair = np.linalg.norm(agent_pos - chair_pos)\n    vel_penalty = np.linalg.norm(agent_vel - desired_vel)\n    reward = -1.5 * to_goal\n    if to_chair < 1.1:\n        reward -= 15\n    reward -= 0.2 * vel_penalty\n    return reward\n",
    "rejected": "``python\n import numpy as np\n \n def reward_function(agent_state, platform_state, desired_speed):\n     # Calculate the distance between the agent and the platform\n     distance = np.linalg.norm(agent_state - platform_state)\n     \n     # Calculate the reward based on the distance and desired speed\n     if distance <= 1:\n         return -distance / desired_speed\n     else:\n         return 0\n     \n # Example usage\n agent_state = [np.array([0, 0]), np.array([-1, 0])]\n platform_state = [np.array([0, 0]), np.array([2, 3])]\n desired_speed = 5\n \n print(reward_function(agent_state, platform_state, desired_speed))"
  },
  {
    "prompt": "Mission: Drone explores the theater. \n Objectives: Detect static obstacle (drapes) and avoid collision. \n Conditions: Agent's state, drape's state and desired speed are obtainable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, chair_pos, goal_pos):\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    if dist_chair < 1.2:\n        return -30\n    return -dist_goal\n",
    "rejected": "def reward_function(agent_state, drape_state, desired_speed):\n     # Calculate distance from agent to drape\n     distance = math.sqrt((agent_state[0] - drape_state[0])**2 + (agent_state[1] - drape_state[1])**2)\n     \n     # If agent is within 5 meters of the drape, give a small reward\n     if distance <= 5:\n         return -distance\n     \n     # Otherwise, give a larger reward for closer proximity\n     else:\n         return max(0, desired_speed - distance)\n     # Example usage\n agent_state = [x for x in range(10)]  # Agent starts at position 0\n drape_state = [random.randint(-10, 10) for _ in range(10)]  # Drape starts with random positions\n desired_speed = 3  # Desired speed\n     reward = reward_function(agent_state, drape_state, desired_speed)\n print(f\"Reward: {reward}\")"
  },
  {
    "prompt": "Mission: Drone explores the theater. \n Objectives: Detect dynamic obstacle (hangings microphones) and avoid collision. \n Conditions: Agent's state, microphone's state and desired speed are obtainable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, chair_pos, goal_pos):\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    if dist_chair < 1.2:\n        return -30\n    return -dist_goal\n",
    "rejected": "class Reward:\n     def __init__(self):\n         self._reward = 0\n         def step(self, observation, action):\n         # Calculate reward based on current observation and agent's actions\n         # This is just an example implementation; you should replace it with your own logic\n         if 'dynamic_obstacle' in observation:\n             reward = -1  # Penalize for detection of hanging microphone\n         else:\n             reward = 1  # Reward for avoiding collision\n             self._reward += reward\n         return self._reward\n     # Create an instance of the reward class\n rewards = Reward()\n     def main():\n     # Initialize some variables\n     agent_state = None\n     microphone_state = None\n     desired_speed = 5\n         while True:\n         # Simulate environment interaction\n         agent_state = simulate_environment()\n             # Get microphone state\n         microphone_state = get_microphone_state(agent_state)\n             # Compute reward\n         rewards.step(observation=agent_state, action=microphone_state)\n             # Check for termination condition\n         if agent_state['state'] == 'collision':\n             break\n     if __name__ == '__main__':\n     main()\n"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (human) and avoid collision. \n Conditions: Agent's state, human's state and wanted velocity are obtainable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    d_goal = np.linalg.norm(agent_pos - goal_pos)\n    d_chair = np.linalg.norm(agent_pos - chair_pos)\n    v_diff = np.linalg.norm(agent_vel - desired_vel)\n    penalty = -50 if d_chair < 0.8 else 0\n    return -d_goal - 0.2 * v_diff + penalty\n",
    "rejected": "import numpy as np\n class RewardFunction: def __init__(self): self.gamma = 0.99  # Discount factor for future rewards self.epsilon = 1.0  # Exploration rate self.tau = 0.05  # Time delay between updates self.discount_factor = 0.97  # Discount factor for discounting future rewards\n def reset(self): return None\n def update(self, policy, state, action, next_state, reward, done): if not done: next_state_action_value = policy.get_next_state_action_value(state, action) next_state_reward = reward + self.gamma * next_state_action_value next_state_done = not done old_value = reward + self.gamma * next_state_action_value new_value = old_value + self.discount_factor * (next_state_done - old_value) policy.update_policy(state, action, new_value, next_state_done) self.discount_factor *= self.tau return state, action, new_value\n # Example usage\n def explore_drone(): policy = { 'state': [0, 0, 0], 'action': ['forward', 'stop'] } reward_function = RewardFunction() reward_function.reset()"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect  dynamic obstacle (people) and avoid collision. \n Conditions: Agent's state, people's state and desired speed are obtainable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    d_goal = np.linalg.norm(agent_pos - goal_pos)\n    d_chair = np.linalg.norm(agent_pos - chair_pos)\n    v_diff = np.linalg.norm(agent_vel - desired_vel)\n    penalty = -50 if d_chair < 0.8 else 0\n    return -d_goal - 0.2 * v_diff + penalty\n",
    "rejected": "import numpy as np\n \n # Define the reward function\n def reward_function(agent_state, target_speed):\n     # Assume the agent's current state is represented by an array [x, y] where x and y are coordinates\n     # We will represent the target speed as a float value\n     \n     # Calculate the distance from the agent to the target speed\n     distance = np.linalg.norm(agent_state[:2] - target_speed)\n     \n     # Reward increases if the agent detects the obstacle within the specified speed limit\n     reward = 10 * (distance < 5)\n     \n     return reward\n     # Example usage:\n agent_state = [10, 20]  # Initial state of the agent\n target_speed = 30  # Desired speed of the target obstacle\n     reward = reward_function(agent_state, target_speed)\n print(\"Reward:\", reward)\n     ### Explanation:\n - **Distance Calculation**: `np.linalg.norm(agent_state[:2] - target_speed)` calculates the Euclidean distance between the agent's position `[x, y]` and the target obstacle's position `[x, y]`.\n - **Reward Threshold**: A threshold of `5` is set to ensure the agent avoids detection when it detects the obstacle within this range.\n - **Reward Increase**: If the agent detects the obstacle within the speed limit (`distance < 5`), the reward increases by `10`."
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect static obstacle (basket hoops) and avoid collision. \n Conditions: Agent's state, basket hoop's state and desired velocity are usable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    smoothness = np.linalg.norm(agent_vel - desired_vel)\n    collision_penalty = np.exp(-dist_chair) * 10\n    return -dist_goal - 0.1 * smoothness - collision_penalty\n",
    "rejected": "Note: The reward should be negative because the agent has to avoid collisions with obstacles, which would result in a loss of energy and points. \n The absolute value ensures that the negative reward reflects the actual distance from the basket hoop rather than just the difference in states. \n \n Please note that the above code snippet only provides an example of how to define the reward function. You need to implement the rest of the functions based on the problem description provided. Here is the complete implementation:\n \n ```python\n import numpy as np\n \n class Drone:\n     def __init__(self):\n         self.state = None  # Initialize the state of the drone\n         self.basket_hoop = None  # Initialize the state of the basket hoop\n         self.velocity = 0.0  # Initialize the velocity of the drone\n         def move(self, dx, dy):\n         # Move the drone by the given amount\n         self.velocity += dx + dy  # Add the movement vector to the velocity\n         def detect_obstacle(self):\n         # Check if there is any obstacle in the environment\n         # This could involve checking for objects like basketball hoops or other obstacles\n         pass  # Placeholder for the detection logic\n         def update_state(self):\n         # Update the state of the drone based on its previous position and direction\n         pass  # Placeholder for updating the state logic\n         def check_collision_with_basket_hoop(self):\n         # Check if the drone collides with the basket hoop\n         # If so, reset the velocity and return True\n         # Otherwise, return False\n         pass  # Placeholder for the collision detection logic\n    \n # Example usage\n agent = Drone()"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect static obstacle (scoreboards) and avoid collision. \n Conditions: Agent's state, scoreboard's state and desired velocity are obtainable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    d_goal = np.linalg.norm(agent_pos - goal_pos)\n    d_chair = np.linalg.norm(agent_pos - chair_pos)\n    v_diff = np.linalg.norm(agent_vel - desired_vel)\n    penalty = -50 if d_chair < 0.8 else 0\n    return -d_goal - 0.2 * v_diff + penalty\n",
    "rejected": "The agent's state is represented as a 3D numpy array (x, y, z) and the obstacle's state is represented as a 3D numpy array (x, y, z). The reward function should be designed to maximize the probability of the agent avoiding the obstacles. In the context of this problem, the agent's goal is to stay within a safe distance of the obstacles. The safe distance is defined as the maximum of the Euclidean distance between the agent and the two obstacles. The agent's reward should be -1 if the agent is outside the safe distance of both obstacles, -0.5 if the agent is within the safe distance of one obstacle but not the other, and 0.5 if the agent is within the safe distance of both obstacles. Your task is to implement a reward function that satisfies these conditions. Use the provided code snippet as a starting point and modify it to achieve the desired reward function. import numpy as np def reward_function(state, obstacle1, obstacle2): \"\"\"\" Calculate the reward for an agent that tries to avoid two static obstacles during flamp. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (climbing ropes) and avoid collision. \n Conditions: Agent's state, climbing rope's state and desired velocity are usable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    d_goal = np.linalg.norm(agent_pos - goal_pos)\n    d_chair = np.linalg.norm(agent_pos - chair_pos)\n    v_diff = np.linalg.norm(agent_vel - desired_vel)\n    penalty = -50 if d_chair < 0.8 else 0\n    return -d_goal - 0.2 * v_diff + penalty\n",
    "rejected": "agent is happy if agent reaches target agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent moves forward Rewards: Agent is happy if agent reaches target Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle Rewards: Agent is happy if agent reaches target and agent moves forward and agent is not hit by obstacle"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with volleyball nets. \n Conditions: Agent's state, volleyball net's state and desired velocity are obtainable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    speed_diff = np.linalg.norm(agent_vel - desired_vel)\n    if dist_chair < 1.0:\n        return -25\n    return -dist_goal - 0.3 * speed_diff\n",
    "rejected": "def calculate_distance(s_agent, s_obstacle): d_agent_obstacle = math.sqrt(s_agent[0] ** 2 + s_agent[1] ** 2); d_agent_goal = math.sqrt(s_agent[0] ** 2 + s_agent[1] ** 2); return d_agent_goal; def main(s_agent, s_obstacle, s_map, s_goal): d_agent_obstacle = calculate_distance(s_agent, s_obstacle); d_agent_goal = calculate_distance(s_agent, s_goal); reward = 10 * (d_agent_obstacle - d_agent_goal); return reward"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with hanging flags. \n Conditions: Agent's state, flag's state and desired velocity are accessible. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    smoothness = np.linalg.norm(agent_vel - desired_vel)\n    collision_penalty = np.exp(-dist_chair) * 10\n    return -dist_goal - 0.1 * smoothness - collision_penalty\n",
    "rejected": "That a simple function."
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (moving players) and avoid collision. \n Conditions: Agent's state, player's state and desired velocity are accessible. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    smoothness = np.linalg.norm(agent_vel - desired_vel)\n    collision_penalty = np.exp(-dist_chair) * 10\n    return -dist_goal - 0.1 * smoothness - collision_penalty\n",
    "rejected": "The reward functionis complex and calculated based on differen component"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Avoid collision with loose balls. \n Conditions: Agent's state, ball's state and desired velocity are usable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    smoothness = np.linalg.norm(agent_vel - desired_vel)\n    collision_penalty = np.exp(-dist_chair) * 10\n    return -dist_goal - 0.1 * smoothness - collision_penalty\n",
    "rejected": "import numpy as np\n \n class Arena:\n     def __init__(self):\n         self.grid = np.zeros((40, 40), dtype=int)  # Initialize the grid\n         self.balls = []  # List to store all balls in the arena\n         self.agent_position = None  # Position of the agent\n         def add_ball(self, x, y):\n         \"\"\"Add a new ball at coordinates (x, y).\"\"\"\n         self.balls.append((x, y))\n         def detect_collision(self, agent_x, agent_y):\n         \"\"\"Detect if there is a collision with any ball.\"\"\"\n         for ball_pos in self.balls:\n             dx, dy = ball_pos[0] - agent_x, ball_pos[1] - agent_y\n             if abs(dx) > 1 or abs(dy) > 1:\n                 return True\n         return False\n         def update_agent_position(self, agent_x, agent_y):"
  },
  {
    "prompt": "Mission: Drone explores the sports hall. \n Objectives: Detect dynamic obstacle (flying balls) and avoid collision. \n Conditions: Agent's state, flying ball's state and desired velocity are usable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    to_goal = np.linalg.norm(agent_pos - goal_pos)\n    to_chair = np.linalg.norm(agent_pos - chair_pos)\n    vel_penalty = np.linalg.norm(agent_vel - desired_vel)\n    reward = -1.5 * to_goal\n    if to_chair < 1.1:\n        reward -= 15\n    reward -= 0.2 * vel_penalty\n    return reward\n",
    "rejected": "The reward function is:  ```python"
  },
  {
    "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect dynamic obstacle (patients) and avoid collision. \n Conditions: Agent's state, patient's state and desired velocity are accessible. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    to_goal = np.linalg.norm(agent_pos - goal_pos)\n    to_chair = np.linalg.norm(agent_pos - chair_pos)\n    vel_penalty = np.linalg.norm(agent_vel - desired_vel)\n    reward = -1.5 * to_goal\n    if to_chair < 1.1:\n        reward -= 15\n    reward -= 0.2 * vel_penalty\n    return reward\n",
    "rejected": "def reward(): pass "
  },
  {
    "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect dynamic obstacle (doctors) and avoid collision. \n Conditions: Agent's state, doctor's state and desired velocity are usable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    speed_diff = np.linalg.norm(agent_vel - desired_vel)\n    if dist_chair < 1.0:\n        return -25\n    return -dist_goal - 0.3 * speed_diff\n",
    "rejected": "def reward(): return 100"
  },
  {
    "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect static obstacle (wires) and avoid collision. \n Conditions: Agent's state, v's state and wanted velocity are accessible. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    d_goal = np.linalg.norm(agent_pos - goal_pos)\n    d_chair = np.linalg.norm(agent_pos - chair_pos)\n    v_diff = np.linalg.norm(agent_vel - desired_vel)\n    penalty = -50 if d_chair < 0.8 else 0\n    return -d_goal - 0.2 * v_diff + penalty\n",
    "rejected": "def reward_function(agent_state, vehicle_state, velocity):\n     # Reward increases as the agent moves closer to the target velocity\n     if abs(velocity - 0.5) < 1:\n         return 2\n     else:\n         return 1\n     # Example usage:\n agent_state = [0, 0, 0]  # Initial position of the agent\n vehicle_state = [0, 0, 0, 0]  # Initial position of the drone\n velocity = 0.6  # Target velocity\n     print(reward_function(agent_state, vehicle_state, velocity))  # Output should be 1"
  },
  {
    "prompt": "Mission: Drone explores the hospital. \n Objectives: Avoid collision with medical equipment stands. \n Conditions: Agent's state, medical equipment's state and desired velocity are usable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    smoothness = np.linalg.norm(agent_vel - desired_vel)\n    collision_penalty = np.exp(-dist_chair) * 10\n    return -dist_goal - 0.1 * smoothness - collision_penalty\n",
    "rejected": "class Drone:\n     def __init__(self):\n         self.state = \"safe\"\n         self.velocity = 0\n         def explore(self, agent_state, medical_equipment_state, desired_velocity):\n         # Check if there is any medical equipment near the drone\n         for medical_equipment in medical_equipment_state:\n             if medical_equipment[\"type\"] == \"medical equipment\":\n                 distance = abs(agent_state[\"x\"] - medical_equipment[\"position\"][\"x\"])\n                 angle = math.atan2(medical_equipment[\"position\"][\"y\"] - agent_state[\"y\"], medical_equipment[\"position\"][\"x\"] - agent_state[\"x\"]) * 180 / math.pi\n         \n                 # If the drone is within range of medical equipment\n                 if distance < 50 and angle <= 45 or distance > 50 and angle >= 65:\n                     self.velocity = desired_velocity\n                     return True\n             # Check if the drone can move towards the desired velocity without colliding with medical equipment\n         if (abs(agent_state[\"x\"] - medical_equipment_state[0][\"position\"][\"x\"]) + abs(agent_state[\"y\"] - medical_equipment_state[0][\"position\"][\"y\"])) <= 30 and abs(agent_state[\"z\"] - medical_equipment_state[0][\"position\"][\"z\"]) <= 5:\n             self.velocity = desired_velocity\n             # If the drone does not have enough time to reach the target speed\n         if self.velocity < desired_velocity:\n             self.velocity = desired_velocity * 0.9\n             # If the drone reaches the desired velocity but still has no medical equipment nearby\n         if self.velocity != desired_velocity and not check_medical_equipment_nearby(agent_state, medical_equipment_state):\n             self.velocity = desired_velocity\n             return False\n     def check_medical_equipment_nearby(agent_state, medical_equipment_state):\n     # Check if the drone is"
  },
  {
    "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Detect wheelchairs and avoid collisions. \n Conditions: Agent's state, wheelchair's state and desired velocity are accessible. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    to_goal = np.linalg.norm(agent_pos - goal_pos)\n    to_chair = np.linalg.norm(agent_pos - chair_pos)\n    vel_penalty = np.linalg.norm(agent_vel - desired_vel)\n    reward = -1.5 * to_goal\n    if to_chair < 1.1:\n        reward -= 15\n    reward -= 0.2 * vel_penalty\n    return reward\n",
    "rejected": "That a simple function."
  },
  {
    "prompt": "Mission: Pilot a drone through a hospital. \n Objectives: Detect static obstacle (staff) and avoid collision. \n Conditions: Agent's state, staff's state and desired velocity are accessible. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    d_goal = np.linalg.norm(agent_pos - goal_pos)\n    d_chair = np.linalg.norm(agent_pos - chair_pos)\n    v_diff = np.linalg.norm(agent_vel - desired_vel)\n    penalty = -50 if d_chair < 0.8 else 0\n    return -d_goal - 0.2 * v_diff + penalty\n",
    "rejected": "The first element represents the current position of the drone\n - The second element represents whether the drone has reached the target point or not.\n \n The fourth line defines the goal of the agent. The agent needs to reach the target point at the end of the episode.\n \n Finally, in the fifth line, we have the rewards that the agent receives when reaching the target point. For example, if the agent reaches the target point, the agent receives a reward of +100 points. If the agent crashes into a static obstacle, the agent receives a negative reward of -10 points. The agent also receives feedback from the staff about their safety, represented by a value between 0 and 1. If the agent reaches the target point while the staff is safely inside the room, the agent receives a positive reward of +20 points. If the agent crashes into the staff, the agent receives a negative reward of -15 points. \n \n Rewards can be negative because the agent may have been pushed away by the staff. \n \n To make things more interesting, let's add some randomness to the reward function. Specifically, the agent receives a random amount of reward between -5 and 5 points depending on the distance between the agent and the target point.\n \n Your task is to complete the code with the reward function. Remember that the agent's state and action spaces need to be adapted accordingly."
  },
  {
    "prompt": "Mission: Operate the drone in a hospital. \n Objectives: Avoid collision with defibrillators. \n Conditions: Agent's state, obstacle's state and desired velocity are usable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    smoothness = np.linalg.norm(agent_vel - desired_vel)\n    collision_penalty = np.exp(-dist_chair) * 10\n    return -dist_goal - 0.1 * smoothness - collision_penalty\n",
    "rejected": "import numpy as np\n \n def reward_fn(agent_state, obstacle_state, desired_velocity):\n     # agent_state = [x, y, theta]\n     # obstacle_state = [x, y, theta]\n     \n     # Check if there are any obstacles within range\n     obstacles_within_range = np.linalg.norm(agent_state[:2] - obstacle_state[:2], axis=1) < 1\n     \n     # If there are no obstacles, check if we hit one\n     if not obstacles_within_range:\n         return 0\n     else:\n         # Calculate distance from agent to closest obstacle\n         dist_to_obstacle = np.linalg.norm(agent_state[:2] - obstacle_state[:2])\n         \n         # Calculate time taken to reach closest obstacle\n         t = (dist_to_obstacle / agent_state[2]) * 3600\n         \n         # Calculate reward based on distance and time\n         reward = max(0, dist_to_obstacle + 10)\n         \n         return reward"
  },
  {
    "prompt": "Mission: Drone explores the hospital. \n Objectives: Avoid  any collision with wall-mounted emergency devices. \n Conditions: Agent's state, emergency device's state and wanted velocity are accessible. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, chair_pos, goal_pos):\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    if dist_chair < 1.2:\n        return -30\n    return -dist_goal\n",
    "rejected": "class Drone:\n     def __init__(self):\n         self.state = 'safe'\n         self.velocity = (0, 0)\n         self.distance_to_wall = float('inf')\n         def move(self, desired_velocity):\n         if self.state == 'safe':\n             # Check for collisions with wall-mounted devices\n             if abs(desired_velocity) <= 1 or math.sqrt((desired_velocity[0]**2) + (desired_velocity[1]**2)) < 1:\n                 self.state = 'safe'\n             else:\n                 # Move forward by desired_velocity\n                 self.velocity = (desired_velocity[0], desired_velocity[1])\n                 self.distance_to_wall = 0\n         elif self.state == 'collision':\n             # Calculate distance to wall\n             self.distance_to_wall = math.sqrt((desired_velocity[0]**2) + (desired_velocity[1]**2))\n             # Reward based on distance to wall\n             if self.distance_to_wall > 0.5:\n                 return -math.inf\n             else:\n                 return 1 / self.distance_to_wall"
  },
  {
    "prompt": "Mission: Operate the drone in a hospital. \n Objectives:  Avoid collision with lamps. \n Conditions: Agent's state, obstacle's state are usable. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    speed_diff = np.linalg.norm(agent_vel - desired_vel)\n    if dist_chair < 1.0:\n        return -25\n    return -dist_goal - 0.3 * speed_diff\n",
    "rejected": "import numpy as np\n \n def reward(agent_state, obstacle_state):\n     \"\"\"\n     This is a reward function for an autonomous drone operating in a hospital environment.\n         Parameters:\n     - agent_state (numpy.ndarray): An array representing the current state of the drone, containing values from 0 to 1 where 0 means no obstacle and 1 means it's in front of an obstacle.\n     - obstacle_state (numpy.ndarray): An array representing the current state of the obstacles, also containing values from 0 to 1 where 0 means no obstacle and 1 means an obstacle is present.\n         Returns:\n     - float: A scalar value that represents the reward based on the proximity to obstacles and avoiding collisions with lamps.\n     \"\"\"\n         # Define the threshold for avoiding collisions with lamps\n     collision_threshold = 0.5\n     \n     # Calculate the distance between the drone and the closest obstacle\n     dist_to_obstacle = np.sqrt(np.sum((obstacle_state[:, None] - agent_state[None, :])**2))\n     \n     # If the drone is closer than the threshold, add a small positive reward\n     if dist_to_obstacle < collision_threshold:\n         return 100\n     else:\n         return -100\n     # Example usage:\n agent_state = np.array([0.7, 0.3])\n obstacle_state = np.array([[0.8, 0.6], [0.4, 0.9]])"
  },
  {
    "prompt": "Mission: Drone indoor exploration in a hospital. \n Objectives: Avoid to collide with static obstacle such as beds. \n Conditions: Agent's state, obstacle's state and wanted velocity are accessible. \n Write a reward function in Python.",
    "chosen": "import numpy as np\n    def reward(agent_pos, agent_vel, chair_pos, goal_pos, desired_vel):\n    dist_goal = np.linalg.norm(agent_pos - goal_pos)\n    dist_chair = np.linalg.norm(agent_pos - chair_pos)\n    speed_diff = np.linalg.norm(agent_vel - desired_vel)\n    if dist_chair < 1.0:\n        return -25\n    return -dist_goal - 0.3 * speed_diff\n",
    "rejected": "The reward functionis complex and calculated based on differen component"
  }
]
